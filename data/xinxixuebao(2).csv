﻿2014-04-28,高正确率的双语语块对齐算法研究,"俞敬松王惠临,吴胜兰","高质量的自动对齐双语语块,对于机器翻译系统,特别是计算机辅助翻译系统的性能提高有重要作用,而且对于人工翻译以及辞典编纂也都有巨大的应用价值。该文提出基于单词间粘合度与松弛度的语块划分评分方法以及双语语块划分的双向约束算法,使得源语言和目标语言的语块的划分与对齐能相互促进。与传统方法相比,因为无需事先进行双语语块划分,而是在搜索最佳对齐时动态地考察划分效果,故可以减少边界划分错误对对齐结果的影响。该算法获得了远超过传统算法的高正确率。","语块对齐,机器翻译,平行文本,双语对齐"
2012-04-15,面向机器翻译的句类依存树库构建及应用,"王慧兰,张克亮","该文以汉英机器翻译为应用目标,以概念层次网络理论的语义网络和句类分析方法为理论基础,探讨了句类依存树库构建的理论和标注实践等问题,描述了构建树库所需的概念类别标注集和句类关系标注集。并通过与已有汉语树库进行对比,以汉语显性轻动词句的标注为例,分析了汉语句类依存树库的特点。该文在应用层面定义了面向汉英机器翻译的融句法语义信息于一体的“句类依存子树到串”双语转换模板,尝试基于汉语句类依存树库提取汉英转换模板。","机器翻译,概念层次网络理论,句类依存树库"
2012-08-09,基于信息熵和词频分布变化的术语抽取研究,"李丽双,王意文,黄德根","在分别研究了基于信息熵和基于词频分布变化的术语抽取方法的情况下,该文提出了一种信息熵和词频分布变化相结合的术语抽取方法。信息熵体现了术语的完整性,词频分布变化体现了术语的领域相关性。通过应用信息熵,即将信息熵结合到词频分布变化公式中进行术语抽取,且应用简单语言学规则过滤普通字符串。实验表明,在汽车领域的语料上,应用该方法抽取出1300个术语,其正确率达到73.7%。结果表明该方法对低频术语有更好的抽取效果,同时抽取出的术语结构更完整。","术语抽取,信息熵,词频分布变化"
2012-06-07,基于并列结构的部分整体关系获取方法,"夏 飞,曹馨宇,符建辉,王 石,曹存根","部分整体关系是一种基础而重要的语义关系,从文本中自动获取部分整体关系是知识工程的一项基础性研究课题。该文提出了一种基于图的从Web中获取部分整体关系的方法,首先利用部分整体关系模式从Google下载语料,然后用并列结构模式从中匹配出部分概念对,据此形成图,用层次聚类算法对该图进行自动聚类,使正确的部分概念聚集在一起。在层次聚类基础上,我们挖掘并列结构的特性、图的特点和汉语的语言特点,采用惩罚逗号边、去除低频边、奖励环路、加重相同后缀和前缀等5种方法调整图中边的权重,在不损失层次聚类的高准确率条件下,大幅提高了召回率。","部分整体关系,图模型,并列结构,层次聚类,边权重"
2013-03-20,可扩展的网页关键信息抽取研究,"郭少华,郭 岩,李海燕,刘 悦,张 瑾,程学旗","该文提出了一种可扩展的网页关键信息抽取框架。该框架很好地融合了模板无关的全自动信息抽取算法和基于模板的信息抽取算法,从本质上提高抽取精度和抽取效率。该框架中的一些关键环节可根据需求进行替换,因此该框架具有很好的可扩展性。同时,该文还提出了模板的正交过滤算法。将该算法引入基于模板的抽取算法中,能够从本质上提高生成的模板的准确性。实验结果验证了上述结论。","关键信息,信息抽取,可扩展框架,正交过滤"
2013-02-28,基于短文本信息流的回顾式话题识别模型,"周 泓,刘金岭,王新功","近几年来,短文本信息流广泛应用于一些全民媒体,它在公开传递信息同时携带了丰富且具有极大价值的信息资源。该文提出了一种回顾式话题识别模型,改进了权值计算方法,有效提取了具有较强分辨话题能力的关键词,在聚类过程中将BIC值作为话题类别合并依据,提高了聚类的准确率。通过进行时间段分隔和去掉孤立点信息提高了算法的效率。实验结果表明,该方法有效地提高了短文本信息流的话题检测准确率和效率。","短文本,信息流,话题识别,聚类"
2013-06-27,文本流多粒度主题结构建模研究,"陈 千,郭 鑫,王素格,张 虎","主题检测近年来在文本挖掘和自然语言处理领域得到了广泛的应用,对主题进行结构建模是主题检测的基础。为了对文本流中的多粒度主题进行建模,提出一种基于语义层次树的主题结构模型。该模型利用领域本体的特点,将主题同本体作一一映射,结合概率理论,将概念集里的概念用主题树的叶子节点表示,每一层中的节点均是下一层节点的多项分布,使之更适合描述文本流中多粒度的主题结构。为了便于构建主题的空间结构,提出主题的相似度和事件相关度计算方法。该文结尾设计了实验构造真实新闻文本流数据上的主题树。实验结果表明,该结构模型能够体现主题丰富的多粒度空间语义特征。","主题检测,多粒度主题建模,文本流"
2013-03-12,基于预测误差扩展的可逆文本水印算法,"费文斌,唐向宏,王 静,林新建","为了避免在水印嵌入后造成文本内容的永久性改变,该文借鉴图像中可恢复水印的思想,将预测误差扩展应用于文本文档,提出了一种基于预测误差扩展的可逆中文文本水印算法。该算法以句子为单位,通过上下文搭配度大小选择可替换的词语,最后利用预测误差扩展和混沌序列,实现水印的嵌入。研究结果表明该算法不仅具有较高的安全性,而且能有效地提取水印和无损地恢复出原始文本。","可恢复水印,预测误差扩展,文本文档,搭配度,混沌序列"
2013-05-06,一种处理结构化输入输出的中文句法分析方法,"赵国荣,王文剑","中文句法结构复杂,特征维数较高,目前已知最好的汉语句法分析效果与其他西方语言相比还有一定的差距。为进一步提高中文句法分析的效率和精度,该文提出一种采用二阶范数软间隔优化的结构化支持向量机(StructuralSupportVectorMachines,StructuralSVMs)方法对基于短语结构的中文句法进行分析,通过构造结构化特征函数ψ(x,y),体现句法树的输入信息,并根据中文句子本身具有的强相关性,在所构造的ψ(x,y)中增加中文句法分析树中父节点的信息,使ψ(x,y)包含了更加丰富的结构信息。在宾州中文树库PCTB上的实验结果表明,该文方法与经典结构化支持向量机方法以及BerkeleyParser相比可取得较好的效果。","中文句法分析,加权上下文无关文法,结构化SVM,二阶范数软间隔优化"
2014-05-14,融合事件信息的中文问答系统问题语义表征,"魏楚元,湛 强,樊孝忠,毛 煜,张大奎","复杂类问题理解是中文问答系统研究的难点,基于组块的问句分析方法将整个问句转化为若干组块,降低了问句分析的难度和复杂性。针对以含有事件(动作)信息的复杂类问题,提出基于语义组块的中文问答系统问题语义表征模型,采用语义组块的思想将问题的语义成分定义为疑问焦点块、问题主题块和问题事件块三个语义组块,对问句中的事件语义信息,建立了问题事件语义结构,将一个问句表征为一个基于语义组块的问题语义表征结构,用于问答系统的问题理解。通过序列标注学习方法实现问题语义表征中语义组块自动标注。实验结果表明:问题语义组块标注效果较好,问题语义表征模型获取了问题的关键语义信息,为语义层面上的问题理解提供基础。","复杂类问题,事件,问题语义表征,语义组块,问题理解"
2012-08-23,一种扩展式CRFs的短语情感倾向性分析方法研究,"乌达巴拉,汪增福","短语情感倾向性分析是文本情感分析的重要研究内容。该文将短语情感倾向性分析问题视作序列标注问题,利用条件随机场模型实现短语的情感倾向性判断。条件随机场模型是利用序列特征处理序列标注问题的经典方法,然而现有条件随机场模型无法将词语的情感倾向性分析与短语的情感倾向性分析相结合,从而造成准确率不高。因此,该文提出一种扩展式条件随机场模型YACRFs。该模型在链式条件随机场模型的基础上进行扩充,将词语情感倾向性分析与短语情感倾向性分析有效地结合起来,引入了情感词汇、短语规则模板以及词性等特征。与传统的规则方法和统计分类方法进行对比实验,该文提出方法取得了最高准确率81.07%。进一步地,在应用于句子情感倾向性分析的实验中得到了94.30%的准确率。实验结果表明,该文所提出的YACRFs模型能够显著提高短语情感倾向性判断结果的准确率。","短语,情感倾向性分析,条件随机场"
2014-08-11,基于迭代两步CRF模型的评价对象与极性抽取研究,"张 盛,李 芳","微博作为一种新兴媒体,已经在人们生活中扮演了一种不可或缺的角色。如何从大量微博中抽取出有意义的评价对象并识别出正确的情感倾向显得越来越重要。本文在传统的CRF模型基础上,提出了两步CRF模型及迭代两步CRF模型,对评价对象和极性进行抽取。两步CRF模型在COAE2014评测语料上取得了0.505的F值,迭代两步CRF模型通过不断增加训练语料,提高了召回率,使得F值达到了0.513,同时提高了模型的稳定性。实验对比了当前主流的几种方法,结果证明了本文提出的方法是行之有效的。","迭代,两步CRF,评价对象"
2012-04-16,Web藏文文本资源挖掘与利用研究,"刘汇丹,诺明花,马龙龙,吴 健,贺也平","该文结合链接分析技术和藏文编码识别技术,使用网络爬虫实现对互联网上藏文文本资源的挖掘,分析了Web藏文文本资源的分布情况。统计数据显示,国内藏文网站50%以上在青海省;约87%的藏文网页集中分布在31个大型网站中;人们正在逐步弃用旧有藏文编码,使用Unicode编码来制作网页。利用HTML标记、栏目归属、标点符号等自然标注信息对这些文本进行抽取,可以构建篇章语料和文本分类语料,可以抽取互联网藏文词库,进行词频统计和训练藏文语言模型,结合双语词典和搜索引擎技术抽取双语平行语料。这些语料可用于藏文分词、命名实体识别、信息检索、统计机器翻译等研究领域。","Web,语料,文本挖掘,信息抽取,藏文信息处理,中文信息处理"
2013-04-08,基于音素混淆网络的蒙古语语音关键词检测方法的研究,"飞 龙,高光来,鲍玉来","蒙古语语音识别系统的词表很难覆盖所有的蒙古文单词,并且随着社会的发展,蒙古文的新词和外来词也越来越多。为了解决蒙古语语音关键词检测系统中的集外词检测问题,该文提出了基于音素混淆网络的蒙古语语音关键词检测方法,并采用音素混淆矩阵改进了关键词的置信度计算方法。实验结果表明,基于音素混淆网络的蒙古语语音关键词检测方法可以较好地解决集外词的检测问题。蒙古语语音关键词检测系统采用改进的置信度计算方法后精确率提高了6%,召回率提高了2.69%,性能得到明显的提升。","蒙古语,关键词检测,集外词,混淆网络,音素混淆矩阵"
2013-07-02,藏文Web网络环境下的搜索策略研究,"陈新一,夏建华,杜玉祥,万福成,于洪志","该文分析了藏文Web网络的度分布和最大度优先搜索算法存在的问题,提出了搜索效率更高的二分度搜索算法和双遍历器的二分度与最大度同步搜索算法。根据社区划分原理,设计和构建了藏文Web社区环境下的搜索算法,实验结果表明,其平均搜索步数和平均查询信息量都优于实验中其他搜索算法。","藏文Web网络,度分布,最大度链路,双遍历器,社区划分"
2014-01-26,藏文排序优先级算法研究,"边巴旺堆,卓 嘎,董志诚,武 强,王龙业","为了顺利实现藏文词语的排序算法,在藏文构件元素识别算法的基础上,该文通过建立藏文规则函数和定义藏文构件的优先级,提出了基于笛卡尔积数学模型的现代藏文音节的优先级算法。该方法既遵循藏文构词原则,又符合语法规则,同时为藏文词语排序算法提供了新的研究思路。最后该文用藏文“”系的所有满足语法规则的词语对本算法进行测试,结果表明该算法符合现代藏文词典的排序要求,且排序效率更好。","规则函数,优先级,藏文,算法"
2012-10-08,基于词典、规则的斯拉夫蒙古文词切分系统的研究,"史建国,侯宏旭,飞 龙","斯拉夫蒙古文是蒙古国现行的文字,又称为西里尔蒙古文或新蒙古文。蒙古文词干和词缀包含着大量信息,斯拉夫蒙古文词切分是斯拉夫蒙古文信息处理众多后续工作的基础。该文尝试了将词典和规则结合的方法对斯拉夫蒙古文进行词切分。首先预处理部分蒙古文词,然后基于词典切分高频和部分不符合规则的词。最后对剩余的词,用切分规则生成多个候选的词切分方案,然后在这些方案中选出最优方案。通过两种方法的有机结合,发挥各自的优点,得到了性能较好的斯拉夫蒙古文词切分系统。","斯拉夫蒙古文,词切分,词典,规则"
2013-09-26,第十四届机器翻译峰会(MT Summit XIV)综述,"张霄军,刘 群",,
2013-10-16,语言计算的重要国际前沿,"孙茂松,刘挺,姬东鸿,穗志方,赵军,张钹,吾守尔·斯拉木,俞士汶,朱军,李建民,刘洋,王厚峰,吐尔根·依布拉音,刘群,刘知远","该文在互联网规模语言信息处理的语境下,从语言计算基础模型、语言分析、语言资源建设、机器翻译、文本内容理解与问答等多个方面,对国内外相关重要动态进行了评述,讨论了语言计算的若干前沿问题及其对中文信息处理近期研究工作所提出的要求。","语言计算,研究前沿,评述,中文信息处理"
2012-02-07,语言网络研究进展,"韩普,王东波,路高飞,苏新宁","语言网络作为一个新的研究领域,其研究正在迅速崛起,目前已经吸引了不少领域的研究者们的关注。该文首先简要介绍了语言网络的特点、常用的统计特征以及相关的网络模型;其次,根据语言构成单位以及当前语言网络研究热点,将语言网络分为语音网络、共现网络、依存句法网络、概念语义网络,并详细介绍了各类语言网络研究的主要进展。最后总结了语言网络研究的现状并给出了展望。","语言网络,小世界现象,无尺度分布"
2013-05-15,基于语言模型的有监督词义消歧模型优化研究,"杨陟卓,黄河燕","词义消歧是自然语言领域中重要的研究课题之一。目前,有监督词义消歧方法已经是解决该问题的有效手段。但是,由于缺乏大规模的训练语料,有监督方法还不能取得满意的效果。该文提出一种基于语言模型的词义消歧优化模型,该模型采用语言模型优化传统的有监督消歧模型,充分利用有监督和语言模型两种模型的消歧优势,共同推导歧义词的词义。该模型可以在训练语料不足的情况下,有效的提高词义消歧效果。在真实数据上表明,该方法的消歧性能超过了参加SemEval-2007:task #5评测任务的最好的有监督词义消歧系统。","数据稀疏,模型优化,有监督模型,语言模型,参数估计"
2012-10-08,多种语义特征在突发事件新闻中的共指消解研究,"庞宁,杨尔弘","提高突发事件应对的关键在于快速地收集和提取相关新闻报道中的有用信息,共指消解是信息提取研究的重要子任务。该文采用最大熵模型对汉语突发事件新闻报道中的共指现象进行消解,综合对比了语义类特征、语义角色特征,以及基于维基百科的语义相关特征,重定向特征及上下文特征在测试集上的效果。实验结果表明,除单纯使用语义角色特征会使系统F值下降1.31％以外,其余各种语义知识对共指消解模型的结果均有所提高。","中文信息处理,突发事件,共指消解,语义特征,最大熵模型"
2012-11-29,Weighted-Tau Rank: 一种采用加权Kendall Tau的面向排序的协同过滤算法,"孙建凯,王帅强,马军","已知的面向排序的协同过滤算法主要有两个缺点:计算用户相似度时只考虑用户对同一产品对的偏好是否一致,而忽略了用户对产品对的偏好程度以及该偏好在用户间的流行度; 进行偏好融合和排序时需要中间步骤来构建价值函数然后才能利用贪婪算法产生推荐列表。为解决上述问题: 我们利用类TF-IDF加权策略对用户的偏好程度及偏好流行度进行综合考量,使用加权的Kendall Tau相关系数计算用户间的相似度;进行偏好融合与排序时则使用基于投票的舒尔茨方法直接产生推荐列表。在两个电影数据集上,本文提出的算法在评测指标NDCG上的效果要明显优于其他流行的协同过滤算法。","协同过滤,面向排序,加权KendallTau,舒尔茨方法"
2013-07-17,基于吸收马尔可夫链的子话题发现方法,"魏明川,朱俊杰,张瑾,张凯,程学旗,任彦","受互联网文本信息话题内容多元性,演化性等特点的影响,传统的话题检测模型对子话题粒度的选取和检测质量很难保证。针对该问题,该文提出一种基于吸收马尔可夫链的子话题划分算法,该算法对基于网页聚类生成的话题关键词进行组合生成子话题,并以吸收马尔可夫链对子话题进行吸收衍化,进行重排序生成结果子话题。实验结果表明,该算法能同时保证生成子话题的重要性和多样性。","子话题划分,话题关键词,吸收马尔可夫链"
2012-12-15,特定事件微博与新闻报道话题对比研究,"周振宇,李芳","该文描述了基于特定事件的新闻报道和微博在话题层面的对比研究。首先利用LDA话题模型抽取两种媒体上关于特定事件的话题,然后提出了话题关注度、差异度、演化度的定义和计算公式,改进了不同媒体话题差异度的计算方法,最后,选取四个不同种类的事件,进行实验对比与分析,结果显示,关于同一事件,1)微博上评论性话题较多,话题关注度值比较接近;新闻报道上事实性话题较多,话题关注度值差异较大;2)微博与新闻报道对评论性话题词汇差异度大,事实性话题词汇差异度小;3)微博上评论性话题持续时间较长,内容变化较少;新闻报道上事实性话题持续时间较长,内容变化较少。","话题模型,微博,新闻报道,对比"
2013-01-06,社交网络中的社团结构挖掘,"范超,王厚峰","社交网络已经成为现代人们在线交流并交换信息的重要途径之一。以国内的人人网为例,大量的年轻人,尤其是学生,以此为平台,相互讨论感兴趣的话题。人与人之间因为学习关系、工作关系、共同的兴趣等诸多因素关联起来;以大学生交流为主体的社交网则更有可能因为在相同院、系、所而关联在一起,从而呈现出社团结构。该文以人人网的真实数据,使用CNM算法来验证这一假设;同时,还利用社会网络的结构知识对CNM算法作了改进,提高了社团发现的精度。所挖掘的社团结构关系还表明,高校不同院系和学科形成的社团具有各自的特点。","社交网络,社团结构,社团挖掘,人人网"
2012-08-10,基于用户意图识别的查询推荐研究,"罗成,刘奕群,张敏,马少平,茹立云,张阔","信息检索的效果很大程度上取决于用户能否输入恰当的查询来描述自身信息需求。很多查询通常简短而模糊,甚至包含噪音。查询推荐技术可以帮助用户提炼查询、准确描述信息需求。为了获得高质量的查询推荐,在大规模“查询-链接”二部图上采用随机漫步方法产生候选集合。利用摘要点击信息对候选列表进行重排序,使得体现用户意图的查询排在比较高的位置。最终采用基于学习的算法对推荐查询中可能存在的噪声进行过滤。基于真实用户行为数据的实验表明该方法取得了较好的效果。","查询推荐,用户意图挖掘,摘要点击模型"
2012-12-14,搜索引擎用户行为与用户满意度的关联研究,"刘健,刘奕群,马少平,张敏,茹立云,张阔","用户满意度是以用户为中心的搜索引擎性能评价的一个重要分支,区别于传统基于查询与文档相关性的评价方法,基于用户满意度的性能评价能够更加全面、客观地对搜索引擎性能进行评价。该文通过设计搜索实验平台,在尽量不影响用户正常搜索过程的前提下收集用户的搜索行为及其满意度评价,通过用户行为分析的方法挖掘用户群体行为特征与用户查询满意度之间的关联关系。相关结论对提高搜索引擎性能、改善用户查询体验具有一定的参考意义。","搜索引擎,用户行为分析,用户满意度"
2013-01-28,一种基于内存的高效在线数据处理服务框架,"林祥辉,张瑾,黄康平,许磊,许洪波,程学旗,程工","在海量数据处理环境下,传统的基于中心数据库的架构已经无法满足大规模的数据处理应用中高并发高数据读写的需求,而串行的工作模式也使得数据分析的时效性得不到有效的保证,已经严重地影响了用户体验。该文从应用架构的角度出发,提出了一种基于内存的高效在线数据处理服务框架,通过多索引的高效数据存取方法和基于发布/订阅模式的数据访问控制机制,在有效减少用户对中心数据库的读写请求的同时提高了数据处理的时效性。实验结果表明该文提出的基于内存的高效在线数据处理服务框架能够有效提高数据库的响应速度,缩短数据处理延时。","海量数据,数据处理,在线缓存,发布/订阅"
2013-03-22,引入集成学习的最大熵短语调序模型,"何钟豪,苏劲松,史晓东,陈毅东,黄研洲","基于最大熵的括号转录语法模型具有翻译能力强、模型训练简单的优点,成为近些年统计机器翻译研究的热点。然而,该模型存在短语调序实例样本分布不平衡的缺点。针对该问题,该文提出了一种引入集成学习的短语调序模型训练方法。在大规模数据集上的实验结果表明,我们的方法能有效改善调序模型的训练效果,显著提高翻译系统性能。","最大熵,短语调序,不平衡分类,集成学习"
2013-09-06,现代汉语常用动词释义对比研究--以《现代汉语词典》(第六版)和《重編國語辭典修訂本》为例,"刘珺,徐德宽,马梦佳,陈淑梅","动词在语言中的地位十分重要,而汉语更是一种动词性语言。了解动词的释义,是研究动词的一个重要途径。该文采用《现代汉语词典》(第六版)和《重編國語辭典修訂本》为对比研究的材料,参考新汉语水平考试大纲,选取三者共有的动词词条进行研究,着重对比两本词典对动词的释义,找出普通话和台湾地区所用标准语两者在动词方面的差异,减少两岸交流中因词义不同所产生的误会,更好地促进两岸交流。","常用动词,释义对比,释义差异"
2012-03-26,维吾尔语音素的声学特征分析,"王辉,努尔麦麦提·尤鲁瓦斯,吾守尔·斯拉木","该文对不同语速下,人工标注的维吾尔语连续语音语料中各音素进行共振峰频率、音长、音强的统计分析,并完成辅-元结构下的塞音、塞擦音的声学特征分析。该文通过美尔频率倒谱系数与共振峰频率等声学特征的融合及模型状态数的修改,对维吾尔语音素识别的声学模型进行了改进,并验证了不同声学特征对音素识别的影响。相比于基线系统,改进后声学模型的识别率取得一定提升。同时,利用语音学知识分析维吾尔语易混淆音素产生原因,为音素识别声学模型的进一步改进提供参考依据。","维吾尔语,声学特征,特征融合,语速"
2012-07-13,最大熵和条件随机场模型相融合的藏文人名识别,"加羊吉,李亚超,宗成庆,于洪志","藏文人名识别是藏文信息处理领域研究的难点之一,其识别效果直接影响到藏文自动分词的精度和相关应用系统的性能,包括藏汉翻译、藏文信息检索、文本分类等。该文在分析藏文人名构成规律和特点的基础上,提出了一种最大熵和条件随机场相融合的藏文人名识别方法。实验表明,该方法可以获取较好的识别效果,在我们的测试集上F-测度值到达了93.08%。","藏文人名识别,最大熵,条件随机场"
2012-02-03,计算机识别藏语虚词的方法研究,"高定国,扎西加,赵栋材","藏文虚词的研究是藏文信息处理技术中词、句及语义研究的基础,而计算机自动识别藏文虚词又是藏语虚词研究的前提。该文在论述藏语虚词在藏语文本中的作用和使用方法的基础上,分析了计算机识别藏语虚词的难度,提出了一个计算机识别藏语虚词的方法,并用2525句典型藏文句子进行了验证,对结果进行分析发现藏文虚词识别的正确率高达97.0768%。","识别,藏语,虚词"
2012-07-01,维吾尔文网页研究及Android维文浏览器的实现,"邓俊,吾守尔·斯拉木,艾尼宛尔·托乎提,袁廷磊,赵志成","通过二次修改WebKit内核来定制浏览器功能是当前嵌入式应用开发的热点。在研究Android平台浏览器引擎WebKit的基础上,综合分析多款浏览器在访问维吾尔文网站时出现的显示问题,找出访问维文网页时显示异常的原因。最后根据维吾尔文文字特点进行研究、设计维文浏览器架构,提出在应用层开发维文网页渲染引擎,实现Android平台的维吾尔文浏览器。","Android,浏览器,WebKit,维吾尔文,网页"
2014,"内容丰富多彩,阐述深入浅出--评《统计自然语言处理》(第2版)",俞士汶,,
2014-05-19,多策略机器翻译研究综述,"李业刚,黄河燕,史树敏,冯冲,苏超","该文全面综述和分析了多策略机器翻译的研究。根据所采用策略方式的差异,我们将多策略机器翻译分为系统级策略融合和模块级策略融合。在分别介绍了不同的翻译方法后,着重介绍了系统级策略融合和模块级策略融合各自具有代表性的研究工作。最后,对多策略机器翻译的研究进行了展望。","机器翻译,多策略机器翻译,融合机器翻译,混合机器翻译,多引擎机器翻译"
2013-05-02,微博检索的研究进展,"卫冰洁,王斌,张帅,李鹏","随着微博的快速发展,微博检索已经成为近年来研究领域的热点之一。该文首先以TREC Microblog数据为基础,从分析微博文档和微博查询两方面出发,得出微博检索与传统文本检索之间的两点不同: 一是微博文档相较于网页具有很多独有的特征;二是微博查询属于时间敏感查询,即在排序时除了考虑文本的语义相似度,还需要考虑时间因素,将这类方法统称为时间感知的检索技术。这两点差异使得已有的信息检索技术不能满足微博搜索的需求。该文主要介绍了近年来这两方面的相关研究: 首先描述了微博本身的多种特征以及基于这些特征提出的检索方法;然后以传统信息检索过程为主线,分别介绍了将时间信息用于文本表示、文档先验、查询扩展三方面的排序模型,最后总结了已有工作并且对未来研究内容进行了展望。","微博检索,时间信息,微博特性,文本表示,文档先验,查询扩展"
2012-06-22,向上学习方法改进移进归约中文句法分析,"朱慕华,王会珍,朱靖波","基于移进归约的句法分析系统具有线性的时间复杂度,因此在大规模句法分析任务中具有特别实际的意义。然而目前移进归约句法分析系统的性能远低于领域内最好的句法分析器,例如,伯克利句法分析器。该文研究如何利用向上学习和无标注数据改进移进归约句法分析系统,使之尽可能接近伯克利句法分析器的性能。我们首先应用伯克利句法分析器对大规模的无标注数据进行自动分析,然后利用得到的自动标注数据作为额外的训练数据改进词性标注系统和移进归约句法分析器。实验结果表明,向上学习方法和无标注数据使移进归约句法分析的性能提高了2.3%,达到82.4%。这个性能与伯克利句法分析器的性能可比。与此同时,该文最终得到的句法分析系统拥有明显的速度优势(7倍速度于伯克利句法分析器)。","中文句法分析,移进归约分析,伯克利句法分析器,向上学习,无标注数据"
2012-11-20,基于归约的汉语最长名词短语识别方法,"钱小飞,侯敏","该文提出了最长名词短语(MNP)的操作性定义,分析了其构造和分布特征,并设计了一种基于baseNP归约的识别方法,利用MNP结构特性及起始有定成分、语义核心等语言学特征,缓解了最长名词短语长距离依赖与模型观察窗口受限的矛盾。开放测试取得了88.68%的正确率和89.21%的召回率;归约方法全面提升了识别性能,特别是将多词结构的调和平均值提高1%,优化幅度达6%以上,并且对长距离复杂结构有着更好的识别效果。","最长名词短语,识别,归约,基本名词短语"
2012-09-27,基于有监督学习的医古文叙述性术语语义标注,"丁长林,白宇,蔡东风","对自由文本形式的中医古籍文献(医古文)进行标注,是对其进行深入分析的前提,语义标注技术是实现该目的的方法之一。该文将中医古籍文献中包含的术语分为名称性术语以及叙述性术语。在分析叙述性术语特点的基础上,将对其语义标注转化为基于有监督学习的短句序列标注或分类问题,并提出了名词性术语规约操作以及基于知网的替换操作两种预处理方法。最后该文通过实验对比了三种学习模型及四种特征选择算法,并证明了问题转化的可行性以及两种预处理方法的有效性。","语义标注,叙述性术语,有监督学习,中医古籍文献"
2013-03-19,一种基于图模型的维基概念相似度计算方法及其在实体链接系统中的应用,"张涛,刘康,赵军","实体链接是指将文本中具有歧义的实体指称项链接到知识库中相应实体的过程。该文首先对实体链接系统进行了分析,指出实体链接系统中的核心问题—实体指称项文本与候选实体之间的语义相似度计算。接着提出了一种基于图模型的维基概念相似度计算方法,并将该相似度计算方法应用在实体指称项文本与候选实体语义相似度的计算中。在此基础上,设计了一个基于排序学习算法框架的实体链接系统。实验结果表明,相比于传统的计算方法,新的相似度计算方法可以更加有效地捕捉实体指称项文本与候选实体间的语义相似度。同时,融入了多种特征的实体链接系统在性能上获得了达到state-of-art的水平。","实体消歧,实体链接,语义相似度计算,排序学习,随机游走"
2013-06-27,强语义模糊性词语的情感分析,"张志飞,苗夺谦,岳晓冬,聂建云","语义的模糊性给词语的情感分析带来了挑战。有些情感词语不仅使用频率高,而且语义模糊性强。如何消除语义模糊性成为词语情感分析中亟待解决的问题。该文提出了一种规则和统计相结合的框架来分析具有强语义模糊性词语的情感倾向。该框架根据词语的相邻信息获取有效的特征,利用粗糙集的属性约简方法生成决策规则,对于规则无法识别的情况,再利用贝叶斯分类器消除语义模糊性。该文以强语义模糊性词语“好”为例,对提出的框架在多个语料上进行实验,结果表明该框架可以有效消除“好”的语义模糊性以改进情感分析的效果。","情感分析,语义模糊性,粗糙集,贝叶斯分类"
2013-06-09,基于MapReduce的并行PLSA算法及在文本挖掘中的应用,"李宁,罗文娟,庄福振,何清,史忠植","PLSA(Probabilistic Latent Semantic Analysis)是一种典型的主题模型。复杂的建模过程使其难以处理海量数据,针对串行PLSA难以处理海量数据的问题,该文提出一种基于MapReduce计算框架的并行PLSA算法,能够以简洁的形式和分布式的方案来解决大规模数据的并行处理问题,并把并行PLSA算法运用到文本聚类和语义分析的文本挖掘应用中。实验结果表明该算法在处理较大数据量时表现出了很好的性能。","概率主题模型,MapReduce,并行,语义分析"
2013-03-11,统计机器翻译和翻译记忆的动态融合方法研究,"汪昆,宗成庆,苏克毅","在融合翻译记忆和统计机器翻译的整合式模型的基础上,该文提出在解码过程中进一步地动态加入翻译记忆中新发现的短语对。它在机器翻译解码过程中,动态地加入翻译记忆片段作为候选,并利用翻译记忆的相关信息,指导基于短语的翻译模型进行解码。实验结果表明该方法显著提高了翻译质量: 与翻译记忆系统相比,该方法提高了21.15个BLEU值,降低了21.47个TER值;与基于短语的翻译系统相比,该方法提高了5.16个BLEU值,降低了4.05个TER值。","统计机器翻译,基于短语的翻译模型,翻译记忆,模型融合,动态加入翻译记忆短语对"
2013-01-27,利用句法短语改善统计机器翻译性能,"孙水华,丁鹏,黄德根","短语表是基于短语的统计机器翻译系统的一个核心组成部分,基于启发式方法抽取到的短语表受单词对齐错误和未对齐词的影响严重,同时抽取到的短语也并非句法意义上的短语。该文提出一种基于EM(Expectation-maximization)算法的双语句法短语抽取方法来抽取双语句法短语,此方法可以通过不断迭代的方式使各参数值达到最优。通过加入双语句法短语、增加新特征、重新训练三种不同的方法,将获得的双语句法短语与基于短语的统计机器翻译方法结合以提高统计机器翻译系统的性能。结果表明: 三种方法都不同程度提高了译文的BLEU(BiLingual Evaluation Understudy)值,其中增加新特征方法提高了0.64个点。","统计机器翻译,EM算法,双语句法短语"
2014-03-17,基于统计专用字符的维、哈、柯文文种识别研究,"买买提依明·哈斯木,吾守尔·斯拉木,维尼拉·木沙江,努尔麦麦提·尤鲁瓦斯","在Unicode编码方案中维、哈、柯文字符安排在阿拉伯字符区域,三种语言中共享字符比较多,跟阿拉伯字符区域混在一起,没有专用的语言ID。在信息检索和自然语言处理领域对维、哈、柯文的识别、处理带来不便。该文首先分析并总结了维、哈、柯文三种语言中的专用字符、复合字符、某些字符在某种语言中出现形势的独特性等特征,然后在此基础上设计了维、哈、柯文种识别算法。 实验结果表明该文提出的文种识别算法的正确率在文本多于70词时达到96.67%以上。","文种识别,专用字符,复合字符,维文,哈文,柯文,Unicode"
2014-04-17,关于维吾尔语口语语料的三音子选取方法研究,"徐宝龙,努尔麦麦提·尤鲁瓦斯,吾守尔·斯拉木","在大词汇量连续语音识别应用中,优质的语音训练语料是所有识别工作的基础和前提, 能否挑选出覆盖更多语音现象的语料是提高语音识别性能的关键。该文在多种维吾尔文口语化传播平台中采集了大量口语句子语料,并考虑协同发音的影响和常用词的适用性,根据评估函数对语料筛选。经过筛选后的语料包含的三音子更加均衡和高效,囊括的语音现象更加全面,为训练准确而牢靠的语音模型打下了稳固的根基。","维吾尔语,语音识别,语料库,三音子"
2012-10-25,藏文停用词选取与自动处理方法研究,"珠杰,李天瑞","停用词的处理是文本挖掘中一个关键的预处理步骤。该文结合现有停用词的处理技术,研究了基于统计的藏文停用词选取方法,通过实验分析了词项频率、文档频率、熵等方法的藏文停用词选用情况,提出了藏文虚词、特殊动词和自动处理方法相结合的藏文停用词选取方法。实验结果表明,该方法可以确定一个较合理的藏文停用词表。","藏文停用词,词频统计,文档频数,熵"
2012-10-13,维吾尔语多词领域术语的自动抽取,"田生伟,钟军,禹龙","多词领域术语抽取是自然语言处理技术中的一个重点和难点问题, 结合维吾尔语语言特征,该文提出了一种基于规则和统计相结合的维吾尔语多词领域术语的自动抽取方法。该方法分为四个阶段: ①语料预处理, 包括停用词过滤和词性标注; ② 对字串取N元子串, 利用改进的互信息算法和对数似然比率计算子串内部的联合强度, 结合词性构成规则, 构建候选维吾尔语多词领域术语集; ③ 利用相对词频差值, 得到尽可能多的维吾尔语多词领域术语; ④ 结合C_value值获取最终领域术语并作后处理。实验结果准确率为85.08%, 召回率为 73.19%, 验证了该文提出的方法在维吾尔语多词领域术语抽取上的有效性。","维吾尔语,多词领域术语,互信息,对数似然比率,相对词频差值"
2014-03-24,一种女书手写字符规范字形自动生成方法,"李波,王江晴,魏红昀,孙阳光,王新年,徐凌","中国女书是具有鲜明民族特色的文字,目前国内还没有公认的女书规范字库。针对手写体文字规范化字体生成过程多采用人工修正方式、效率低下的现状,该文设计了一种女书手写字符规范字自动生成方法。基于手写文字样本,提取其单像素骨架,并结合字符轮廓信息进行骨架畸变点校正;然后提取骨架特征点和笔段,根据笔段连通性和交角情况建立笔段关联矩阵;基于笔段关联矩阵由笔段恢复笔画,获取笔画路径关键点序列;最后基于三次Bezier曲线重绘字符笔画并均匀加粗,形成笔画粗细一致、平滑无毛刺、无畸变的规范字体。实验结果表明,该方法自动便捷,效果良好,效率优于人工方式,经改进后可以推广到其他手写字符的规范化过程。","女书,规范化,笔段"
2013-01-14,基于OpenType技术的方块苗文字库研究,"莫礼平,周恺卿,蒋效会","方块苗文是民间苗文的代表,其信息处理研究对于保护民间苗族文化遗产和弘扬苗族文化有着重要意义。字库开发是方块苗文信息处理研究内容的重要部分。根据方块苗文字库开发的实际需要,该文以文字结构分析为基础,提出了基于Unicode标准的方块苗文编码方案设计思想,介绍了方块苗文字符字模制作的基本步骤,并以标签定义、操作符定义和变换规则定义为重点,探讨了基于OpenType技术的方块苗文字库设计和开发的方法。测试结果表明,方块苗文OpenType字库具有文件小、易扩充等优点,能够解决英文、汉字和方块苗文的混排问题。","方块苗文,Unicode标准,字符编码,OpenType技术,字库"
2013-04-08,基于逆向匹配的电子商务网站实体模板半自动构建方法,"傅彦,徐昭邦,夏虎,周俊临","Web页面中的主题信息一般分布比较集中,可利用网页的这一特性进行网页主题信息的自动提取。网页源代码中的HTML标签不规范,使得正向匹配难以生成嵌套结构准确的DOM树,该文提出一种通过逆向匹配的方法,构建完整的网页源代码DOM树。通过对DOM树进行剪枝,删除无关节点,对保留下来的信息块的节点标签进行人工选择与唯一性判定,从而生成提取模板。该方法能够实现对电子商务网站源网页中的主题信息进行提取,是一种半自动、通用的方法,可用于信息检索系统中的信息采集。","逆向匹配,DOM树,模板构建,信息提取"
2012-12-06,基于文本特征的短文本倾向性分析研究,"程南昌,侯敏,滕永林","语篇倾向性分析是倾向性分析的较高层次领域。根据文本篇幅和结构可以将语篇分为短文本和长文本。该文以网络商品评论作为样本研究短文本倾向性分析的特点和策略。根据倾向极性在文中的决定性因素的不同表现,短文本可以分为含显性归总句、含隐性归总句、含特征词以及一般文本四类,针对不同类别文本采用不同的处理策略。在此基础上,运用词典、规则的方法构建了语篇倾向性分析系统CUCsas,该方法在第四届中文倾向性分析评测(COAE2012)中取得了较好成绩。","短文本,文本特征,归总句,倾向性分析,词典与规则"
2012-09-20,基于D-S证据理论的微博客蕴含交通信息提取方法,"张恒才,陆锋,仇培元","微博客消息中经常蕴含大量实时交通信息,有望与现有实时交通信息采集方式形成互补。该文针对微博客消息语义模糊性及用户描述差异性问题,提出了一种微博客消息蕴含交通信息的D-S证据理论提取方法。该方法首先构建微博客消息蕴含交通状态信息评价体系,利用百科知识提高评价精度,然后定义微博客消息源的基本概率分配函数,通过证据合成与证据决策,实现微博客消息蕴含实时交通信息的甄别与融合。实验结果表明,该方法能够对微博客消息蕴含实时交通信息的可信度进行有效判断,并能够在最大程度上利用不同微博客用户发布消息的信息内容,且较之传统的文本聚类融合方法具有更高的准确率。","微博客,交通信息,文本聚类,证据理论,维基百科"
2013-03-18,基于上下文的话题演化和话题关系抽取研究,"章建,李芳","自动挖掘大规模语料中的语义信息以及演化关系近年来已受到广大专家学者的关注。话题被认为是文档集合中的潜在语义信息,话题演化用于研究话题内容随时间的变化。该文提出了一种基于上下文的话题演化和话题关系抽取方法。分析发现,一个话题常和某些其他话题共现在多篇文档中,话题间的这种共现信息被称为话题的上下文。上下文信息可以用于计算同时间段话题间的语义关系以及识别不同时间段中具有相同语义的话题。该文对2008年～2012年两会报告以及2007年～2011年NIPS科技文献进行实验,通过人工分析,利用话题的上下文信息,不但可以提高话题演化的正确率,而且还能挖掘话题之间的语义关系,在话题演化的基础上,显示话题关系的演化。","话题,话题上下文,话题演化,话题关系"
2013-07-12,维基百科中翻译对的模板挖掘方法研究,"段建勇,闫启伟,张梅,胡熠","双语翻译对在跨语言信息检索、机器翻译等领域有着重要的用途,尤其是专有名词、新词、俚语和术语等的翻译是影响其系统性能的关键因素,但是这些翻译对很难从现有的词典中获得。该文针对维基百科的领域覆盖率和结构特征,提出了一种从维基百科中自动获取高质量中英文翻译对的模板挖掘方法,不但能有效地挖掘出常见的模板,而且能够发现人工不容易察觉的复杂模板。主要方法包括三步: 1)从语言工具栏中直接抽取翻译对,作为进一步挖掘的启发知识;2)在维基百科页面中采用PAT-Array结构挖掘中英翻译对模板;3)利用挖掘的模板在页面中自动挖掘其他中英文翻译对,并进行模板评估。实验结果表明,模板发现翻译对的正确率达90.4%。","双语翻译对,维基百科,模板挖掘,信息抽取"
2012-11-01,槽填充中抽取模式的优化方法,"沈晓卫,李培峰,朱巧明","在传统的信息抽取中,模式匹配已经被证实为简便而有效的方法,而依存路径也是最为常用的模式之一。在槽填充任务中就有众多的参与者引入了以依存路径为基础的模式匹配方法;该文就针对该方法中存在的包括模式平衡性,模式抽取方式和模式筛选策略等方面的问题,提出了模式裁剪、模式转置、模式扩展和模式语义定义等主要的优化方法并实现了相关系统,在TAC-KBP2010的目标语料上进行了测试。该文提出的方法F值为20.8%,比基准系统的14.3%提高了6.5%。","槽填充,模式优化,信息抽取"
2014-06-11,基于字符的中文分词、词性标注和依存句法分析联合模型,"郭振,张玉洁,苏晨,徐金安","目前,基于转移的中文分词、词性标注和依存句法分析联合模型存在两大问题: 一是任务的融合方式有待改进;二是模型性能受限于全标注语料的规模。针对第一个问题,该文利用词语内部结构将基于词语的依存句法树扩展成了基于字符的依存句法树,采用转移策略,实现了基于字符的中文分词、词性标注和依存句法分析联合模型;依据序列标注的中文分词方法,将基于转移的中文分词处理方案重新设计为4种转移动作: Shift_S、Shift_B、Shift_M和Shift_E,同时能够将以往中文分词的研究成果融入联合模型。针对第二个问题,该文使用具有部分标注信息的语料,从中抽取字符串层面的n-gram特征和结构层面的依存子树特征融入联合模型,实现了半监督的中文分词、词性标注和依存句法分析联合模型。在宾州中文树库上的实验结果表明,该文的模型在中文分词、词性标注和依存分析任务上的F1值分别达到了98.31%、94.84%和81.71%,较单任务模型的结果分别提升了0.92%、1.77%和3.95%。其中,中文分词和词性标注在目前公布的研究结果中取得了最好成绩。","联合模型,中文分词和词性标注,依存句法分析,词语内部依存结构,半监督学习"
2014-06-20,基于联合音变还原和形态切分的形态分析方法,"张海波,蔡洽吾,姜文斌,吕雅娟,刘群","传统的形态分析方法,一般是先进行音变还原工作,再进行形态切分工作。音变还原工作的好坏直接影响形态切分工作的优劣,两者之间存在错误传播的问题。鉴于传统形态分析方法存在的错误传播问题,该文提出了基于联合音变还原和形态切分的形态分析方法。该方法通过使用具有双重功能的联合标签,同时实现了音变还原及形态切分的功能。由于该方法不依赖于黏着语的特有的语言学规则,因此便于扩展到新的语言上。结果表明,联合音变还原和形态切分的形态分析方法要优于传统的先进行音变还原后形态切分的形态分析方法,能够很好地解决先音变还原后形态切分带来的错误传播问题。","形态分析,音变还原,形态切分"
2014-06-20,基于字的分布表征的汉语基本块识别,"李国臣,党帅兵,王瑞波,李济洪","汉语的基本块识别是汉语句法语义自动分析中的重要任务之一。传统的方法大多数直接将汉语基本块识别任务转化成词层面的一个序列标注问题,采用CRF模型来处理。虽然,在许多评测中得到最好的结果,但基于词为标注单位,在实用中受限于自动分词系统以及汉语词特征的稀疏性。为此,该文给出了一种以字为标注单位,以字为原始输入层,来构建汉语的基本块识别的深层神经网络模型,并通过无监督方法,学习到字的C&amp;W和word2vec两种分布表征,将其作为深层神经网络模型的字的表示层的初始输入参数来强化模型参数的训练。实验结果表明,使用五层神经网络模型,以[-3,3]窗口的字的word2vec分布表征,其准确率、召回率和F值分别达到80.74%,73.80%和77.12%,这比基于字的CRF高出约5%。这表明深层神经网络模型在汉语的基本块识别中是有作用的。","汉语基本块,分布表征,深层神经网络,序列标注"
2014-06-11,"“A+一+X,B+一+Y”构式的分类及释义模板","刘洪超,詹卫东","该文以现代汉语中的“A+一+X,B+一+Y”格式为例,介绍了构建《现代汉语构式知识库》的初步工作。“A+一+X,B+一+Y”格式可根据其表义功能不同分为三个大类,十个小类。该文重点阐释了该构式表达“因果倚变义、事物交错义、状态交替义、动作行为交替义、周遍大量义、让步小量义”等6种意义的判定条件及相应的释义模板。","“A+一+X,B+一+Y”,构式知识库,释义模板"
2014-06-15,中文非投射语义依存现象分析研究,"郑丽娟,邵艳秋,杨尔弘","汉语是一种语序灵活的语言,句子变式很多,基于传统依存树的投射现象还不能很好解决某些句式的语义理解问题。文章以10000个句子的汉语语义依存图库为基础,验证并明确了汉语非投射现象的客观存在性,考察了汉语句子中存在的非投射现象,并从语言学和句子深层语义理解的角度对非投射现象进行了归纳和解释。文章总结了7类出现非投射现象的情况,包括小句宾语句、比较句、主谓谓语句、紧缩复句、代词、动补谓语句以及注释短语或复句。这对于自动语义依存标注有重要的指导作用。","语义分析,语义依存,非投射结构,依存图"
2014-07-21,汉语核心框架语义分析,"石佼,李茹,王智强","汉语核心框架语义分析是从框架语义角度,通过抽取句子的核心框架,获取汉语句子的核心语义骨架。该文将核心框架语义分析分为核心目标词识别、框架选择和框架元素标注三个子任务,基于各个子任务的不同特点,采取最大熵模型分别对核心目标词识别与框架选择任务进行建模;采用序列标注模型条件随机场对框架元素标注任务进行建模。实验在汉语框架网资源的10 831条测试语料中显示,核心目标词识别和框架元素标注F值分别达到99.51%和59.01%,框架选择准确率达到84.73%。","汉语框架网,核心框架语义,语义分析"
2014-06-20,基于分层输出神经网络的汉语语义角色标注,"王臻,常宝宝,穗志方","语义角色标注是自然语言处理中的一项重要任务。当下针对中文语义角色标注的主流做法是通过基于特征的统计机器学习实现的。然而,统计机器学习的方法需要引入经验性的人工特征,这在一定程度上增加了工作量。深度学习在自然语言处理领域的应用使得特征的自动学习成为可能。文章尝试了一种适用于语义角色标注的深层神经网络架构,该模型能自然地推广到其他标注任务。实验表明,深度学习算法能够有效地用于语义角色标注任务,但是我们仍然发现,模型对语义层面知识的学习是相当有限的,基于深度学习的方法还不能取代基于人工特征的统计机器学习算法。","语义角色标注,深度学习,特征向量"
2014-06-24,基于语义解析的中文GIS自然语言接口实现研究,"周俊生,曲维光,许菊红,龙毅,朱耀邦","该文对基于语义解析的中文地理信息系统(GIS)自然语言接口实现技术与方法进行了探索性的研究。首先,我们针对一个具体GIS应用领域设计和开发了一种函数式的形式化意义表示语言GISQL和一个中文语义解析标注语料库;然后,我们通过引入混合树作为隐变量用于构造输入句子与输出表示结构之间的对应关系,提出了一种基于含隐变量的感知器模型的语义解析算法。在开发的中文语义解析标注语料库上的实验结果显示,该文提出的语义解析算法的F1值达到了90.67％,明显优于baseline系统。更重要的是,该文的研究证明了基于语义解析方法实现中文GIS的自然语言接口是一种有效可行的途径。","地理信息系统,自然语言接口,语义解析"
2014-06-15,语法和语义相结合的中文对话系统问题理解研究,"黄沛杰,黄强,吴秀鹏,吴桂盛,郭庆文,陈楠挺,陈楚萍","针对中文口语问句的表达多样性对对话系统问题理解带来的挑战,该文采用“在语法结构之上获取语义知识”的设计理念,提出了一种语法和语义相结合的口语对话系统问题理解方法。首先人工编制了独立于领域和应用方向的语法知识库,进而通过句子压缩模块简化复杂句子,取得结构信息,再进行问题类型模式识别,得到唯一确定问题的语义组织方法、查询策略和应答方式的句型模式。另一方面,根据领域语义知识库,从源句子中提取相应的语义信息,并根据识别到的句型模式所对应的知识组织方法进行语义知识组织,完成对问句的理解。该文的方法被应用到开发的中文手机导购对话系统。测试结果表明,该方法能有效地完成对话流程中的用户问题理解。","问题理解,对话系统,句型模式,中文"
2014,面向政治新闻领域的中文文本校对方法研究,"张仰森,唐安杰,张泽伟","政治新闻领域内文本错误多为语义级错误。在研究新闻领域文本政治性差错的语言表述特征的基础上,分析了报刊新闻中政治性差错的表现类型,构建了面向各类错误侦测的词库和知识库。通过研究政治新闻文本的语言学特征,提出了一个政治性差错文本错误侦测规则的一般形式化模型,采用统计与规则相结合的策略实现政治新闻领域文本的语义校对。实验结果显示,该方法的召回率为65.5%,精确率为80.5%,具有较好的应用前景。","政治新闻,文本校对,查错模型"
2014-06-20,汉语动词资源馆(Chinese Verb Library)的构建,"汪梦翔,王厚峰,刘杨,饶琪","该研究以动词的语义聚合层次为核心,构建了含有四个语义层级的动词分类系统,依托生成词库理论、语义格框理论和构式语法理论,从事件结构、语义格框架、物性角色、句法格式四个角度来对动词的自身属性以及组合性等特征进行描述,从而构造出能够解释并深入描述动动之间、动名之间甚至是超常搭配用法的汉语动词资源馆Chinese Verb Library(CVL)。实验表明,该研究可以为句法分析、语义角色标注、尤其是揭示隐含的谓间关系提供新的支持。","汉语动词资源馆,生成词库论,事件结构"
2014-06-20,词位重构与平行语言资源的再生性建设,"萧国政,高精鍊,双文庭,姬东鸿,郭婷婷,吴泓渺","该文以大数据意识为背景,通过语言学范畴“词位”内涵、外延的重构,以自然和人工平行性语言资源为基础,提出和讨论语言资源的再生性建设命题。并以期“通过资源建构资源”的再生性模式,推动语言资源多类型、高覆盖、跨语言快速发展及语言应用理论建设。","词位重构,平行语言资源,资源再生性建设,语言信息处理理论"
2014-06-20,汉语显式篇章关系分析,"丁彬,孔芳,李生,周国栋","篇章关系分为显式和隐式两种。显式关系的显著特征是篇章的基本单元之间存在显式连接词。针对汉语显式篇章关系,构建了包括汉语连接词识别和篇章关系分类的显式篇章关系分析平台。该文选取汉语宾州树库(Chinese Penn Treebank, CTB)中的500篇文本进行了汉语显式篇章关系标注;结合连接词的中心词,采用最大熵分类器构建了汉语连接词识别模块,其性能F1值达到了66.79%;基于连接词及其词性等上下文特征,构建了篇章关系分类器,其在最顶层4大类语义关系上的分类性能的F1值为91.92%。","连接词识别,语义关系分类,最大熵分类器"
2014-06-15,广义话题结构理论视角下话题自足句成句性研究,"尚英,宋柔,卢达威","话题自足句是在广义话题结构理论的基础上定义的。话题自足句的成句性是广义话题结构的重要性质之一。该文在38万字不同语体的广义话题结构语料库中对话题自足句的成句性进行了实证性调查,发现有少量话题自足句不成句,对不成句现象进行了分析、分类,并提出了使其成句的办法。这将进一步完善广义话题结构理论,并能提高使用话题自足句的应用系统的性能。","广义话题结构,话题自足句,成句性"
2014-06-20,基于协同训练的文本蕴含识别,"任函,万菁,吴泓缈,冯文贺","针对文本蕴含的训练数据不足的问题,该文提出了基于协同训练的文本蕴含识别方法。该方法利用少量已标注的蕴含数据和大量未标注数据进行协同训练。为此,该文利用改写视图和评估视图,从结构和非结构两个角度考察蕴含关系,并将语义树核分类器和基于统计特征的分类器应用于两个视图,同时利用协同训练的结果训练一个综合分类器,用于对新数据进行预测。实验表明,基于协同训练的蕴含识别方法能在少量训练数据的情况下获得较好的识别性能。","文本蕴含识别,协同训练,语义树核"
2014-06-11,基于单文本指代消解的人物家庭网络构建研究,"顾静航,朱苏阳,钱龙华,朱巧明","人物家庭网络是社会关系网络中的一个重要组成部分,因此,如何高效准确地提取出人物的家庭网络具有重要研究意义。该文在前人工作的基础上提出一种基于单文本指代消解技术的人物家庭关系抽取方法,以此扩大人物家庭关系抽取的范围,进而提高人物家庭网络的召回性能。该文还提出了一种基于人物虚拟边的家庭网络评估指标,用于更合理地评价构建出的人物家庭网络的性能。在大规模中文语料Gigaword上的实验表明,该方法可以较为准确地抽取出人物的家庭关系,进而提高人物家庭网络的召回性能,从而为社会网络分析提供基础数据。","社会关系网络,家庭网络,单文本指代消解"
2014-06-24,基于图排序的词汇情感消歧研究,"杨亮,张绍武,林鸿飞,宋艳雪","词汇情感消歧是文本情感倾向性分析的关键技术之一。该文在分析比较了词汇情感消歧和词义消歧异同后,从情感分析角度出发,提出了基于图排序的词汇情感消歧方法。该方法通过自动获取和人工校正相结合的方式获得多情感词汇,然后根据语义关系构建词义关系图,进而在词义关系图上迭代计算直至收敛,最后选择多情感词汇的词义中权值最大的词义作为结果输出,从而实现情感消歧。该文分别在新浪微博语料库和情感语料库上验证了该方法的有效性。","多情感词汇,图排序,情感消歧"
2014-06-04,基于模糊集合的汉语主观句识别,"宋洪伟,贺宇,付国宏","主观句识别的工作在诸如情感分类和意见摘要等意见挖掘系统中占有很重要的地位。在该文中,我们提出一种基于情感密度的模糊集合分类器以识别汉语主观句。首先,我们利用优势率方法从训练语料中抽取主观性线索词;然后,为了能更好的表达一个句子的主观性,我们利用抽取出的主观性线索词计算出每个句子的情感密度;最后,我们结合情感密度的特点实现了一个三角形隶属度函数的模糊集合分类器以识别主观句。我们在NTCIR-6中文数据中做了两组实验。实验结果表明我们的方法具有一定的可行性。","主观句识别,情感密度,模糊集合,优势率"
2014-06-15,中文微博用户性别分类方法研究,"王晶晶,李寿山,黄磊","该文旨在研究中文微博用户的性别分类问题,即根据微博提供的中文文本信息对注册用户的性别进行识别。虽然基于微博的性别分类已经有一定研究,但是针对中文的性别分类工作还很缺乏。该文首先提出分别利用用户名和微博文本构建两个分类器对用户的性别类型进行判别,并对不同的特征(例如,字特征、词特征等)进行了研究分析;其次,在针对用户名和微博文本的两个分类器的基础上,使用贝叶斯融合方法进行分类器融合,从而达到采用这两种文本分类信息同时对用户性别进行性别判断。实验结果表明该文的方法可以达到较高的识别准确率,并且分类器融合的方法明显优于仅利用用户名或者微博文本的分类方法。","性别分类,新浪微博,文本分类,社交网络"
2014-06-17,一种基于弱监督学习的论坛帖子对话行为分类方法,"孙承杰,林磊,刘秉权","论坛帖子对话行为分类可以明确每个帖子在当前线索中的角色,有助于重构论坛线索中的对话关系,提高论坛信息检索的效果。该文提出了一种基于弱监督学习的论坛帖子对话行为分类方法,把帖子的对话行为分类作为线索的序列标注问题来解决。该方法的特点是只要指定合理的特征约束,就可以训练对话行为分类模型。方法在CNET和edX数据集上的分类精确率分别达到75.6%和60.7%,优于有监督的条件随机域方法。","弱监督学习,特征约束,对话行为分类,论坛线索结构分析"
2014-06-11,社交网络账号的马甲关系辨识方法,"樊茜,许洪波,梁英","正确辨识网络账号的马甲关系,能够维护网络环境的安全与和谐,抑制网络中不法行为和虚假信息。基于文本挖掘的作者身份识别一直受到广泛关注,但对社交网络中文本作者关系鉴别的研究较少,该文提出了一种社交网络账号的马甲识别方法,基于网络语言的风格和账号关系,分别提取网络文本特征和账号之间的回复关系频次两组特征构成特征集合,同时基于账号组合构建训练样本向量空间,鉴别网络账号的马甲关系。结合论坛数据对所提方法进行了实验验证,准确率达到80%,结果表明该方法具有较高的马甲辨别准确率。","马甲识别,语言风格,关系特征,社交网络"
2014-06-28,基于条件随机场与时间词库的中文时间表达式识别,"吴琼,黄德根","该文提出一种统计与规则相结合的时间表达式识别方法。首先,通过分析中文文本中时间表达式的词形、词性和上下文信息,采用条件随机场识别时间单元而非时间表达式整体,避免了中文时间表达式边界定位不准确的问题;然后,从训练语料中自动获取候选触发词,并依据评价函数对候选触发词打分,筛选出正确的触发词完善触发词库;最后,根据时间触发词库与时间缀词库,制定规则对时间表达式边界进行定位。实验结果显示开式测试F1值达到98.31%。","CRF,规则,时间触发词,时间缀词"
2014-06-28,基于句法语义特征的中文实体关系抽取,"郭喜跃,何婷婷,胡小华,陈前军","实体关系抽取的核心问题是实体关系特征的选择。以往的研究通常都以词法特征、实体原始特征等来刻画实体关系,其抽取效果已难再提高。在传统方法的基础上,该文提出一种基于句法特征、语义特征的实体关系抽取方法,融入了依存句法关系、核心谓词、语义角色标注等特征,选择SVM作为机器学习的实现途径,以真实新闻文本作为语料进行实验。实验结果表明该方法的F1值有明显提升。","句法特征,语义特征,实体关系抽取,SVM"
2014-06-06,基于话题检测的自适应增量K-means算法,"李胜东,吕学强,施水才,孙军","根据话题检测任务的定义和特点,本文分析了传统的增量聚类算法和K-means算法的优缺点,提出了基于话题检测的自适应增量K-means算法,设计了话题检测实验,实验结果证明了该算法提高了话题检测性能,具有良好的应用前景。","话题检测,增量聚类,K-means算法,话题检测与跟踪评测"
2014-06-18,基于文本聚类的语言韵律和节奏风格特征挖掘,"贺湘情,刘颖","该文以朱自清、汪曾祺和刘亮程的散文作品为语料,旨在从文本的韵律和节奏出发,采用文本聚类的方法来挖掘出新的能够代表作品风格的特征。实验表明,以句末用字韵母的n元组合、分句句长的n元组合、标点符号和整句句长作为风格特征,能成功地将这三位作家的作品区分开来。其中刘亮程句尾韵的舌位高于汪、朱二人,朱自清对韵脚的选择不如刘、汪二人丰富。汪曾祺的分句长最短,且最为讲究句式长短的对齐;刘亮程兼顾长短句的交错,节奏更富于变化;朱自清的句长变化最为平稳。","特征挖掘,韵律,节奏,文本聚类"
2014-06-01,基于词项—句子—文档三层图模型的多文档自动摘要,"熊娇,王明文,李茂西,万剑怡","应用图模型来研究多文档自动摘要是当前研究的一个热点,它以句子为顶点,以句子之间相似度为边的权重构造无向图结构。由于此模型没有充分考虑句子中的词项权重信息以及句子所属的文档信息,针对这个问题,该文提出了一种基于词项—句子—文档的三层图模型,该模型可充分利用句子中的词项权重信息以及句子所属的文档信息来计算句子相似度。在DUC2003和DUC2004数据集上的实验结果表明,基于词项—句子—文档三层图模型的方法优于LexRank模型和文档敏感图模型。","图模型,多文档自动摘要,句子相似度,词项—句子—文档图"
2014-06-11,基于依存句法分析的社会媒体文本挖掘方法——以饮食习惯特色分析为例,"任彬,车万翔,刘挺","在进行社会媒体文本挖掘时,传统的基于词表的方法,存在准确率较低、词表难获得等问题。该文提出一种基于依存句法分析的文本挖掘方法,通过规则匹配的方式从社会媒体文本中提取信息。该方法不依赖词表,且实验证明了相比基于词表的方法在准确率上有大幅提高。应用基于依存句法分析的文本挖掘方法,我们在微博文本上进行了饮食习惯特色分析,实现了性别、地区、时间等维度的饮食习惯特色分析并可进行交叉分析,最终用词云的方式展示了结果。","依存句法分析,文本挖掘,社会媒体,饮食习惯特色分析"
2014-06-10,基于LM算法的领域概念实体属性关系抽取,"刘丽佳,郭剑毅,周兰江,余正涛,邵发,张金鹏","针对非结构化自由文本中关系模式比较复杂,关系抽取性能不高的问题,该文提出了利用BP神经网络的优化算法-LM算法,对非结构化自由文本信息中的领域概念实体属性关系进行抽取。首先对语料进行预处理,然后利用CRFs模型对领域概念的实例、属性和属性值进行实体识别,然后根据领域中各类关系的特点分别进行特征提取,构造BP神经网络模型,利用LM算法抽取相应关系。和适用于二分类问题的SVM相比,人工神经网络优化算法自主学习能力强,识别精度高,更适用于多分类的问题。通过几组实验表明,该方法在领域概念实体属性关系抽取方面取得了良好的效果, F值提高了12.8%。","BP神经网络,LM算法,属性关系抽取"
2014-06-11,怎样用物性结构知识解决“网球问题”？,"袁毓林,李 强","“网球问题”指怎样把racquet(网球拍)、ball(网球)和net(球网)之类具有情境联想关系的词汇概念联系起来、发现它们之间的语义和推理关系。这是一个自然语言处理和相关的语言知识资源建设的世界性难题。该文以求解“网球问题”为目标,对目前比较主流的几种语言词汇和概念知识库系统(包括WordNet、VerbNet、FrameNet、ConceptNet等)进行检讨,指出它们在解决“网球问题”上还都存在一定的局限性,着重分析它们为什么不能解决“网球问题”。进而指出基于生成词库论的名词物性结构知识描写体系可以解决“网球问题”,主张用名词的物性结构知识和相关的句法组合知识来构建一种以名词(实体)为核心的词汇概念网络,以弥补上述几种知识库系统的不足,为自然语言处理提供一种可资参考的词汇概念知识库体系。","“网球问题”,Word Net,Verb Net,Frame Net,Concep tNet,物性结构"
2014-06-11,基于汉语音位发音想象的脑机接口研究,"杨晓芳,江铭虎","该文提出了一个基于汉语音位发音想象的脑机接口系统框架,使得受试者使用脑机接口系统时能更加自然和流畅。三名受试者参与了本实验研究,实验过程中受试者被要求想象四个汉语元音和四个辅音音位的发音部位及语音发音,以及一个不作想象任务的控制条件,同时记录其脑电数据。在数据处理阶段,本文对采集到的头皮脑电数据进行了频域、时域、空域分析,以提取出音位发音想象效应最优化的特征向量用于提高每两个条件间的配对分类效果。实验结果表明,音位发音想象效应的最优脑电频段为2~10Hz,时段为刺激呈现后300~500ms,头皮空间分布主要集中在感觉运动皮层区域。音位发音想象任务和控制条件相比具有较高的分类正确率,最高可达83%,为基于音位发音想象的汉语脑机接口系统研究提供了理论基础。此外,刺激材料间的Jaccard距离和分类正确率的高度相关性表明,音位发音想象任务可被视为复杂的发音器官运动想象任务,并且可由人脑感觉运动皮层区域的脑电信号来解码预测。","脑机接口,运动想象,音位发音"
2014-06-11,语言同现网、句法网、语义网的构建与比较,"赵怿怡,刘海涛","网络方法应用于语言研究是语言研究大数据时代的新趋势。语言是一个多层级的符号系统,选择哪种语言单位作为网络节点,选择哪种语言单位间的关系作为网络联结,影响到语言网络的结构和功能。该文梳理了以汉语词为单位,以同现、句法、语义关系为联结依据的几类网络构造方法,并针对同一文本构造三类网络发现: 句法网络的网络直径、平均路径长度远小于同现网络,实词在语义网络中占据中心节点位置。这提示我们网络分析方法的应用仍要以可靠的语言学理论为指导,从语言学内部出发才能更好解释各类语言网络的差异。","同现网,句法网,语义网"
2014-06-11,基于语块和条件随机场(CRFs)的韵律短语识别,"钱揖丽,冯志茹","该文提出一种基于汉语语块这一浅层句法信息,并利用条件随机场模型的中文文本韵律短语边界预测方法。首先介绍语块的定义和标注算法,然后在进行了语块结构标注以及归并处理的语料上,利用CRFs算法生成相应模型对韵律短语进行识别。实验结果表明,基于语块信息的CRFs韵律短语识别模型的识别效果优于不利用语块结构的模型,其F值平均能够提高约十个百分点。","韵律短语,边界预测,语块结构,条件随机场"
2014-06-12,利用扩展标记集的词结构分析,"孙 静,方 艳,丁 彬,周国栋","该文给出了一种与传统分词不同的词法分析选择,提出了一种利用扩展标记集来实现词内部结构分析的方法。首先阐述了词的内部结构特点,把结构中的前后缀视为特殊的词,进而通过识别出每一个词的前后缀来识别词的内部结构。方法是把词内部结构识别问题转换成序列标注问题,通过扩展标记集,采用CRF模型来实现词的内部结构分析。最终实验表明,无论是在总体性能上,还是在各层结构的识别上都取得了较高的准确度。","扩展标记集,词结构分析,前后缀,序列标注问题"
2014-06-11,量化词语的领域特征,"刘冬明,杨尔弘","词作为最小的语义单位,同领域之间具有复杂的关系,特别是较为常用的词,通常难以明确界定其所属领域。在某些应用中并非必须确定词和领域的明确关系,仅仅依赖词的领域性的量化值就能够取得较好的效果。该文根据大规模语料库中词的关联信息,采用无指导的方法,对词的领域性进行量化,其结果可以作为词的一种特征应用于文本分类、话题检测、信息检索等相关的自然语言处理中。最后,通过和常用的特征——TFIDF在话题检测应用中进行对比,证明了其有效性。","词的领域性,话题检测,TFIDF"
2014-06-11,基于Word Embedding语义相似度的字母缩略术语消歧,"于 东,荀恩东","该文提出基于Word Embedding的歧义词多个义项语义表示方法,实现基于知识库的无监督字母缩略术语消歧。方法分两步聚类,首先采用显著相似聚类获得高置信度类簇,构造带有语义标签的文档集作为训练数据。利用该数据训练多份Word Embedding模型,以余弦相似度均值表示两个词之间的语义关系。在第二步聚类时,提出使用特征词扩展和语义线性加权来提高歧义分辨能力,提高消歧性能。该方法根据语义相似度扩展待消歧文档的特征词集合,挖掘聚类文档中缺失的语义信息,并使用语义相似度对特征词权重进行线性加权。针对25个多义缩略术语的消歧实验显示,特征词扩展使系统F值提高约4%,使用语义线性加权后F值再提高约2%,达到89.40%。","字母缩略术语,术语消歧,Word Embedding,语义相似度"
2014-06-28,基于量词的名词概念获取研究,"王 萌,俞士汶","概念获取是自然语言理解领域中重要的研究课题。该文提出了一种基于汉语量词的名词概念描述方法,设计并实现了一个权重计算方案。通过聚类实验探索了量词对名词语义区分的作用和贡献,实验结果表明基于量词的名词概念表达方式是有效的,可以区分大部分名词概念。","概念获取,量名搭配,量词,聚类"
2014-06-11,汉语语义选择限制知识的自动获取研究,"贾玉祥,王浩石,昝红英,俞士汶,王治敏","语义选择限制刻画了谓语对论元的语义选择倾向,是一种重要的词汇语义知识,对自然语言的句法、语义分析具有重要作用。该文研究汉语语义选择限制知识的自动获取,提出基于HowNet和基于LDA (Latent Dirichlet Allocation)的两种知识获取方法,对方法进行了实验对比与分析。实验表明,前者所获取的知识可理解性更好,后者所获取的知识应用效果更好。两种方法具有很好的互补性,我们提出了一个二者的融合方案。","语义选择限制,知识获取,How Net,LDA(Latent Dirichlet Allocation)"
2014-06-11,汉语语义倾向语料库的建设,"杨 江,李 薇,彭石玉","该文从研究背景、设计思路、标注体系和方法、加工步骤等方面介绍了汉语语义倾向语料库的建设过程。该语料库是一个以研究语言主观性表达为目的的共时、非平衡、单语标注语料库,依据语言主观性多维度描述体系而设计,规模为100万字,配备有集检索与统计、结果检查与可视化于一体的专用语料库工具箱系统,具有可用性大、标注质量高、语言学理据强等特点。","语义倾向,语料库,主观性,建设"
2014-06-25,面向微博文本的情绪标注语料库构建,"姚源林,王树伟,徐睿峰,刘 滨,桂 林,陆 勤,王晓龙","文本情绪分析研究近年来发展迅速,但相关的中文情绪语料库,特别是面向微博文本的语料库构建尚不完善。为了对微博文本情绪表达特点进行分析以及对情绪分析算法性能进行评估,该文在对微博文本情绪表达特点进行深入观察和分析的基础上,设计了一套完整的情绪标注规范。遵循这一规范,首先对微博文本进行了微博级情绪标注,对微博是否包含情绪及有情绪微博所包含的情绪类别进行多标签标注。而后,对微博中的句子进行有无情绪及情绪类别进行标注,并标注了各情绪类别对应的强度。目前,已完成14000条微博,45431句子的情绪标注语料库构建。应用该语料库组织了NLP&amp;CC2013中文微博情绪分析评测,有力地促进了微博情绪分析相关研究。","情绪语料库,语料库构建,情绪标注,微博文本"
2014-06-11,基于HowNet的航空术语语义知识库的构建,"张桂平,刁丽娜,王裴岩","语义知识库的构建是自然语言处理基础性工作,对于语言信息的处理有重要的作用,但面向特定领域的语义知识库的构建还是一个难点。该文在分析了航空术语的基本特点的基础上,根据HowNet和KDML描述语言构建了面向航空领域的术语语义知识库,并在构建航空术语知识库的过程中总结形成了构建航空术语知识库的基础规则、动态角色/特征的选择规则。在文章最后对所构建的术语进行了相似度的计算,取得了较好的结果。","航空术语,How Net,语义知识库,KDML"
2014-06-11,基于话题链的汉语语篇连贯性描述体系,"周 强,周骁聪","汉语简洁灵活的意合型篇章组合结构,对传统的基于关联词的篇章连贯性描述体系提出了新的挑战。该文引入话题链描述形式,设计不同类型的话题评述关系集,构建了以话题链为主,融合关联词语和其他连贯形式描述机制,覆盖话题评述、并列、因果、转折四大类关系的汉语语篇连贯性描述体系。在清华句法树库TCT上进行的验证实验,发现话题链和关联词语分别覆盖了约76%和50%的汉语复句,并且两者经常同时使用,初步证明了这个体系在句子连贯性描述方面的可行性和有效性。","话题链,话题评述关系,连贯性描述体系,汉语语篇分析"
2014-06-11,从广义话题结构考察汉语篇章话题认知复杂度,"卢达威,宋 柔,尚 英","语言理解问题从认知的角度已有大量的研究,但针对汉语的研究却很少。由于认知实验操作复杂,不容易大规模复制,因此难以量化其结论的普遍性以及对语言事实的覆盖度。该文尝试模拟人补足汉语篇章片段中话题-说明信息的过程,建立广义话题结构认知机模型,并通过认知机对大规模汉语语料进行定量分析,考察汉语标点句的话题认知所需的记忆资源及认知局限性。用作统计特征量的广义话题结构特征有标点句的深度、话题结构内折返度、话题栈深度、话题栈折返度、搁置区使用量。统计数据可从认知行为的视角得到合理解释。该文一方面揭示了说汉语者的话题认知能力的表现和局限性,另一方面又说明了广义话题结构认知机是话题认知的合理模型。","广义话题结构,认知机,认知复杂度,标点句,话题自足句,汉语篇章"
2014-06-15,统计机器翻译删词问题研究,"李 强,何燕龙,栾 爽,肖 桐,朱靖波","该文对基于短语的统计机器翻译模型的删词问题进行研究与分析,使用人工评价的方式将删词错误分为3类。该文通过两种方法,即基于频次的方法和基于词性标注的方法,对源语言句子中关键词汇进行识别。通过对传统的短语对抽取算法中引入源语言对空关键词汇的约束来缓解删词错误问题。自动评价方法以及人工评价方法证明,该方法在汉英翻译任务以及英汉翻译任务中显著的缓解了删词错误问题,同时得到一个精简的短语翻译表。","统计机器翻译,删词问题,人工评价"
2014-06-11,融合格框架的基于语块的依存树到串日汉统计机器翻译模型,"吴培昊,徐金安,谢 军,张玉洁","该文提出了一种融合格框架的日汉基于语块的依存树到串统计机器翻译模型。其基本思想是从日语依存分析树获取格框架,在翻译模型的规则抽取及解码中,以日语格框架作为约束条件,指导依存树的句法结构重排,调整日语和汉语的句法结构差异,实现格框架与日汉依存树到串模型的融合。实验结果表明,该文提出的方法可有效改善日汉统计机器翻译的句法结构调序和词汇翻译,同时,还可有效提高日汉统计机器翻译的译文质量。","日汉机器翻译,格框架,依存树到串模型,句法结构"
2014-06-16,翻译规则剪枝与基于半强制解码和变分贝叶斯推理的模型训练,"高恩婷,段湘煜,巢佳媛,张 民","统计机器翻译一般采用启发式方法训练翻译模型。但启发式方法的理论基础不够完善,因此,会导致翻译模型规模庞大以及模型参数精确率不高。针对以上两个问题,该文提出一种基于变分贝叶斯推理的模型训练方法,形成更精确的精简翻译模型。该方法首先通过强制解码对齐语料,然后利用变分贝叶斯EM算法获得模型参数。该文的实验语料为NIST汉英翻译任务数据,实验结果显示,基于句法(基于短语)的统计机器翻译中,超过95%(76%)的规则被剪枝,且BLEU值显著提高。","机器翻译,规则剪枝,半强制解码,变分贝叶斯"
2014-06-05,微博汽车领域中用户观点句识别方法的研究,"潘艳茜,姚天昉","该文主要研究如何自动识别微博中用户对各品牌汽车进行评价的句子。针对微博中汽车宣传信息较多而由真正汽车用户发出的观点句所占比例很小的特点,该文提出了结合微博和汽车评论语料的基于SVM模型的分类方法。选取的特征包括词语、评价词个数、与评价对象有关的词语以及微博相关特征。实验表明,评价词特征和部分微博相关特征可有效提高分类器性能,使用微博和汽车评论两种语料进行训练的分类器性能要比仅使用微博语料的方法好。","微博,观点句识别,意见挖掘,SVM"
2014-06-14,基于深度学习的微博情感分析,"梁 军,柴玉梅,原慧斌,昝红英,刘 铭","中文微博情感分析旨在发现用户对热点事件的观点态度。已有的研究大多使用SVM、CRF等传统算法根据手工标注情感特征对微博情感进行分析。该文主要探讨利用深度学习来做中文微博情感分析的可行性,采用递归神经网络来发现与任务相关的特征,避免依赖于具体任务的人工特征设计,并根据句子词语间前后的关联性引入情感极性转移模型加强对文本关联性的捕获。该文提出的方法在性能上与当前采用手工标注情感特征的方法相当,但节省了大量人工标注的工作量。","深度学习,微博情感分析,递归神经网络,自编码"
2014-05-31,蒙古语短语结构树的自动识别,"乌 兰,达胡白乙拉,关晓炟,周 强","句法分析在自然语言信息处理中处于非常关键的位置。该文在描述蒙古语特点的同时提出蒙古语句子中短语结构分析难点。根据蒙古语自身特点,归纳了短语标注体系,建立了蒙古语短语树库,尝试实现蒙古语句子的自动分析。初次开发的句法分析器的分析准确率达到62%,自动分析器的测试结果表明该分析器能在较大程度上辨别出短语结构类型,能生成句法树结构,但在短语结构内部关系方面的识别效果还有很大改进空间。最后总结了分析器近期能解决的相关问题。","蒙古语,短语结构语法,句法树库,自动识别"
2014-06-11,基于错误驱动学习策略的藏语句法功能组块边界识别,"王天航,史树敏,龙从军,黄河燕,李 琳","藏语句法功能组块分析旨在识别出藏语句子的句法成分,为后续句子级深入分析提供支持。根据藏语的语言特点,该文在藏语句法功能组块描述体系基础上,提出基于错误驱动学习策略的藏语功能组块边界识别方法。具体思路为,首先基于条件随机场(Conditional Random Fields,CRFs)识别组块,然后分别基于转换规则的错误驱动学习(Transformation-based Error-driven Learning,TBL)及基于新特征模板的CRFs错误驱动学习进行二次识别,并对初次结果进行校正,F值分别提高了1.65%、 8.36%。最后通过实验分析,进一步将两种错误驱动学习机制融合,在18073词级的藏语语料上开展实验,识别性能进一步提高,准确率、召回率与F值分别达到94.1%、94.76%与94.43%,充分验证了本文提出方法的有效性。","错误驱动学习,藏语句法功能组块,组块边界识别,CRFs,TBL"
2014-06-11,基于多策略的藏语语义角色标注研究,"龙从军,康才畯,李 琳,江 荻","语义角色标注研究对自然语言处理具有十分重要的意义。英汉语语义角色标注研究已经获得了很多成果。然而藏语语义角色标注研究不管是资源建设,还是语义角色标注的技术探讨都鲜有报道。藏语具有比较丰富的句法标记,它们把一个句子天然地分割成功能不同的语义组块,而这些语义组块与语义角色之间存在一定的对应关系。根据这个特点,该文提出规则和统计相结合的、基于语义组块的语义角色标注策略。为了实现语义角色标注,文中首先对藏语语义角色进行分类,得到语义角色标注的分类体系;然后讨论标注规则的获得情况,包括手工编制初始规则集和采用错误驱动学习方法获得扩充规则集;统计技术上,选用了条件随机场模型,并添加了有效的语言特征,最终语义角色标注的结果准确率、召回率和F值分别达到82.78%、85.71%和83.91%。","藏语,语义角色标注,TBL,CRFs"
2014-06-11,维吾尔语语音检索技术研究,"张力文,努尔麦麦提·尤鲁瓦斯,吾守尔·斯拉木","随着大数据时代的到来,各种音频、视频文件日益增多,如何高效地定位关键敏感信息具有非常重要的研究意义。目前研究人员对针对英语和汉语的语音检索技术进行了深入的研究,而针对维吾尔语的语音检索技术还处于起步阶段。该文对维吾尔语语音关键词检索技术进行了研究并采用了大词汇量连续语音识别、利用聚类算法将多候选词图转换为混淆网络、倒排索引、置信度以及相关度的计算等技术和方法,对维吾尔语语音检索系统进行了研究与搭建。最后在测试集上对该系统进行测试,测试结果显示,在语音识别正确率为82.1%的情况下,检索系统的召回率分别达到97.0%和79.1%时,虚警率分别为13.5%和8.5%。","维吾尔语,语音检索,语音识别,词图,混淆网络,倒排索引"
2014,基于感知器算法的维吾尔语词性标注研究,"帕提古力·依马木,买合木提·买买提,吐尔根·依布拉音,卡哈尔江·阿比的热西提","维吾尔语自动标注是维吾尔语信息处理后续句法分析、语义分析及篇章分析必不可少的基础工作。词性是词的重要的语法信息,假如一个词的词性无法确定或一个词给予错误的词性,对后续句法分析造成直接的影响。本文使用感知器训练算法和viterbi算法对维吾尔语进行词性标注,并在词性标注时利用词的上下文信息作为特征。实验结果表明,该方法对维吾尔语词性标注有良好的效果。","词性标注,感知器算法,维吾尔语词性标注"
2014-06-02,现代维吾尔语常用词统计关键技术研究,"艾孜尔古丽,努尔艾合买提,玉素甫·艾白都拉","本文研究了构建现代维吾尔语语料库的关键技术与方法,特别是现代维吾尔语语料库的构建,并对现代维吾尔语语料预处理技术,现代维吾尔语语料统计技术,现代维吾尔语词干提取技术,现代维吾尔语数据分析技术进行了研究;研制了现代维吾尔语常用词候选表, 从词语的使用频度和词语的分布两方面对词语进行了基本考察,将维吾尔语词语的“词种数、频次、频率、文本数、词长”作为常用词候选表的依据。","现代维吾尔语,语料库,常用词候选表,计量分析"
2014-06-05,基于规则的越南语命名实体识别研究,"闫丹辉,毕玉德","命名实体识别是信息抽取的重要研究内容,主要包括对组织机构名、地名和人名的自动识别。针对英语和汉语的命名实体识别研究开始较早,主要采用基于规则和基于统计的方法进行识别,但目前国内还少有针对越南语命名实体识别的研究。该文分析了越南语命名实体的语言学特点,对其分类并进行了形式化表达,提出了一种基于规则的越南语命名实体识别方法,实验结果显示,该方法能够达到较高的识别准确率。","命名实体识别,越南语,规则"
2014-06-10,基于跨场景推理的事件关系检测方法,"杨雪蓉,洪 宇,陈亚东,王潇斌,姚建民,朱巧明","事件关系检测是一项面向事件之间逻辑关系的自然语言处理技术。事件关系识别的核心任务是以事件为基本语义单元,通过分析事件的篇章结构信息及语义特征,实现事件逻辑关系的深层检测。该文首次建立一套事件关系检测的任务和研究体系,包括任务定义、关系体系划分、语料采集与标注、评价方法等。同时,该文提出了一种跨场景推理的事件关系检测方法,该方法认为,具有相同事件场景的“事件对”,往往具有相同的事件关系类型。该文提出的基于跨场景推理的事件关系检测方法在针对四大类事件关系类型的检测精确率为54.21%。","事件关系,框架语义,事件场景向量,事件场景"
2014-06-15,添加冒号和分号分类标签特征的汉语逗号分类,"李艳翠,谷晶晶,周国栋","标点分析在句子和篇章分析中有重要作用,其中逗号的功能分类是标点分析的重点和难点。该文研究添加冒号和分号分类标签为特征的逗号自动分类。首先给出逗号、冒号和分号的分类方法;然后介绍基于此分类方法的逗号、冒号和分号标点分类语料库;最后分别考察添加冒号类别标签、分号类别标签以及同时添加冒号和分号类别标签为特征的逗号分类结果。实验结果表明,三种情况下的逗号分类正确率均有不同程度的提高。","逗号分类,冒号标签,分号标签,篇章分析"
2012-07-19,开放式信息抽取研究进展,"杨 博,蔡东风,杨 华","从大规模非结构化文本中自动地抽取有用信息是自然语言处理和人工智能的一个重要目标。开放式信息抽取在高效挖掘网络文本信息方面已成为必然趋势,按关系参数可分为二元、多元实体关系抽取,该文按此路线对典型方法的现状和存在问题进行分析与总结。目前多数开放式实体关系抽取仍是浅层语义处理,对隐含关系抽取很少涉及。采用马尔科夫逻辑、本体结构推理等联合推理方法可综合多种特征,有效推断细微完整信息,为深入理解文本打开新局面。","开放式信息抽取, 联合推理, 文本理解"
2012-04-12,焦点的韵律表达及认知加工研究综览,"厚露莹,贾 媛","焦点是语言学界广泛关注的问题。随着实验语音学与心理语言学的发展,国内外对焦点的韵律表达及认知加工方面的研究发展迅速,主要涉及焦点的语音与音系表征、焦点与重音的对应关系,以及句子理解中焦点加工与韵律加工的大脑机制等问题。该文从这一角度对相关研究进行回顾与总结,介绍该领域的发展状况及主要研究方向并提出见解和评论,以期对今后的研究有所启发。","焦点,重音,韵律加工,事件相关电位"
2013-01-20,社会媒体短文本内容的语义概念关联和扩展,"肖永磊,刘盛华,刘 悦,程学旗,赵文静,任 彦,王宇平","随着微博、照片分享等社会化媒体的快速发展,每天产生了大量的短文本内容如评论、微博等,对其进行深入挖掘有重大的应用价值和学术意义。该文选取微博作为例子,详细阐述我们提出的方法。微博信息流因其简短和实时的特性而具有非常大的价值,已经成为市场营销,股票预测、舆情监控等应用的重要信息源。尽管如此,微博内容特征极其稀疏、上下文语境提取困难,使得微博信息的挖掘面临着很大挑战。因此,我们提出一种基于Wikipedia的微博语义概念扩展方法,通过自动识别那些与微博信息语义相关的Wikipedia概念来丰富它的内容特征,从而有效提高微博信息数据挖掘和分析的效果。该文工作首先通过可链接性剪枝、概念关联和消歧,发现微博信息中重要的n-gram所对应的Wikipedia概念;其次,采用基于概念-文档关联矩阵的NMF分解(非负矩阵分解)方法获取Wikipedia概念之间的语义近邻,为微博信息扩展相关的语义概念。基于TREC 2011的微博数据集和Wikipedia 2011数据集进行实验,与已有两个相关研究工作比较,该文提出的方法取得了较好的效果。","短文本,概念,非负矩阵分解,锚文本,语义相似度,概念消歧,Wikipedia"
2012-03-30,基于多层协同纠错的中文层次句法分析,"蒋志鹏,关 毅,董喜双","层次句法分析是一种简单快速的完全句法分析方法,该方法将句法分析分解为词性标注、组块分析和构建句法树三个阶段。该文将其中的组块分析细分为基本块分析和复杂块分析,利用条件随机域模型代替最大熵模型进行序列化标注。由于层次句分析中错误累积问题尤为严重,该文提出了一种简单可行的错误预判及协同纠错算法,跟踪本层预判的错误标注结果进入下一层,利用两层预测分数相结合的方式协同纠错。实验结果表明,加入纠错方法后,层次句法分析在保证解析速度的同时,获得了与主流中文句法分析器相当的解析精度。","层次句法分析,条件随机域模型,组块分析,多层协同纠错"
2013-05-26,基于文本和视觉特征融合的Web动画素材标注,"邱兆文,吴 瑕,陈海燕","为了提高对Web动画素材的组织、管理,该文提出了基于文本特征和视觉特征融合的Web动画素材标注算法。首先利用自动提取的Web动画素材上下文信息,结合Web动画素材名称、页面主题、URL以及ALT等属性组成特征集,提取出文本关键字;然后利用视觉与标注字之间的相关性,对自动提取的标注字进行过滤,实现Web动画素材的自动标注。实验表明该文提出的基于文本特征和视觉特征融合的Web动画素材标注算法可有效地应用于Web动画素材自动标注。","Web动画素材,文本特征,视觉特征,语义标注"
2013-09-10,基于句式结构的高效语法图解标注系统,"杨天心,彭炜明,宋继华","为支持基于句式结构的大规模树库建设与研究,该文设计了人机结合的可视化语法图解标注系统,通过句式结构的框架约束和词汇知识库的底层支持有效规范了标注结果的结构层次和词性标记,在一定程度上保证了树库标注的一致性和高效率。该文从实践角度介绍了基于句式结构的语法图解标注系统在辅助构建大规模汉语树库中的操作模式和功能。","树库,句本位语法,句式结构,图解标注"
2012-04-02,一种迭代式的概念属性名称自动获取方法,"汪平仄,曹存根,王 石","属性是一种用于描述概念和鉴别概念的特殊知识。属性名称是表示属性的专有名词。该文提出了一种基于前后缀迭代的方法,从Web网页中获取概念的属性名称。该方法的每一次迭代分为两个阶段: (1) 从现有种子属性集中选择合适的前后缀,构造词汇-句法模式,从Web网页中提取候选属性;(2) 采用基于相似性的验证模型对候选属性进行验证,以扩充现有属性集合。该文提出了一组验证模型对候选属性进行验证,比较各个模型的优缺点,并在地域类和商业主体类概念上分别得到了平均92.9%和90.7%的准确率,以及对原有种子属性集合近100倍的扩充率。","概念,属性,属性前缀,属性后缀,属性元,知识获取"
2013-10-24,完全加权正负关联规则挖掘及其在教育数据中的应用,"余 如,朱朝阳,黄名选","完全加权数据模型的特点是其项目权值分布在各个事务记录中,随着事务记录的不同而变化。现有的加权负关联规则挖掘算法不能适用于完全加权数据模型。该文提出一种新颖的基于概率比和兴趣度的完全加权正负关联规则的挖掘算法,探讨了算法在教育信息化数据中的应用。算法以概率比代替传统的置信度,采用支持度-概率比-兴趣度架构衡量完全加权正负关联规则,获得很好的挖掘效果。以真实的教育数据和文本数据为实验测试集,与现有正负关联规则挖掘算法比较,该文提出的算法更有效、更合理,具有较高的理论价值和应用前景。","概率比,兴趣度,完全加权关联规则,文本挖掘"
2013-02-08,维基百科中争议性文章的发现方法研究,"常天舒,林鸿飞","维基百科收录的文章和参与编辑的用户日益增多,其中不乏一些用户对同一条目持有不同的见解。该文旨在发现维基百科中的争议性文章,通过维基百科提供的历史信息,在传统的挖掘方法基础上,对具有特殊属性的用户角色进行总结并融合到排序模型中,探讨这类用户对争议性文章挖掘的作用。在 16-745篇文章组成的数据集上进行了实验,除传统的PRF和NDCG评价外,该文给出了更直观的排序结果,与其他基准模型相比有较大的提升。","维基百科,争议度排序,社会网络分析"
2013-02-06,基于带汇点流形的面向属性抽取式观点摘要,"徐学可,谭松波,刘 悦,程学旗","该文研究面向在线顾客点评的面向属性抽取式观点摘要问题。传统方法主要考虑如何抽取属性相关观点,该文提出进一步考虑观点的富含信息(informativeness)、重要性(salience)及多样性 (diversity)这三方面要求。该文提出了一个基于带汇点的流形排序的一体化的摘要抽取模型,在一体化的流形排序过程中同时考虑三方面要求。 在餐馆点评数据上的实验表明了所提出三方面要求的合理性及摘要抽取模型的有效性。","在线顾客点评,面向属性抽取式观点摘要,带汇点的流形排序,属性观点联合模型。"
2012-04-08,基于句法特征的评价对象抽取方法研究,"戴 敏,王荣洋,李寿山,朱 珠,周国栋","评价对象抽取是情感分析任务中一个重要的子任务。该文使用基于条件随机场模型的监督学习方法实现英文的评价对象抽取。为了更好的捕捉评价对象和情感词之间的关系,引入句法分析用以加入丰富的句法特征提高评价对象抽取性能。实验中,我们在两个不同的数据集上考查了句法特征对评价对象抽取性能的影响,并做了详细的分析比较。实验结果表明,将句法特征应用在评价对象抽取任务中能够取得不错的效果,明显提高了评价对象的抽取召回率。","情感分析,评价对象,句法特征,条件随机场"
2013-01-10,一种基于类别先验信息的问题检索语言模型,"吉宗诚,王 斌","社区问答系统已经积累了大量的以层次类别结构进行组织的问题答案对。为了能够重用这些非常宝贵的历史问题答案对资源,设计出一个非常有效的问题检索模型至关重要。在该文中,我们在语言模型建模的框架下提出了一种新的基于问题类别先验信息的方法来提高相似问题检索的性能。特别地,我们将叶子类别语言模型看作是Dirichlet超参来对一元语言模型的参数进行加权,从而提出了一种新的基于类别先验信息的语言模型。该方法具有严格的数学推导依据。在来源于Yahoo! Answers的真实的大量数据集上做了实验比较和分析,实验结果表明我们提出的方法比之前简单的线性插值的方法具有非常显著的性能提升。","社区问答,问题检索,类别,类别先验信息,语言模型"
2013-03-13,基于词语关联度的查询缩略,"陈炜鹏,付瑞吉,胡 熠,秦 兵,刘 挺","冗长查询指用户提交的句子成份复杂的查询。当前的搜索引擎对于关键字的检索取得了较好的结果。但是对于冗长的查询,如果将所有词作为关键字进行检索,往往只能返回相当有限的结果。我们尝试利用关键词之间的词语关联度,发现语义蕴含,删除“信息量”小的关键词,提高检索的效果。对于实验结果,我们分别从“面向机器”和“面向用户”两个角度进行评价。在“面向机器”的评价部分,我们根据搜索引擎返回结果的标红率和结果数进行自动评价;在“面向用户”的评价部分,我们对搜索结果文档进行人工评价。实验结果表明,我们的方法能够明显提高检索结果的数量和质量。","查询缩略,词语关联度,评价方式"
2013-01-20,交互式问答系统中待消解项的识别方法研究,"张 超,孔 芳,周国栋","交互式问答系统能够与用户进行对话式交互进而处理用户提出的一系列问题。交互式问答技术是近些年来问答技术的一个热门方向。该文首次深入研究交互式问答中待消解项的识别方法。根据语料统计了交互式问答中待消解项的分布情况并进行相关实验,运用前人研究的启发式规则与平面特征相结合的方法在交互式问答中测试识别待消解项的性能。结合交互式问答的特点提出了专有名词的两个基于交互式问答特点的特征,并在TREC QA问题集语料中进行相关实验。实验结果表明,代词、有定名词用已有的方法识别效果较好,在加入本文提出的新特征后,在专有名词上也取得了较好的效果。","交互式问答,待消解项识别,指代消解"
2012-12-20,基于标签混合语义空间的音乐推荐方法研究,"闫 俊,刘文飞,林鸿飞","目前,音乐越来越受到人们的青睐。如何为用户推荐符合用户需求的歌曲成为了很多音乐网站、电台以及其他相关音乐媒介关心的话题。针对这个问题,该文选取了社会化标签作为推荐方法的主要依据,首先将其分别映射到流派、情感和上下文信息三个语义空间中,然后在三个空间分别计算用户和歌曲的相似度,最后通过不同方法将三个空间的相似度进行融合从而对用户进行歌曲推荐。实验表明,融合不同空间相似度的推荐方法得到了很好的效果。","音乐推荐,社会化标签,语义空间"
2013-01-18,基于多特征微博话题情感倾向性判定算法研究,"刘全超,黄河燕,冯 冲","社交网络舆情分析是一种新的研究趋势,而其中微博话题的情感倾向性判定是社交网络舆情分析中的热点。针对微博内容特征以及微博间转发、评论关系特征,构建情感分析用词典、网络用语词典以及表情符号库,设计基于短语路径的微博话题情感倾向性判定算法,以及基于多特征的微博话题情感倾向性判定算法,并进一步利用微博间的转发和评论关系对基于多特征的微博话题情感倾向性判定算法进行优化,其微平均正确率与F值分别达到85.3%和79.4%。","微博, 微博话题, 情感分析, 观点分析, 情感倾向性"
2012-06-25,藏语自动分词中的几个关键问题的研究,"完么扎西,尼玛扎西","在分析现有的藏语自动分词方法基础上,该文通过分析藏文构词规则、句法结构、词的前后词性关系、后加字的添接法和格助词的用法等来重点研究了未登录词、紧缩词和交集型歧义的识别及处理方法,并提出了“重组法”,“排除—还原法”和“词性规则法”三种方法。经测试,在文学类、诗歌类、医学类和新闻类等大小为1M的藏语语料中未登录词、紧缩词和交集型歧义的识别准确率分别达到99.84%、99.95%和92.02%。","未登录词,紧缩词,交集型歧义"
2013-01-10,语义词特征提取及其在维吾尔文文本分类中的应用,"吐尔地·托合提,艾克白尔·帕塔尔,艾斯卡尔·艾木都拉","基于机器学习的文本分类中,维吾尔文传统分词方法表现出非常明显的不足和局限性。该文使用另外一种维吾尔文自动分词方法dme-TS。dme-TS中,不再以词间空格作为切分标记提取词特征,而是用一种组合统计量(dme)来度量文本中相邻单词之间的关联程度,并以dme度量的弱关联的词间位置作为切分点,提取对学习算法真正有意义的语义词特征。实验结果表明,用dme-TS提取文本特征可以降低特征空间的维度,同时也能有效的提高传统以单词为特征的分类算法的性能。","维吾尔文分词,词特征,dme-TS,语义词特征,文本分类"
2012-12-18,书法字库的设计实现与管理,邱明锋,"通过对书法字库现状分析,阐述了书法字模创作、纸质书法字模二值化处理,数字书法图像切分、曲线轮廓描述与编码的过程;应用OpenType字形技术的开源性来设计书法字库的特殊性和添加连笔、行气、章法、风格等脚本;利用字库编辑软件与脚本工具生成OpenType书法字库。提出了解决当前网页、手机浏览缺失书法字体的思路,是设计出与世界文字信息交流相匹配的书法字库,并实现其在操作系统中的管理,使中国传统的书法文化能在电子信息交流中更好地得到传承和发展。","书法信息传播,图像二值化,字形技术,书法字库,字体管理"
2013-04-15,论贵州古彝文编码字符集构建,"吴 勰,禄玉萍,王明贵","依托彝文古籍文献开展古彝文字符整理和规范研究,建立古彝文编码字符集有很高的要求和极大的工作量。这项工作有助于实现古彝文规范化应用,为古彝文信息技术开发提供基础保障。实现这一构想: 需要最大限度地搜集整理古彝文字符,广泛听取彝文专家建议和意见,经过充分的科学论证,对搜集的古彝文字符进行甄别、查重、筛选和择定,剔除大量古彝文异体字形,在此基础上规范古彝文的字量、字形、字音和字序,实现计算机技术处理古彝文字符信息的规范化。","古彝文,编码字符集,彝文信息处理"
2012-11-29,基于规则的汉语名名组合的自动释义研究,"魏 雪,袁毓林","该文以现代汉语(特别是网络搜索词)中的名名组合为主要研究对象,探索一种基于规则的汉语名名组合的自动释义方法。其研究步骤为: (1)利用《现代汉语语义词典》中名词的语义类别,来建立名名组合的语义类组合模式;(2)在“生成词库论”中物性角色思想的指导下,用名名组合中某个名词的施成角色或功能角色作为释义动词,来揭示这两个名词之间的语义关系;(3)以语义类组合模式为单位构建名名组合的释义模板,并汇集成名名搭配数据库;(4)利用《知网》资源,来获取具体名词的施成角色和功能角色,建立汉语名词知识库。在这两个数据库的基础上,我们初步实现了一个汉语名名组合的自动释义程序。","名名组合,自动释义,释义动词,语义类,释义模板,施成/功能角色"
2013-06-11,衔接性驱动的篇章一致性建模研究,"徐 凡,朱巧明,周国栋,王明文","该文系统地探索了衔接性理论对篇章一致性建模的作用。不同于目前有监督的基于实体和篇章关系网格的模型,该文提出的无监督模型揭示了系统功能语法中主位—述位结构理论对于篇章一致性建模的重要性,同时显示了基于主位和指代消解两种过滤机制对于篇章一致性建模的适用性。在三种不同文体的国际基准语料上进行的句子排序和文本摘要一致性检测任务实验表明主位—述位结构和指代消解信息能使篇章一致性检测准确率得到显著提升。","篇章衔接性,篇章一致性,主位—述位结构,指代消解"
2013-04-08,动词引出新支话题的语用功能研究,"季 翠,卢达威,宋 柔","汉语是一种话题显著的语言。汉语篇章中,同一话题会多次延续,也可能发生话题转换。该文讨论一种话题转换现象: 原话题的说明中的某个成分成为新话题,但该新话题及其说明并不构成原话题的说明或原话题说明的一部分。这种话题可称为新支话题。该文对动词按照词汇语义进行分类,揭示动词将其宾语引出成为新支话题的能力所在。文章给出了《围城》中动词引出新支话题的全部实例的词汇语义分布统计。","新支话题,动词,分类体系"
2013-01-20,英语情态句的情感倾向性分析,"陈仲帅,刘 洋,禹晓辉","该文研究了英语情态句的情感倾向性分析问题。情态句是英语中的常用句型,在用户评论文本中占有很大的比例。由于其独有的语言学特点,情态句中的情感倾向很难被已有的方法有效地分析。在该文中,我们借助词性标签进行了情态句的识别,并提出了一种情态特征用于帮助情态句情感倾向性的分析。为了进一步提高分析效果,我们还给出了通过合并同义情态特征来缓解情态特征稀疏性问题的方法。实验结果表明,在二元及三元情感倾向性分类问题上,该文提出的方法在F值上较经典分类方法分别有4%及7%的提高。","情感倾向性分析,意见挖掘,情感分类,情态句"
2012-12-12,汉语框架网中未登录词元的框架选择,"陈学丽,李 茹,王 赛,王智强","汉语框架网的低覆盖率导致汉语句子中存在许多未登录的词元,严重制约着汉语的框架语义分析任务。针对未登录词元的框架识别问题,该文借助同义词词林的词义信息,提出基于平均语义相似度计算及最大熵模型两种方法,采用静态特征与动态特征相结合的特征选择方法。实验证明,这两种方法都能有效地实现未登录词元的框架选择,基于相似度计算的方法(TOP-4)获得78.61%的准确率;基于最大熵的方法结果可达87.29%,同时在新闻语料上达到了75%的准确率。","汉语框架网,未登录词元,词义信息"
2013-10-22,微博信息传播网络的结构属性分析,"王晓明,王 莉,杨敬宗","当前微博迅速流行,由于它交互结构的复杂性,其研究分析难度较大,该文提出了一种新颖的方法分析微博信息传播网络的属性。首先定义了信息源的概念,针对6个不同主题事件的微博传播结构,对各信息传播网络结构进行了可视化分析,并给出了信息源分布特征分析。带有时间标签的信息传播网络通常是有向非循环图,定义了3种信息传播微元结构,分别对应信息分散、信息聚集、信息传递。利用斯皮尔曼等级相关系数研究了它们之间的关联度,发现3种结构间有相当大的差异,基于这3种关系分析了信息传播网络的演变情况,得出信息分散结构在各时间片上的数量最多。","信息传播网络,网络社区,微元结构,网络可视化"
2013-03-28,中文微博客的垃圾用户检测,"李赫元,俞晓明,刘 悦,程学旗,程 工","微博客的出现改变了我们获取信息的方式。然而,大量垃圾消息却此起彼伏,危害着微博的健康发展。该文研究了中文微博客中的垃圾用户检测问题。我们首先对垃圾用户的行为进行了分析,提出了基于用户图、用户资料、微博内容的3大类7种检测特征。随后,讨论了基于SVM分类器的垃圾用户检测方法。最后,我们对采集的微博数据进行了标注,并评价了分类器的效果。实验表明: 分类器具有较高的准确率和召回率,该文提出的特征具有较好的区分度。","微博客,垃圾用户,检测"
2013-03-17,基于传播模拟的消息流行度预测,"万圣贤,郭嘉丰,兰艳艳,程学旗","社交网络中的消息流行度预测问题对于信息推荐和病毒式营销等应用具有重要意义。该文提出了一种基于传播模拟的消息流行度预测方法,首先使用最大熵模型学习并预测用户转发消息的概率,然后使用独立级联传播模型在真实的社会网络上模拟消息的传播过程,从而完成消息流行度的预测。该方法的优点在于更充分的利用了社会网络的结构和用户特征信息。该文在Twitter数据集上的实验结果表明,相对于基准方法,该文提出的方法具有更高的准确率和稳定性。","流行度预测,传播模型,最大熵模型"
2013-03-15,长尾查询搜索性能评价方法的研究,"霍 帅,张 敏,刘奕群,马少平,金奕江,茹立云","各大搜索引擎公司都致力于准确而快速的帮助用户找到信息目标,搜索性能评价变得非常重要,而目前尚无对长尾查询性能评价的方法。该文通过分析长尾查询结果数据,提取了长尾查询三种类型特征,并对特征进行叠加分析。进一步地针对数据集的严重不平衡问题提出两种数据平衡方法。最后提出并改进了长尾查询评价方法。在真实搜索引擎结果数据集上的实验验证了所提出的评价方法取得一定的评价效果,其中对不相关文档的评价取得较高的准确率。","长尾查询,搜索引擎性能评价,自动评价方法"
2012-02-24,机器翻译自动评价综述,"李良友,贡正仙,周国栋","随着机器翻译的发展,对其质量进行评测的自动评价方法也越来越受重视。发展至今,各种评价方法与技术层出不穷,采用何种分类标准来组织和描述它们也是一个很大的挑战。根据核心技术的不同,该文重点介绍了三类主流的自动评价方法,包括: 基于语言学检测点的方法、字符串匹配的方法和基于机器学习的方法。论文分别阐述了这些类别中颇具代表性的方法的工作原理并分析了各自的优缺点。此外,受限参考译文下的评价技术虽然不是主流的方法,但是其对提高自动化程度和评价性能的作用不能忽视,所以该文将其作为特殊的类别做了阐述。然后,汇报了近年来衡量自动评价方法的国际评测结果。最后,总结了自动评价的发展趋势和有待进一步解决的相关问题。","机器翻译,自动评价,自动评价分类"
2013-05-02,TSRM藏文拼写检查算法,"珠 杰,  李天瑞,刘胜久","拼写检查作为文本处理中的重要内容,在字处理软件、文字识别、语音识别、搜索引擎等领域具有广泛的应用。该文以藏文语音特性建立的字组织法为依据,以藏文音节规则为模型,提出了藏文音节规则模型(TSRM)的藏文音节拼写检查算法,并通过2组实验验证了算法的有效性。在没有考虑梵音转写藏文的情况下,拼写错误检查的准确率可以达到99.8%。","藏文音节,藏文规则,拼写检查"
2012-10-19,基于FUG的藏语句法形式化描述,"扎西加,多 拉","针对藏语自然语言形式化的实际需求,分析了用复杂特征描述藏语句子的必要性,引入了复杂特征集和合一运算的概念。以形式化为出发点,以现代语言学理论为后盾,以实例举证的方式对藏语词汇、句法、语义的规则及句子合一运算提出了探索性的研究思路,并且采用框式表示的方法,力求从形式化的角度为藏语自然语言处理提供便利。","藏语句法,复杂特征集,句子结构,语义信息"
2012-04-25,藏文构件元素识别算法研究,"边巴旺堆,卓 嘎,陈延利,武 强","要实现藏文排序算法,必须解决组成藏文音节的构件元素识别,然后由构件元素的优先级进行排序。本文通过对藏文的文字结构、书写规律以及文法规则的研究,设计了符合现代藏文的构件元素识别算法。在该算法中对藏文特殊音节的二义性、双元音和缩写等问题进行了处理。实验表明该算法能够满足实际藏文构件元素识别的需要。另外,为了在国家编码标准下输入的藏文词语也能利用本算法正确识别其构件元素,在算法中做了相应处理。","藏文,算法,语法规则,构件元素"
2012-10-10,一种维吾尔语联机手写识别系统,"热依曼·吐尔逊,吾守尔·斯拉木","该文介绍一种维吾尔语联机手写体识别系统。其针对维吾尔语词语的书写特点采用了基于多分类器融合的系统和方法,分别使用混合高斯模型模拟整词的静态特征和隐马尔科夫模型模拟书写笔迹的动态特征,有效地提升了识别系统的准确率。在第一期实验中,整词识别率达到97%;第二期的实验中,整词识别率达到99%。","维吾尔文,手写体,词语建模,HMM,GMM"
2012-04-17,基于能量变化率的汉语塞音检测算法,"张连海,陈 斌,屈 丹,李弼程","针对爆发谱特征不稳定的问题,论文提出了一种基于能量变化率的汉语塞音检测方法。该方法首先基于Seneff听觉谱提取了一组描述音段能量变化率特性的参数,然后采用Fisherface方法进行特征变换,变换后的特征采用K近邻(KNN)分类器进行分类,实现了塞音的检测,最后利用留一法对模型性能进行交叉验证。实验结果表明,干净语音塞音检测准确率可以达到96.39%,信噪比10dB的语音塞音检测准确率可达到88.07%,模型具有较好的稳定性和泛化性能。","塞音检测,能量变化率,发音特性,Seneff听觉模型"
2012-05-16,语音声学参数自动标注/提取系统简介,"周学文,呼 和","该文重点介绍了一个实用的语音声学参数自动标注/提取软件系统。使用该系统能够极大地降低语音参数标注和采集的错误率,有效提高语音声学参数库研制效率,确保实验方法和实验数据的可重复性和可验证性,从而推动语音声学参数数据库研制和语音声学实验研究工作的规范化和标准化。","声学参数,自动标注,自动提取"
2012-01-05,话题转换方式和句子长度对边界声学参数的影响,"吴 倩,王 蓓","该文研究了不同话题转换方式和句子长度对边界处停顿、边界前延长量及音高重置的影响。语料是由两个句子构成的小语篇,通过改变第二个句子控制两种句子长度(短和长)和三种话题转换方式(延续、精述和转折)。20位发音人的语音分析结果显示: (1) 话题转换方式和句子长度对停顿及音高重置都有调节作用,但对边界前词的时长延长量没有显著影响。另外,两因素间没有交互作用。主要表现为: 边界后句子越长,句间停顿越长,且边界处的音高重置越大。从话题延续、话题精述到话题转折,停顿时长呈增长趋势,且音高重置度增大; (2) 停顿时长与边界前延长量存在较弱的负相关,与音高重置则存在较弱的正相关; (3) 相较于男性发音人,女性发音人对话题转换方式更为敏感,且更倾向于用停顿和音高两种声学线索标记话题转换方式。句长效应则在男女发音人中都稳定存在。以上结果表明,句长对边界处声学参数的影响基于底层发音机制,而话题转换方式的影响则是语言中信息传递的需要。","话题转换方式,句子长度,韵律边界"
2012-11-17,基于语义分类的比较句识别与比较要素抽取研究,"周红照,侯明午,侯 敏,滕永林","比较是人们常用的评估不同事物优劣、异同的表达方式,利用机器识别比较句并进一步抽取比较要素是语言信息处理领域一项新颖又有实用价值的课题。该文依据比较句与比较要素之间是一种“你中有我,我中有你”的共生关系,将比较句识别与比较要素抽取两个任务合二为一完成;根据词意分类,构建由领域词典、情感词典、标记词典、普通词典构成的词典系统;根据汉语比较句句义分类,构建比较句识别与比较要素抽取规则库。以第四届中文倾向性评测(COAE2012)发布的测试语料为实验对象,该系统取得了较好的实验(评测)结果。","语义分类,词典与规则,比较句识别,比较要素抽取"
2013-04-15,面向动态主题数的话题演化分析,"方 莹,黄河燕,辛 欣,魏骁驰,庄 琨","话题演化用于自动分析话题变化趋势,具有较高的应用和研究价值。ILDA(Infinite Latent Dirichlet Allocation)模型在LDA(Latent Dirichlet Allocation)模型的基础上增加了狄利克雷过程,除了能获取隐变量,更重要的是能完成超参的动态更新和主题数的变动。而已有的话题演化研究中,话题的主题数需要事先指定且无法变动,基于ILDA模型的方法则可以针对性地解决该问题。构建的话题演化分析系统可实现如下功能:各周期内按不同主题分类、相邻周期间的主题进行关联、按时间顺序计算子话题强度。实验显示,基于ILDA模型的参数动态更新符合实际需求,话题演化分析过程完善可行。","主题模型,无参混合模型, 狄利克雷过程,话题演化"
2013-03-16,中文信息处理的词法问题——以句本位语法图解树库构建为背景,"彭炜明,宋继华,俞士汶","该文对比了句本位语法图解树库与中文信息处理现行词法规范在分词单位和词类标注两方面的差异,指出目前自动词法分析与句法分析的若干脱节之处,梳理了图解树库中关于临时造词、惯用语等特殊结构的标注策略和语言学理据,并探讨了“依句辨品”和“指称化”等汉语词类相关理论在中文信息处理中的实现方式。","中文信息处理,临时造词,句本位语法,图解树库"
2012-03-27,基于谓词及句义类型块的汉语句义类型识别,"王 倩,罗森林,韩 磊,潘丽敏","从现代汉语语义学角度,可将句义类型划分为简单句义、复杂句义、复合句义和多重句义4种。作为在整体上对句义结构进行描述的方式之一,句义类型识别是对汉语句子进行完整句义结构分析的重要步骤。该文基于谓词及句义类型块提出了一种汉语句义类型识别的方法,实现了4种句义类型的识别。该方法先通过句中谓词的个数进行初步识别判断出部分简单句,再对剩余的句子先用C4.5机器学习的方法得到句中谓词经过的最大句义类型块的个数,再结合句法结构中顶端句子节点进行判决,最终给出剩余句子的句义类型判定结果。实验采用BFS-CTC汉语标注语料库中10221个句子进行开集测试,句义类型的整体识别准确率达到97.6%,为基于现代汉语语义学的研究奠定了一定的技术研究基础。","句义类型识别,句义类型,语义分析,自然语言处理"
2012-12-13,中文篇章级句间语义关系体系及标注,"张牧宇,秦 兵,刘 挺","篇章句间关系(Discourse Relation)是篇章级语义分析的重要内容,该文在英文篇章句间关系研究的基础上分析了中英文间的差异,总结了中文篇章级语义分析的特点,并在此基础上提出面向中文篇章句间关系的层次化语义关系体系,对句间关系类型进行详细描述。为了验证体系的合理性和完备性,我们在互联网新闻语料上进行了标注实践,分析了标注中遇到的难点并给出解决方案,为进一步的中文篇章级语义分析工作奠定基础。","中文篇章级语义分析,句间关系,语义体系,语料标注"
2012-03-08,基于HNC理论的词语相似度计算,"吴佐衍,王 宇","该文运用自然语言处理的概念层次网络(Hierarchical Network of Concepts,HNC)理论提出了一种词语相似度计算方法。该方法利用HNC理论词汇层面联想的概念表述体系,根据HNC映射符号的编码规则和符号映射理论,综合概念内涵、概念外部特征、概念类别和组合符号来计算词语的相似度,并与基于知网的词语相似度算法和人工的主观判断的相似度进行了比较分析。实验结果表明,该方法能够较好地反映词语之间的语义差别,与人的直观判断基本一致,是一种有效可行的方法。","概念层次网络,语义相似度,中文信息处理"
2012-03-15,依存树到串模型中引入双语短语的三种方法,"谢 军,刘 群","依存树到串模型使用基于HDR片段的翻译规则。HDR片段是由中心词及其所有依存节点组成的树片段。这种翻译规则可以较好地捕捉语言中的句子模式和短语模式等组合现象,但在捕捉非组合现象(如习惯用语或固定搭配)方面存在不足。这类非组合现象易于由短语捕捉。为了更好地改善依存树到串模型的性能,本文提出了三种引入双语短语的方法,分别为引入句法短语、引入泛化句法短语及引入非句法短语。实验结果表明,同时使用句法短语、泛化句法短语及非句法短语时,可以将依存树到串模型的性能显著提高约1.0 BLEU值。","统计机器翻译,依存树到串模型,泛化句法短语,非句法短语"
2012-03-14,一种基于改进隐马尔克夫模型的词语对齐方法,"刘 颖,姜 巍","该文在基本隐马尔克夫模型的基础之上,利用句法知识来改进词语对齐,把英语的短语结构树距离和基本隐马尔克夫模型相结合进行词语对齐。与基本隐马尔克夫模型相比,这个模型可以降低词语对齐的错误率,并且提高统计机器翻译系统BLEU值,从而提高机器翻译质量。","短语结构树距离, 隐马尔克夫模型, 词语对齐, BLEU值"
2012-02-20,基于判别式分类和重排序技术的藏文分词,"孙 萌,华却才让,才智杰,姜文斌,吕雅娟,刘 群","本文提出一种基于判别式模型的藏文分词方法,重点研究最小构词粒度和分词结果重排序对藏文分词效果的影响。在构词粒度方面,分别考察了以基本字丁、基本字丁－音节点、音节为最小构词粒度对分词效果的影响,实验结果表明选定音节为最小构词粒度分词的F值最高,为91.21%;在分词结果重排序方面,提出一种基于词图的最短路径重排序策略,将判别式解码生成的切分结果压缩为加权有向图,图中节点表示音节间隔,而边所覆盖的音节作为候选切分并赋予不同权重,选择一条最短路径从而实现整句切分,最终分词结果的F值达到96.25%。","判别式,藏文分词,构词粒度,重排序"
2012-03-06,基于词典和统计相结合的维吾尔语拼写检查方法,"麦合甫热提,艾山·吾买尔,麦热哈巴·艾力,吐尔根·伊布拉音,张 健","该文通过研究国内外相关的拼写错误查错和纠错方法的理论,再结合维吾尔语自身的特点,提出了基于词典和统计相结合的维吾尔语拼写查错方法。首先,提出基于词典的方法进行词库和词干提取的拼写检查;其次,提出基于N元语法的词缀连接有效性判断模型,对未登录词提出基于N元语法的拼写检查模型;最后,结合以上几种方法各自的优点提出基于混合策略的拼写检查方法,该方法在准确性和检查结果可靠性等方面得到了较显著的提高。","维吾尔语,拼写检查,词典,N元语法"
2012-12-20,基于特征耦合泛化的药名实体识别,"何林娜,杨志豪,林鸿飞,李彦鹏,唐利娟","药名识别的直接目的是从生物医学文本中寻找药名。目前,药物相关研究不断出现,远远超出了维护人员更新药物信息数据库的速度,这就迫切需要一种自动提取药物信息的技术。该文采用了一种基于特征耦合泛化(FCG)的半监督学习方法生成药名词典,然后将药名词典和条件随机场结合进行药名实体识别。首先我们用模板的方法构造了一个药名词典,然后用FCG方法对词典去噪,最后将去噪后的词典用在测试集上进行药名实体识别,得到了76.73%的F值。","药名识别,机器学习,特征耦合泛化,CRF"
2012-12-01,多语种网络文本快速新词抽取,"刘冰洋,刘 倩,张 瑾,刘欣然,程学旗","从网络文本中提取新词是网络信息处理中的一个重要问题,在信息检索、文本挖掘、词典编纂、中文分词等领域中都有重要应用。本文提出了一种与语言无关的快速新词提取算法,首先针对后缀树的数据结构将多语言文本进行统一编码,然后使用改进的统计方法在双后缀树上以线性时间统计重复串与邻接类别,并计算字符串的整体度,同时通过剪枝大幅度减少计算量,在中、英文语料上较好地实现了新词的抽取及排序。","新词, 邻接类别, 字符串整体度, 后缀树, 多语言"
2012-12-01,基于PageRank的中文多文档文本情感摘要,"林莉媛,王中卿,李寿山,周国栋","文本情感摘要任务旨在对带有情感的文本数据进行浓缩、提炼进而产生文本所表达的关于情感意见的摘要。该文主要研究基于多文档的文本情感摘要问题, 重点针对网络上存在同一个产品的多个评论产生相应的摘要。首先,为了进行关于文本情感摘要的研究,该文收集并标注了一个基于产品评论的中文多文档文本情感摘要语料库。其次,该文提出了一种基于情感信息的PageRank算法框架用于实现多文档文本情感摘要,该算法同时考虑了情感和主题相关两方面的信息。实验结果表明,该文采用的方法和已有的方法相比在ROUGE值上有显著提高。","摘要,情感,多文档"
2012-01-06,《同义词词林》在中文实体关系抽取中的作用,"刘丹丹,彭 成,钱龙华,周国栋","语义信息在命名实体间语义关系抽取中具有重要的作用。该文以《同义词词林》为例,系统全面地研究了词汇语义信息对基于树核函数的中文语义关系抽取的有效性,深入探讨了不同级别的语义信息和一词多义等现象对关系抽取的影响,详细分析了词汇语义信息和实体类型信息之间的冗余性。在ACE2005中文语料库上的关系抽取实验表明,在未知实体类型的前提下,语义信息能显著提高抽取性能;而在已知实体类型的情况下,语义信息也能明显提高某些关系类型的抽取性能,这说明《词林》语义信息和实体类型信息在中文语义关系抽取中具有一定的互补性。","中文实体关系抽取,树核函数,同义词词林,语义信息"
2012-12-01,基于核心词和实体推理的事件关系识别方法,"杨雪蓉,洪 宇,马 彬,姚建民,朱巧明","事件关系识别是一项面向文本信息流进行事件关系判定的自然语言处理技术。事件关系识别的核心任务是以事件为基本语义单元,通过分析事件的篇章结构信息及语义特征,实现事件逻辑关系的浅层检测(即判定任意事件之间是否存在逻辑相关性)。该文通过利用同一话题下事件的核心词及实体的分布特性,针对同一话题下事件关系识别任务,提出一种基于核心词和实体推理的事件关系识别方法。实验结果显示,该文方法明显优于基于事件语义依存线索的事件关系识别方法,F值获得了15.34%的提升。","实体分布,核心词分布,虚拟相关事件,事件关系"
2013-10-20,多特征文本蕴涵识别研究,"赵红燕,刘 鹏,李 茹,王智强","文本蕴涵识别是解决自然语言中存在的同义异形问题的有效途径。虽然国内外学者已经提出了很多文本蕴涵识别模型,但影响文本蕴涵识别的因素错综复杂,识别准确率普遍不高。该文把文本蕴涵识别看作二元分类问题,抽取词汇特征、句法依存关系特征及FrameNet语义知识库特征的多种特征构造特征矩阵,训练SVM分类器,实现文本蕴涵识别。该方法在国际文本蕴涵识别技术评测RTE3的测试集上进行测试,蕴涵正例识别准确率达到了78.1%,高于RTE3评测2-ways的最高结果。","文本蕴含识别, 句法依存关系,FrameNet"
2012-12-05,CICF: 一种基于上下文信息的协同过滤推荐算法,"鲁 凯,张冠元,王 斌","协同过滤能够满足用户的偏好,为用户提供个性化的指导,是当前互联网推荐引擎中的核心技术。然而,该技术的发展面临着严重的用户评分稀疏性问题。用户评分历史中包含着丰富的上下文信息,因此该文通过利用两种上下文信息对评分稀疏性问题进行了有益的探索: 利用物品之间的层次关联关系挖掘用户的潜在喜好;对用户评分的短期时间段效应进行建模。并提出了基于两种上下文信息的统一模型CICF。通过在Yahoo音乐数据集上的实验表明,CICF相比传统协同过滤算法能够显著提高预测效果;并通过在不同稀疏度的训练集上的实验证实了CICF能够有效地缓解评分稀疏性问题。","三协同过滤,上下文信息,隐参数模型"
2012-12-20,LDA-CF: 一种混合协同过滤方法,"廉 涛,马 军,王帅强,崔超然","推荐系统是一种克服信息过载的重要工具,其中最流行的方法是协同过滤。该文提出一种结合潜在因素模型和邻域方法的混合协同过滤方法LDA-CF。我们首先将评分矩阵转换成伪文档集合,使用LDA(Latent Dirichlet Allocation)主题模型发现用户和物品潜在因素向量;然后在低维潜在因素空间计算用户和物品相似度;最后采用邻域方法预测未知评分。在MovieLens 100k数据集上的实验表明: 在评分预测任务中,LDA-CF取得的MAE性能指标优于传统的邻域方法。因此,LDA可以有效地从评分矩阵中发现对计算相似度十分有用的用户和物品低维特征表示,在一定程度上缓解了数据稀疏问题。","推荐系统,协同过滤,主题模型"
2012-03-01,一种基于作者建模的微博检索模型,"李 锐,王 斌","近年来,微博的发展令人瞩目,微博检索已经成为一个重要的研究课题。而微博具有文本内容短、更新快、融合社交网络等特点,这些特点使微博的检索不同于传统的web检索。该文首先分析了传统的向量空间模型、概率模型以及基本的语言模型直接用于微博检索将面临的问题;接着在语言模型框架下提出了利用作者信息对微博内容进行扩展的思想,即利用作者信息重新估计微博的语言模型;然后针对话题模型在短文档训练中存在的问题,提出了使用作者的文档话题模型来进一步扩展微博的内容;最后在TREC公开数据集上进行了实验。实验结果表明,可以通过合理使用作者信息来有效的提高微博检索的效果。","微博,作者模型,微博检索,平滑"
2012-01-09,基于转化的互联网广告技术研究,"顾智宇,秦 涛,王 斌","基于转化的互联网广告方式根据用户在浏览广告后的购买等行为对广告效果进行衡量,极大利用了互联网广告的独特优势,成为了未来互联网广告发展的趋势。该文介绍了基于转化的互联网广告的运行方式,分析了其行业应用,进一步地总结了该领域的当前研究成果,包括基于转化的竞价机制设计、转化率预测、基于转化的广告排序等。最后在此基础上,分析了存在的问题并展望未来的研究方向。","互联网广告,转化率,按行动付费广告"
2013-08-20,基于层叠CRF模型的词结构分析,"方 艳,周国栋","传统的中文分词就是识别出每个词的边界,它忽略了汉语中词与短语分界不清这一特点。在理论上,语言学家对词边界的确定往往各持己见,各语料库的分词标准不能统一,在实践中也不能完全满足具体应用的需求。该文给出了基于层叠CRF模型的词结构自动分析方法,能够以较高的精确度获得词的边界信息和内部结构信息。相比于传统的分词,词的结构分析更加符合汉语词法与句法边界模糊的事实,解决了语料库标准的不一致性以及应用的不同需求。","中文分词,内部结构,分词标准,层叠CRF"
2013-10-06,复句关系词规则生成系统中的冲突检测与处理,"杨进才,谢 芳,王中华,胡金柱","复句中的关系词对研究复句中各分句的语义关系有着重要意义,在基于规则的关系词自动识别中需要大量的规则,并且规则库是动态变化和不断完善的,向规则库中入库规则时会出现规则冲突和入库错误的情况,该文探讨如何在入库时识别产生冲突的规则,并对规则进行相关的处理。对复句的普通规则、连用词规则、普通句式规则、连用句式规则四类规则进行了形式化的表示与存储,在此基础上设计了关系词检测、约束类型检测、约束条件检测、结论检测的检测流程。提出了两种冲突处理方式——优先级方式和有向无环图方式,对两种方法进行了比较。利用该检测方法和有向无环图的处理方式,入库了千余条规则。实验表明,利用该方法冲突规则的检测和处理正确率达到100%。","复句关系词,规则冲突,有向无环图"
2013-08-19,否定与不确定信息抽取研究综述,"邹博伟,周国栋,朱巧明","否定与不确定表达在自然语言中广泛存在,正确识别此类信息并将其与准确信息分开处理,在信息抽取、情感分析、文本挖掘等自然语言处理任务中具有重要研究价值。自从2008年BioScope语料库发布以来,针对否定与不确定信息抽取研究举办了多次大规模评测会议和学术论坛,为采集语料、明确任务及性能评测等提供了交流平台,否定与不确定信息抽取逐渐成为自然语言处理领域的研究热点。该文简要介绍了否定与不确定信息抽取的研究背景、任务定义、相关语料等,并通过回顾和分析该领域的研究现状,展望未来的发展趋势。","否定信息,不确定信息,自然语言处理"
2013-06-26,事件关系检测的任务体系概述,"杨雪蓉,洪 宇,陈亚东,姚建民,朱巧明","事件关系检测是一项面向文本信息流进行事件关系判定的自然语言处理技术。事件关系检测的核心任务是以事件为基本语义单元,通过分析事件之间的语义关联特征,实现事件逻辑关系的识别与判定,包括关系识别(即识别有无逻辑关系)和关系判定(即判定逻辑关系类型,如“因果”关系)。目前,专门面向事件的逻辑关系分析与处理,尚未形成一套完整的研究体系。针对这一问题,该文借助篇章分析、事件抽取和场景理解等相关领域中的概念与数据资源,尝试建立一套事件关系检测的任务和研究体系,包括任务定义、关系体系划分、语料采集与标注、评价方法等。同时,该文着重分析和对比了事件关系检测与篇章关系检测的差异,并给出了事件关系检测任务的难点与挑战。","事件关系检测,篇章分析,事件,论元,语义关系"
2013-07-14,基于评论质量的多文档文本情感摘要,"林莉媛,王中卿,李寿山,周国栋","文本情感摘要任务旨在对带有情感的文本数据进行浓缩、提炼进而产生文本所表达的关于情感意见的摘要,用以帮助用户更好地阅读、理解情感文本的内容。该文主要研究多文档的文本情感摘要问题,重点针对网络上存在的同一个产品的多个评论进行摘要抽取。在情感文本中,情感相关性是一个重要的特点,该文将充分考虑情感信息对文本情感摘要的重要影响。同时,对于评论语料,质量高的评论或者说可信度高的评论可以帮助用户更好的了解评论中所评价的对象。因此,该文将充分考虑评论质量对文本情感摘要的影响。并且为了进行关于文本情感摘要的研究,该文收集并标注了一个基于产品评论的英文多文档文本情感摘要语料库。实验证明,情感信息和评论质量能够帮助多文档文本情感摘要,提高摘要效果。","情感摘要,多文档,评论质量"
2013-09-12,基于微博的情感倾向性分析方法研究,"高 凯,李思雨,阮冬茹,刘邵博,周二亮,乔世权","随着微博等新型社会网络媒体的发展,人们在网络上传播着对各类话题的情感,社会网络也因此成为了挖掘社情民意的有效平台。传统文本分析算法难以适应篇幅短小、内容琐碎且富含情感特征的微博等短文本挖掘的需要。该文提出基于情感单元和评价对象分析的微博情感倾向性分析方法,通过基于词性共现概率计算的情感单元和情感评价对象抽取,计算情感单元的情感度,建立博主个性化及情感倾向性分析模型,完成情感倾向性分析。实验结果及分析验证了上述算法的有效性。","社会网络,短文本挖掘,情感单元,评价对象"
2013-08-22,基于全局变量CRFs模型的微博情感对象识别方法,"郝志峰,杜慎芝,蔡瑞初,温 雯","微博行文具有较大的自由性,其中情感对象识别是一个困难的问题,尤其是情感对象未显性出现情况下的情感对象识别,暂未发现有效解决方法。该文针对这一难题,结合中文微博的特点,提出了一种改进的条件随机场的模型。该模型把情感对象识别看作一个序列标记问题,通过在传统的CRF序列标记模型上增加情感对象的全局节点,有效地结合上下文信息、句法依赖以及情感词典,从而可以识别出微博中的情感对象。该方法的优势在于能够应用于情感对象未显性出现的情况。实验结果表明该方法比现有方法能更有效地识别出微博中的情感对象。","条件随机场,微博,情感对象识别,信息抽取,情感分析"
2013-07-29,基于用户分析的微博用户影响力度量模型,"张绍武,尹 杰,林鸿飞,魏现辉","微博用户影响力作为影响力研究在微博领域的延伸,已逐渐成为一个研究热点。该文在传统影响力度量指标的基础上,结合微博价值、消息传播过程中产生的影响力扩散以及用户的活跃程度,提出了三种新影响力度量方法,包括微博影响力、行为影响力以及活跃度影响力。此外,通过有效融合上述三种新度量方法提出了新的微博用户影响力度量模型。最后,针对不同影响力度量指标,该文对它们的内部关系进行分析,并阐述了影响力度量指标之间关联程度及形成原因。","用户影响力,新浪微博,传播路径"
2013-07-13,面向中文文本的情感信息抽取语料库构建,"戴 敏,朱 珠,李寿山,周国栋","情感信息抽取是情感分析中的一个重要子任务。虽然该任务已经开展有一段时间,但是面向中文文本的情感信息抽取任务研究才刚刚起步。目前中文文本的情感信息抽取面临的首要困难在于现有的相关中文语料库还非常有限。为了更好开展中文文本的情感信息抽取研究,该文重点研究了中文语料标注体系,构建一个规模较大、标注类型丰富的中文情感信息抽取语料库。除了常见语料库标注的情感倾向性、评价对象、情感词等信息外,重点标注了评价对象的省略、无情感词情感句表达及极性转移等情况。由语料信息统计可知,该文所指出的特殊现象(例如,评价对象的省略)在中文情感表达中是非常普遍的,开展这方面的研究很有必要。该文所构建的中文文本语料库将为中文情感信息抽取任务提供语料基础。","情感分析,情感信息抽取,中文语料库"
2013-07-18,特征和实例迁移相融合的跨领域倾向性分析,"孟佳娜,于玉海,赵丹丹,孙世昶","在情感倾向性分析中,经常会发生由于领域知识的变化引起的分类精度下降的问题。为解决此类问题,该文提出了一种基于实例和特征相融合的知识迁移方法,首先通过三部图构建了源领域和目标领域的领域依赖特征词之间的关联,并得到一个公共的语义空间来对原有的向量空间模型进行重建,然后再通过带偏置的马尔科夫模型,建立源领域和目标领域实例之间的关联,从而有效的将源领域学习到的情感倾向性知识迁移到目标领域中,高于其它方法的实验结果验证了算法的有效性。","跨领域倾向性分析,迁移学习,偏置的马尔科夫模型"
2013-07-21,基于多层次语言特征的弱监督评论倾向性分析,"牛 耘,张 黎,王世泓,魏 欧","该文提出一种基于多层次语言特征的弱监督的情感分析方法, 先以少量情感词构成初始情感词典,用这些种子词汇作引导,根据评论文本在单词、短语及句子级别的语言特征结合上下文挖掘目标文本中潜在的具有情感倾向的词汇/短语。通过自训练不断扩充情感词典,最终得到一个具有领域特征的情感词典,并用所得到的情感词典对目标文本的情感倾向进行判断。与其他方法在同一数据上的结果相比,该方法以很小的词典规模取得了最高的F-score,并且得到的情感词含义明确。方法用于不同领域也取得了较高的精度,表明方法具有较好的领域适应性。","情感分析,多层次语言特征,弱监督算法,情感词典"
2013-05-05,Zipf定律与网络信息计量学,"刘胜久,李天瑞,珠 杰","作为文献计量学重要定律的Zipf定律已在许多领域得到较广泛的应用,网络信息计量学伴随着网络信息的激增而受到人们越来越大的关注。该文结合搜索结果数量的分布情况,提出了在网络信息计量学中仍然存在Zipf定律的猜想,并采用公开的词语集在几个代表性的搜索引擎中进行实验验证,证实了搜索结果数目近似服从Zipf定律的结论,其中Baidu与So搜索结果的Zipf指数为0.003。","Zipf定律,Zipf指数,搜索引擎,网络信息计量学"
2014-01-05,基于翻译模型的查询会话检测方法研究,"张振中,孙 乐,韩先培","查询会话检测的目的是确定用户为了满足某个特定需求而连续提交的相关查询。查询会话检测对于查询日志分析以及用户行为分析来说是非常有用的。传统的查询会话检测方法大都基于查询词的比较,无法解决词语不匹配问题(vocabulary-mismatch problem)——有些主题相关的查询之间并没有相同的词语。为了解决词语不匹配问题,我们在该文提出了一种基于翻译模型的查询会话检测方法,该方法将词与词之间的关系刻画为词与词之间的翻译概率,这样即使词与词之间没有相同的词语,我们也可以捕捉到它们之间的语义关系。同时,我们也提出了两种从查询日志中估计词翻译概率的方法,第一种方法基于查询的时间间隔,第二种方法基于查询的点击URLs。实验结果证明了该方法的有效性。","查询会话检测,词语不匹配问题,查询日志"
2013-06-27,基于话题相关的文档集的无向基本要素网络的连通性探讨,"杨 华, 陈 波","基于数量有限的文档,该文构建以基本要素中的head和modifier为节点的无向网络UBEN,调查了话题相关文档的UBEN的连通性,指出了话题相关的文档的UBEN具有的特性。讨论停用词对UBEN连通性的影响,比较了相关文档集和随机文档集的UBEN的联通特性的差异,指出了连通性在一定程度上是文档之间内容相关导致的融合结果。结论对多文档自动文摘和信息检索等任务有一定的意义。","话题相关文档集,自动文摘,复杂网络,连通性,信息融合"
2013-09-05,一种基于区分能力的多类不平衡文本分类特征选择方法,"张延祥,潘海侠","文本分类中的不平衡数据问题在现实应用中比较普遍。传统的特征选择方法在不平衡问题上倾向于多数类而忽略稀有类。针对这种倾向性该文提出了一种主导性分析量化方法,并基于对该方法的优化提出了一种基于类别区分能力的特征选择方法,即DA(Discriminative Ability)方法,该方法使用文档概率的最小绝对值差作为评分标准,一定程度上保证了特征选择在稀有类与多数类上的公平性。实验表明,DA优于CHI、IG、DFICF,尤其在F1宏平均指标上,DA在不平衡问题上能够取得更好的降维效果。","文本分类,不平衡问题,特征选择,主导性分析,区分能力"
2013-07-14,面向文本分类的特征词选取方法研究与改进,"李国和,岳 翔,吴卫江,洪云峰,刘智渊,程 远","中文特征词的选取是中文信息预处理内容之一,对文档分类有重要影响。中文分词处理后,采用特征词构建的向量模型表示文档时,导致特征词的稀疏性和高维性,从而影响文档分类的性能和精度。在分析、总结多种经典文本特征选取方法基础上,以文档频为主,实现文档集中的特征词频及其分布为修正的特征词选取方法(DC)。采用宏F值和微F值为评价指标,通过实验对比证明,该方法的特征选取效果好于经典文本特征选取方法。","文本文档,特征词,特征选取,文本分类"
2013-06-26,知识进化算法及其在关联分类中的应用,"梁红硕,刘云桥,赵 理","针对传统的关联分类算法在构造分类器的过程中需要多次遍历数据集从而消耗大量的计算、存储资源的问题,该文提出了一种基于知识进化算法的分类规则构造方法。该方法首先对数据集中的数据进行编码;然后利用猜测与反驳算子从编码后的数据中提取出猜测知识和反面知识;接着对提取出来的猜测知识进行覆盖度、正确度的计算,并根据不断变化的统计数据利用萃取算子将猜测知识与反面知识进行合理的转换。当得到的知识集中的知识的覆盖度达到预设的阈值时,该数据集中的知识被用来生成分类器进行分类。该方法分块读入待分类的数据集,极大地减少了遍历数据集的次数,明显减少了系统所需的存储空间,提高了分类器的构造效率。实验结果表明,该方法可行、有效,在保证分类精度的前提下,较好地解决了关联分类器构造低效、费时的问题。","知识进化,猜测,反驳,关联分类"
2013-09-27,中文的同形异码字问题,张小衡,"同一个字符拥有不同的计算机内部代码,这意味着有两个或两个以上字形在人的眼中是同一个字,而计算机却认为是不同的字。这种“人机看法不一致”会给语言信息处理带来混乱,导致信息检索不全,统计数字不准,字词分类排序不一致等情况。该文结合Unicode实例专题讨论当前计算机上存在的中文同形异码字问题,包括 (a) 私人造字公有化所形成的同形异码字,(b) 兼容编码所形成的同形异码字,(c) 建立专门的笔画部首表而形成的同形异码字,(d) 半宽和全宽字形分别编码而造成的同形异码字等,并探讨解决问题的方法。","中文字符,同形异码,Unicode"
2013-09-09,编码与同义词替换结合的可逆文本水印算法,"林新建,唐向宏,王 静","从通信编码的角度,该文探讨一种利用编码方法和同义词替换相结合的可逆文本篡改检测水印算法。以可替换同义词为标志对文本进行分组,提取分组文本特征生成认证水印信息;利用霍夫曼编码和纠错编码对同义词库各词进行编码,利用同义词替换技术完成水印的嵌入。在接收端,利用分组文本特征和霍夫曼编码,实现水印文本的篡改定位,利用纠错码实现可替换同义词的还原恢复。仿真实验表明,算法嵌入的水印具有良好的不可见性和较强的鲁棒性,在实现对文本篡改定位的同时,较好地实现了可替换同义词无损还原。","编码,同义词替换,可逆文本水印,定位篡改"
2014-12-29,基于统计的汉字字形视觉重心计算,"邓晓健,李 彬,张俊松","该文提出了一种汉字字形视觉重心的计算方法。首先收集常用汉字图像样本,通过图像预处理,提取出样本汉字的连通区域视觉平衡中心;然后招集被试对样本汉字进行视觉重心标注;再利用统计建模的方法,构建出连通区域视觉平衡中心和汉字整体视觉重心之间的关系模型。与相关方法比较,文中方法考虑了汉字视觉重心依赖于人的主观体验这一因素。该方法能广泛应用于汉字特征提取、汉字结构设计与优化等应用领域。","书法汉字,连通区域,视觉重心,回归分析"
2013-08-15,基于复述技术的汉语成语翻译方法研究,"罗 凌,陈毅东,史晓东,苏劲松","汉语成语是汉语的精华,拥有特有的语言形式,并经常出现在汉语中。但是由于汉英统计机器翻译训练语料中成语的稀疏性和现今大多机器翻译系统并没有对成语进行特殊的处理和研究,在汉英机器翻译中成语的翻译并不理想。针对该问题,本文提出了基于复述技术的两种方法来提高汉英统计机器翻译系统中成语翻译的能力。方法1: 测试集成语复述替换;方法2: 训练集成语复述替换。实验结果表明,方法1可以解决成语未登录词问题,提高成语翻译能力。方法2可以解决训练语料中成语稀疏问题,改善翻译训练模型。","统计机器翻译,成语翻译,复述"
2014-12-25,面向微博搜索的时间敏感的排序学习方法,"王书鑫,卫冰洁,鲁 骁,王 斌","近年来微博检索已经成为信息检索领域的研究热点。相关的研究表明,微博检索具有时间敏感性。已有工作根据不同的时间敏感性假设,例如,时间越新文档越相关,或者时间越接近热点时刻文档越相关,得到多种不同的检索模型,都在一定程度上提高了检索效果。但是这些假设主要来自于观察,是一种直观简化的假设,仅能从某个方面反映时间因素影响微博排序的规律。该文验证了微博检索具有复杂的时间敏感特性,直观的简化假设并不能准确地描述这种特性。在此基础上提出了一个利用微博的时间特征和文本特征,通过机器学习的方式来构建一个针对时间敏感的微博检索的排序学习模型(TLTR)。在时间特征上,考察了查询相关的全局时间特征以及查询-文档对的局部时间特征。在TREC Microblog Track 20112012数据集上的实验结果表明,TLTR模型优于现有的其他时间敏感的微博排序方法。","时间敏感,排序学习,微博搜索"
2013-04-08,基于本体的航空领域问答系统,"张克亮,李伟刚,王慧兰","该文设计并实现了基于本体的航空领域问答系统,该问答系统采用面向领域本体的问题分类方法和结构化语义信息提取方法,将自然语言问题转换为SPARQL查询语句,在本体知识库中检索答案。实验结果表明,该系统能够处理该领域内的大部分常见问题,取得了82.97%的平均准确率。","问答系统,本体知识库,问题类型,结构化语义"
2013-07-19,维吾尔语三音节词中元音和谐的声学特征分析,"古力努尔·艾尔肯,祖丽皮亚·阿曼,地里木拉提·吐尔逊","该文从提高语音合成自然度的实际需求出发,首次从实验语音学的角度从《维吾尔语语音声学参数库》中统计出了333个三音节词,其中再筛选了93个全和谐词和半和谐词,并对其元音的宽带共振峰模式、共振峰值、音高、时长和音强等韵律参数进行了统计分析,归纳了其共振峰、音高、时长和音强分布特点来考察元音和谐的基本声学特征,总结出了一些重要的规则和结论,为参数式或波形拼接式语音合成系统中调整合成前的元音和谐问题提供了重要的参考依据。","三音节词,元音和谐,声学分析,语音合成,维吾尔语"
2013-04-08,现代汉语虚词用法知识库建设综述,"张坤丽,昝红英,柴玉梅,韩英杰,赵 丹","现代汉语虚词用法繁杂多样,虚词用法的研究对汉语语义理解及语法分析起着非常重要地作用。该文在分析虚词及词汇知识库研究现状的基础上,对三位一体的现代汉语虚词用法知识库中虚词用法词典、虚词用法规则库和虚词用法标注语料库的建设过程进行了详细描述,对虚词知识库现存的问题进行了分析。利用已经构建的现代汉语虚词知识库,对虚词用法自动识别进行了研究,并对现代汉语虚词知识库的应用进行了初步的探讨。","虚词用法知识库,虚词用法词典,虚词用法规则库,虚词用法标注语料库"
2013-04-08,基于依存语法构建多视图汉语树库,"邱立坤,金 澎,王厚峰","树库是自然语言处理中一项重要的基础资源,现有树库基本上都是单视图树,支持短语结构语法或者依存语法。该文提出一套基于依存语法的多视图汉语树库标注体系,仅需标注中心语和语法角色两类信息,之后可以自动地推导出描述句法结构所需的短语结构功能和层次信息,从而可以在不增加标注工作量的前提下获得更多语法信息。基于该体系,构建了北京大学多视图汉语树库(PMT)1.0版,含有64000句、140万词,支持短语结构语法和依存语法两个视图。","多视图树库,依存语法,短语结构语法"
2013-04-08,词汇计量研究与常用词知识库建设,"俞士汶,朱学锋","面向自然语言处理的词汇语义研究应该以词汇的计量研究为基础。该文在评述汉语词汇计量研究的主要成果以后,提出一个汉语常用词知识库的建设任务,并给出常用词表的构造性定义、词表常用性的定量评价方法以及“部件词”的概念,最后介绍现代汉语常用词知识库的总体设计和已经做的工作。期望常用词知识库的建设能为汉语词汇语义学研究、为中文信息处理事业的发展做出贡献。","汉语常用词知识库,《中国语言生活状况报告》,综合型语言知识库,《现代汉语语法信息词典》,部件词"
2013-04-08,面向计算的现代汉语双音词分离及其语法意义与特性研究,薛宏武,"汉语缺乏严格的形态束缚,在句子里双音词经常由于嵌入相关句法成分而呈现出分离。为了提高自然语言处理中语段分词、词性标注及基于规则的句子语义计算的准确性与有效度等,文章系统考察了现代汉语里典型词与离合词的分离现象,挖掘并刻画出它们各自形成的语法动因、条件、意义以及分离而成的成分的语法特性等,从而对现代汉语中双音词的分离做出了系统多层级的理论思考。文章指出典型词的分离是语用作用的结果,分离结构的意义是主观的;而离合词分离是句法语义作用的产物,意义是客观的。","面向计算,双音词,分离,语法意义与特性,现代汉语"
2013-04-08,汉语语义场网络中的无标度分布现象,"杨 华,姬东鸿,萧国政","语义场是词语意义联系在一起构成的语义系统。一门语言的所有子语义场合在一起,就是该语言的语义场。探索用复杂网络来表示汉语的语义场,基于联想场的概念,该文提出用复杂网络表示汉语的语义场。该网络的节点度,节点权值与边权值均服从无标度分布。展示结点度、结点权值、边权值在一定范围的内容,观察到一些在网络视角才能发掘出的现象。该文将较特别的现象展示给语言学界的专家们,期望引起共鸣,得到对这些现象的更合理解释。","语义场,复杂网络,无标度分布"
2013-04-08,HowNet与CCD映射方法研究,"向春丞,穗志方,詹卫东","本体映射是解决本体异构问题的关键方案。该文以HowNet和CCD中的名词性概念为例,首先利用机器学习技术发现初始映射关系,主要包括特征选择、样本集合划分、分类器选择等步骤;然后考虑本体的整体结构信息,利用相似度传播算法,对初始映射关系进行全局调整。实验表明,最终的一对一和一对多映射关系的准确率分别达到了94%和87.5%。","本体映射,机器学习,分层抽样,相似度传播算法"
2013-04-08,基于特征序列的语义分类体系的自动构建,"陈 刚,刘 扬","词义知识表示主要依赖属性描述或分类描述,这两种方式各有所长,但不同表示之间相互转换的可行性与现实状况还未被关注。在属性描述的基础上,该文引入序关系的思想,提出基于特征序列的概念与方法,以此来模拟、分析概念涵义从一般到特殊的渐次生成过程,发掘尚未显性化的中间概念,自动构建出一个语义分类体系。以HowNet(2000版)数据为例,实验表明该方法可以生成一个性质优良、覆盖完全的新的语义分类体系,并反映此前的属性描述在语言知识工程实践中不易察觉的一些问题。","词义知识,属性描述,分类描述,序关系,特征序列,语义分类体系"
2013-04-08,基于本体和依存句法的词汇语义关系标注及评价方法研究,"熊 晶,支丽平,袁 冬","为弥补传统的语义标注方法在词语或句子成分之间关系描述方面的不足,该文提出了一种基于本体和依存句法的非结构化文本语义关系标注算法。算法以句子为单位,综合POS(Part of Speech)、语义辞典、语言学特征等因素对句子中词汇的语义关系进行识别,利用词语间的依存关系对词语进行语义组合,从而实现词汇语义关系标注。结合语义标注过程中的语义匹配度、语义丰富度等特征,设计了评价算法,用以衡量标注结果的正确性。实验结果表明,该标注算法能获得较高的准确率,在大规模语料下效果尤为显著。","语义标注,本体,非结构化文本,依存句法"
2013-04-08,内容标签和关系标签相结合的汉语篇章标注规范,"王 荀,李素建,王宇昕","篇章标注是自然语言处理中的重要任务,很多其他任务,如自动摘要、机器问答等都可以通过篇章标注得到对文本内容和语义的认识,从而获得更好的结果。与此同时,篇章理解的理论如篇章修辞结构(RST),向心理论(CT)等与实际问题的结合并不紧密,难以实用。该文中我们参考现有的语言学理论和一些语篇标注库(如RST-DT,PDTB),并结合自然语言处理任务特点,提出了一套用于篇章标注的汉语标注体系。这个体系能够比较准确和全面地描述出篇章的内容和逻辑关系,并很好地服务于实际任务的需要。","篇章语义标注,修辞结构理论,关系标签,内容标签"
2013-04-08,汉语篇章级小句关系的标注体系,"吴云芳,徐艺峰,王恺然","句际关系自动分析属于篇章语义学研究的范畴,虽然英语句际关系的研究已有大量工作,但汉语句际关系的自动分析还只是刚刚起步。该文在RST理论框架下,结合汉语特点,提出了完整的汉语篇章级小句关系标注体系。将汉语话题和逻辑关系置于同一个框架下进行描述,将小句关系划分为事件附属关系和事件逻辑关系两大类。逻辑关系又包括6个中类、15个小类。目前已在人民日报语料上完成了8000个句子的小句关系标注。抽取出其中1000个句子检测了双盲标注的一致性,揭示了汉语意合性语言小句关系标注的困难;并基于标注数据对关系类型进行了定量分析,指示了汉语句际关系自动分析将面临的重点和难点。","句际关系,小句关系,语料库标注"
2013-04-08,基于图式的文本蕴涵识别初探,"倪盛俭,姬东鸿","文本蕴涵识别是大部分自然语言信息处理应用的核心。该文尝试探讨文本蕴涵识别中涉及的(意象)图式理据。通过对选自语料例子的分析,显示各类图式是如何成为文本蕴涵识别的理据的。图式包括物性结构、理想认知模型、框架、脚本等。这些图式都是表示语义信息的结构。从广义上讲它们都可以纳入语义特征的范畴,都可能成为蕴涵关系成立的理据。基于图式的文本蕴涵识别研究结合有关图式库的构建,有望为突破文本蕴涵识别瓶颈作出重要贡献。","文本蕴涵识别,图式,物性结构,理想认知模型,框架,脚本"
2014-02-17,基于框架语义的隐式篇章关系推理,"严为绒,朱珊珊,洪 宇,姚建民,朱巧明","篇章关系分析是一种专门针对篇章语义关系及修辞结构进行分析与处理的自然语言理解任务。隐式篇章关系分析是其中重要的研究子任务,要求在显式关联线索缺失的情况下,自动检测特定论元对之间的语义关系类别。目前,隐式篇章关系分析性能较低,主流检测方法的准确率仅约为40%。造成这一现状的主要原因是: 现有方法脱离论元的语义框架进行关系分析与检测,仅仅局限于特定论元特征的关联分析。针对这一问题,该文提出一种基于框架语义的隐式篇章关系推理方法,这一方法有效利用了框架语义知识库(即FrameNet)和相关识别技术,实现了论元语义框架的自动识别,并在此基础上,借助大规模文本数据中框架语义关联关系的分布概率,进行论元语义一级的关系判定。实验结果显示,仅仅利用第一层框架语义知识,即可提高隐式篇章关系检测性能至少5.14%;同时,在考虑关系类别平衡性的情况下,这一方法能提高至少10.68%。","篇章关系,隐式篇章关系,框架语义"
2013-04-08,基于意群划分的中文微博情感倾向分析研究,"桂 斌,杨小平,朱建林,张中夏,肖文韬","微博作为一种新兴的社交网络平台,逐渐成为公众发布个人信息,获取实时信息,表达个人观点的新平台。针对微博情感倾向判断的问题,提出了一种基于意群划分的中文微博情感倾向分析(STDSG)方法。引入意群的概念,提出微博意群划分算法,根据意群间的关系,考虑否定词、程度词及标点符号的对情感倾向分析的影响,提出计算微博意群情感倾向的方法。在给定的数据集上,实验结果准确率达到了80.1%,总体性能优于基于情感词典的方法及基于支持向量机的方法。","微博, 意群, 情感倾向"
2013-04-08,基于文本纹理特征的中文情感倾向性分类,"许歆艺,刘功申","随着互联网的发展,社交网络、电子商务等已经成为人们关注的焦点,对社交网络的文本进行情感倾向性分析和挖掘变得越来越重要。该文针对网络上的中文文本,提出一种基于文本纹理特征的情感倾向性分类方法。通过测试多种文本纹理特征对文本情感倾向性的影响,成功将文本纹理特征融入情感分类中。通过计算各类特征与文本的情感倾向性的相关度,对特征进行降维。相对于基于词频的情感倾向性分类方法,查准率平均提高了10%左右。","中文文本分类,情感倾向性,文本纹理,SVM"
2013-04-08,基于观点袋模型的汽车评论情感极性分类,"廖 健,王素格,李德玉,张 鹏","该文针对网络评论倾向分级问题,提出了一种基于观点袋模型和语言学规则的多级情感分类方法。通过分析句子中的词性搭配关系,设计了12种抽取特征-观点搭配模式,并对存在问题给出了解决策略。依据汉语用词特点和词汇在汽车领域的特殊用法,提出搭配四元组的情感倾向极性值计算方法。在此基础上,利用获取的搭配四元组及其情感倾向极性,建立文本的向量化表示,并构造了权重计算公式。最后,利用文本余弦相似度计算方法实现对评论文本的五级情感极性分类。通过在COAE2012任务3的汽车数据集上进行的测试,取得了较好的分类结果。","情感分类,观点袋模型,词性搭配"
2012-05-02,腾讯微博的内容生成模式分析,"李亚平,曹 润,童 露,梁 循,倪志豪","随着Web 2.0 时代网络技术的快速发展,社交类网站用户大规模增加。该文选取腾讯微博近两万名用户,抓取了他们所有的微博数据,对腾讯微博的用户内容生成模式进行分析和研究。我们从微博用户贡献分析、基于时间的用户活跃度分析以及微博影响三个角度出发,对微博的数量、微博的原创与转发、微博发布的周模式与日模式、微博转发影响力以及对影响微博转发的因素进行研究。总结出微博用户内容生成的一些特点,如用户内容贡献呈现一种“90-10”规则,不同类型的用户有着不同的“微博风格”,微博用户每日微博发布数有着明显的周模式与日模式等。相关分析结论对于进一步深化研究微博的用户内容生成模式具有一定参考意义。","微博,用户内容生成,模式分析"
2012-04-18,一种基于因子图的搜索广告转化预测模型,"顾智宇,秦 涛,王 斌","基于转化的广告方式在应用和研究中逐渐得到重视,采用该方式的搜索广告在广告排序时需要对候选广告的转化概率进行预测,以提高广告的转化率,优化搜索引擎的广告收益。该文在对搜索广告中影响转化的各特征进行提取与分析的基础上,提出了描述广告、查询、用户三个因素与转化事件关系的概率因子图模型,并基于该模型对广告转化进行预测。最后我们使用从某商业搜索引擎采集的实际数据对预测模型进行评价并与朴素贝叶斯方法进行对比,实验结果表明,三类因素对转化具有不同程度的影响,我们提出的因子图模型可以较好地预测广告的转化。","搜索广告,概率预测模型,CPA广告"
2013-05-15,基于随机森林的产品垃圾评论识别,何 珑,"目前的产品垃圾评论识别方法只考虑评论特征的选取,忽略了评论数据集的不平衡性。因此该文提出基于随机森林的产品垃圾评论识别方法,即对样本中的大、小类有放回的重复抽取同样数量样本或者给大、小类总体样本赋予同样的权重以建立随机森林模型。通过对亚马逊数据集的实验结果表明,基于随机森林的产品评论识别方法优于其他基线方法。","产品垃圾评论,不平衡问题,随机森林"
2013-04-08,一种基于排序学习方法的查询扩展技术,"徐 博,林鸿飞,林 原,王 健","查询扩展作为一门重要的信息检索技术,是以用户查询为基础,通过一定策略在原始查询中加入一些相关的扩展词,从而使得查询能够更加准确地描述用户信息需求。排序学习方法利用机器学习的知识构造排序模型对数据进行排序,是当前机器学习与信息检索交叉领域的研究热点。该文尝试利用伪相关反馈技术,在查询扩展中引入排序学习算法,从文档集合中提取与扩展词相关的特征,训练针对于扩展词的排序模型,并利用排序模型对新查询的扩展词集合进行重新排序,将排序后的扩展词根据排序得分赋予相应的权重,加入到原始查询中进行二次检索,从而提高信息检索的准确率。在TREC数据集合上的实验结果表明,引入排序学习算法有助于提高伪相关反馈的检索性能。","信息检索,查询扩展,伪相关反馈,排序学习"
2013-04-08,搜索日志中中文人名自动识别,"王 玥,吕学强,李 卓,舒 燕","搜索日志中人名识别一直是日志挖掘中的一个重点和难点,其结果好坏直接关系搜索引擎的检索效率和准确率。由于分析了长文本中人名识别方法在搜索日志中使用存在很多困难与不足,因而该文提出了一种在搜索日志中识别中文人名的方法。该方法将搜索日志中人名内部用字的概率特征引入条件随机场,再根据搜索日志的特点计算人名可信度提取搜索日志中的中文人名。在搜狗查询日志上进行实验,正确率平均达到了81.97%、召回率平均达到了85.81%,综合指标F值平均达到了83.79%。","人名识别,搜索日志,条件随机场,可信度"
2013-04-08,现代汉语词汇历时检索系统的建设与应用,"荀恩东,饶高琦,谢佳莉,黄志娥","词汇是语言系统中最具活力的子系统。在语言演化的过程中,词汇的历时变化是语言学、历史学、社会学等多学科所关注的信息。我们收集了时间跨度约为60年的同质新闻语料。基于自然语言处理技术我们开发了现代汉语词汇历时检索系统。基于该平台可以利用频率、累积和与累积频率等方法从微观和宏观的角度上对词汇的语义、语用等方面进行研究。","历时信息,词汇演化,历时计算,语料库"
2012-12-28,一种融合聚类和时间信息的微博排序新方法,"卫冰洁,史 亮,王 斌","随着微博的快速发展,微博检索已经成为近年来研究领域的热点之一。微博检索与传统文本检索在两个方面明显不同: 一是微博具有自己的特点,表现在文本短和内容中具有主题概括词(称为Hashtag);二是微博排序中除了考虑文本和语义相似度,还需考虑时间信息。根据这两点区别,该文在统计语言模型的基础上,使用聚类进行文本扩展,并将Hashtag信息运用到聚类过程中。同时,因为微博数据集中具有Hashtag的微博个数不超过13%,针对这一现象,该文还提出了一种扩展微博Hashtag的方法,最终提出了基于聚类的三个模型。然后通过定义文档先验将时间信息加入到提出的三个检索模型中,得到融入聚类和时间信息的三个模型。最后基于TREC Microblog数据的实验结果证明,融合聚类信息和时间信息的模型在MAP和P@30上有明显提高,分别提高7.1%和11.6%。","微博检索,Hashtag,聚类,时间,语言模型"
2013-06-27,藏语拉萨话单音节嗓音声学参数分析,"陈小莹,艾金勇,于洪志","该文对藏语拉萨话单音节的嗓音特征进行了实验研究,实验首先对藏语拉萨话单音节进行语音标注,然后根据语音标注的位置信息,利用对应的程序提取音节结构中的元音和辅音的嗓音声学参数,对基频、开商和速度商分别统计分析,并做了显著性分析。实验结果表明不同元音和辅音的嗓音参数与发声方式以及其在音节中位置有关,元音和音节结构的不同会显著影响开商和速度商的值,但对于基频数据的影响并不显著。同时嗓音参数之间也存在一定的关联性,即基频和开商、速度商之间是反比关系,开商和速度商之间是正比的关系。","拉萨话,嗓音特征,基频,开商,速度商"
2012-11-09,“蒙古语名词语义信息词典”的开发与应用,"海银花,那顺乌日图","2009年至今,“蒙古语名词语义信息词典”(以下简称为“名词语义词典”)通过几年的开发目前词典基本成形,并且有了显著的新进展。其新进展主要体现在词条的扩充、属性字段的增添及其初步应用。该文概要介绍“名词语义词典”的研发过程,实例说明这部词典的新进展和初步应用情况。","蒙古语名词,语义信息词典,开发,应用"
2013-06-14,藏语三音动词短语自动抽取研究,"赵维纳,李 琳,刘汇丹,普布顿珠,吴 健","藏语三音节复合动词短语(以下简称三音动词短语)能产性强,使用频率高,结构不稳定,给藏语文本处理带来很多麻烦。针对这些特点,该文提出了一种统计和规则相结合的三音动词短语的自动抽取算法。首先,从三音动词短语的结构出发,以构成三音动词短语的动语素作为标志,获得三音动词短语候选项。然后,利用统计算法和语言规则库对候选项进行过滤,获得三音动词短语。实验结果表明,统计和规则结合的方法可以有效地从未经标注的藏语语料中获取三音动词短语。","中文信息处理,藏语信息处理,词汇获取,藏语三音节复合动词短语"
2013-03-27,基于维吾尔语词干词缀粒度的汉维机器翻译,"米莉万·雪合来提,刘 凯,吐尔根·依布拉音","汉语到维吾尔语的自动机器翻译有着重要的现实意义。目前对于汉维统计机器翻译方法的研究相对空白。该文提出了一种以维吾尔语为词干词缀粒度的汉维机器翻译方法。该方法利用维吾尔语形态分析后的词干词缀作为翻译的基本单位,并且根据其黏着语特性提出了一种基于有向图的维吾尔语“词干-词缀”语言模型。基于开放语料的实验证明我们的词干词缀翻译模型以及语言模型显著优于之前的基于词粒度的模型。","维吾尔语,机器翻译,汉维翻译,词干,词缀,形态分析"
1987,计算机中文信息库中的关联操作算法,"彭平,何克抗","组织大量数据, 建立信息存贮系统的最主要目标之一, 是能够让用户有效地使用其中的任一信息子集, 这就需要信息库系统除了存贮信息之外, 还要具备较强的信息数据操作能力。",
1987,用于理解的语义词典的构造和设计,"张小龙,姚天顺","在人工智能比较活跃的领域中, 自然语言理解已日益引起计算机科学、语言学和心理学等方面专家的重视。在自然语言处理和理解中, 从机器翻泽系统到各种语言理解系统的建立, 它们都需要对句子进行词法和句法分析, 查找为翻译或理解所建立的词典。不论怎样把以前所建立的系统分为第一代或第二代系统, 这些系统大都是基于句法结构。由于仅靠句法结构还不够, 不能完全解决机器翻译或语言理解方面的问题, 因此, 在美国曾使机器翻译研究一度中止。大量的实践证明, 目前, 自然语言理解以句法为基础的研究已转为以语义为基础的研究。因而, 为了适应以语义为基础的研究, 我们就得率先进行语义词典建立方面的探讨和研究。",
1987,设计汉语段落理解机构的研究,"冯寅,王开柱","自然语言问题回件系统早在60年代就有人着于研制了。从事这项研究的最初动机是试图寻找一个能够真正理解语言深层意义结构的语言理解模型。虽然在设计机器模拟人类理解语言的过程中面临着大量的问题, 但在70年代,仍出现了一系列较为成功的自然语言问题回答系统。在近期,一些较为成形的自然语言识别理论以及自然语言处理系统的设计方法也陆续出现。本文将介绍一个正在我校研制的汉语问题回答系统CQAES（Chinese Question Answer Experimental System）的汉语段落理解机构。CQAES将专门理解描述有教师、学生进行教学和突验的中文段落并回答涉及到亥段落内容的有关问题。R.Alterman曾针对一些具体的动作相互间的关系。给出了7种适应于描述一部分具体动词的意义联贯关系。我们将此思想发展成为一个描述一系列事件间的意义联贯关系。但是, 这些关系不象R.Alterman那样具体地揭示某些动词间的意义联贯关系, 而是具有某种通常意义的事件间的意义联贯关系。",
1987,一种高性能汉字内码的设计,"贾雷,郭肇德,郝伟刚","本文给出了一种新型汉字内码结构, 它具有中西文兼容的处理功能,并且,保证不发生中西文混淆, 从而支持了汉字进入未作修改的西文软件。这种汉字内码已经在IBM PC/XT上的高兼容性汉字操作系统AC-DOS（Advanced Chinese Disk Operating System）上得到实现。在AC-DOS系统的支持下, 原西文系统支持的许多种软件如各种应用软件、各种编泽软件、行编辑、全屏幕编辑、数据库软件和计算机网络等, 不需作修改, 就能做到象处理西文一样处理汉字信息，同时, 仍保持西文的全部功能。",
1987,普通话音节合成系统的研究,"杨顺安,许毅,曹剑芬","今天, 随着计算机科学技术的进步, “ 第五代计算机” 或“ 智能计算机” 的呼声口益高涨。通过自然语音来和计算机交流信息, 这显然是智能计算机应该具备的一种功能。",
1987,浊音的基频同步分析及普通话的音素——音节识别方案(摘要),"严普强,施昊","本文讨论了两个问题 1）针对浊音信号的近似周期性, 建议采用锁相环同步分析的方法进行谱估计。其优点在计算效率和分析精度, 同时还能克服语调变化、分析帧位置变化引人的分析误差。2）针对汉语发音规律的特点, 建议以音素一音节为识别对象。共优点在于计算效率和可以在不同层次充分利用语音、语言的规律。",
1987,书面汉语自动分词系统—CDWS,梁南元,"本文在大量统计的基础上, 论证了计算机自动分词是可行的。CDWS The Mordern Printed Chinese Distinguishing Word System）是作者设计的一个有较高切分精度、可实用的现代书面汉语自动分词系统, 它采用了词尾字构词检错技术及若干有效的纠错知识, 配置了知识库和临时词典, 显著的降低了错误切分率。",
1987,论中国少数民族文字信息处理设计原则,赵珀璋,本文首先介绍了我国各民族语言文学特点。接着讨论了少数民族文字信息处理设计原则。最后提出了少数民族文字字符集编码结构设计和划分小字符集、中字符集及大字符集的依据。,
1987,多文种电脑,李金铠,"世界进人信息时代, 研制能够处理世界各种文字信息的电脑—多文种电脑已成为国际科学技术上的一个难题。",
1987,一种适用于B-HIVE多机系统的并行排序算法,聂采涛,"本文提出了一种将M个单一分布的元素在B一HIVE多机系统上进行排序的算法。B一HIVE多机系统是一个以ALPHA结构为互连网络的, 由N个处理机构成的多机系统。该算法是上升算法与顺序排序算法的综合应用,其时间复杂度为:将该算法同应用在以CCC（Cube-connected lgcles）为互连网的多机系统上的, 与所谓最有效的排序算法相比, 本文所提出的算法具有较好的时间复杂度。",
1987,IBM Token Ring局部网上的pc网络程序的汉化,"吴炬,邱光谊","本文简单介绍了可在IBM Token Ring局部网上运行的PC NET-WORK的网络程序, 分析了开发汉字网络程序的途径和需耍解决的问题, 提出了以服务器为中心实现汉字操作系统的思想。",
1987,中华杯中文电脑公开赛圆满结束,陈一凡,"为了促进汉字键盘输入技术的优化, 推动中文电脑系统的发展, 由中国中文信息学会、中园科协青少年部、科技日报社、中央电视台、计算机世界报社、中国人才报社、天津市科协、大连市科教委、大连市科协联合发起举办的中华杯中文电脑公开赛, 自1987年8月10日开始,经过21天的紧张训练和比赛, 于9月1日圆满结束, 取得了良好的成绩。",
1988,浅谈汉英终端的发展和方略,"梁先宇,孔令人","文章在剖析目前汉英终端现状及其技术方索的基础上, 采用全新的体系结构、设计方案和相应技术措施,阐述了汉英终端的发展和方略, 并根据长期研究和实践的成功经验, 侧重介绍了KT系列科泰汉英仿真终端的主体思想, 实现原理及其独有的优裕环境, 双机双控体系, 复盖变通能力, 三重链接仿真, 西文汉化通用、简便易行等特征, 推出具有中国特色的产品。",
1988,PC/AT及其兼容机上C—Xenix的总体设计及实现,"孙玉方,陈一清,吴健,郑蕾,曾昱满,周晓萱","由于PC/AT及其兼容机有着比较丰富的软件资源, 因币配备多用户的Xenix是比较适宜的, 为了满足国内用户之急需, 我们开发了中英文兼容的C一Xenix信忽处理系统。本文重点讲述了C一Xenix的设计原则及主要实现枝术。",
1988,一个计算机辅助汉字点阵制作系统,樊建平,"这篇文幸详细描述一个东阵汉字库制作系统的结构及组成其各个棋块的具体实现算法和方法。系统的输入为扫描仪扫入的一“ 张” 由几个汉字组成的点阵字稿, 猜出为具有一定规格的一个点阵字库。系统由以下几个模块组成从一“ 张” 字稿中分刻单字的棋块、处理汉字点阵中的“ 污点” 及“ 缺点” 棋块、汉字点阵的规格化模块、汉字笔划边沿光滑模块及字库管理维护模块。文章的最后给出了一些由该系统处理得到的汉字点阵字样。",
1988,论汉语拼音、三拼、双拼、简拚的统一表达形式,王晓龙,"采用拼音精入的中文拾态处理系统上存在看拼音字毋、三拼、双拼、简拼等多种表达形式, 有些系统之间对同一音节的简码定义相互矛质, 给用户带来了很多麻烦。为解决这种沉乱现象, 本文在对拼音音节分布规律分析的基础上,以符合人们习惯、容易记忆、方便用户、不增加重码率为设计准则, 提出了一种可包含拼音字毋、三拼、双拼、简拼各种表达方式功能的统一表达形式, 使在以拼音为基础的各种汉字翰入系统上实现同一人机界面成为可能。该表达形式已在IBM PC等机型上实现。",
1988,计算机藏文信息处理的研究与设计,"于江苏,葛小冲","该文介绍了藏文信息的特点, 提出采用了78个藏文符号作为现代藏文信怎处理交换用的8位一字节编码方案, 设计出一种藏文字符输入健盘, 在IBM一PC/XT微机上用INTEL一8088汇编语言开发出一种独具特色的藏文操作系统。该系统与西文MS一DOS兼容, 支持多种计算机语言如FORTRAN、PASCAL、C、BASIC宏汇编等对藏文信息进行处理。在该系统下, 用户可以对藏文信息的处理进行二次开发。",
1988,论计算机用中国汉字字形的标准化,"石云程,江晓红","本文论述了中国汉字字形的特点和历代汉字学家对社会用字所作的统一化、规范化的成就, 并且通过一些史例阐述了标准化是中国汉字发展的必然趋势, 而计算机用点阵汉字字形标准化又是汉字信息处理技术发展的必然需要。最后本文还介绍了三种国家标准点阵汉字字形风格和特点。",
1988,论绘图仪用汉字库的数据特征及存贮结构,刘晓基,"本文提出了绘图仪用汉字序的数据特征及其存贮结构, 并且进一步提出了关于这种汉字序的压缩存贮方法和笔划路径优化方法, 从而使得这种汉字库更加易于推广应用。目前, 本文所提出的汉字序已经在国内各领域得到很好的应用。",
1988,利用笔划密度函数等特征的手写汉字识别的讨论,"王林泉,陈刚","为了对手写汉字的识别分类的方法进行探讨, 本文提出了一种分类处理能力强, 速度比较快, 算法上能尽量克服手写变形影响的手写汉字的识别分类方法。一般要求正介文字在候补文字上进行选择, 候补文字的数额要小。我们利用手写汉字笔划密度函数特征、粗网格特征、粗外围特征以及几个特征的拜用来对文字进行识别和分类, 用计算机进行模拟和验证。由实验可以知道利用各自的每一个特征, 分别有其相应的分类能力, 差不多到第五位或第七位分类能力可达到100%,如果将三个特征并用, 正如所予期的, 分类能力可以提高, 到第三位就可达到100%,这是因为各个特征分别从不同的方面吸收了手写的变形。",
1988,民族文字信息处理系统关于时域和地域的兼容性和通用性,"许寿椿,孙文玲","近几年来, 蒙藏维哈朝彝等文种的微机系统相继开发成功, 在资料存赊、检索, 企事业管理及言语统计等多种应用领域都取得一些成绩, 关于系统开发方法、编码结构等方面也开始了较深人的研讨。本文根据系统应用中提出的问题, 针对少数民族文字发展中较多变化的情况和跨界民族内文化交流的需耍, 提出了系统关于时域和地域的兼容性或通用性问题。",
1988,关于E-ch/A世一汉/英机器翻译系统及其汉英目标语的综合,李维,本文分两部分。第一部分介绍我们设计的E一ch/A机器翻译中目标语的综合问题。,
1988,现代汉语统计频度及其在电脑音轨输入系统中的应用,陶沙,"本文先介绍和分析音频和词频的若干参数, 接着从四个方面论述频度在电脑音轨输入系统中的应用问题。研制中文信息处理系统, 安在定量分析的基拙上进行定性分析, 必须做到科学性和实用性兼备。文中提到的“ 现代汉语语音健位图” 和“ 信息处理用现代汉语词语某” , 已在中国人民大学语言文字研完所机房微机IBM PC/XT上实现, 名为《音轨RD一86型电脑输入系统",
1987,我国汉字识别研究的进展,张炘中,"作为模式识别和人工智能一个分支的汉字识别技术, 自七十年代末, 我国一些大学、研究所开展研究以来, 至今已有十年历史。十年来, 从无到有, 从几个单位少数人探讨进展到有一定规模的研制队伍在认真探索, 从纯原理、方法研究发展到理论、方法、模拟实验、识别系统齐头并进。特别是最近两、三年, 汉字识别的研究在印刷体汉字、联机手写汉宇、手写印刷体汉字等领域全面开展起来, 取得了一些可喜的初步成果, 已有一些指标赶上甚至超过世界水平。可以说, 我国汉字识别的研究就要摆脱摸索阶段, 开始向实用系统研制的道路前进了。",
1987,简讯,,中国计算机学会中文信息技术专业委员会于1987年10月成立。该专业委员会的研究方向是中国语言文字处理的计算机系统和以计算机为工具研究语言文字处理技术。它将同其他学会开展广泛的横向合作。,
1987,一个面向OA的印刷汉字OCR实用系统,"杨力,武玉朴,张佩芳,王成山,梁刚","本文叙述一个采取以“统计模式识别”为主, 以“结构模式识别”方法为辅的识别技术路线实现的以办公室自动化（OA）为应用环境的一级印刷汉字文本识别系统，该系统从实用化角度出发, 采用页式文本图象扫描输入，输入后将图象文本分割成单个汉字, 并根据汉字的结构特点, 抽取了汉字的内层, 外层,局部等多个特征。识别采用多级分类方法。识别结果形成一个国标区位码文件，系统软件建立了一种与用户间的友好界面。该系统是在IBM PC/XT上实现的, 对印刷字样识别率>99%, 对各类实际的办公行文其统计识别率>95%, 识别速度为1-2字/秒。",
1987,汉字识别的特征点法及其一种应用,"张炘中,阎昌德,刘秀英","本文提出了一种基于特征点的汉字识别新方法。汉字笔划上的端、折、歧、交点和汉字背景上的关键背景点称为汉字特征点。和以往不同的是本文把笔划特征点和关键背景点两者结合起来, 直接根据特征点本身的信息来识别。汉字特征点反映了汉字结构的本质特征, 集中了主要的结构信息。用特征点来识别汉字,能消除汉字中非结构信息的不稳定性对识别的影响, 能浓缩汉字信息, 减少存贮量, 提高识别速度。",
1987,识别多种印刷体汉字的TTWK法,"方应谦,李明禄,林欣,邹敏华","本文提出了“ 脱壳透视” 的分类原理, 该分类原理对同一汉字集上不同字体的汉字的分类一致性较好, 抗干扰能力强, 是多种印刷体汉字自动识别中较为满意的分类方法。在识别特征的选取上, 我们提出了“ 特征稳定度” 概念, 从这个概念出发, 提取了每个汉字的稳定识别特征和同字异体的公共识别特征, 将每个汉字的复杂结构抽象成一个具有典型特征的框架模型（简称“ 稳定框架原理” ）有效地提高了识别率, 压缩了存储空间。根据这些原理, 我们以OKIFAX7700传真机作扫描器, 在CROMEMCO SYSTEM THREE上实现了对GB2312一级字3755个四号宋体及黑体汉字98.57%的识别率及3.24字/秒的识别速度。",
1987,按印刷汉字笔划尖端数粗分类的方法及汉字识别系统,"史瑞琦,王庆人","汉字识别研究最早开始于六十年代中期,从那以后,世界上许多国家都做了大量的工作,目前,日本已研制出几种商品化的汉字阅读机,其中东芝公司研制的OCR光电字符阅读机,对单一印刷体的二千汉字字符集达到99%的识别率,识别速度为100字/秒,但他们使用的是大型计算机和专用集成电路、造价昂贵, 这种文字阅读机, 从它的使用普及性和我国目前的国情来看, 都是不合适的。我国目前把手写和印刷体汉字直接输人到计算机的研究工作还处于摸索阶段, 在微型机上实现汉字识别系统的研究起步更晚, 我们在印刷体汉字识别方面做了一些工作。本文将把作者的一些工作向读者做一介绍。",
1987,高识别率印刷体汉字识别方法的研究,"崔国伟,舒文豪","本文提出了用改进的粗外围法和对汉字点阵在水平和垂直方向上投影进行WALSH变换以抽取汉字特征的方法。研究表明, 这种方法的处理速度快, 具有较强的抗干扰能力和较高的识别率。利用这种方法, 在由理光FX-120型传真机和IBM PC/XT微机组成的印刷汉字识别系统上, 对二级国标6763个二号印刷体汉字进行多次识别验证, 取得了很好的结果。",
1987,汉字识别的点跟踪包含配选法,"郭宝兰,张彩录","本文提出了一种新的汉字识别方法——点跟踪包含配选法, 它是在汉字识别的包含配选法, 双重包含配选法的基础之上, 为克服其内存空间较大的缺点提出来的。包含配选法视被识别文字笔划（或空白）包含标准辞书的程度, 以包含度为判决准则, 来判定文字的所属, 它的标准辞书是一全域（或半域）模板, 需要较大的内存空间。点跟踪包含配选法的标准辞书, 只存贮文字抽样范围内一些特征点以及它们的关系属性, 用关系文法描述为一单支关系树。采用这种识别方法, 可以压缩内存空间, 有利于提高正确识别率和识别速度。按上述想法完成的汉字识别模拟系统经实验, 得到了满意的结果。",
1987,汉字的属性关系图描述及一种基于这种描述的偏旁抽取方法,"董弘,丁晓青,吴佑寿","本文通过对汉字结构特点的分析, 给出了一种稳定的以汉字识别为目的的汉字的属性关系图表示。这种表示在汉字结构描述的同时引入属性, 从而使描述更加完整, 也使结构分析大为简化。基于这种表示, 本文给出了一种汉字偏旁抽取方法。这种方法把偏旁抽取归结为从整个汉字属性关系图表示中求取同构子图或子图匹配, 在求取同构子图或子图匹配时充分利用了反映汉字结构特点的属性, 从而使计算量大大减少。",
1987,汉字的自动笔划分析,"何厚存,盛焕烨",本文提出一种以汉字的“笔划”来描述汉字以及对汉字进行自动分析的方法。它是以结构分析（句法分析）为基础的汉字识别过程中非常重要的一步。,
1987,线状图形的SLSA法提取,"顾新理,汪燮华","在汉字的结构模式识别中, 对汉字进行笔划间的分割、提取和表示是必不可少的。评价这些方法的优劣常基于这两个标准：第一, 笔划提取的正确性和表示的合理性, 其次是得到这个结果的时间。在手写体汉字识别中, 由于汉字模式经常受到书写者的不同及书写条件等因素的影响, 使得书写同一笔划的形状各不相同, 甚至产生变形。这里我们提出一种称为SLSA(Straight-Line Sequence Approximation)法来提取汉字笔划, 它能大大减少提取笔划所需的时问, 并且不产生变形。最后结果用Freeman代码来表示。这种方法已用于在IBM-PC/XT机上实现的手写体汉字识别模拟系统中, 效果极为满意。",
1987,汉字笔划的微结构分析,"路浩如,杨源远,李璇","本文提出了一种汉字微结构分析原理作为笔划提取的策略基础。微结构是由汉字边缘点在指定的四个基本方向上所引的最长“纤维”构成的编结组织。它包含了汉字结构的信息, 特别是笔划段及其主方向的信息。从微结构分析可以鉴别结构纤维的主次和真伪, 固而能够容易地滤除微结构中的各类噪声。微结构分析原理的充分有效性在汉字处理中得到证实。",
1987,用于手写印刷体汉字识别的二维扩展属性文法,赵明,"计算机汉字字符识别（CCR)作为一种汉字输入手段, 在中国、日本、东南亚等使用大量汉字的国家和地区, 一直受到高度的重视。随着近年来计算机中文文字处理能力的增强和中文输出技术的进步, 也相应地要求输入技术取得新的突破。另一方面, 硬件技术的发展正在给CCR提供越来越多的支持。现在可以说, 解决问题的关键几乎全在于软件方面, 而其中最主耍的又在于能否提出更有效的方法。",
1987,编者话,,"汉字识别在我国的研究已有十年历史。十年来, 研究内容逐渐深入, 研究领域不断扩大, 研究队伍日渐增加, 到目前, 已经取得一些可喜的初步成果。可以说, 我国汉字识别的研究就要摆脱摸索阶段, 开始向实用系统研制的道路前进了。",
1988,构造MPHF的改进回溯方法,周有文,"为了给大型关键字集合构造MPHF, 本文在现有工作的基础上, 对回溯过程作了多处改进。提出了限制阈值增长和相对回溯距离的回溯控制算法:该算法的策略是, 为了限制阈值的过快增长, 回溯点宁可舍近求远：在不引起阈值增长情况下, 尽可能选择序号大的回溯点；当计算点序号大于某额定值conp后, 确保所选回溯点序号不小于某额定值conv 。在建立MPHF的数学描述并证明有关越界定理后, 提出了一系列回溯越界处理方法。本文所述改进回溯方法已在微机上用PASCAL语言实现, 其大量计算结果表明, 所提出的理论是正确的, 方法是行之有效的。",
1988,中文语料库的表格式查询语言,"徐赤裔,何克抗","本文提出了一种便于对中文语料序中的各种词条进行查询和检索的表格式查询语言；介绍了这种语言的形式定义、对各种关系运算的描述方式以及对语料库进行实际查询检索的应用实例。实验结果表明, 这种语言简单、实用而又具有较强的功能。",
1988,中国古文字信息系统CAW—DOS的设计,钱培德,"本文论述了中国古文字信息处理系统CAW一DOS的设计过程。该系统以IBM PC/XT微型计算机为支撑环境, 能分别地或混合地对现代汉字和多种古文字信息进行处理。文章介绍了CAW一DOS的内码方案和输入码方案, 给出了CAW一BIOS中输入模块和输出模块的结构及主要算法。",
1988,汉字代码体系,黄忠置,"本文在分析现有汉字代码体系的基础上, 提出汉字编码的目标, 实现编码目标应遵循的原则, 并令析了一种实现编码目标的代码体系1一2码方素。",
1988,类声母类韵母概念应用于孤立汉字语音识别,"程俊,易克初,文成义,胡征","本文提出了类声母类韵母概念, 并将两段码书的矢量量化技术应用于孤立汉字语音识别系统。考虑到类圣母与类韵母的短时谱属两种不同类型, 各采用不同的帧长和帧间隔进行分析, 相应生成类声母段码书和类韵母段码书用以压缩数据量。理论分析和实验表明, 该方法优于通用码书识别方法, 使系统性能有明显改善, 而且计算代价下降, 便于进一步扩大大词汇量。用这一思想构成字表为16个单音的识别系统, 特定人语音识利精度达98%以上, 非特定人语音识利精度为91.3%",
1988,建立维汉文处理系统的若干问题与处理,"冯家宁,张文江","本文讨论建立兼用英汉文与维文及以阿拉伯文字为基础的文字处理系统的一些基本问题, 包括健盘编码、字母内码及其在字符设备上的代码的区别、字符排列方式、I/O控制及高层软件兼容性。希望有助于计算机处理少数民族文字的深入开发。#br#在我国的少数民族文字中以阿拉伯文字为基础的就有维吾尔、哈萨克、柯尔克孜等好几种。这些拼音文字的共同特点是它们的读写顺序与英汉文相反—从右向左, 但其中的数字又是从左向右同一字母在词头、词中、词尾和独立情况下的写法以下我们称为字符又有不同, 甚至不同的字母也有不同的表示宽度。国内外对这些文字与英文或汉字的混合处理系统都已有所研完并在不断完善。结合我们自己的工作与所掌握的其他材料, 主要有如下四个方面的考虑。由于相似性, 为叙述方便, 我们就主要以维文为例, 对基本系统来说不同文字的区别只是字形及其数目",
1988,科研动态管理应用系统的设计与实践,刘怡芬,"管理是一门科学, 它的目标是研究如何使人、财、物、信息资源处于最佳运用状态, 以充分发挥其效益。本文着重从理论和实践上论述依据信息系统工程理论, 把管理目标剖析为各类实测性指标信息, 利用数据库管理系统和微机中文信息处理技术, 实现动态管理的设计思想及系统功能的简介。",
1988,发展我国语言工程产业的纽带——中文信息学会自然语言处理专业委员会简介,王广义,"近年来, 我们专委的学术研讨活动几乎涉及到自然语言处理的各个领域。例如机器翻译、自然语言理解、言语分析与合成、言语数据统计与分析、计算机辅助教学等。在历次学术研讨会上, 国内从事语言工程项目的专家、学者发表大量的具有较高水平的学术论文、系统研制成果报告、介绍了国内外最新的语序信息处理, 特别是机器翻译的新理论、新方法及发展趋势。这些卓有成效的学术交流, 对推动我国语言信息处理系统的研制及其应用、产业化转化发展方向么，起了积极的作用。",
1987,ECT-2英汉机器翻译系统的一些特点,王广义,"ECT-2英汉机器翻译系统, 是中国社会科学院语言所与天津计算机研究所签约研制的应用型机器翻译系统, 其产品定名为《天语》（以下简称《天语》系统）。",
1987,论汉字内码与汉字数据类型,张轴材,"汉字化是NLS（National Language Support）潮流中的重要组成部分。汉字化的中心任务是以汉字内码为基础, 树立汉字数据类型。本文提出了理想汉字内码的若干特征, 并以此为基础，对主要的实用汉子的内码进行了分析比较, 进而阐明了汉字内码与汉字数据类型的区别与联系, 指出了汉字数据类型与相应操作的匹配问题。",
1987,信息处理用规则汉语,"鲁川,梁镇韩","由于电子计算机和通信卫星的问世, 人类开始了从工业化社会向信息化社会的过渡。我国从1987年开始实施的《高技术研究发展纲要》规定要组织精干力量不失时机地研究和开发的高技术之一就是信息技术。",
1987,中文叙词表系统的设计与分析,"庄牧彬,苏东庄","本文通过对中文叙词表结构的分析并用抽象代数为工具进行数学推导, 给出了一种叙词表结构的形式化描述, 指出可利用叙词表自身的内在关系将一个大的叙词表（集）划分成若干个彼此独立的小叙词集。并以此为基础提出了一种用于检验叙词表结构正确性的多值关系矩阵算法。最后简要论述了计算机处理中文叙词表的建表和维护以及系统的实现等问题。",
1987,汉语言的几个统计规律,"王德进,张社英,刘源","本文在现代汉语字、词频统计的基拙上, 对汉语的一些统计规律进行了探讨, 认为字、词的序号分布都不服从Zipf分布规律, 并提出了新的概率分布公式，汉语的字和词都较好地服从于这一分布。另外, 本文还计算了汉字和词的一阶熵、词的静态和动态平均词长、以及汉语书面语的冗余度估计值等。",
1987,一种新颖实用的汉字编码及中文系统,"钱伟长,曹家麟,冯麒孙,廖大平,徐永晋","汉字宏观字形编码（简称钱码）是一种简单易学、通俗易懂的新颖计算机汉字输入法, 在1986年全国首届汉字评测活动中被评为A类方案。",
1987,中英文数据库管理系统的实现技术,"陈松乔,吴兴林","本文扼要介绍中南工业大学独立研制的IBM-PC/XT微型机中英文关系型数据库管理系统（CDBMS）的设计和实现技术。着重讨论索引技术、窗口技术和表格自动处理的理论, 提出了B'-树和带合并运算的自然连接的定义, 介绍了窗口检索和表格自动生成、处理的基本方法, 并给出了某些简要算法, 最后评价了这些算法在时向和空间方面的效率。",
1987,对维文信息处理用三项标准的探讨,吾守尔·斯拉木,"当今世界, 信息处理和交换已采用了现代化手段, 各种文字信息处理和交换的标准化已成为信息业的头等大事。维吾尔文字信息处理的标准化不仅对我区及国内信息处理事业和全国性信息处理网络和通讯有着重要的意义, 而且对国际有关国家和地区进行科学技术和文化交流有着同等重要的意义。它关系到了子孙后代, 关系到我区及国内有关部门信息处理的发展, 关系到维文信息处理的标准化、系统化、网络化及其推广应用和普及。",
1987,法汉题录机器翻译试验,乔毅,"本方案包括词典分析和语法分析两大组成部分, 其间的关系可以既有界限又可以互相穿插。因此, 这就要求词典应当扩大分类信息的容量, 其中包括：语法信息、语义信息和转换信息。",
1987,具有连续变倍能力的高质量汉字生成技术,"周根林,黄宜华,唐琦","本文介绍了在通用微机上实观的具有连续变倍能力的高质量汉字字形生成技术, 叙述了可变倍文字库的自动建立及点阵恢复算法。已完成的结果表明, 文字库容量小, 变倍后保真度高, 生成速度快。该字形生成系统已纳入文书排版编辑软件, 由此构成的高级汉字办文系统及汉字轻印刷系统已在微机级实现。",
1987,GCMT—85德汉机器翻译系统简介,王承宏,"GCMT-85是一个试验型的德汉全文机器翻译系统。设计这个系统主要是为了考察德语分析中的特殊问题并找出解决的办法, 探索德汉转换的有效方式, 同时, 试图针对机器翻译规则系统编制中的某些带有普遍性的问题提出一些见解。",
1988,关于语义辞典构造的一些初步设想,"黄昌宁,陈祖舜","语言作为组织知识的结构, 是个网状结构。其词汇部分组织的是已经认识的知识, 是一种静态结构。辞典则是被某一定群休认作规范知识（所谓常识）的那个部分, 它依赖于具体语言而其内核则是不依赖具体语言的“ 概念结构”。换言之, 辞典乃是概念结构的外显形式。#br#在各种概念中, 可以区分出一组最基本、最原始的概念, 所有其余概念都可以用它们来刻划。#br#知识包括常识具有层次结构, 辞典是这种层次结构的最底层。支撑着上面各层知识结构, 形成一个完整的知识体系。#br#本工作正是按上述设想来构造一个机器用的辞典, 它显然是更大计划的一小步。",
1988,中文输入中语法分析技术的应用,俞士汶,"北京大学计算机研究所研制了一种以词和基础的中文语句输入方法。为了减少选择同特征词的麻烦, 输入方法使用了语法分析技术, 取得了一定的效果。本文叙述了方法的梗概、应用语法公式的原理以及加速语法分析的剪枝算法。",
1988,汉语自动分词及歧义组合结构的处理,"李国臣,刘开瑛,张永奎","现代汉语计算机自动分词是中文信息处理领域所特有的一个重要研究课题, 机器翻译（MT）、自然语言理解（NLU）、情报检索（IR）等许多工作都需以自动分词作为基础。#br#本文对现有的几种计算机自动分词界法进行了简要的分析和评价, 提出了一种新的分词算法—“联想—回溯法”（Association — Backhacking Method）, 简称AB法, 并对这种基于知识的分词算法进行了详细的讨论。在此基础之上, 我们着重论述了歧义组合结构的切分策略,提出了许多处理歧义结构的实用切分规则",
1988,汉语词汇的分层统计模式,刘杰,"汉语词频统计是一件耗资费力、旷以时日的工作, 其困难主要来自统计单位—词难以确定。由于汉语分词方面本身的困难, 加之统计者的目的和出发点各不相同, 分词标准在现阶段很难求得一致, 这就使得汉语词频统计成果在应用上带有较大的局限性。#br#为了使词频统计成果尽量能适应不同研究领域人员的需要, 本文建议采用分层统计的办法, 即首先对语料抽样作宽式的切分和统计, 得出中间结果, 再由不同专业的用户在此墓袖上进行再统计, 得出直接应用于专项研究的数据和成果。文章论述了分层统计模式的作法、依据和优点, 提出了制定宽式切分标准的一般原则。",
1988,汉语句法分析器SYNAC,"马六生,邢汉承","SYNAC(The Syntactic Analyzer for Chinese)是基于一定语义分析的、以定子句文法作为描述工具的汉语句法分析器, 能分析比较复杂的汉语句子。采用了填槽技术和反向推理策略。本文着重介绍SYNAC设计的基本思想和主要方法, 最后, 作了一些评价。",
1988,机器翻译中词典和文法的关系,董振东,"为机器翻译设计和研制的词典和文法, 是机译系统的核心, 是机译研究中的关健。词典和文法的关系及其处理一直是机译研究者所关心的问题。本文将从四个方面来论述它们的关系：（1）个性与共性的关系；（2）静态与动态的关系；（3）语法与语义的关系；以及（4）信息与算法的关系。这几个关系中既涉及到词典和文法客观存在的关系, 也涉及到我们对两者的主观处理的关系。",
1988,德汉机器翻译GCAT系统的设计原理和方法,冯志伟,"本文介绍了德汉机器翻译系统的基本原理和方法, 指出了在汉语生成过程中, 汉语句子各成分的词组类型、句法功能、语义关系、逻辑关系之间存在着的极为错综复杂的各种联系。汉语语法的特点, 就在于汉语句子中词组类型与句法功能之间没有明确的对应关系, 而且, 词组类型与句法功能相同的成分, 它们的语义关系或逻辑关系还可能不同。因此, 汉语的自动生成就不能仅局限于依据其词组类型, 而且还要依据句法功能、语义关系、逻辑关系, 这样才能区分同形结构, 从而生成正确的汉语句子。",
1988,日汉机器翻译中词的自动切分技术,"王启祥,王锡江,陈未竞","本文阐述NDJCMT系统中词的自动切分技术, NDJCMT是我们实现的一个日汉机器翻译实验系统, 词的自动切分是日语词素分析、句法及语义分析的基础, 是一项日本语计算机信息处理的基础性研完课题, 它涉及对语言本身的研究。日语和汉语类似, 词及词之间无分隔符, 通常假名、汉字混写, 给词的切分造成了困难。作者根据日语的特点, 提出了一种“ 句节数最少” 词的自动切分方法, 使用语言编程且在一机上获得实现。","机器翻译,自动分词,句节数最少法"
1988,编者的话,,"计算语言学是一门年轻的交叉学科, 顾名思义, 它的任务是利用计算机来对语言学进行",
1989,论汉语自动分词方法,"揭春雨,刘源,梁南元","文章简单考查了目前中文信息处理领域中已有的几种主要的汉语自动分词方法, 提出自动分词方法的结构模型ASM(d,a,m)，对各种分词方法的时间复杂度进行计算, 对于时间复杂度对分词速度的影响, 以及分词方法对分词精度的影响也进行了分析；同时指出并论证在自动分词中设立“ 切分标志”是没有意义的。",
1989,汉语拼音词汇输入处理系统,"丁天怀,汉字乐,蓝安东","本文介绍一种能减少汉语拼音输入时的同音现象及提高输入速度的处理系统。即在国际通用标准健盘上以词组为单位进行“ 盲打” 输入, 并由系统特有的处理软件对可能出现的同音字加上标记, 最后对整篇文章进行修改与编辑。试脸结果表明, 一篇文章中大约有三分之一的汉字被注上了需要检验的同音标记。但实际上真正需要修改的错别字只占全文总汉字数的七分之一, 也就是说, 大约85%的拼音音节可以一次准确地转换成汉字。修改所花的时间约为整篇文章输入时间的一半。",
1989,多语种(英法俄德日汉)数词的自动翻译,李竹,"本文介绍了一个多语种（英法俄德日汉）数词的自动翻译系统, 探讨了不同语言间数词系统的共性与差异, 证明了媒介语方法在机器翻译中的可行性。数的概念和具体的数都具有普遮性, 而每一种语言都具有自己的数词系统。不同语言间数词的共性是翻译的基础, 从共性出发, 我们可以找到一个自然的中立的表示, 并以此为媒介, 进行不同语言间数词的转换。媒介语方法是进行多语可逆翻译的最佳途径。本系统通过对不同语言数词结构的分析, 选阿拉伯数字为媒介语, 从而简化了翻译过程, 实现了多语间数词的准确翻译。",
1989,简易日汉兼容系统MCDOS的设计与应用,"朱学锋,朱万森,俞士汶","根据对中日两国汉字的编码体系及字形的调查, 论述了用中国汉字字模作为日本汉字模的代用品的可行性。将CCDOS稍加改造, 使之成为一种简易的日汉兼容系统MCDOS, 为日汉机器翻译系统的研究提供了一个接近实际的环境。已经利用MCDOS开发了日汉机器词典, 并解决了MCDOS和科学院计算中心开发",
1989,VAX/VMS操作系统的汉化,杨国纬,"本文介绍采用高位为1的国标码作为内码对VAX/VMS操作系统进行全面、彻底汉化的方案, 从分析VAX/VMS系统结构开始, 着重介绍汉化的起步与方法, 以及汉化工作的主要内容。",
1989,汉字结构属性库系统,樊建平,"这个属性库系统主要面向字形设计及结构方法的汉字模式识利。同时具有一般属性库系统的一些功能。系统全部采用菜单式的用户界面, 用户可以根据",
1989,一类表格图象的分割及处理,"高章舜,朱志莹","在哈工大开发的传真机一微机报表识别处理工作站中, 以传真机为图象输入装置, 本文提出了一个表格图象分割用数学模型, 并按这个数学模型提取了表格图象的几何参数, 分割出了表格图象中的各数据项, 并对数据项中每一手写体数字进行了分割处理。同时, 消除了噪声。本文提出的分割算法不依赖于表格图象内部的表格线, 因而有很强的适应性。",
1989,《三声码》的优化方向,段宁华,"本文就《三声码》在全国首届编码方案评测中的表现, 并结合移植于APPLE-II型微机与IBM-PC/XT微机（长城0520）后二年多的应用情况, 探讨《三声码》的基本素质与优先方向。",
1989,WB技术在汉字编码领域的应用——H输入法,"黄喜祥,扈佩成",本文论述了汉字字词库WB技术在编码识别领域的应用、编码技术纳入WB汉字处理系统工程的原理、技术关键、主要技术指标及其综合社会效益。,
1989,托忒蒙文与维、哈、柯、汉、英文字混合兼容处理研究,吴宗尧,本文研究在IBM PC/XT及其兼容机上对左、右、竖三个方向上书写的六种文字进行处理的一般原理和方法；研究在选定的M-24微机操作系统的中断处理级和系统功能调用两级上实现左、右、竖三种屏幕编辑方式；找出屏幕映封、主/辅字符处理规则、键盘输入和屏幕转换方法；解决这些文字在不同方向上混合兼容处理、不等宽字符处理、字符编码等问题并在汉字操作系统CC-DOS上实现。,
1989,一个MS—DOS的SPOOLing系统,钱培德,"为MS-DOS操作系统设计一个专门的SPOOLing系统, 以实现数据处理和I/O处理的并发进行, 这是十分必要的。由于中文信息处理系统在I/O方面的特点, 所以这种系统更有必要配置SPOOLing系统。本文介绍一个基于MS-DOS的SPOOLing系统的设计和实现, 包括它的总体结构、用户界面、数据结构和有关模块的算法。另外还对该系统的进一步升级作了讨论。事实证明, SPOOLing系统可大大提高整个系统的效率。",
1989,利用句法结构分析的联机手写体汉字识别,"张圣训,赵晓瑜","本文提出利用句法结构分析对限制性手写体汉字进行联机识别的方法。首先, 对汉字的笔划进行编码, 平滑滤波和数据压缩等予处理；然后, 利用其结构信息, 采用模糊开关和自动机理论, 建立一种新型自动机——条件有限状态自动机, 以得出手写体汉字的笔划顺序码；最后, 利用汉字笔划的顺序码在识别字典中检索, 查找标淮汉字的机内码, 从而完成整个识别过程。",
1989,系统软件中的汉字字符数据类型及多字符集支持功能,陈明源,"本文从目前汉字系统编码混乱这一事实出发, 认为走出这一困境的根本出路, 是对计其机软件进行更进一步的改造：首先, 需要承认中国及世界上存在多个不同民族的字符集。进而, 对程序设计语言而言, 需要为汉字及其它民族文字, 增加相应的字符数据类型；对信息系统而言, 则是增加多字符集的支持能力。这种改造的后果, 应是保证汉字信息交换码与处理码的统一。",
1989,自然语言的歧义与机器翻译的对策,俞士汶,"歧义是自然语言的特征之一。开发机器翻译系统, 不仅要研究一种语言内部的歧义, 而且还要研究两种不同语言间的歧义。本文从不同的角度考察了这些歧义现象及其对机器翻译的影响, 总结了在机器翻译系统中为了得到恰当的译文所采用的几种办法, 并且提出了一种观点或者说方法, 它允许译文的歧义同原文的歧义相对应。",
1989,中国中文信息学会速记学会在京成立,,"中国中文信息学会速记学会于一九八九年三月十八日在京正式成立。这是中国中文信息学会成立的第十个专业委员会。唐亚伟、张长松为主任, 陶沙、徐培汀为副主任。委员31人,其中常务委15人。",
1988,联机手写汉字识别的理论与实践,"刘迎建,戴汝为","本文以句法模式识别的理论为指导, 以大量实践为基础, 给出在第四代计算机基础上联机手写汉字识别的形式化和工程化的方法。从汉字识别角度提出, 汉字应分成笔段、笔划、字根、字、词组五个层次, 分别用模糊属性文法对各个层次进行描述。提出启发式模板引导强制匹配方法, 并给出了对根和字的具体匹配过程。在词组的识别中, 提出对词组求距离的概念和利用上下文的粗分类方法,描述了词组的存贮结构和词组的匹配过程。利用本文的方法, 可以联机识别从楷书至行书范围的手写体汉字, 识别率的提高仅与增加各层次的模板有关, 与识别方法无关。",
1988,汉字识别中的笔段直接抽取算法研究,"周根林,曾庆凯,王绪龙","汉字信息处理领域中急待解决汉字自动阅读技术的开发。在实现结构分析法识别多字体印刷汉字时, 笔划抽取是关健所在。本文提出了从汉字点阵中直接抽取笔划特征的新算法, 省去了细化过程。与国外同类研究相比, 处理速度和正确率均有较大提高。对国标一级汉字的抽取结果是令人满意的。",
1988,多字体印刷汉字识别系统的研究,"郭东民,舒文豪","本文对多体字印刷汉字识别问题, 从文字输入、予处理、分割、特征抽取直至分类识别等几个方面进行了全面的讨论。并在此基础上, 建立了多体字印刷汉字识别系统。在该系统中, 充分考虑到多字体印刷汉字的特点, 使用了改进的笔划穿越特征, 汉字外部和内部面积等特征, 在克服噪声、字位移及异体同字的分类一致性等方面都取得了较好的效果。利用该识别软件和同一个字典, 可以识别混合在一篇文稿中的黑、宋两种字体的印别汉字。经多次试验, 平均识别率大于99%。",
1988,关于成立“中文信息处理研究所”的决定,,"北京教授讲学团第六次委员会, 讨论了成立“ 中信息处理研究所” 的请示报告, 认为",
1988,格语法与自然语言处理,张潮生,"本文讨论了在自然语言处理系统中使用格语法的优点与存在问题。由于标准不易确定, 格的使用存在不少混乱之处。格语法表达意义时所达到的深度还比较浅, 通常带有较多语言表层的痕迹。格语法还有其体局限性。尽管在理论上存在这些不足, 作为工程上使用的方法, 格语法与其他一些方法相比, 可在质量与代价之间保持校好的权衡。经过修改与扩充, 格语法可在每然语言处理领城中继续发挥作用。",
1988,一种数据代码压缩方法,孔祥柏,"随着信息系统应用的发展, 人们逐渐感觉到需找到一种有效方法来表示信息, 减少信息的冗余量。本文提出一种数据压缩方法, 并给出这种方法应用于英文及汉字信息压缩的测试结果。",
1988,面向算法领域的自然语言理解,"朱瑞香,袁晓洁","算法领域的自然语言理解主要是理解算法描述的处理及其规定的数据类型与结构等, 用于由算法自动地生成程序。在分析了各种算法书写的基础上,我们定义了一种用于描述算法的类自然语言, 并给出了理解类有然语言的方法。该方法是利用理解知识, 通过关健字的匹配实现理解的。我们还根据理解方法研制一个理解系统, 作为自动编程系统的一部分。理解系统是用PASCAL语言写成, 在IBM一PC/AT计算机上实现。本文介绍了上述内容。",
1988,计算机汉字输入的本质,黄地,"计算机汉字输入是一个极其困难的问题, 编码输入是无法根本解决的。实际上, 汉字输入是一个过程, 只有将汉字处理的各种技术作为一个整体综合进这一过程, 才能圆满地解决汉字输入。本文提出通用型汉字终端汉字输入的最高目标和设计原则。在此基础上提出一套将汉字处理的各种技术综合进汉字输入这一过程的技术以及基于知识的汉字输入系统的设计思路。以此为指导设计的汉字输入管理系统可生成所有的编码、提示和词组及它们的混合的汉字输入法。最有意义的结果是面向用户, 从而得以极大地简化汉字输入这一极复杂的问题, 有效地提高输入速度。",
1988,手写印刷体汉字部件的抽取,赵明,"本文介绍了识别手写印刷体汉字的一方法中轮廊折线跟踪、笔段抽取和基础部件抽取阶段的工作。为了克服结构方法对关键结构过于敏感和串形决策不稳定的缺点, 采用了多冗余有引导的部件抽取策略。当根据局部结构难于作出判断时, 不勉强进行判断, 而是将各种可能作为候选带到后级, 利用后阶段的结构归纳进行判断, 从而提高了对崎变的容纳能力。#br#本方法可用作结构识别方法中的粗分类方法。",
1988,首届全国计算语言学学术会议在京召开,,,
1990,机器翻译词义辨识对策,"李维,刘倬","词义辨识是机器翻译中最困难的问题之一, 不夸大地说, 它是全自动高质量机器翻译成败的关键。本文结合许多实例, 详细探讨了解决这个问题的各种方法及其相互关系, 这些方法包括成语加工、语法分析、语义分析、语境分析、代指分析、互补分析、中性词义以及一词多译等。它们的有机配合可以有效地解决绝大多数多义区分问题, 从而为机译系统的实用化奠定了基础。所有这些方法都已在JFY-IV英汉机译系统中得到广泛运用和验证。",
1990,CC-DOSV4.0显示模块的关键算法,钱培德,"本文指出了CC一DOS V4.0显示模块中的关键算法, 提出了显示模块的数学模型。文章介绍了软字库和汉卡字库的结构, 讨论了计算软字库字模地址和和汉卡字库字模地址的算法。",
1990,用于多语种机译系统的中文词典设计,"朱美英,内田裕士,袁琦","本文是中日合作机器翻译项目日方基本词典工作组朱美英女士1989年3月在日本情报处理学会自然语言研完会上发表的论文自然语言处理研究会资料,(71 一6(1989)) 。论文阐述了日本国际情报化协力中心（CICC）和中国软件技术公司(CSTC)共同开发的中文词典的设计方针及其词典结构。受作者委托由中日合作项目中方基本词典工作组袁琦先生将日文原稿翻译成中文, 并请朱美英女士对译文做审阅和校对。在此对内田先生和未美英女士表示感谢, 由于他们的支持和帮助, 这篇论文得以尽快地与国内读者见面。",
1990,基于规则的汉语自动分词系统,"姚天顺,张桂平,吴映明","本文通过对汉语自动分词难点的分析, 讨论了词频和词结合力的关系, 提出了一套机械切分与语义校正的汉语自动分词方法。系统包括建立绝对切分标志符表, 变长度最大匹配法、2一3一1优先规则集、固有歧义切分和组合歧义切分校正方法等。最后列举描述语义校正规则的实例。系统作为CETRAN.A的一部令, 在SUN8一280工作站上实现。",
1990,一个高效实用的汉字识别予处理子系统,"薛舒,欧阳北辰","汉字图象的输入和予处理是汉字识别系统的必要组成部分。本文给出了一个实用的予处理子系统, 实现了图组的扫描输入和单字切分、外围粗特征提取的并行处理, 大大提高了效率。文中还对字切分和特征提取的方法作了进一步的探讨。",
1990,计算机自动分析量词短语的方法及规则,曹敏,"本文对现代汉语量词进行了分析和总结, 从组合关系的角度分析量词在句段中的地位, 根据量词与其它词的搭配关系总结出了有关分析量词短语的语法规则。在量词中, 有一些量词本身之间是同音词, 由于它们的语法功能相同, 所以单纯用语法上下文已无法区分它们了, 必须求助于语义上下文及不同量词与名词的搭配不同来分析它们, 本文利用量词与不同语义的名词的搭配关系来区分同音量词,这些方法及规则可应用于同音词的自动识别, 汉外机器翻译等方面, 我们已在微机止用C语言实现了同音量词自动识别系统。",
1990,APPLE汉字系统设计,张民,"本文主要论述了汉字系统的设计与实现, 同时也论述了拼音码压缩、小字序及假脱机技术。",
1990,蒙语文艺节目微机管理系统总体设计与实现,"张主,嘎日迪,桑杰",本文论述了微机蒙语文艺节目管理系统的总体设计思想、系统结构、数据库及其结构以及软件设计中的技巧等。,
1990,从信息论角度探讨《红楼梦》的作者,"徐秉铮,蔡伟鸿","本文从(1)词的相关性上下文的相关性;(2)字符数的统计和（3）字符串（相当于词或词组）的统计等三方面来判断《红楼梦》前八十回和后四十回是否同一作者所用方法是LZM(Lempel一Ziv一Welch）比压缩算法。结果如下表所示：表中代表字争出现频率, 代表字符出现频率。从表中结果, 我们可以初步得出给论《红楼梦》前八十回与后四十回写作风格有明显的不同, 表现为前八十回词汇量大, 喜欢用短字符, 文章前后相关性弱。后四十回则词汇量小, 喜欢用长字串, 文章前后相关性强。从这里可以判断, 《红楼梦》很可能是由两位作者所完成。",
1990,UNIX国际化及中文应用环境,"孙玉方,杨建平,陆拓实,郑蕾","本文概述了UNIX国际化的现状, 介绍了UNIX System V国际语言扩充MNLS的功能和结构以及基于MNLS的中文应用环境CAE的功能和特点, 并对于UNIX国际化的实现思想、手段和国际标准化等提出了一些看法。",
1990,汉化技术与实用技巧的探讨——汉化FoxBASE数据库系统的几点问题,"陈镐缨,孙斌,杨立新","FoxBASE是一种新型的微机数据库管理系统, 它运行在IBM PC微型机上, 与dBASE一3数据库管理系统相比, 不仅具有工作效率高、运行速度快、功能强等特点, 而且它的命令完全与dBASE一3兼容, 后者的程序及数据库无需用户做任何修改就能在FoxBASE环境下运行。#br#由于系列数据库管理系统易学易用, 表现出了其非凡的生命力。它目前在我国已拥有大量的用户, 各单位中正在发挥作用的各种各样的信息管理系统绝大多数都是用dBASE一3开发的。但是, 随着计算机应用领域的迅速深入和扩大, 要求各种信息管理系统处理的信息量越来越大, dBASE一3在库信息量增大的情况下运行速度急剧下降的缺陷也随之越来越突出。许多富有实践经济的软件开发人员已着手寻求新的一代管理系统开发工具。事实上, 目前在微型机上运行的数据库管理系统己远非dBASE系列一种了, 并且绝大多数新型的数据库管理系统都具有较更为强大的功能, 各种性能指标均散发出诱人的信息。然而, “ 先入为主” 这一人类不成文的“ 定律” 是难以为人们的意志为转移的—众多的一用户不愿意舍弃多年来得心应手的工具, 大量的现役信息管理系统不容推翻重新开发。在许多新近涌现出的微机数据库管理系统中, 给它们带来了一线光明, 它无论从运行速度、功能扩充上讲, 还是从命令、数据的兼容而言, 都充分体现出它的优势, 完全可以充当一的换代产品。为此, 我们选定了该软件为汉化对象。经过一段时间的努力, 终于顺利完成汉化工作及用户试用阶段, 用户反映良好, 并通过了陕西省高教局鉴定。专家们认为“ 该软件与国内其他几种类似的汉化软件比较, 在运行可靠性、汉字的显示速度及屏幕的稳定性方面均处于领先地位, 在我国具有极为广阔的应用前景。”",
1990,应用DCG文法分析汉语,陈敏,"“ 把” 字句是现代汉语中一个典型而复杂的句型。“ 把” 字句的研究历来是汉语语法学界的重大研完课题。本文介绍了一个汉语句法分析器, 可以接收一般的主谓宾句, 并对“ 把” 字句的一个子集作了详细的分析, 文章重点于“ 把”字句的宾语是谓语动词的受事的情况作了详细的介绍, 并将其分成几种形式加以实现。分析器使用限定性子句文法, 是用语言写成的。分析器以句型驱动为主, 并带有一定的语义分析, 其结果是一个表示了该句子的语义和语法结构的注释树结构。",
1990,汉语计算机自动分词知识,梁南元,"汉语分词是汉语言计算机处理的一项不可缺少的工作。使用自动分词知识可以进一步提高自动切分精度, 满足高标准的需求。本文在[1][2][3]的研究基础上, 介绍了一些行之有效的自动分词知识。根据对48092 个汉字的语言材料统计结果表明统计材料分社会科学和自然科学两部分, 这些自动分词知识可以处理左右的歧义切分字段。",
1990,汉英机译研究(二)一种实用的汉语切分方法…链接表法,"周依欣,吴蔚天","本文是根据信息传递的完整性和完全性以及一个词的词义的不可叠加性或简单叠加性即转义性, 做为切分的依据。按此提出了链接表切分方法, 并采用了一种称做模式字典的辅助工具以解决某些常见的歧义问题。共应用范围十分广泛, 除能用于书面语, 也能用于口语的全文切分工作。从而迈出了汉语机器理解的第一步。",
1990,有关蒙文台式印刷系统的技术问题,"嘎口迪,张主,刘彦文,林诒洪,韩迈,王世伟,魏建平","本文主要论述了《蒙文台式印刷系统》研制过程中的有关技术问题,诸如坐标系、系统内码处理、窗口—视区对映射、视缓—视区映射、蒙文的特珠处理、字库技术等等。#br#近几年, 随着我国实行“ 对外开放, 对内搞活经济” 政策以来, 对各级机关的办公效率和办公质量都提出了新的要求。办公信息的骤增, 办公手段的落后, 办公形式的刻板和办公效率的低下, 使得实现办公自动化已成为刻不容缓的任务。特别是少数民族地区这个问题更加突出。为适应这种局面, 同时开发利用好边疆地区的信息资源, 我们联合开发了办公自动化的主要组成部分—蒙古文台式印刷系统。下面将有关技术问题介绍如下。",
1990,基于文字结构特征的快速平滑细化方法,"唐松,郭椿标,郑南宁","本文提出了一种针对文本图象的平滑细化方法, 该方法以汉字的结构特征为基础, 由平滑和细化两大部分组成。平滑方法依据近邻象素相关函数, 滤除了影响汉字结构特征的噪声。细化才法又分为快速边缘检测算法和内外边缘同步细化算法, 该算法的显著特点, 是利用汉字边缘特征, 状态字和连通图指导快速细化。同时采用判别树形式使条件判断所需时间最少。该处理方法失真小, 速度快。",
1989,试论基于RAM的汉库字结构,钱培德,"汉字库是汉字信息处理系统中的一种重要数据结构, 它的结构设计得是否合理, 会直接影响到整个系统的性能。本文对基于的RAM的静态型汉字库和动态型汉字库的结构, 分别作了详尽的论述。文章还提出了描述基于RAM的汉字库性能的方法, 并讨论了与汉字库结构有关的算法。",
1989,中文科技术语中的歧义结构及其判定方法,冯志伟,"本文是作者最近提出的“ 潜在歧义论” 的第二部分, 说明了在中文科技术语的PT-结构实例化的过程中, 可以产生四种不同的树形结构：无歧义结构、歧义消除结构、歧义结构和非法结构；文中提出了不同的转换鉴定式来判别这些不同的歧义结构。",
1989,中文信息处理系统的性能准则——内部码、汉化程度及其它,"戴瀛洲,李成国","计算机科学技术是当代信息社会进步的有力杠杆。这一高科技产品的发展之快, 使得最古老的象形文字之一——汉字的计算机处理研究也具有了世界的规模。中国等国都在大最生产具有中文信息处理能力的计算机, 仅在国内, 就有超过50种以上的计算机系统和500种以上的汉字输入方案。这些系统各有特色并被广泛使用, 然而并不统一。另一方面,ISO/IEC、JTCI/SC2倡导的关于NLS(National Language Support)标准化的研究已在多8位信息编码体系方面取得重要进展, 具有统一字符集合、不同民族和文字的信息共享的计算机系统正在成为现实。基于上叙考虑, 中文信息处理的标准化研究既是信息技术方面的重要基础工作, 也是中文计算机开发应用实际迫切的需要。",
1989,一种基于语义和句法的书面汉语分析系统的研究与实现,"许亚因,吴佑寿,葛成辉,丁晓青","本文提出一种中心词驱动的书面汉语分析方法。把汉语句子看成中心词和它的附属成分组成的递归结构。利用中心词和附属成分之间的语义联系与约束关系, 对句子进行语义-句法分析, 找出其里层结构, 从而得到句子意义的机内表达。",
1989,用加权最小二乘估计LPC分析和识别不送气塞辅音,"丁公元,严普强","本文提出加权最小二乘估计LPC算法, 并用于分析、识别汉语普通话不送气塞辅音/b,d,g/。观察表明, 用加权最小二乘估计LPC分析塞音爆破起始谱的关于不同发音部位总体谱特征, 比较接近声学理论所预言的模式。另外, 做了用单帧爆破起始谱区分不同发音部位塞音的实验, 实验包括特定人识别和不定人识别两部分。结果表明, 对特定人, 用加权最小二乘估计LPC和常规LPC相结合时, 识别结果比单独使用LPC时提高6%以件, 达96.8%，而三人“ 留一人在外” 的不定人识别结果则表明, 用加权最小二乘估成LPC比常规LPC识别率提高3.1%, 而同时用二者作特征时, 识别率比单独用LPC提高约11%, 达89.6%。",
1989,计算机自然语言处理,"占传兴,顾新理","系统的有效性取决于系统中包含的智能化程度, 如视觉、听觉、自然语言处理及对话系统等。一个有效的系统应能模拟人类的问题回答和推理过程,并能分析人们对问题的回答。本文介绍一个简单的自然语言接口NLI(National Language Interface), 它能接受、分析用户用自然语言的输入。使用PROLOG语言, 我们正在IBM-PC机上实现这个系统。","选择主题,标识,句法分析,语意分析"
1989,信息处理交换用蒙古文系列标准的制定原则、方法和技巧,"嘎日迪,张主,桑杰",本文主要论述了《信息处理交换用蒙古文七位和八位编码图形字符集》（GB8045-87）、《信息处理交换用蒙古文字符集键盘的字母区布局》（GB8046-87）、《信息处理交换用蒙古文16X12、16X8、16X4点阵字模集和数据集》(GB7422.1-2-87)、《信息处理用蒙古文24点阵字模集及数据集》等系列标准的制定原则、方法、技巧等内容。,
1989,1989年自然语言处理学术研讨会纪要,,"中国中文信息学会自然语言处理专委, 于1989年5月17日至19日在京举行了《1989年自然语言处理学术研讨会》。参加会议的有来自全国各地的科研机构及高等院校的代表50多人（开幕式报到人数79人）。会议收到学术论文、报告共25篇。内容包括机器翻译、人工智能以及汉语信息处理等诸多领域。其中有关机器翻泽的论文占了一半以上。这反应出我国机器翻译近几年又有了新的发展。有关汉语信息处理的学术研究也呈现出空前活跃的新局面。",
1990,智能汉字字形设计技术及一个试验性系统ICCDS,樊建平,"本文介绍计算机用于设计高质量汉字字形、特别是汉字字模字稿的研究结果包括“ 基于汉字结构码量化传统书法知识的方法” 的介绍以及用该方法解决合体汉字的自动设计问题为解决实现“ 参量图形学方法” 并且具有直观字形修改功能的字形设计系统所遇到的困难, 提出了“ 受限参量图形学方法” 以及“ 字形动态分解概念。” 同时给出实现这些方法及概念的试验性系统的总体框架及各组成部分的功能说明。",
1990,智能型计算机辅助汉语教学系统,"何克抗,李秀兰","本文提出了智能型计算机辅助汉语教学系统的设计与实现方法。讨论了该系统中的领域知识表示、教学知识表示、学生模型的建造以及教学决策等问题。提出了新领的适合于汉语领域知识表示的“ 网络关系给构” 表示法, 根据模糊教学理论建立了可用于对学生能力作出综合评价和进行教学决策的“ 模糊逻辑”的模型。",
1990,汉语句子描述中的复杂特征,冯志伟,"本文说明了我国学者提出的“ 多值标记函教” 与国外学者提出的“ 复杂特征” 在实质上是一致的, 而汉语句于的自动处理特别需要采用这种“ 复杂特征”来进行描述。文章具体分析了用“ 复杂特征” 描述汉语句子的必要性, 提出了描述汉语的“ 特征值” 系统以及静态特征和动态特征的概念。",
1990,一种快速汉字文本回显算法,"蒋沪生,陈泳章","本文在认真研究了英文字处理系统中回显算法的设计思想后, 针对中文信息处理的特点提出了一种新的汉字文本回显算法, 该其法以当前屏幕显示文本和修改后新的需显示的文本为已知条件, 得到一组以最优方式完成回显工作的编辑操作序列, 从而提高了汉字文本回显速度, 改善了中文字处理系统的性能。",
1990,中文词输入的自适应系统,蒋贤春,"中文输入是计算机中文化中最基本的问题之一。本文将要介绍一种用于中文输入的自适应系统, 该系统具有两个最基本的特点第一, 系统提供一种简学的中文编码, 使用者不用任何学习就能掌握输入方法, 第二, 系统具有自我适应的功能。通过系统内部的决策机制, 大量的重码被压缩。",
1990,汉字输入中的重码自动区分理论,"王锡龙,黄希琛,邹志刚","本文在讨论了当前汉字输入中的机助区分重码方式的基础上, 给出了汉字机内有动处理重码方式, 引出了双字自动处理重码方式的两个命题, 并证明其正确性。并以此扩展到单字和多字自动处理重码方式, 给出了算法, 从而解决了汉字输入中的重码自动区分问题。在《长光汉字智能输入系统》中, 重码区分效率达到。",
1990,汉字形母输入的探讨,杨国纬,"本文为拼形类汉字输入方案的研制工作提出了一些原则, 在研究过程中如始终自觉地遵循这些原则, 将有助于加快研制过程和提高所研制输入方案的性能。提出了建立汉字字形字母的问题, 并进行了有关的讨论。最后简单介绍字母法汉字输入方案, 它是在本文所述的原则指导下, 进行汉字形母输入方案研制工作的一次尝试。#br#在本文中提出的一些原则, 主要是针对拼形类的汉字输入方案为对象的。其中、有的部分虽然过去也被有的研制者零星地采用过, 但由于不是全面地、有意识地、有约束力地贯穿在整个研制过程中, 因而效果也较为有限。如果在研制过程中全面地遵循这些原则, 将大大有助于进一步提高拼形类输入方案的易学易记性能。",
1990,多字体多字号印刷汉字识别方法的研究,"崔国伟,舒文豪,李仲荣","本文对多体多字号印别汉字识别的方法进行了研究, 本文提出的方法是首先对不同字号印刷 汉字进行归一化处理, 再抽取汉字四周笔端数特征、改进粗外围特征、笔划穿插次数特征和投影变换特征, 然后对组合特征进行多级分类识别。实验在IBM一PC AT 微型机上进行, 结果表明, 实验系统在识别实际印别文本时识别率大于98%。",
1990,中文信息处理在中国的发展,"苏东庄,袁琦","我国在1958年研制成功第一台大型电子计算机。之后, 从中俄机器翻译着手, 开始了中文信息处理的研究。到1975年, 围绕中文文字排版系统和汉字编码输人开始了中文信息处理的理论研究和实用开发。1978年以来, 不论是在理论研究和实用技术、实用系统的研究开发方面都取得了巨大的进展, 从各个方面推动了计算机在中国的应用和推广。",
1989,JFY-IV机器翻译系统概要,"刘倬,傅爱平,李维",全面介绍大型实用外汉机译系统JFY-IV的总体设计思想、语言学理论基拙、算法设计策略以及上机实验结果。,
1989,汉语中的兼类词、同形词类组及其处理策略,"孙茂松,黄昌宁","本文从计算语言的角度, 系统地总结汉语中的歧义现象——兼类词和同形词类组, 对之进行了比较深入的研究，并且结合汉语自动句法分析, 给出了相应的处理策略。",
1989,基于版面分析的文本管理系统,"王玉,张炘中,苏东庄","本文介绍了基于版面分析的文本管理系统的建立。文章详细叙述了版面分析的原理, 提出一套以知识为基础的判定规则及建立在这些判定规则之上的版面分析算法。",
1989,“信息处理用现代汉语分词规范”的若干问题探讨,揭春雨,"汉语信息处理技术的重点已由单个字符处理过渡到词处理和句子处理, 过渡的基点是分词, 一个统一的分词规范国家标准对于众多信息处理系统之间的兼容性具有不言而喻的重要意义。目前, “信息处理用现代汉语分词规范（国家标准）”正在制订和审定。本文讨论了几个与此相关的问题, 分析了汉语信息处理用词的特点, 对“分词规范”和汉语拼音正词法作了比较, 对“分词规范”和民族语用心理习惯的关系也进行了探讨, 在此基础上, 作者对“分词规范”提出了一些个人意见和建议。",
1989,书面汉语自动分词的“生成——测试”方法,黄祥喜,"词链现象是书面汉语自动分词的困难所在, 本文针对词链现象的复杂性, 提出了一种“生成—测试”分词法。这种方法以知识为基袖, 它通过词典的动态化、分词知识的分布化、分词系统和句法语义系统的协同工作等手段实现了词链的有效切分和汉语句子切分与理解（生成格结构）的并行。“生成—测试”方法反映了人的分词和理解过程。",
1989,利用句法、语义循环递归网络实现汉语拼音→汉字转换,"汤建华,徐近霈","本文论述了利用语言学知识对汉语语音识别进行后处理的研究, 通过对汉语句型及语言学特点的综合分析, 结合汉语语音识别中拼音→汉字转换的具体问题, 提出了一种适合于任意现代化汉语句型的句法, 语义循环递归网络分析方法、并用C语言在IBM-PC/XT机上实现, 在所建立的有8000词库的拼音→汉字转换系统中使用该方法对《汉语五百句》中的所有句型进行分析得出满意效果, 对汉语同音词的正确辨析率达96%。",
1989,计算机辅助编制机器辞典,朱学锋,"本文总结了笔者开发用于自动翻译和自然语言处理的机器辞典的实践经验。利用软件技术, 建立了一个良好的界面, 可以方便地录入日文汉字与假名、可以大大压缩录入的信息量、可以自动生成日语词汇的某些词法信息、可以自动生成汉语词汇的检索特征, 从而提高了辞典管理的自动化程度, 取得了良好的效果。",
1989,中国中文信息学会基础理论专业委员会全国第三届汉字及汉语语音识别学术会议记要,,中国中文信息学会基础理论专业委员会主办的第三届全国汉字及汉语语音识别学术会议于1989年9月17日至21日在江西省九江市举行。,
1990,中文词的自动办理,"王永成,苏海菊,莫燕","本文综述了中文词处理方面的进展, 并系统地科学地提出了最新的分词算法思想利用切割标志, 将中文切割成一些词段再用词典对分割出的词段进行匹配抽词恰当地处理切分与抽词后留下的字串：应用“ 解答树” 及若干介词原则及切分规则或具体的分词知识以解决不同形式切分及其人工选择问题。",
1990,键位相关速度当量的研究,"陈一凡,张鹿,周志农","本文从二百多万个实验数据中统计分析出通用小键盘连续击键键位相关速度当量矩阵。键位相关速度当量表是优化汉字键盘输入键位设计的人机工程基础据, 也为自动浏定汉字键盘输入方法速度素质提供了科学依据。本文还介绍了采集数上述数据的实验设计原理和数据处理所采用的方法。","键位相关速度当量,最小值归一,时间相对"
1990,普通话音节间的协同调音现象及其合成模拟,杨顺安,"自然语流中, 音节间显现出复杂的协同调音现象。如果忽视这种现象,语音合成的输出语句是不会自然的, 要开发连续语的或不认人的语青识系统, 也是不太可能获得成功的。本文首先通过对大量双音节的语谱图的观测和分析, 归纳出十种类型的音节音联的协同调音规律, 利用“ 普通话语音合成软件” , 增添了协同调音规则, 初步合成出比较清晰和自然的多音节词语。",
1990,用非精确结构匹配法识别手写印刷体汉字,"吴智彪,夏莹,孙承鉴","本文为手写印刷体汉字识别提供了一种新的解决方法。在研究过程中, 从汉字图象的输入到识别结果的获取, 建立了一整套基本完整的识别实验系统。系统选择四边形状特征作为粗分类的基本特征, 提出汉字最稳定的结构是笔划段之间相对位置关系的思想。在粗分类时引入集合运算, 提高了粗分类的正确率和分类能力, 在细分时用快速合并笔划段的方法获取汉字笔划段作为细分特征。最后对于关系结构图的匹配提出了一种新的匹配方法一相关属性关系图启发式匹配,这种方法利用了汉字样本知识, 建立具有相关属性的关系图, 在其指导下, 完成非精确的结构匹配, 该系统在386微机上用汇编语言实现, 对1千个手写常用汉字识别率达百90%以上, 速度是每字2秒。",
1990,可移植的汉语人—机接口系统的研究与实现,"许志端,李堂秋,蔡经球","可移植汉语人一机接口是我国界当前研究的热点之一。本文通过分析汉语的特点, 对在微机上建造中小型应用系统的可移植汉语人一机接口, 提出了一种切实可行的实现方法, 文中还介绍了反用这一方法实现的实验系统的具体结构。",
1990,基于汉字结构码量化传统书法规则知识方法的实现,樊建平,"在〔1,2〕中我们提出基于汉字结构码量化传统书法规则知识方法来完成合体汉字字形的自动设计问题。本文详细介绍该方法的具体实现技术。主要包括整个问题模型建立、结构码树、总体控制过程、初始空间划分、修改算法设计、字形测试项设笠及相应的评价函数以及控制策略等问题。",
1990,采用Fuzzy ISODATA聚类方法识别语音,王迎庆,"本文利用Fuzzy ISODATA聚类分析的方法进行语音识别。此方法是把训练模式的样本进行分组Fuzzy ISODATA聚类分析, 以求得样本的聚类中心, 再把待识别语音与采类中心作隶属程度运算, 用最大隶属度原则识别语音。文中还对几种模糊采类方法用于语音识别作了比较。",
1990,点阵汉字矢量化技术,"赵志远,崔先国,张明金","点阵汉字矢量化是汉字汉息处理中的一个重要问题。本文提出了点阵汉字矢量化的类型步骤及方法, 并着重讨论了空心字矢量化中笔划整形和笔划光滑二个影响汉字效果的重要问题的处理方法, 还给出了算法实例。采用点阵汉字矢量化技术, 作者在IBM一PC机上开发了绘图仪汉字系统, 效果较好。",
1991,语义表达的一些性质,张潮生,"本文简略地讨论自然语言处理系统中的语义表达的一些问题, 内容涉及语义表达的性质、概念的分解、语义原语的选择、不同种类概念的相互制约、格形式表达的问题, 等等。",
1991,键位分布合理指数与动态平均码长综合指标的自动测定,"陈一凡,张鹿","本文引进速度码长概念作为测定汉字键盘输入系统速度素质的客观指标, 以解决当前汉字字、词输入、软件智能处理的输入系统对评佑技术提出的新课题。对速度素质的计算机自动测定摆脱了由操作员作竞赛性测定人为素质的干扰,为汉字键盘输入评测提出新的理论依据和提供了测试软件。","键位相关速度当量,输入动态平均码长,输入速度码长,键位分布合理指数"
1991,字符集的序性,许寿椿,"国际标准ISO/DP 10646中, 把文字规定为用于书写语言的图形字符的完备集。本文作者认为, 这个完备集应该是有序完备集。词典中的词以及其它类型的字符串习惯上总按确定的顺序排列。本文讨论了英文、拉丁壮文、欧洲拉丁字母系文字、蒙文、阿拉伯文和维吾尔文、朝鲜文的序性。讨论着重于编码字符串的序性与传统词典顺序的一致性。不幸的是, 除英文外, 前述的许多文字缺少这种一致性。字符集的序性在许多编码标准中被忽视了。实际上, 除藏文外的文字, 编码字符串与传统词典序的一致性大多可以通过合理编码获得。",
1991,汉字字形编辑器,樊建平,本文介绍一个计算机辅助的字形设计系统——“汉字字形编辑器”（以下称“ 字形编辑器” ）的实现动机、功能、实现技术以及其支持的受限参量图形学方法、动态字形分解概念。,
1991,汉字词组的快速排序研究,"张钟澍,全大克","本文提出的按汉字笔划权值为序对汉字词组的排序方法不仅有很快的运算速度, 而且内存开销较少。文中详细介绍了汉字笔划权值的转换方法以及用汇编语言实现的技术要点, 并给出了改进的桶排序算法描述以及一些汉字词组的排序实验结果。",
1991,对汉字字形规律的再认识,王力德,"本文从汉字编码角度出发,对汉字字形结构进行了分析,通过与西文文字结构层次的比较,在汉字的字根和笔画之间发现了一个新的层次一一连通图,并运用图论的基本原理对汉字字形规律进行重新认识, 将汉字看做连通图的集合, 以连通图做为构成汉字的字形元件——“形元”,以“形元”编码, 分类科学, 简单易学, 记忆量少。这种思想是对汉字字形的结构的一种新的认识, 可以用于汉字编码和手写汉字识别。汉字字形编码（包括部件,字根,角形等）难学难记的问题迄今未能解决得很好,为彻底解决汉字编码向全民族普及的问题,笔者认为有必要跳出传统观念, 对汉字字形换一个角度重新认识。",
1991,计算机用汉字字号规格的理论分析与计算,"林川,叶世融","本文考虑了一些事例：铅活字的字号规格使铅字适应了活字排版的工艺方式, 照相排字的大小规格也同样如此。故作者认为计算机排版系统使用的汉字尺寸规格也必须适应计算机的排版工艺。由进一步的计算, 两种不同的字号计算方法得到了相同的一组计算机用字号数据。",
1991,面向整个华语社区的1K级汉字键盘的设计原则,许家梁,"利用键盘, 通过手动, 向计算机输入中文信息, 是中文输入的基本方式。键盘的设计、输入方法的设计是中文信息处理标准化的基本课题之一。",
1991,一种用于话句识别系统的文法分析技术,林道发,"在实用语音识别系统中, 为了尽可能提高识别准确度, 可以借助词语间的约束关系来辅助话句的判识。本文以直观方式介绍一种适用于中等词汇量、特定主题、分离词话句识别的文法分析技术。最后简要介绍了我们在语音识别系统中应用这个技术的情况。",
1991,关于汉字识别粗分类并行算法的研究,"李侠民,刘小林","利用汉字外围轮廓特征进行粗分类是汉字识别中有效的粗分类技术,为了提取一个汉字的外围轮廓特征, 本文提出了两种VLSI并行算法和结构：一个是用树网实现的并行算法；另一个是带有树结构的线性脉动阵列, 同时分析了该两种算法的时间复杂度。",
1991,试论键盘管理模块标准,吕强,"中文信息输入技术的多样性和用户需求的复杂性, 要求具备中文信息输入能力的键盘管理模块具有便于扩充和修改的特点。本文提出了一种键盘管理模块标准结构, 能满足上述要求。它具有内外一致、高低一致的特点。",
1991,条形码与中文信息处理,"张凡,徐伟中,任煜,崔玉华,杨建锋","八十年代, 条形码在许多领域得到应用。作为一种可印制的二进制语言, 条形码也可用于中文信息处理。条形码汉字输入系统是依据汉字的国标码设计的, 与一般的整字输入方法相比, 有很多优点。它比较灵活, 用户可随意安排字模的位置, 因而适合专业打字员及排字工, 而且不要求掌握计算机知识。另外, 条形码系统还有高的可靠性和较低的价格。条形码技术也可作为中文信息的传输媒介,起到网络的作月。这方面有必要进一步研究。",
1991,联想功能在APPLE上的实现,张民,"本文主要论述了联想输入, 联想字典的结构与调度, 同时也介绍了驱动器常开方法。",
1991,基于连接关系的汉语词典信息的推断,"朱美英,内田裕士,俞士汶",日本和印度几西亚、泰国、中国、马来西亚正在合作开发多国语言机器翻译系统。本文介绍这个项目中的正在研究开发的汉语询典开发辅助系统的词典信息推断功能。,
1991,一个英语单词的格式化算法,"陈震杰,闵珍晖,夏振华","本文介绍了一个用于自然语言处理系统的英语单词的格式化处理算法。算法以词词缀切分策略为基础, 目的是减少系统机器字典的冗余信息、提高系统的识词能力。在附录中给出了一个用Turbo-Prolog语言实现的格式化程序。",
1991,An Algorithm to Format English Words,,,
1991,《中文信息学报》第二届编委会第一次会议纪要,,"《中文信息学报》第二届编委会第一次会议于一九九一年三月二十八日在北京信息工程学院召开。这届编委会由23人组成，实到18人, 因公因事请假10。",
1991,汉语组合类型语法,"翟成祥,王岩冰,张家重,徐家福","组合类型语法是从计算机处理汉语的角度, 在范畴语法的基拙上提出的一种汉语的形式语法理论。本文介绍了其基本思想及所建立的汉语语法体系, 并给出了一些描述实例。",
1991,信息处理用现代汉语语义分析的理论与方法,张普,本文论述了语义知识的获取在汉语信息处理的句处理阶段所处的关键地位。指出语义知识与句法知识、语用知识的获取已使汉语信息处理在知识工程和智能化系统的开发方面进入实质性阶段。,
1991,制订《信息处理用现代汉语常用词词表》的原则与问题的讨论,"梁南元,刘源,沈旭昆,谭强,杨铁鹰","本文讨论了《信息处理用现代汉语常用词表》(以下简称《常用词表》)的制订方法。提出按照《信息处理用现代汉语分词规范》, 以定量原则为主,定性原则为辅的原则进行选词, 《常用词表》首次提出选词函数的术语, 并创造性地使用两个不同选词函数共同选词, 使所选词条均匀分布性更好：以定量为原则的收词方法客观真实地反映了社会实际用词的规律, 尽可能地避免了传统主观方法建立词典时的不足；采用联想的定性方法做为定量标准的补充, 使《常用词表》中词条更加完整避免和减小了在词频统计中由于分类、选材、抽样、分词等引起的背景干扰。《常用词表》收词规范、收词频率高、覆盖率高, 为“现代”各个时期、各个专业所通用。经验证, 覆盖率在98.5%以上。",
1991,自然语言理解中的音字流自动分词,"王晓龙,王开铸,白小华","本文讨论了自然语言理解中的语音流和文字流的自动分词问题；构造了汉语理解的层次化模型；提出了把反馈信息限定为最简形式从而使分词层与语义无关的思想以及词串排序的三种策略：按可能性大小排序, 按运算时间长短排序,以及上述两种的综合；介绍了一种分词精度极高的分词方法FWF；并且给出了实现算法和实验结果。FWF分词方法已经在语句级键盘输入、声音输入、手写汉字输入系统上使用。",
1991,直接映射式字符检索算法,杨宪泽,"现有的检索算法, 大多数建立在比较基础上, 效率不高。散列算法考虑了关键字与信息记录存贮地址间关系, 效率较高。但是, 散列算法必须在不同情况下分析关键字, 才能构造好的Hash函数, 保证较高效率。本文在文献[5,6]基础上, 提出了一个新的字符检索算法。这一算法关键字（字符）与信息记录存贮地址直接映射, 不实施反复比较操作, 时间复杂性达到0(1), 适宜今后在计算机中文信息处理中广泛应用。",
1992,计算机多文种信息处理及中西文字的计算机计量,王懋江,"本文从计算机多文种信息处理的角度讨论了用计算机将日汉英技术词典改造成汉日英词典的若干问题, 定义了词组的语种表示长度及语种表示能力等概念, 并以计算机实际数据比较了中西文字的强弱点。实测数据表明, 中文排序要比西文排序快得多",
1992,CDSA模型及其在关系数据库自然语言接口中的实现,"吴照林,高广峰","本文通过对传统的自然语言处理方法的分析, 针对自然查询语言的结构, 提出了一种基于结构分析的语言模型—CDSA(Concept Dependence on Structure Analysis)模型, 并描述了作者实现的一个基于CDSA模型的关系数据库自然语言接口的实验软件NLQI（Natural Language Query Interface）",
1992,中文词组的快速查找算法,张钟澍,"本文提出按数据文件的关键字（中文词组）中每个汉字笔划数的权值之和进行分桶存贮, 在查找中文词组串时, 也采取按桶检索的查找算法, 并证明了该查找算法的平均化费为O(N), 优O(N log N)于的二分查找算法。",
1992,关于二值图象结构补偿放大平滑的一点注记,"徐佩珍,顾景文","本文对二值图象结构补偿放大平滑中的补偿模式阵进行讨论, 给出了一种简单而统一的确定补偿块元素的算法, 算法可用于汉字信息和图象处理领域。",
1992,拼音-汉字转换输入中的结构识别方法,万建成,"本文提出并讨论了拼音汉字转换输入法中的常用词搭配结构的识别方法。该方法在词组的水平上, 在常用格配结构约束下利用少量的词属性特征, 可有效的解决许多单字词的识别问题, 这其中包括一些单纯句法难以识别的同音单字词。",
1992,一种汉字键盘输入编码方案,程良鸿,"本文提出一种汉字键盘输入编码方案, 采用十一种笔画部件和汉字拼音首字母描述汉字。文中通过对国标6763个汉字的实际编码与计算机辅助统计评测结果, 证明该方案规则简单, 重码率低, 效率高。",
1992,基于词组的智能化汉字输入系统CIIIS/2的设计,"吕强,钱培德","本文给出了基于词组的智能化汉字输入系统— 的设计细节,其中有关单字输入技术和联想输入技术是对现有输入技术的优化和扩充, 前后链输入枝术和词组编码输入技术具有一定的创新。本系统具备一定的自学习和自适应能力, 从而能表现一定的智能水平。对于智能化在汉字输入方面的应用, 本文做了有益的探索和尝试。",
1992,印刷体汉字识别技术在我国的发展和应用,"张炘中,沈兰生","本文论述了我国印刷体汉字识别技术研制的三个阶段, 指出了我国印刷体汉字识别系统的特色, 并提出了今后发展的动向。",
1992,蒙古文信息处理全息内部码的编码设计,拉西吉格木德,"本文首先提出, 蒙古文信息处理系统必须具有, 包括能对蒙古文文献资料进行分析研究内在的较完备的通用功能其次较为全面地分析了蒙古文字的属性、特点、规律及信息含量最后设计出了蒙古文信息处理全息内部码的编码方案。",
1992,唯物码汉字输入法在CCDOS4.0中实现,,"汉字是一种方块平面图形文字, 与拼音文字有显著区别。古往今来, 汉字字典的查字方法多种多样, 到了今日的电脑时代, 电脑汉字输人法已有数百种, 但电脑汉字输人法与字典的检索法却互相脱节。在字典中用一种方法查字, 在电脑中用另一种方法输入汉字。能否寻找到一种途径, 人们学会查字也就会输入汉字呢, 这将为人们的学习工作带来很大方便。",
1992,语料库、知识获取和句法分析,"黄昌宁,苑春法,潘诗梅","在这篇文章中, 我们将介绍一种基于语料库的汉语句法分析系统。这里, 我们用以进行句法分析的知识主要是从有句法标注的语料库中获得的。我们的工作注重在知识获取及表达句法分析的算法。在句法分析中我们也用到了语法知识, 即依存语法四公理。此外, 我们也提出了依存语法第五公理来支持我们的汉语句法分析系统。",
1992,多语料库作法之中文姓名辨识,"张俊盛,陈舜德,郑萦,刘显仲,柯淑津","专用名词虽然只占中文文章中的词的百分之一到百分之二, 但是, 如果不对这些专用名词加以处理, 将会形成自动分词的错误的大部分。本文首先描述了包括中文姓名辨识的分词方法, 然后介绍其实验结果。最后, 文章讨论了中文姓名辨识被遗漏和误判的原因, 升提出未来的研究方向。",
1992,机译知识自动获取,"韩向阳,陈肇雄,张祥","机译知识获取是机器翻译系统研究中一个至关重要又极难解决的问题。目前多数机译系统的知识库都是由人工静态建立的, 系统不能自行对其进行扩充。本文介绍了一个智能机译系统IMT/EC来统中用多种途径进行知识获取的学习机制的设计与实现。这一机制实现了自动知识获取及动态知识维护, 使IMT/EC来统有护强的适应能力。",
1992,XMMT英汉机器翻译系统,"李堂秋,高庆狮,倪子伟","本文通过英汉机器翻译系统的设计思想和结构, 阐述如何综合应用人工智能技术, 研制翻译质量高, 通用性好, 能扩展成多种语言翻译系统的一种方法。系统设计的侧重点在于如何提高系统的翻译质量。文中给出了主要模块的算法原理, 在附录部分还给出了这个系统的一些翻译例子。",
1992,QHFY英汉机器翻译系统的词典设计,"陈圣信,包培文","本文介绍了清华大学自动化系和外语来研制QHFY的英汉机器翻译系统的词典系统设计。#br#该词典系统由一部有40000 多单词和词组的主词典, 若干辅助词典和一部专业词典组成。在主词典的结构中, 重点介绍了控制符和特征字。前者控制词处理流程, 后者提供分析和生成用的句法信息和语义信息。#br#词典查询中用二级索引查询算法以提高系统的翻译速度。本文还讨论了改进词典系统的若干方法, 如生词处理, 压缩容量, 以及结构和算法的改进等。",
1992,日汉机译系统中有关汉语生成的几个问题及处理方法,"陈群秀,李咏玖","机器翻译系统中, 源语的夺析和目标语的生成是必须解决的两大问题。在日汉机译系统中,由于汉语和日语分属不同的语系语族, 是表达方法差别较大的两种语言, 所以生成时有一些问题需妥考虑和解决。本文作者结合在研究工作中的一些体会, 提出日汉机译系统中有关汉语生成的个问题及处理的办法, 供同行们共同切磋讨论。",
1992,基于短语结构文法的分词研究,"韩世欣,王开铸","本文在分析现有各种分词年法的基础上, 遵循自然语言理解的层次模型, 着重对短语结构文法及其形式化描述进行了探讨。提出了汉语自动分词在短语层的理解模型及短语层分词语义相关原则, 最后给出了短语结构分词法法的机器实现。实验结果表明, PSG法在提高分词精度上具有相当好的效果。",
1992,非过程性规则描述语言NRDL,"赵振西,刘秀芬","规则描述语言是自然语言处理系统的开发环境中十分重要的知识获取工具。本文介绍一种非过程性的、面向汉语理解的规则描述语言NRDL, 详细说明了NRDL的语言结构、规则书写方法、实现机制和功能特点, 并且介绍了在计算机使用的情况。",
1992,汉语叙述文中的小句前部省略现象初析,宋柔,"汉语篇章中省略的现象十分常见。大多数的省略发生在句子的前部, 本文中称为前部省略。本文从这一现象出发, 提出了汉语叙述文基于前部省略的树形结构, 称作参照树, 分析了参照树的某些形式规律和语义规律。这一工作的目标是为用计算机处理汉语篇章提供形式化模型和算法设计的依据。目前的结果是初步的和粗糙的, 但已经展示出了一系列需要在计算语言学范围内加以深入研究的课题。",
1992,编者的话,,"1991年11月20一22日在杭州召开了全国计算语言学第一届联会学术会议。会议由中国中文信息学会计算语言学专委会与自然语言处理专委会、中国人工智能学会自然语言理解学会、中国计算机学会人工智能与模式识别专委会自然语言理解学组和中文信息技术专委会计算语言学与机器翻译学组、北京语言学会计算语言学研究会以及杭州电子工业学院等七个单位共同发起。到会的有来自大陆和港台地区的大专院校、研究所和新技术产业等多个单位的名专家和代表, 会上宣读论文篇。",
1992,汉语分析的语义网络表示法,"刘东立,唐泓英,王宝库,姚天顺","本文讨论机界翻译和自然语言处理中汉语分析的中间媒介表示, 重点是给出汉语信息的几个主要语言环节在语义网络中的表示方法, 通过实例说明它们是如何处理的。",
1992,一种生成复杂特征集句法树的汉语句法分析方法与系统实现,"赵铁军,李生,周明","本文根据汉语句法结构的特点, 提出了一种生成具有复杂特征集的多叉树的汉语句法分析方法。实现上, 提出了分阶段处理、部分人机对话、句法和语义知识并用等策略, 实现了一个初步实用的系统。本文最后给出了部分实验结果",
1992,具有文法分析功能的智能中文教学系统,"陈晓钢,陈增武,胡上序","本文介绍了一种智能计算机辅助中文教学系统。作者为系统引入了一种基于规则的自然语言形式文法, 使系统具备了文法分析功能及自然语言的理解能力。以基于规则的文法为基础建立起来的文法推理机、文法知识库及富于特色的学生模型, 使该教学系统能象中文专家那样对学生的中文作业进行分析、评价, 并给予学生纠错指导。实验表明, 该系统为学生提供了良好的中文学习环境, 具有很强的智能特征",
1992,汉语句法分析的交互激活竞争模型,"李粟,陈永明","本文根据联结主义提出的交互激活竞争理论, 把句法分析看做为一个通过竞争机制从句法规则集中选取合适句法规则的过程, 并建立了一个汉语句法分析的网络模型。这一模型的特点是能用最少单元把任何短语文法转换成网络表达能根据一般原则对网络赋权 网络具有较好的收敛性。这一模型已成功地应用于若干汉语基本句型的分析和歧义词的处理。本文还对通常的网络函数计算公式作了改进, 使网络能克服“ 层次障碍”的影响, 并保持原有的特性。",
1992,自动分词软件质量的评价模型,"曹焕光,郑家恒","汉语自动分词是中文信息处理的基础性工作。近年来, 我国已研制了十多种自动分词软件, 但尚无评价此类软件的模型和方法。本文针对分词软件的特点, 提出了自动分词软件质量的评价准则, 并对这类软件质量的两个重妥特性令词正确率和切分速度给出了定量的度量方法。",
1992,汉字字形向量轮廓压缩算法的设计与实现,"黄宜华,王绪龙,袁春凤","本文讨论一种从点阵式汉字字形由软件自动抽取向量轮廓字形的算法。文中在轮廓点的步进、向量拟合等方面提出了许多新的设计方法和实现技术, 并讨论了一种有效的向量轮廓数据二次压缩算法。本系统业已实现, 压缩后的向量字形配合相应的还原技术可广泛地应用于各种中文计算机系统中。",
1991,题词,"钱为长,陈力为",,
1991,纪念《中文信息学报》创刊五周年,本刊编辑部,"中国中文信息学会主办的全国性学术创刊物——《中文信息学报》创刊已整整五年了,它是随着中文信息处理事业的发展而成长起来的。五年来, 在广大作者、读者的关心和支持下, 刊登论文内容日益丰富、全面, 水平逐步提高。学报不仅为我国从事这一领域及相关领域的科研工作者提供了学术交流园地, 同时, 对振兴和发展我国中文信息处理技术, 开展国际学术交流活动, 促进我国国民经济建设也作出了应有的贡献。",
1991,机器翻译系统中一种规则描述语言(CTRDL),"王宝库,张中义,姚天顺","本文的目的在于为汉英机器翻译系统（CETRA）提供一种处理中文文本的规则描述语言（CTRDL）。这是一种受限的语言, 用以定义中文文本的分析和生成文法。实际上任何特定语言的文法都可用这种元语言的形式系统加以描述。",
1991,汉字键盘输入的认知模型,"张侃,陈一凡","本文从汉字键盘输入方法评测的重要意义和目前遇到的困难出发, 说明以工程心理学的方法建立的汉字键盘输入认知模型是设计完整的评测系统和优化输入方案的有效手段。",
1991,实现汉字简繁体自动转换的一种方法,"杜宇,何克抗","本文提出并实现了一种利用上下文构词关系进行汉字简繁体自动转换的方法。与现有其他转换方法相比, 可以有效地解决“多义对应”问题并可显著提高转换速率。",
1991,汉语自动分词实用系统CASS的设计和实现,"揭春雨,刘源,梁南元","近年来, 汉语自动分词成力中文信息处理的一大热门课题, 其研究进展令人关注。本文主旨, 是通过介绍CASS系统的设计和实现, 阐述汉语自动分词实用系统的一般性原理, 包括系统的总体结构、自动分词算法和分词词典的实现、各种多义切分字段的识别和处理, 等等。CASS系统由总控程序、自动分词程序、设施管理程序、分词词典和知识库等五大部分组成。自动分词算法程序选用正向增字最大匹配法ASM(+1,+1,+1)实现, 该算法的嵌套调用, 可以识别出各种多义切分字段, 包括任意多重的交集型多义字段。这个算法经过相应的运行控制, 可以实现其它各种分词方法。",
1991,汉字假名变换技术及其应用,"朱学锋,俞士汶","本文详细分析了将日语汉字变换为假名的困难及现实可能性。笔者开发了一种汉字假名变换技术, 并将其应用于英日汉对照的计算语言学词典的开发。本文对应用结呆进行了分析。本文还探讨了这项技术的潜在应用领域。",
1991,关系数据库汉语查询接口的设计与实现,"吕光楣,陈清波","汉语接口一直是我国AI界研究的热门课题之一。本文首先对汉语接口的可行性进行了论证, 然后在分析了自然查询语言功能特征的基础上, 提出了以词汇为基础, 以语义特征为先导的综合处理技术。为使接口能进行移植, 又引入了数据库模式字典, 设置移植和学习模块, 使之重建专用字典, 与新库连接。最后给出了实验系统的结构和流程图, 并作出该系统的性能评价和测试结果。",
1991,一个手写印刷体汉字识别实验系统,"陈玲,陈学德,郑重,青木由直","本文在充分考察了手写汉字和中国大汉字集特点的基础上, 提出了一组用于手写印刷体汉字识利的分类特征, 它们是长笔划分布类型、各类笔划的数目、交叉点数目和折点数目。利用这组特征进行匹配就可直接识别出GB2312-80汉字集中的绝大部分汉字, 再通过一个基于知识的推理过程即可进一步识别出已被分成类组的少数剩余汉字, 这种将统计分类与基于知识的推理识别相结合的两级识别方法具有较高的效率。一个适应性较强的汉字笔划和特征点抽取方法也被设计, 它是SLSA方法的改进, 与机器学习功能相配合, 大大提高了特征抽取的正确率。我们根据上述思想建立了一个手写印刷体汉字识别实验系统, 并获得了较好的实验结果。",
1991,编码字符集中子集的完整性,许寿椿,"本文提出编码字符集中的完整性问题。（一）中给出两类编码实例。一类严格遵从“一个字符只分配给一个码位”或“任何字符都不重复分配码位”（ 简称一符一码）的节约原则。另一类以一符两码可多码的方式, 以码位的牺牲换取子集的完整性。（二）中解释了子集完整性概念和意义。说明了有意义的子集通常是现实中某子系统的反映。多文种编码字符集的许多子集往往与某自然语言系统相关联,这种子集的完整性, 也就与相应语言文字的系统性相关联。据此提出了完整性条件, 此较了完整性得失, 说明了完整性的某种相对性。（三）中指出不少字符集, 含ISO 10646 DP版及DIS版, 的一符一码原则损害了若干子集的完整性。在那里,拉丁文字圈中除英文以外的各国家、各民族的文字字符子集大多被肢解了, 只有字母表是英文字母表（含元素2X26=52个）子集者例外。斯拉夫文字圈、阿拉伯文字圈情况相似。文中指出汉语拼音字母子集, 无论在汉字编码的中国国家标准中还是国际标准10646中放严重肢解了。",
1994,《自然码》双拼键盘设计合理的研究,"杨道沅,董小国,董红,陈丹","本文提取了周志农先生的《自然码》双拼数据和键盘分布, 根据健盘设计原则, 详细的从静态, 动态两方面对《自然码》双拼健盘进行了研究, 从而提出了我们对《自然码》双拼键盘是否合理的看法。",
1994,语料库与知识获取模型,"张敏,罗振声","在计算语言学中, 知识作为句法分析的数据支持起着重要的作用, 如何利用计算机从现实世界中获取知识一直是人工智能领域探讨的重要课题, 本文描述了如何从语料库这一客观真实数据资料获取有用知识, 用以帮助汉语句子中依存关系的分析, 在依存语法作为语言模型的基础上, 探讨了向上依存关系的二元同现矩阵, 以及知识库的组成和结构, 在获取词跟词之间的依存关系的同时, 还获取了词跟类、类跟类之间的依存关系",
1994,汉字库多级存贮系统的分析,"陆建明,钱培德","本文通过实验和统计论证了汉字库多级存贮结构的合理性文章将汉字库结构分为静态内存型、动态内存型和静动态结合型三种, 并详细地讨论静态内存型最后根据统计和论证, 给出了相应的算法",
1994,汉语文本读入中音字转换的知识集成模型和时间同步搜索算法,"徐近霈,高枚","本文提出一种集成声学和语言学特别是词间近郁关联和远部语法规则知识的组合概率模型, 以及将这一模型用于音字转换过程的时间同步的动态规划搜索算法,并实验验证了所提模型和搜索算法的有效性",
1994,机器词典的信息表示及在汉英机器翻译中的实现,"李生,赵铁军","在这篇文章中, 我们强调了机器词典对于机器翻译等自然语言处理系统的重要意义高质量的机器词典需要花费巨大的劳动词典知识来源于语言学研究和计算语言学的工程实践, 其编撰标准、信息表示及编码等均应考虑工程实现的效果本文结合作者正在研制的汉英机器翻译系统, 介绍了机译词典的有关实现问题, 并讨论了词典知识在汉语分析和汉英转换中的作用",
1994,日语用言格框架在机译系统中的应用,雍殿书,"本文介绍了基于格文法的日语动词、形容词及形容动词格框架在日汉机器翻译系统中的重要应用, 为建立实用化的日汉机器翻译系统提供了有力的技术支持。",
1994,汉语句型自动分析和分布统计算法与策略的研究,"罗振声,郑碧霞","汉语句型的自动分析与分布统计是继我国汉字字频统计和词频统计之后的又一重要的基础性研究课题本文就以结构特征为标准的句型系统, 提出以谓语为中心的句型成分分析与句型匹配相结合的分析算法与策略, 讨论了句型成分及其短语边界的识别与判定方法, 给出了有关竣义结构的处理策略, 以及实验模型的测试结果与分析。",
1994,机器可读词典的快速查找技术,张永奎James R Cowie,"本文叙述了用于直接访问由可变长记录组成的顺序式词典文件中各个词条的一种方法。这种方法以trie索引为基础, 避免了重新组织词典文件。Trie索引是一个深度可变的多层次索引, 深度的控制取决于为存放索引可提供的合理的内存量, 本方法可满足查找一个词条只需要一次磁盘访问的理想要求。",
1994,一个中文数据库管理系统界面,"林耀森,张少润",本文提出一个中文数据库管理系统界面的设计方案及其语言它使中文数据库用户能容易地、自然地执行数据定义、查询和处理该语言具有英语数据库用户使用的、已成为工业标准的结构查询语言SQL的全部功能,
1994,基于N联字的汉字识别后处理研究,"苗兰芳,张森,周昌乐","为了提高汉字识别率, 本文提出了在单个汉字的初级识别后, 利用N联字的上下文关系, 对初级识别中拒识或不确定的汉字语段作进一步确认的一种方法, 阐明了N联字后处理方法的基本思想, 给出了实现此方法的数据库的结构设计方案和理论算法, 分析了理论上可提高的识别率, 最后给出了一个N联字汉字识别后处理系统模型。","汉字识别,N联字,数据库,后处理"
1994,无编码通用词库的高倍逻辑压缩和反向查询技术原理,黄希琛,"标准词库是机内自动识别重码和词码输入方式的重要数据基, 但词库庞大,微机内存有限, 使词库不能全部装入内存使用本文介绍无编码通用词库的高倍逻挥压缩技术, 可使词库全部装入内存使用其次, 介绍依输入词码生成待定词及利用反向查询方式查询词码所映象的词的技术原理该技术可使汉字输入系统所有的编码方案不设计词码, 并使用同一个词语库, 这为设计通用智能汉字输入系统莫定了基础",
1994,语音代码──汉字智能转换研究,万建成,"在综述了语音代码一汉字智能转换(IPC)研究的发展和技术现状后, 本文就其研究的范畴问题提出作者的观点, 其中包括变换的形式定义, 研究所涉及的范围和困难问题, 变换效果的评价等, 在评价方面, 提出了转换的完备性、唯一性、攻义性、本原性攻义和非本原性歧义的概念文章就进一步研究提出了作者的建议, 希望本文提出的问题能起引起有关的讨论, 在充分认识研究的必要性和困难的基础上, 将其引向全面深入的发展。",
1994,NL句法分析中超语法符合现象的处理,"翁富良,周斌,吴立德","本文回顾并总结了句法层次上六种不同类型的超语法符合(extra grammaticality,简称EG)现象及它们的恢复原则, 描述了如何用扩充的GLR分析器(Extended GLR Parser简称EGLR)完整解决其中四种、部分解决其余两种现象的技术。在处理后两种现象时, 要涉及语义问题。","句法分析,GLR分析,超语法符合,词汇范畴"
1994,汉字黑体字形衍生系统的设计与实现,"岳华,蔡士杰,顾进,武港山,严伟荣",计算机在电子印别、广告、包装等领域应用的迅速扩展迫切需要众多的高质量的曲线轮廓汉字库。本文介绍一种通过黑体笔划中心线抽取、笔划轮廓切割、笔划粗细变换、曲线拟合等处理技术来自动生成一族具有黑体风格、但笔划粗细不同的字形的方法。该方法具有低成本、高效率和高质量等特点。,"笔划粗细衍生,曲线轮廓,笔划中心线,汉字库自动生成"
1994,一个实验性的汉语篇章理解系统,"崔耀,陈永明","本系统从世界现象的组成和人类的记忆结构特点出发, 结合汉语的具体情况, 从意义分析的角度将汉语的词汇分为描述性的词、过程性的词、辅助性的词三类。这三类词分别描述了世界现象中的事实、事件以及语言本身所具有的特性。在此基础上形成了汉语的篇章理解所依赖的知识表示和知识组织形式, 即以事实一事件网络为基本结构的记忆模型通过这个模型建立了汉语篇章理解系统的知识库, 以及与之相应的加工和管理机制系统对汉语篇章的分析是以词为引导进行的。汉语的词直接对应于事实一事件网络中的节点和辅助词表中的词项这些节点和词项综合了语法的、语义的、语用的知识,并且能根据处理的需要及时地为分析过程提供预期本系统通过阅读, 对自己的知识库进行动态的自我管理。在阅读了有关七种鸟类的汉语故事之后, 系统能够学习到有关鸟类的一些新概念, 并能回答相应的问题。汉语篇章理解需要依赖各种知识。这些知识来自语法、语义和语用三个方面为了使计算机能够更好地处理汉语的篇章, 必须对各方面的知识进行合理的组织和管理。由于语言是人们用来描述世界现象, 传递信息的工具, 对于自然语言理解的研究工作有必要从意义分析的角度进行。意义分析就是找出语言是如何对世界现象进行模拟, 进而发现特定的言语活动所描述的有关世界现象特征及其相互关系的过程。本文从汉语的词与世界现象的对应关系出发, 就汉语理解系统的建造进行了初步尝试。",
1994,面向语料库标注的汉语依存体系的探讨,"周明,黄昌宁","实现大规模真实文本的处理, 是信息化社会的迫切要求, 也是国际计算语言学界的一个战略目标目前一项迫在眉睫的任务是建立一套满足大规模真实文本处理的语言处理体系, 包括分词的标准、词的分类体系、句法体系和语义体系。其中句法体系是核心环节。本文提出并论证了依存语法是合乎大规模真实文本处理要求的句法体系, 并结合汉语的特点, 研究了汉语的依存语法, 划分了种依存关系。最后简要讨论了依存语法的一些应用","汉语,依存语法,语料库语言学"
1994,水平线与字符粘连的切分及字符修补,李燕,"本文对文本识别中经常遇到的线段与字符粘连的问题进行了论述, 提出了抽取水平线和对字符进行修补的葬法, 并给出了结果。",
1994,等线体和圆头体曲线轮廓字形的自动生成系统,"武港山,叶晓璐,蔡士杰,陆波","本文叙述了等线体和圆头体汉字曲线轮序字形的自动生成系统该系统从黑体汉字曲线轮廊字形中自动抽取骨架单线体, 结合等线体和圆头体汉字构字规则自动生成多种笔划粗细规格的等线体和圆头体汉字曲线轮屏字库, 具有成本低、速度快、质量好等优点。",
1994,精密曲线轮廓字模的生成方法研究,"瞿洋,张喜斌,苏恩泽","本文介绍了一种精密曲线轮廊字模生成方法这种方法运用图象处理技术,对字符的点阵进行部件分离、边缘发现、边缘跟踪、边缘修改, 以便得到精确的字符边界轮廓然后, 运用本文设计的新的郁边加权平均样条曲线量化方法, 生成高精密度、高压缩比的曲线轮廊字模实验结果表明, 本文设计的方法较好地保留了字符的特征, 具有较高的压缩比和较快的还原速度该方法还可以对其它任意二值图象进行曲线量化","区城填充,边缘发现,边缘跟踪,矢量化,曲线化,局部三次样条插值,加权平均"
1994,多维参数控制法及神笔汉字发生器,"何尔恭,薛开平,温立新,曾锡山","在所有汉字生成方法中“ 部件组字’ 最能有效地压缩存储字形的信息量, 但由于部件受到压缩时的崎变使得它难以生成高质量汉字本文提出了“ 多维参数控制法”有效地克服了上述缺点它已应用于部件组字型汉字库一神笔汉字发生器, 可以快速生成高质贵汉字.",
1994,曲线轮廓汉字的网格适配技术,"胡长原,武港山,张福炎","要提高曲线轮廓汉字的还原质量, 需要在字形还原时采用网格适配技术本文介绍了网格适配的基本原理, 并在分析汉字字形还原失真现象的基础上, 给出了我们自己设计并实现的适合于曲线轮廓汉字特点的二种网格适配方法, 即动态的补象素算法和曲线轮廓汉字的技术","曲线轮廓汉字,网格适配（grid一fitting）,字形扫描变换,Hinting技术"
1994,计算机辅助汉字书写教学的研究──书写汉字库生成系统的研制,"刘禹,何克抗","本文概略地分析了汉字字形的特征, 总结了书写汉字字形库生成系统设计的主要思想, 和来用的主要算法","CAI,汉字字形,笔画分解"
1994,汉语复句的结构分析,张仕仁,"本文详细分析了复句结构, 论述了用盒式图表示复句的形式结构, 用复杂特征集表示复句的意义结构前者直观易懂, 便于非专业人员理解, 后者从深层表示复句的意义, 便于计算机加工处理最后探讨了汉语复句的自动分析方法",
1994,DATALINK资讯交流网系统,"蔡金元,吕粮","本文介绍DATALINK资讯交流网系统的作用和主要功能在国外“信息高速公路,",
1993,关于汉语句型,"唐泓英,姚天顺,王宝库","本文提出了一套汉语信息处理用的简洁的基本句型, 并从汉语分析策略的三个方面, 通过实例深入地分析了句型可以简化的原因, 文章最后给出了现代汉语简单句的28种基本句型。",
1993,汉语孤立字声调的模糊识别方法,"徐士林,Samuel C.Lee","本文应用模糊集合来识别汉语孤立字的声调。孤独字的四声调可被描述成四种模式类的模糊集合。由于四声调的基音轮廓具有其固定模式, 因之在此基础上可构成模糊集合的隶属函数。方法中使用隶属函数为模式分类的判别函数。这些隶属函数既简单又易于计算, 故适宜实时执行。实验结果表明, 总的识别率高于99%。",
1993,统计语言模型及汉语音字转换的一些新结果,郭进,"汉语音字转换是一个重要而困难的问题。语料库语言学为我们提供了新思路。作者们通过建立统计语言模型, 将基于语料库的方法与传统的基于规则的方法结合, 研制了THED新一代音字转换系统。该系统对随机抽取的祈华社新闻语料有不低于95%的带调音节和国标汉字的转换正确率。本文侧重报道该系统在汉语音字转换方面及与此相关的汉语切词和词性标注方面的一些实验结果, 也简要介绍该系统在语料库应用方面的一些思路。",
1993,以成语为范围的词汇支援系统,"林联合,吴杰,吴亮","这篇论文介绍有关调用汉语成语的词汇支援系统。人们给系统一个话题后, 该系统将针对该话题提供一批成语来帮助人们选择和寻找词语。文章还介绍了成语的知识表达方法和自动的语义检索过程。",
1993,汉语普通话声母的分类与识别,"徐秉铮,邱伟","本文提出了一种实时高效的汉语声母识别新方法。采用声母二次识别的策略, 即先用五个时域特征参量将声母分类, 然后用语音分布知识及模板匹配法对类内元素进一步进行处理以最后确定识别结果, 这种新方法已应用于基于声、韵识别的汉语声控打字机, 它也适合于不认人识别系统, 本方案已在IBM PC/XT、286等机上实现, 经训练识别率可达95%以上。",
1993,基于意象知识的消歧体系,"杨莹,李应潭","本文提出了一种可以表示常识及语言知识的意象知识体系。在这种知识的形式化表示基拙上, 给出了NLP中的消歧知识及其表示形式, 以及基于消歧知识的消歧策略。最后, 论述了这种方法实现上的可行性。",
1993,基于神经网络的手写体汉字识别实验系统,"盛立东,何其明","本文针对BP网络的特点, 讨论了手写体汉字特征的抽取, 给出了BP网络的算法, 对于特定人手写体汉字大写“壹”~“ 拾”取得95%以上的识别率。",
1993,特定人主题受限连续汉语人机对话系统的研究,"王跟东,林道发,杨家源","本文描述了一个以火车售票和信息查询事项为对话主题的专人使用的连续汉语人机对话系统原型, 着重讨论语言理解和应答文生成, 这部分是连接语音识别和语音合成的纽带, 起着承上启下的作用。语言理解和应答文生成采用上下文无关文法(CFG), 结合黑板以及局部上下文相关(CSG), 对连续汉语语音识别输出的结果进行理解, 生成人机对话系统中的应答文, 作为语音合成的输入。利用堆栈技术处理不完全信息和信息证实, 使得该系统具有自动提问获取知识的能力和对关健信息进行确认的能力, 从而使人机对话能连续进行。语义处理提取动词等关键信息, 使该系统具有容错性。我们对时间的描述采用模糊数学方法处理, 以适应现实对话中不精确的时间表示。",
1993,基于合一语法的通用句法分析器:设计与实施,"沙新时,吴立德,周斌","本文从建立一个通用的基于合一语法的句法分析器的实际出发, 就特征结构、合一算法、基于合一的语法形式、使用基于合一的语法的方法等方面展开讨论, 对某些模物的东西加以澄清, 对现有的各种方式、方法进行分析比较, 并介绍了我们的方法, 最后给出了结论。",
1993,歧义、系统歧义和语境,钱树人,"本文对歧义现象, 特别对语言片段的歧义理解进行了剖析, 并研究了不同语境对理解歧义的影响。进而提出了默认语境, 系统语境和系统歧义等概念, 并简要地介绍了汉语语言片段歧义分析模型系统CAAMS。",
1993,FPY中的同音词智能识别方法,万建成,"FPY是作者研制的以词为识别单元、以拼音为编码的汉字智能输入系统。本文介绍了该输入系统中的数种同音词的智能识别方法, 其中包括部分汉语短语句法分析方法,和作者首先提出并实现的几种词法和语义相关分析方法。",
1993,拼音语句汉字输入系统InSun,王晓龙,"本文提出了基于键盘的字词愉入技术之上的语句输入的思想, 讨论了拼音语句输入的知识的表达、获取和系统实现, 给出了测试结果及应用情况。",
1993,一种汉语语音多级识别策略,"邝继顺,何鎏藻","本文提出了一个汉语语音识别的三级识别结构, 在以半音节为识别基元的基础上, 将基于语音生成模型的模式匹配法和基对“知识”的特征规则法有效地结合起来, 从一个新的角度来研究汉语语音识别。通过对汉语语音发音方式和拼读规则的分析及实验, 总结出用于声母分类的有效特征及各级判决中的若干判别规则。实验结果表明, 系统达到了较高的性能指标。前半音节的识别率为94.7%, 存贮量和计算量分别和一般全音节识别系统的1/10和1/36。",
1993,汉字在计算机屏幕上的阅读适性讨论,"林川,樊林,薛国光","本文讨论了屏幕显示用汉字的类型, 并着重讨论在计算机终端屏幕上, 影响连续阅读用汉字的阅读适性的一些技术因素, 这就是终端的操作-阅读距离, 屏幕的分辨率以及字面尺寸。",
1993,基于规则的高效索引算法和排序算法,杨宪泽,"本文提出产生式规则索引算法和排序算法, 用以提高智能系统运行速度。索引算法和排序算法都缩短了规则匹配时间。而且, 排序算法不仅适宜规则静态排序, 也适宜规则动态排序。",
1993,手写印刷体汉字的笔段抽取及偏旁识别,胡家忠,"本文采用对汉字点阵图象进行方向变换的方法抽取汉字的笔段, 采用结构分析的方法识别分布于汉字四周的偏旁, 对国标一级汉字中的99类偏旁计一万余字进行了偏旁抽取试验, 当侯选偏旁数96%。",
1993,手写体汉字的基元选定及抽取的新方法,"刘庆波,洪家荣,王开铸","本文介绍了一种手写体汉字识别的基元选定及抽取的新方法。这种方法面向计算机对于二维图形的处理能力, 从汉字的整体结构出发, 选定了易于处理的十六种基元, 辅以特定的抽取方法, 还讨论了特征选择与重码问题的关系, 实验结果证明了该方法的可行性。",
1993,一个基于神经网络的手写文字分类/识别模型,"陈学德,陈玲,曾碚凯,郑重,青木由直","本文设计了一个基于神经网络的手写文字(汉字、数字、字母等)分类/识别模型,给出了该模型的预处理方法、神经网络结构、工作算法和学习算法, 并进行了手写数字识别和手写汉字分类实验。实验结果表明我们所设计的网络结构是较合理的, 所采用的预处理方法是有效的, 能够取得较高的分类/识别效率。",
1993,基于神经网络的相似汉字识别的研究,"梁曼君,石竹","在计算机文字识别中相似汉字识别是一个难题, 我们对Hopfield人工神经网络加以改进, 提出了一种新的HNN网络模型, 并用它来识别相似字, 取得了良好的效果, 为相似汉字识别探索了一条新路。",
1993,智能自适应中文操作系统,"于国荣,何克抗","本文全面分析多种显示适配卡的工作机理, 探索了一种新颖的设计思想, 实现了一种中西文兼容、简繁体汉字兼容, 且具有良好的自适应能力和自适应智能汉化能力的高性能的微机汉字操作系统(IACS)。",
1993,汉字点阵无级压缩的优化算法,"傅晓宇,傅晓玲","本文给出一优化的无级压缩变换算法, 可将宋体、楷体、黑体、仿宋体的48x48点阵汉字无级压缩为48点阵以下的任意点阵, 且压缩变换后的点阵结构完整, 字中相应笔划粗细均匀, 字形美观。",
1993,中文文本压缩的自适应算法,"贺前华,徐秉铮,彭磊","本文初步分析了中文文本的存储结构特征, 并将其应用于文本压缩。对LZW(Lemple Ziv Welch)算法进行了两方面的改进:1.采用变码长编码, 对短文本的压缩有显著的效果；2.建立一删除规则, 当码本加满以后对码本进行删除整理, 使编码过程一直能够积累输入文本的相关信息, 对较长文本, 其压缩效果比基本LZW算法有显著改善。",
1993,PE3200系列机汉字显示系统方案与实现,何钟林,"介绍一种适于中小型机的全软件汉字显示系统的设计与实现。该系统采用面向通用智能图形终端的方案, 使用汉字编译方法, 根据汉字输入码, 生成由图形终端命令组成的汉字显示文件。通过离线预处理设计和在线点阵重定义显示方式来提高汉字显示速度。系统还提供汉字库管理和造字功能。该系统可解决由中小型机组成的过程控制系统、仿真系统和数据处理系统中的汉字显示问题。",
1993,统一汉字库的研究,周浩华,"本文从模拟人写字的思维过程的观点出发, 用抽象的方法描述汉字结构的本质, 提出一个构造统一汉字库的模型。这个模型基于汉字知识库，论述了汉字的表示、统一汉字库的设计和构造方法。",
1993,限制汉语语法分析中歧义性的启发式方法,"邰晓英,童頫","汉语语法、语义分析中的歧义性是计算机理解汉语的难点之一。本文提出根据对汉语单词用法的规则化描述, 建立各个单词的启发式规则, 用以对句法分析中歧义结构进行约束的一种分析机制。",
1993,N元汉字字词编码输入的最短码长和速度上限,"王晓龙,王轩","本文介绍了计算N元字词编码最短码长的方法, 给出了理论证明和实现算法, 并在IBMPC和工作站上就52万字和180万字的样本数据对N取24种不同的码元情况下进行了计算, 给出了各种常见的输入方案的码长下限。基于上述计算结果, 本文还就最短码长精度、码元N与编码效率、词数与码长的关系以及录入员平均输入速度的上限进行了讨论。",
1993,大规模逻辑神经网络印刷体汉字识别系统,"杨国庆,吕军","逻辑神经网络是一种采用快速学习算法、RAM阵列实现的数字网络。本文描述了采用这种网络模型实现的印刷体汉字识别系统。这是一个初步实用的系统, 可识别大约4000个不同字号的宋体汉字及其它字符, 其识别率达99%, 甚至对于实际书刊, 其识别率也能达到95%左右。系统使用了大约384,000个神经节点, 是一个复杂的大规模神经网络。和其它同类系统相比, 具有适应性、稳固性好、学习速度快以及可用数字集成电路全硬件并行实现等优点。",
1993,自由手写阿拉伯数字识别方法与系统,"谭鹤良,谢兵兵,陈明武","本文提出一种应用贯空特征码识别自由手写阿拉伯数字的方法, 在研究了大量的手写数字贯穿码的基础上, 得到了14个分类决策条件, 并将它们应用于数字识别系统, 收到了很好的识别效果。",
1993,英文生成系统词汇结构与词项位的逻辑描述,"卡世力,姚天顺","本文介绍了汉英机器翻译系统（CETRAN)中从中间语言生成英语的生成系统的词汇结构, 在逻牌上描述了生成系统各个层次之间的生成原理, 并提出一种应用于生成系统的转换方法——词项位生成法。为了说明清楚, 整个叙述列举了从中间语言图到目标语句子转换实例。",
1993,CEMT—Ⅲ系统中汉语兼类问题的处理,"赵铁军,毛成江,张民,李生","汉语中词的兼类是一个普遍存在的现象。任何工程化的汉语句法分析系统都不能回避这个重要而难以解决的歧义问题。本文根据汉英机器翻译系统CEMT-III的有2万词条的机器词典进行了统计, 其中兼类词占7.7%, CEMT-III系统采用多级渐进处理策略, 将确定性推理和非确定性推理相结合, 实现了汉语词的兼类自动消除机制。",
1993,汉字认知心理研究对机器自动识别汉字的启示,"韩布新,陈一凡","几项认知心理学实验研究从不同角度一致证实, 方块汉字的四个等分象限所含的字形特征信息童不同。在人类识别汉字时作用也不一样。其中以左上象限最重要, 右下象限的作用则要弱得多。本文结合部件的象限位置频率, 讨论了这些结果对汉字机器识别的一些启示。",
1995,页面描述语言PostScript字库机制的一个层次式实现模型,"胡长原,张福炎","将国际流行的页面描述语言PostScript扩充以支持高质量的汉字页面的输出, 是近年来国内外对PostScript语言及其解释器研究的一个热点。本文提出了一个PostScript字库机制的层次式实现模型。该层次模型从语言的描述能力到字库解释器的实现, 充分体现了作者的两个目标：一是兼容和增强PostScript语言对页面上正文输出的灵活高效的描述能力, 并推广到大字符集, 多文种的应用场合；二是在解释器的字库技术上突破Adobe Type 1的局限, 使解释器能支持多种实用的字库技术, 特别是国内的多种曲线轮廓汉字库技术。本文同时提出了将字库技术从页面描述语言中独立出来的研究与开发方法。","页面描述语言（Page Description Language),解释器(Interpreter),字库(font)"
1995,基于文本句法的文本生成模型,"张晓龙,姚天顺","本文引用语言学及符号学理论, 综合自然语言文本生成所涉及的知识, 构造了计算机文本生成的理论模型。文中探究了文本句法中的微观整合性和宏观整合性以及它们对文本生成的作用。这个以文本结构模型和文本意义模型为主体的文本生成理论框架,为进一步的计算机语言生成研究提供了方法依据。","语义分析,自然语言理解,符号学,文本生成"
1995,基于笔划特征的宋体字形衍生方法,"严伟荣,蔡士杰","本文介绍了基于笔划特征的轮廓汉字字形衍生方法, 并叙述了宋体轮廓汉字衍生系统在微型计算机上的设计与实现。","笔划特征,黑度,宽度因子,模板"
1995,我国汉字识别技术的历史、现状和展望,张炘中,本文论述了我国印刷体汉字识别、手写印刷体汉字识别、联机手写汉字识别的历史和现状。从实用化的角度指出了汉字识别技术今后的发展动向。,
1995,汉语文－语转换中的语言学处理,"蔡莲红,魏华武,周俏峰","本文介绍了一个计算机实现的汉语文-语转换系统, 它以词为单位, 将文本按句子输出, 可保持自然语言的韵律。为了改善合成语音的自然度和可理解度, 我们总结了一些语言学规则, 并应用于该系统中, 得到了很好的测试结果。","汉语语音,文-语转换,分词,韵律"
1995,手写印刷体汉字识别中近似字的区分,胡家忠,"本文详细地分析了手写印刷体汉字中字形相似字的特点。采用对汉字点阵图象进行方向变换的方法, 利用汉字的笔道（黑点）方向和背景（白点）的封闭率特征表现近似字的局部差异, 在相似字特征判定表的引导下, 对字形相近的字进行判别, 取得了令人满意的效果。",
1995,异或哈希算法查找中文词组性能评价,林亚平,"本文根据汉字机内码的特点, 利用异或哈希算法建立中文词库和查找中文词组。根据不同规模的中文词库, 给出了相应的改进算法。对实际的中文词库测试表明, 此哈希算法分布均匀、冲突较少, 且速度快, 因此具有较好的实用性。",
1995,一种优化的并行汉字／字符串匹配算法,"王素琴,邹旭楷","字符串检索指在一个文本Text=t1…tn中找出一个字符串Pat=p1…pm的所有出现。本文给出了在CREW/CRCW PRAM机器模型上并行检索汉字/字符串的算法, 它使用n/m。个处理机, 预处理时间为O(m+|∑|, 并行执行时间为O(m)。","并行算法,文本,模式,字符串检索,搜索状态向量,字符一模式匹配向量"
1995,中华汉字输入编码方案,"周建钦,赵志远,刘世民","本文首先论证了利用西文键盘的汉字输入, 完全根据汉字的音, 形或意等属性编码, 很难达到最优。因而, 我们提出既利用汉字的某些属性编码, 同时又强制固定汉字的一些编码, 使得编码位数达到最优","汉字输入,编码,拼音顺序"
1995,一个高精度的简、繁体印刷体汉字文本识别系统,"张炘中,沈兰生,刘秀英,李燕,闫昌德","本文叙述了一个基于改进的“汉字识别特征点方法”的高精度简、繁体印刷体汉字文本识别系统。引入特征点的方向属性, 明显地提高了“汉字识别特征点方法”的汉字识别率。文中阐述了该系统各主要环节的原理。经过百万汉字真实印刷文本的严格测试,本系统汉字识别率达到97.84%。对质量较高的真实印刷文本, 汉字识别率达到99%以上。",
1995,以压缩词库为数据基的重码自动区分技术,"黄希琛,崔广才","重码自动区分技术是汉字键盘输入技术中的重要研究方向, 是解决编码易学和输入快速之间矛盾的有效方法。本文首先介绍了联想字库的结构和双向联想区分重码的技术原理, 接着阐述了压缩词库的存储结构以及区分重码所采用的查询、生成、匹配和剪切技术。","汉字输入输出,重码处理,信息压缩"
1995,中文姓名的自动辨识,"孙茂松,黄昌宁,高海燕,方捷","中文姓名的辨识对汉语自动分词研究具有重要意义。本文提出了一种在中文文本中自动辨识中文姓名的算法。我们从新华通讯社新闻语料库中随机抽取了300个包含中文姓名的句子作为测试样本。实验结果表明, 召回率达到了99.77%。","中文姓名自动辨识,生词处理,汉语自动分词,中文信息处理"
1995,曲线轮廓汉字字形缩放与还原中几个问题的研究,"黄宜华,袁春凤","本文主要讨论了曲线轮廓字形缩放与还原中两个重要的技术问题。首先描述了一个新的用于提高还原速度的快速封闭区域填充算法。然后, 给出了一个笔划缩放误差调整技术, 它可保持缩放字形笔划的均匀美观；同时, 文中给出了一个完整的字形缩放与还原算法。",
1995,藏文信息处理属性统计研究,"江荻,董颖红","本文统计分析：1、藏字的字长和构词频度；2、藏字的声母和韵母结构方式及频度；3、藏字的位置字符及结构方式。通过统计分析, 从藏字结构方式的量和位置字符的量的度量揭示其质的面貌, 为藏文研究和藏字信息处理应用提供基础数据。",
1995,关于汉字的两个分组查找算法,"周建钦,马述杰,李进忠","处理汉字的以比较为基础的二分查找算法, 其复杂性为O(NlogN)。本文结合概率论知识, 提出汉字的随机分组查找算法和分组散列查找算法, 给出算法描述, 并证明其算法复杂性为O(N), 从而优于二分查找算法。最后给出实验结果。","汉字,二分查找,随机分组查找,分组散列查找,概率分布"
1995,科技汉、日词汇的计算机计量及中日英文字的比较,"王懋江,吴振益","本文对中日两国工业技术词汇的平均词长及排序时间进行了测量并得到大量数据。实测数据表明, 外来语名词（英语名词或其他语种的名词）的意译比音译, 其译名的词长要短得多, 故作者提倡外来语名词意译成汉语名词, 而不赞成音译。",
1996,汉语书面语的分词问题──一个有关全民的信息化问题,陈力为,"汉语的书面语是按句连写的, 词间无间隙。因此在汉语书面语的处理中, 例如, 统计、分析、理解等, 我们首先遇到的问题是词的切分。把按句连写转换为按词连写, 所以, 词的正确切分是进行汉语书面语处理的必要条件它的任何错误都将使处理结果受到或大或小的影响, 有时是严重的影响。",
1996,利用上下文相关信息的汉字文本识别,"夏莹,常新功,马少平,朱小燕,金奕江","为了改善汉字文本识别率, 本文提出了一种基于语料库统计概率的后处理方法, 该方法利用上下文相关信息, 超过词汇对于汉字文本识别, 把具有确定性边界的一个汉字序列多数情况为一个句子作为一个处理单元, 利用统计获得的字字同现概率,采用动态规划方法, 获得了令人满意的效果。","汉字识别,语料库语言学,MARKOV模型,后处理"
1996,基于基因算法的时间规正算法,"贺前华,韦岗,徐秉铮","本文提出了一种适用于孤立字识别的基于基因算法的时间规正算法;详细讨论了其中一些关键技术, 如编码方法、适应度技术、基因操作子设计等。该算法可弥补动态时间规划DTW的某些不足(1)使距离归一化因子M与实际路径相关, 这使不同路径的比较更合理(2)以自然方式提供多条最佳规划路径建立了试验数据库, 在试验结果的基础上提出了算法性能分析模型模板间距离遵循正态分布。通过与DTW及串行多路径搜索法的性能进行比较, 结果表明基因时间规正算法具有明显的识别优势。",
1996,蒙古文信息处理通用系统内部码体系结构详析,拉西吉格木德,"本文, 归纳蒙古文字特殊性和分析现有蒙古文系统后, 指出建立具备通用处理功能, 又与西文、汉字兼客的蒙古文信息处理系统的关健, 在于内部码体系结构的建立上然后, 对几种内部码体系结构设计方案, 进行了详细地分析比较, 并探讨了它们的可行性。",
1996,面向中小学的汉字编码性能指标分析,何克抗,"本文根据社会需求分析和在中小学现场试验的结果, 提出了面向中小学的汉字编码性能指标体系, 在此基础上深入地分析了实现该性能指标体系的具体措施与方法",
1996,现代汉语语法信息词典规格说明书,"俞士汶,朱学锋,王惠,张芸芸","《现代汉语语法信息词典》是为计算机实现汉语分析和汉语生成而研制的一部电子词典。这部电子词典可以在语言信息处理的广泛领域中得到应用。本词典的详细规格说明书的初稿制订于1990年。在八五攻关期间1991年至1995年,一方面严格按照规格说明书进行词典内容的开发, 一方面在开发过程中又对规格说明书进行了局部的调整与修订, 于1995年11月形成的现在的版本。这份规格说明书也是汉语信息处理研究的一项重要成果。#br#现在发表的这份规格说明书共分以下五章第一章介绍词典的设计目标与结构第二章介绍总库的属性字段。第三章介绍各类词库的共同属性字段,第四章介绍各类词库专有的属性字段。第五章介绍《现代汉语语法信息词典》于年月通过专家鉴定时所达到的规模附录给出了面向信息处理的现代汉语词语分类体系的词类代码表。","现代汉语,语法信息,词类,电子词典"
1996,计量语言学统计分析软件系统,"郑玉玲,沈米遐,徐昂","本文介绍近期完成的国家自然科学基金项目藏缅语语料库及比较研究的计量描写的软件系统。该系统建立了我国境内藏缅语族五大语支个语言点扬万词条的开放性词汇语音数据库。研制了语言特征统计, 语言比较研究软件。设计了应用于多种语言谱系分类比较研究的语音对应关系“ 全方位交叉” 算法。对藏语方言的音节、音位、声母、韵母、声词、词素、构词能力和语音结构等余项特征做了分布和对比统计。对藏语乃个方言点做了语音对应关系和音系对比关系的量化描述, 并在此基础上做出具有历时与共时比较研究意义的相关和小相关分析, 得出了语言分类的相关矩阵和聚类分析图表","藏缅语,语料库,计量分析"
1996,建立现代汉语依存关系的层次体系,"刘伟权,王明会,钟义信","依存语法通过分析语言单位内成分之间的依存关系揭示其句法结构本文针对从短语到句群的各级单位内部的各种依存关系展开讨论。提出依存关系普遥存在于各级单位之中, 初步建立了汉语依存关系的层次体系。这一体系覆盖了大部分常见的语言现象, 经检验可应用于句法分析过程中, 作为表示句子结构的一种手段。为了增进体系的完备性和正确性, 显然还有许多工作要做。","句法分析,依据语法,依存体系,汉语"
1996,高速中文PostScript系统研究,"廖恒,吴昭,李三立","PostScript为桌面设计过程中, 排版印刷的后端处理的开放标准。高速中文PostScript系统的实现涉及到多方面的技术我们一方面研究了中文PostScript系统的软件构成和算法另一方面, 设计并实现了基于Intel 80960CA超标量亿次处理器的高速中文PostScript处理硬件系统, 进一步将其扩充为分布式并行处理系统, 并在通用PC平台上实现单机和并行中文PostScript系统此外, 我们还提出了存储压缩等相关技术。本文探讨了上述系统的构成, 并对系统进行了测试和性能评价。测试表明, 我们通过采用这些高速的硬件系统及与其相适应的软件算法, 基本解决了中文’处理速度慢和成本高的问题。","中文PostScript,Desktop publishing ,Parallel Processing,cluster computing ,supercomputing,memory enhancement"
1996,一种基于语言理解的输入方法──智能拼音输入方法,"吴军,王作英,郭进,王政贤","本文介绍“ 智能拼音”一一一种基于“ 语句”理解的快速汉字输入方法。这种方法利用汉语上下文的相关性, 实现拼音到汉字的自动转换, 使用者只需输入相应的拼音码,不用手工选汉字, 系统便才民据上下文自动将相应的汉字给出, 并在整个语句的范围内根据输入内容的变化对结果动态调整, 随时保证语句的正确。这种方法减少了击键次数, 并基本实现了盲打, 极大地提高了汉字输入的速度, 只要会拼音, 不用学习训练就能掌握。","智能拼音输入方法,拼音一汉字自动转换,台尔可夫过程"
1995,规则和统计相结合的汉语词类标注方法,周强,"本文分析了汉语的多类词现象与汉语词类标注的困难, 介绍了汉语词类标注中的规则排歧和统计排歧的处理策略以及规则和统计相结合的处理思路。按此思路设计的软件系统, 对封闭语料和开放语料的标注正确率分别达到了96.06%和95.82%。",
1995,汉字认知模型与形码方案设计,何克杭,"本文在深入分析人类识别汉字认知模型的基础上将认知心理学的理论方法系统地应用于汉字编码的形码方案设计的全过程。首先根据汉字认知模型提出三条相似性原则, 对汉字末级部件进行合理归并；第二, 根据人类联想记忆特征对部件进行科学分类；第三, 运用短时记忆的组块理论和汉字构形理论制定汉字拆分规则；第四, 运用图式理论制定汉字编码规则。整个方案符合中小学生的认知结构和认知特点, 达到较好的规范性、易学性与快速性。经一批试点学校的试验证明, 按认知理论研制的编码方案不仅有易学、易记的特点, 而且能促进中小学的语文教学改革, 有效地提高语文教学的质量和效率。",
1995,部件组合──潜在的汉字结构层次,韩布新,"本文提出了汉字结构中的一个潜在层次——部件组合, 并对其在汉字编码字符集（基本集）中的分布特征进行了统计分析, 发现绝大多数组合的组字次数和频率都很低, 高频组合很少。文中列出了低频组合中的60个高频部件, 以供汉字编码输入参考。最后讨论了部件组合在汉语教学及认知心理学研究等方面的应用意义。",
1995,邮政编码自动识别系统的研制,"盛立东,师春礼","本文从实际应用出发, 在对预处理中存在的问题进行分析解决的基础上, 开发了一套基于神经网络和锯法结构相结合的邮政编码自动识别系统。本系统软件采用C语言编写在486微机上实现。在对13970个扫描数据测试中, 其识别率为98.43%, 产生的误识率为0.136%, 拒识率为1.43%。","二值化,模式识别,神经网络"
1995,一种汉语电子词典的新结构,"刘东立,滕永林,姚天顺","汉语电子词典是汉语机器翻译系统的最基本的组成部分, 其组织结构的好坏对整个系统的效率具有直接的影响。本文提出一种节省存储空间且查询高效的汉语词典存储结构：以领头字为关键字的一级索引结构。通过理论推导和实例说明, 证明了该结构的高效性和实用性。文中对词典的一般组织结构作了简单的介绍, 并通过实例将其与新的词典结构作了比较。","机器翻译,电子词典"
1996,韩国语句子的内面格分析,毕玉德,"格作为解释以谓词为中心·反映其它成分以何种关来伴随述语的语义关来的工具, 对于研究分析表面结构规范、语法多‘ 现比较明显的自然语言昨常有利。本文以韩国语为研究对家, 利用格文法对其句子进行分析研究, 总结出了韩语句子的三个特点, 在充分论证的基础上, 提出了韩语句子的格规则等。本文目的在于建立韩语的句子模型, 即格模型。",
1996,一种汉字书写模拟练习软件的设计,"唐棠,陆兵","本文介绍了利用微型计界机现有输入设备书写汉字的一种学习软件之设计, 它接近真实的汉字书写练习, 有一定的应用价值。","汉语教学,字形设计,中文信色处理"
1996,汉字的两种模型和辅元音分键问题,叶楚强,"本文论述汉字的书写文化模型、汉字的电脑文化模型, 以及汉字的辅元音分健问题, 这些都是中文信息的根本问题。","精密语文学,智能计算机,汉字文化新领域,汉字输入,自然语言理解"
1996,笔迹鉴别的字符予处理与匹配,"刘成林,戴汝为,刘迎建","笔迹鉴别多用匹配方法比较字并的书写风格, 而字符困像的预处理和归一化对匹配是昨常重要的本文介绍笔迹鉴别的字符图像预处理和一种形状匹配方法。预处理主要介绍二值图像的噪声消除和归一化方法。嗓声消除的方法是平滑、轮廓跟踪和填充为保持字符中的书写特征, 点阵的归一化是线性的, 但字符位五和尺度的确定昨常重要。本文给出了三种归一化方法四边定界法、重心对准法和单边定界法, 并在此基拙上用图像匹配方法进行书写人识别的实验。匹配方法是通过距离变换快速实现的。实验结果表明, 重心对·准归一化最适合于笔迹鉴别问题, 距离变换匹配得到的识别率也比较令人满意","笔迹鉴别,预处理,形状匹配,归一化,重心对准,距离变换"
1996,关于汉字的分组排序算法及其复杂性,周建钦,"处理汉字的传统的排序算法, 其复杂性最少为。本丈结合概率论知识, 提出汉字的分组排序葬法, 给出葬法描迷, 并证明其葬法复杂性为, 从而优于传统的排序葬法。最后给出实验结果。","汉字,快速排序,分组排序,概率分布"
1996,汉语短语标注标记集的确定,"周强,俞士汶","本文提出了一个汉语短语标注的基本标记集, 并从句法功能和结构组成方面对不同短语的性质进行了深入的分析和探讨, 以期为汉语短语划分和标注的自动处理和人工校对提供一个统一的处理标准。",
1996,汉字图象的小波分析,"梁玉尧,李多,马争鸣","目前, 我们进行汉字模式识别所使用的汉字是以图象的方式输入计算机。要将其转换为计算机所能识别的字符, 其关键在于输入汉字图象特征的提取。作为图象的汉字有其自身的特点, 它是由较简单的笔划所组成的, 每种笔划又有其较固定的方向性即空问分步的固定性, 所有的汉字都是由几种简单的笔划所组成的。而小波变换为我们提供了一个十分有效的分析图象信色的多分辫率方法, 它可以将原始图象分解为模糊子图家和水平方向、垂直方向、料方向上的子图象。因此, 小波变换为我们分析汉字图象信息提供了一个十分有效的手段。本文正是基于汉字的土述特点, 并利用小波对空问频率的多分辫率分析方法,对汉字图象处理而得到汉字图象特征的。","小波变换,快速小波变换,汉字图象特征"
1996,基于假设检验的手写印刷体汉字识别方法,"黄铁军,胡家忠","为将统计决策方法和句法方法有机结合起来, 本文提出了以部件为基元的基于假设检验的手写印, 体汉字识别方法由统计方法得到候补字集, 利用部件特征的先验知识抽取待识字可能包含的部件并对假设进行验证, 从而不断缩小候补字集, 并逐步完善汉字的结构描述。初步实验表明其分类效果明显。","手写印刷体汉字识别,部件,假设检验"
1996,汉字字形的关系稳定原理,"王开铸,王英伟","本文对汉字的字形描述进行了深入的研究, 并在此基拙土总结得出了汉字字形的关来稳定原理在汉字字形中, 笔划基元的方向、长度、位置等属性均是不穗定的, 而各笔划塞元之间的关来是稳定的。基元间关亲是反映字形本质的因素, 是汉字字形信巴的主体。关来穗定原理作为反应汉字字形本质的重要原理, 除了在研究汉宇字形方面有重要意义之外, 最重要的应用就是对汉字识别的研究提供方向性的指导。","中文信色处理,汉字字形描述,关来稳定原理"
1996,藏文综合编码方案的研究与实现,"彭寿全,黄可,张义刚","目前, 藏文处理系统中普返存在着外字困扰问题, 本文首次实现了彻底排除外字的全新编码方案一一综合编码方案, 它由通行编码方案和外字编码方案两部分组成。通行编码方案按双字节模式编码, 外字编码方案采用组合叠加编码。文中对外字组合叠加编码方案作了深入地研究, 提出了外字运算符、外字描述和顺序输入叠加输出等概念设计了自动造字算法和程序, 解决了外字处理的一来列技术障碍, 并在叩一汉字系统版本一上实现了藏文综合编码方案, 其方法可引仲到其它藏文处理未统。",
1996,汉字输入法类的设计与实现,"赵雷,吕强,杨季文,朱巧明","本文指出了汉字输入法实现技术不足, 提出了一种全新的汉字输入法与汉字平台脱离的松散连接的结构休系, 给出了用面向对象技术来实现汉字输入法类的方法, 描述了汉字输入法类的结构, 简要说明了它的用法, 最后给出了一个用汉字输入法类实现汉字输入法对象的实例。","汉字输入法,类"
1996,语句级汉字输入技术,"王晓龙,王幼龙","本文讨论了包括声音输入、键盘输入、文字识别等各种形式的汉字输入技术的研究和发展, 阐述了按照字、词、语句作为汉字输入技术发展阶段的思怒, 提出了适用于上述各种形式的类码语句歧义处理问题, 该问题可描述为有向图求最短路径的问题。本文讨论了采用语法—语义分析和统计模型的最少元素概率推理方法和控制策略, 在知识库完备或不完备的情况下均可进行正常的推理, 并给出基于当时情况下的最佳结果。本文还简要介绍了几个应用事例。","人机接口,自然语言理解,中文信息处理,语句输入"
1996,数词的语义结构及通用翻译算法,"郭宏蕾,姚天顺","本文给出了数词的语义结构描述及知识表示, 提出高层位数词、子位数词的概念, 较好地刻化了各语种读数的语言规律。在此基础上, 提出一种通用数词翻译算法, 使数词翻译在实现土具有独立于语种的特查",
1998,受限语言子集的理论研究和探索,"宗成庆,宋今,陈肇雄,黄河燕","本文在综述受限语言研究成果的基础上,提出受限语言子集的一种形式化描述模型,并给出其相应的语言特性和数学特性,就受限汉语子集的确定方法问题进行了理论研究和探索。作者希望本文提出的表示模型和确定方法能够引起有关的讨论,并在充分认识受限语言研究的必要性和困难的基础上,将其引向全面深入的发展。",
1998,串频统计和词形匹配相结合的汉语自动分词系统,"刘挺,吴岩,王开铸","　本文介绍了一种汉语自动分词软件系统,该系统对原文进行三遍扫描:第一遍,利用切分标记将文本切分成汉字短串的序列;第二遍,根据各短串的每个子串在上下文中的频度计算其权值,权值大的子串视为候选词;第三遍,利用候选词集和一部常用词词典对汉字短串进行切分。实验表明,该分词系统的分词精度在1.5%左右,能够识别大部分生词,特别适用于文献检索等领域。","中文信息处理,自动分词,软件系统"
1998,中文自动编码原理,陈玉龙,"目前汉字编码设计和处理大都依赖人工作业,不仅效率低,而且性能差。文章指出:汉字编码技术的一项基本改革是系统内部信息结构的改革,以实现电脑对汉字特征信息的程控操作。描述了一种叫做汉字特征码信息结构的基本形式,并提出在“中文自动编码系统”中建立规范化的特征码汉字信息库,作为今后各类汉字编码的特征信息源。文章详细论述了从特征码汉字信息库到字、词编码生成的程序处理方法,从而证实了在汉字编码系统中建立字、词码表是没有必要的。文章最后指出,特征码信息结构不仅有助于实现自动编码操作和提高编码效率,而且也有助于电脑参与编码设计和改善编码性能。",
1998,汉语统计语言模型的N值分析,"张树武,黄泰翼","　N 元语言模型(n - gram) 作为统计语言处理的主要方法,目前在汉语语言处理(词性标注、字符识别、语音识别等) 中已得到广泛的应用。但是,具体N 取何值为较优,目前尚没有明确的定论。本文从对汉语短语语法模式的近似表示、对未登录语词的自动检测与重构能力、和实际的音文转换应用系统性能测试三个方面出发,综合比较和分析了基于汉语词的N 元语言模型中N 值的选择。并得出结论:对于基于真实词的汉语N 元语言模型,N 的取值范围应介于3 至6 之间,且N = 4 为较优。这一结论将有助于汉语统计语言处理的发展。","计算语言学,语言模型,语音识别"
1998,一种新的汉字字频统计方法,游荣彦,"本文用误差估计方法,在给定误差限和置信概率的条件下,解出了汉字字频统计的抽样规模,解出了一种汉字字频统计的抽样规模,提出了一种汉字字频统计的新方法,该方法中所定义的汉字的统计频率具有统计学上的无偏性且较之以前方法具有更小的方差,因而是汉字的使用频率的一种更为精确的估计。","使用频率,统计频率,抽样规模,置信概率,无偏性,有效性"
1998,中文文本压缩的LZSSCH算法,华强,"本文结合中文的特点,从建模编码、自适应索引扩位和最大索引位长等方面对LZSS 算法进行了修改,得到的LZSSCH 算法对以中文为主的中西文混合文本文件的压缩比平均与LZSS 算法高出约8 % ,而其压缩和扩展速度以及可执行程序的大小均与LZSS算法相当。算法无须任何预处理,还可用于压缩其它非拼音文字文本文件。","数据压缩,中文,文本,LZSS算法"
1998,联机中文签名鉴定的一种局部弹性匹配方法,"柯晶,乔谊正","本文提出了一种用于联机中文签名鉴定的局部弹性匹配方法。将签名切分为笔段,以包含特征的笔段为基元。基元抽取之后,首先根据一些比较简单的静态特征,采用动态规划方法寻求待测签名和参考签名基元之间的最优对应关系,根据最优对应关系,再同时考虑签名的静态和动态两类特征,对待测签名和参考签名的基元进行局部弹性匹配。在对680 个测试签名样本的识别实验中,取得了92. 6 %的平均正确识别率,在486/ 25微机上的平均识别时间为0. 9 秒,初步显示了本方法的可行性。","自动签名鉴定,基元抽取,基元对应,局部弹性匹配"
1998,关于汉字的熵和极限熵致编辑部的一封信,,"贵刊1997 年第3 期发表的王忠效先生的《汉语文本压缩研究及其应用》一文中批评我在过去测定的“汉字熵为9165 比特的结论是不科学的”( P61) ,并且指出,我的这一结论导致汉语文本平均压缩比为16/ 9165 ",
1998,中文文本中抽取特征信息的区域与技术,"刘开瑛,薛翠芳,郑家恒,周晓强","本文探讨了各种从中文文本中抽取特征信息的区域和技术。本文以新闻语料、科技论文、公文类文献为例,详细论述了从各类文本中抽取特征信息的区域与技术,对科技论文,还给出了一些可操作的产生式规则。无论对自动标引、自动分类,还是自动文摘的研究者而言,本文的方法与结论都有一定的参考价值。","中文文本,特征信息,文献自动化,公文文档"
1998,中文自动文摘原理与方法探索,"吴岩1,刘挺1,王开铸1,陈彬2","本文首先介绍了自动文摘的研究情况及存在问题,然后给出了计算机自动文摘的一般模型,最后介绍了我们所研究的两种自动文摘的原理和方法,及其实验结果。","自动文摘,机械文摘,理解文摘"
1998,定位格中手写体数字串的提取,"张涛,毛志宏,夏绍玮","本文主要针对手写体数字串和定位格(线) 相粘连的情况,首次提出完整提取这种数字串的方法。首先运用数学形态学运算进行粗处理,去除定位格,得到特征点,然后结合数字的结构特征对它修补,最后进行平滑处理。实验表明本文方法的有效性。","数字串提取,手写体数字,数学形态学,字符重建"
1998,一种混合的中文文本校对方法,"于勐,姚天顺","本文以模式匹配的方法和3 元文法分析的方法为基础,结合语法属性标注和分","中文校对,n元文法,自然语言处理"
1998,语句拼音-汉字转换的智能处理机制分析,"章森,宗成庆,陈肇雄,黄河燕","语句拼音- 汉字转换是中文信息处理研究的一个重要方面,是键盘汉字输入和语音输入的核心技术,其主要特征是对动态输入的拼音串进行词法分析,给出所有可能的汉语句子,然后对这些汉语句子根据上下文环境进行句法分析和语义分析,动态调整句子中的字词,输出最佳结果。近年来,语句拼音- 汉字转换系统大量应用了人工智能技术和机器翻译的理论,以期提高系统转换的准确率和增强系统的智能处理功能。本文分析了语句拼音- 汉字转换系统所采用的核心技术,即知识支持、自动分词和动态调整等,讨论了语句拼音- 汉字转换的处理方法和过程,知识库的组成结构,用于拼音串自动分词的算法和实现,音字转换中动态调整的概率模型等,本文还分析了现有语句拼音- 汉字转换系统在拼音串自动分词和音字转换的动态调整中发生错误的原因,并提出了改进方法。","自动分词,音字转换"
1998,宋代名家诗自动注音研究及系统实现,"穗志方1,俞士汶2,罗凤珠2","本文以160 万字的宋代名家诗为研究对象,介绍了一个宋诗自动注音系统的设计与实现。系统的资源包括语料库、知识库以及信息库;所采用的多音字自动注音策略有以下三种:条件概率策略、互信息策略以及规则策略。本系统的特色是将现代基于统计的语言模型与宋诗自身的音韵特点相结合来实现宋诗的自动注音。实验结果是令人满意的。","计算语言学,古籍整理,语料库,自动注音"
1998,德汉机器翻译中的语义消歧策略,"王永生,柴佩琪,卫蔚","本文首先分析了德语中的语义歧义现象,然后提出了几种借助配价和语义信息进行消歧的策略。这些策略目前都已应用于同济大学开发的TJ TITR 德汉机器翻译系统中。实践证明,它们不仅较好地解决了机器翻译中的语义歧义问题,而且大大提高了系统运行的效率。","机器翻译,动词配价,语义消歧"
1998,关于歧义字段切分的思考与实验,"刘挺,王开铸","通常认为:如果一个字段存在不同的切分形式,则称该字段为歧义字段。假设A ,B ,C 分别代表一个或多个字组成的字串,在字段ABC 中如果A ,AB ,BC ,C 都是词,则称ABC 为 交集型歧义字段。在字段AB 中,如果A ,B ,AB 都是词,则称AB 为组合型歧义字段。交集型歧义字段占字段总数的85 % - 90 %。",
1995,MMT（ODA）项目中基于中间语言的分析和生成的机制,"董亦农,郭锐","由日本联合中国、泰国、马来西亚和印度尼西亚共同开发的多国语言机器翻译（MMT)系统采用了中间语言的方法。本文简要地介绍了该项目的概况, 比较详细地介绍了该MMT系统的中间语言和分析、生成的机制。“概念”是中间语言的最基本的词汇, 本文尝试加以严格定义, 并且对于理想的概念辞典给出完全性、必要性、独立性和协调性的新提法。为了使机器翻译界的专家们能够通过篇幅不长的论文准确而完整地了解该系统的核心, 本文试图用形式化的方法定义系统的分析规则和生成规则的描述语言。本文是作者关于MMT的一些思考和总结, 希望能对今后这方面的工作有所稗益。","机器翻译,MMT,中间语言,分析,生成"
1995,论歧义结构的潜在性,冯志伟,"本文把作者在科技术语结构研究中提出的“潜在歧义论”(PA论)进一步推广到日常语言, 说明在汉语日常语言中也广泛地存在着潜在歧义结构, 而在具体的语言文本中, 许多潜在歧义都消解了。自然语言有歧义性的一面, 又有非歧义性的一面, 潜在歧义论正好揭示了自然语言的歧义性和非歧义性对立统一的规律。潜在歧义论指出了潜在歧义结构本身就包含了消解歧义的因素, 因而这种理论可为自然语言处理提供消解歧义的方法和手段。","潜在歧义论（PA论),PT-结构,实例化,歧义性,非歧义性,歧义消解"
1995,汉语语料的自动分类,"吴军,王作英,禹锋,王侠","语料库语言学的发展要求语料库的规模越来越大。随着电子出版业的迅速发展, 获取大量机读文本建立大规模语料库已成为可能。但是收集来的粗语料是杂乱无章的, 在作加工整理前必须分类。若用手工分类则工作量很大。本文介绍了一种语料自动分类办法。它采用文中提出的语料相关系数的概念, 并利用不同类语料相关系数不同的特点进行分类, 取得了93%的大类分类正确率。","语料库,语料分类,相关系数"
1995,汉语教学系统CTS的设计与实现,"王雍,陈增武,王泽兵","CTS是一个针对以“语法--结构”为大纲, 结合功能法教学的教学策略实现的汉语教学系统。本文详细描述了CTS的课件库组织结构和对象管理机构的管理模型, 并在此基础上介绍了CTS的设计与实现。","课件,超文本,单媒介对象,链接"
1995,汉语输入编码中简码字、词的合理选配,"韩布新,任雪松","本文分析了几种常用汉字编码方案的简码字表, 发现有很多不一致之处。考虑到简码字的合理选取数量、记忆量和键位安排等因素, 提出汉字的使用频度/构词能力级比率要较单纯使用频度指标更为合理。根据这一指标, 在“汉字属性信息数据库”基础上, 找出了78个简码字和120个简码双字词, 并进行了相应的键位安排以便于实际应用。",
1995,自由码及其词组输入处理方法,"何尔恭,曾锡山",本文提出自由输入法的特点及说明。还提出了与自由码输入环境有关的词组输入处理方法。,"汉字信息处理,输入方法,汉字编码,词组输入"
1995,汉字原型与手写汉字识别,姜珊,"本文评述了目前三种汉字的计算机表示和二种传统的汉字结构分析方法。应用拓扑和几何的基本原理, 分析汉字结构及其制约关系。从而, 确定四类组成汉字的基本关系并在此基础上实现了汉字原型, 给出了把汉字原型应用在手写汉字认别的实例。","汉字结构,汉字原型,手写汉字识别"
1997,汉语短语的自动划分和标注,周强,"考虑到传统的基于规则的汉语分析器对大规模真实文本的分析所遇到的困难, 本文在使用统计方法进行汉语自动句法分析方面作了一些探索, 提出了一套基于统计的汉语短语自动划分和标注算法, 它分为预测划分点、括号匹配和分析树生成等三个处理阶段, 其间利用了从人工标注的树库中统计得到的各种数据进行自动句法排歧, 最终得到一棵最佳句法分析树, 从而可以自顶向下地完成对一句句子的短语自动划分和标注, 对一千多句句子的封闭测试结果表明, 短语划分的正确率约为86%, 短语标注的正确率约为92%, 处理效果还是比较令人满意的。","短语自动划分和标注,语料库加工"
1997,时间语义层次结构及理解,"郭宏蕾,姚天顺",,
1997,规则动态选择与路标记忆算法,"宗成庆,陈肇雄,黄河燕","本文提出一个语法分析中的规则动态选择与路标记忆算法。该算法提出了实时记录规则调用频度, 通过调用频度对规则进行动态选择的处理方法以及设置路标信息记忆表以减少回溯次数的语法分析思想。通过对该算法的设计与分析, 深入探讨了快速、高效的语法分析器设计方法以及规则优先次序的动态选择方法。","机器翻译,语法分析,规则选择,回溯"
1997,古诗研究的计算机支持环境的实现,"刘岩斌,俞士汶,孙钦善","北京大学正在开发以全宋诗为首选对象的古诗研究系统——古诗研究的计算机支持环境。本文介绍这个系统的设计与实现。该系统已录入陆游的全部诗作9000余首及部分注释, 计88万字。本文介绍了该系统的基本功能如：检索、阅读、统计和辅助研究工具及其实现技术, 较详细地介绍了其中的全文检索和超文本技术。本文也介绍了利用该系统可以深入开展的一些研究课题, 如：古诗格律研究, 古汉语研究和诗人风格研究, 从而可以辅助解决古诗研究中的一些困难问题。","古诗电子化,全文检索"
1997,骨架汉字字形存储与显示技术,"赵恒,金通,王国瑾","本文通过对骨架技术的分析, 提出了一种基于骨架技术的计算机汉字存储与显示方式, 即骨架汉字字形存储与显示技术。为了控制骨架汉字的笔划形状, 木文运用了两个控制参数, 取得了较好的控制效果。","骨架技术,骨架汉字,字形存储与显示"
1997,语句级音调规律的研究与实现,"吴岩,刘挺,李秀坤,王开铸","本文首先讨论了人的发音规律, 然后介绍了在语句级上提高语音合成质量的小系统, 并对其实现过程进行了形式化描述, 最后给出了本系统已实现了的实例。","文语转换,语音合成,人工智能。"
1997,基于语境类似度的并列成分的判定方法,"简幼良,高健,王秀坤","木文针对日语处理中的疑难问题之一—长句并列成分的系受关来和范围的判定, 介绍了日本长尾真等人提出的关于并列关键字语境类似度的日语并引成分的分析方法。该方法对日语并列的分类、关列关键字的确定、类似性的决定因素及其量化、并列构造范围的求解等进行了详细的讨论, 并给出了算法。我们把这种算法应用到我处开发日中翻译系统“孙悟空”里, 并进行了一定的调整和补充, 取得了比较满意的效果。",
1997,一种实用型汉语词汇处理系统CWP之设计,"唐棠,戎启俊",本文介绍了一种微型计算机汉语词汇处理系统及其分词算法的设计、该系统在中文信息处理技术中有较好的应用价值。,"词库,分词算法,中文信息处理"
1998,汉语句法规则的自动构造方法研究,"周强,黄昌宁","本文对汉语句法规则的自动构造方法进行了一些探索。通过对汉语句法规律的总结和提炼,提出了一套简单灵活的汉语句法元规则描述体系,包括结构元规则集、标记特征表和中心标记表等部件,在此基础上,构造了一个有效的元规则解释器,取得了较好实验效果。","上下文无关语法,元规则,语法构造"
1998,汉英机器翻译中的冠词处理研究,"常宝宝,刘颖,刘群","对于汉英机器翻译而言,由于汉语中缺乏与英语冠词相应的语言范畴,而且面向人的冠词用法规则很难满足机器翻译处理的需要,冠词的误用严重地影响了最终译文的质量。本文提出一种将基于转换的错误驱动的学习机制用于冠词处理的策略,初步实验显示,这种方法可以有效地提高机器译文中冠词使用的准确率。","机器翻译,冠词选择,基于转换的错误驱动学习"
1998,日汉机器翻译系统中的词典讨论,"雍殿书,胡海文,陈家骏,王启祥","本文讨论了日汉机器翻译系统中有关词典的同音词、同型词、兼类词、挑选汉译词以及惯用型处理等几个问题,这些问题的解决将直接影响日汉机器翻译系统的译文质量。","机器翻译,词典,同音词,多义词,惯用型"
1998,一种无约束手写体数字串分割方法,"赵斌,苏辉,夏绍玮","针对无约束手写体数字串中的连笔字符,本文提出以基于识别的分割方法为主,结合运用剖分方法和全局识别方法等多种分割策略的数字串分割方法。这种方法直接针对数字串分割,也可以运用到非数字字符串的分割中,其分割思想对连笔汉字的分割也具有一定指导意义。","分割,字符识别,轮廓,投影,动态规划"
1998,汉语三字词声调的模式分析,"陶维青,徐士林,钟金宏","汉语三字词的声调模式是复杂的。本文对4 男4 女各192 个三字词的声调进行分析,选择各音节的头尾差和相对调位比为特征,在进行特征抽取、统计和分析的基础上,研究了三字调整声调的模式和变调规则。本文的结果对三字词和连续语音声调的识别具有重要价值。","三字词,特征抽取,模式分析"
1998,紧凑的、基于对象的脚本语言——JavaScript,"吴保平,张波","JavaScript 语言是用来编制Web 文档的流行工具之一,它使Web 页不仅可动态显示信息,而且还可交互处理信息。本文旨在较系统地介绍JavaScript:JavaScript 的特点、基本数据类型、语句、函数、对象(Object ) , 库对象模型, 事件以及用J avaScript 和HTML 制作HomePage 的方法。","JavaScript,语言,对象,事件,HomePage"
1998,汉语属性库OLAP系统的设计,"朱巧明,李培峰,吕强,杨季文","本文介绍了一个基于C/ S 结构的汉语属性库联机分析处理(OLAP) 系统的设计。文中详细阐述了汉语属性库的存储结构和系统工作平台。同时,文中给出了客户机端和服务器端应用系统的设计思想及实现方法。","OLAP,汉语属性,C/S 结构,分析,重组"
1998,中文文本自动校对技术现状及展望,"张仰森,丁冰青","本文概述了中文文本自动校对技术的产生背景,分析了预校对文本常见的错误类 型及文本自动校对(自动查错和确认纠错)的难点,探讨了当前商品化的文本校对软件的校对策略和发展趋势。","中文文本自动校对,自动查错,确认纠错"
1998,也谈汉语书面语的分词问题——分词连写十大好处,张小衡,"单词的切分对现代汉语的运用、研究和计算机信息处理等都具有相当重要的意义。本文阐述书面汉语分词连写的十大好处, 并讨论一些实施方面的问题。文章全文分词连写。","汉语,书面语,分词连写"
1998,全文检索为WWW增添生机,卢献华,"Internet 正以200 %的用户增长率迅速发展,成为人们在工作和生活中不可缺少的工具。人们可以通过Internet 收发邮件,查找信息,订购产品。WWW(World Wide Wed 全球信息网)是Internet 的一个重要组织部分,是近年来在Internet 上发展最快的服务之一。WWW 服务器以超文本的形式组织信息,用户可以通过Internet 利用MS2IE、Net scape 等WWW 浏览器方便地浏览这些信息。WWW 上的信息量远远超出人们可以想像的范围,而且仍在快速地增长。据统计,中国的四大互连网Chinanet 、CERnet 、CSTnet 和Gbnet 有32 万以上用户,接入网1000 多家,有12 万以上用户。全球的WWW 服务器数目已经达到几十万,全球的WWW 网页数量每50 多天就会增加一倍。随着信息量的膨胀,人们在信息的海洋中搜索自己所需要的信息的能力显得越来越重要,在这种需求下,越来越多的WWW 服务器开始加入辅助人们进行信息查询的检索工具,而几乎所有的检索工具都采用了相同的方法———全文检索( Full -text ret rieval 或Free - text ret rieval) 。",
1998,手写汉字的集群识别,"姜珊,孙玉方","为了降低单个汉字的分辨率,论文分析了通用的汉字识别模型,并在此基础上建立了适于多字识别的集群识别模型。为了充分论证集群识别模型的观点,本文从理论证明和实验两方面获得支持根据。实验结果表明基于多字识别模型的集群识别能可靠提高对连续文字的识别效果,是手写汉字识别中很有希望的发展方向。","手写体汉字识别,汉字识别模型,集群识别"
1998,图纸矢量化中多向字符的分割,"邹荣金,蔡士杰,张福炎","本文描述了工程图中多向字符串的分割方法,提出一种新的高精度字串的迭代定向算法,有效地解决了倾斜短字串的方向纠正。同时对字线相交时的推理分割方法也作了介绍。研究中使用了图象形态分析的方法,直接在原图象上进行整体识别,保证了图象信息的完整性和可检测性。","工程图纸矢量化,计算机图形学,图象分割,CAD,识别"
1998,基于连续语音识别算法和词树约束的汉语词组语音识别,"杨浩荣,孙甲松,王作英","具有确定词表的词组语音识别是语音识别研究的一个重要方面,应用相当广泛。本文在简单介绍词组语音识别之后给出了一个基于连续语音识别算法和词树约束的汉语词组语音识别方法。这种方法通过在束搜索连续语音识别算法中引入词树约束信息,发挥了连续语音识别算法的优点,并且充分利用了确定词表的约束信息,提高了计算和搜索的效率。然后介绍了约束词树和它的高效存储结构,这种结构提高了约束词树的存储效率和在识别搜索中的检索效率;最后给出实验的结果和讨论并进行简要的总结。","语音识别,词树约束,词组"
1998,基于多知识源的同音词识别方法,"宗成庆,章森,陈肇雄,黄河燕","本文提出了基于多知识源的同音词识别方法。该方法利用上下文条件测试函数实现了不定范围的信息相关处理,并根据词性、语义、位置、音节和词频等多种关联信息进行同音词综合识别,取得了较好的同音词识别效果。","音字转换,同音词识别,上下文分析"
1998,扩展词组数最小法的假名汉字转换,中岛晃,"本文描述了改进后的“词组最小法”、并提出了新算法。它被名为“扩展词组最小法”。重新定义了句子中词组的计算方法。为了实现此目标,从始读到句子假名的全部读入,将词库查询及语法检查的结果以“树”型数据加以保留。采用上述算法后,以假名文字为单位的变换率可达95. 8 %;以词组为单位的变换率可达88. 9 %。","词法分析,词组数最小法,假名汉字转换"
1998,面向EBMT的汉语单句谓语中心词识别研究,"穗志方,俞士汶","在基于实例的汉英机器翻译( EBMT) 系统中,为计算语句相似度,需要对句子进行适当的分析。本文首先提出了一种折中的汉语句子分析方法———骨架依存分析法,通过确定谓语中心词来把握句子的整体结构,然后,提出了一种根据汉英例句集中英语例句的谓语中心词来识别相应的汉语例句的谓语中心词的策略。实验结果是令人满意的。","基于实例的机器翻译,语句相似度,谓语中心词,知识获取,语义匹配"
1998,一类规范文本篇章结构的自动标引,单永明,"本文通过对汉语文本中标题和段的级、标题的型等概念的描述与分析,讨论了汉语文本篇章结构的标引问题,提出了规范文本的概念,并给出了规范文本篇章结构的一种标记方法,在此基础上,讨论并实现了规范文本篇章结构的自动标引,给出了标引算法。","中文信息处理,文本自动分析,自动标引,篇章结构,标引算法"
1998,数据库汉语查询语言的分词研究与实现,"徐九韵,仝兆岐,向逐聪,王新民","在综合考虑数据库查询这一特殊性的基础上,根据查询语句中词汇对数据查询不同贡献程度分级建立分词词典;然后提出了分步- - 正向单扫描的分词方法(DSWS) ,并对该分词方法的时间复杂度进行了分析。","汉语分词,分词词典,数据库查询"
1998,汉字异或动态散列分组查找算法,"王忠效,范植华","本文根据汉字内码特点,提出一个适合汉字信息处理用的汉字动态散列分组查找算法。该算法采用简单的异或散列函数将汉字进行分组,组内取链式结构顺序查找。由于散列均匀,其渐近时间复杂度为O (1) 。","汉字查找,散列查找,散列函数,自适应散列查找"
1997,汉字输入键盘设计方法的研究——兼论标准汉字双拼键盘的设计,"杨道沅,李棣","本文提出了汉字‘权值’的概念, 并以汉字‘权值’做为衡量标准, 以汉字输入双拼键盘的设计为例, 对如何设计一个优秀的汉字输入键盘的方法进行了详细研究。",
1997,论多媒体技术在语言信息处理中的作用,张普,本文分为三部分:,
1997,《现代蒙语词频统计软件系统》的设计与实现,"吉日木图,嘎日迪,赛音,达·巴特尔","本文介绍了《现代蒙语词频统计软件系统》的设计原理与实现方法。重点论述了统计模型的建立, B-树数据库的实现, 同形词标记, 单词统计, 复合词统计, 合并, 频度排序, 使用度排序, 读音排序以及结果输出等语料处理软件系统的设计与实现。","现代蒙语,词频,统计,B-树,数据库,排序"
1997,阿拉伯文字、汉字兼容处理系统,"吴宗尧,李健,艾尔肯,吴丹竹","本文描述阿拉伯文特色, 提出阿文与汉字兼容的一种编码方法, 介绍典型系统Windows3.1 for Arabic与Chinese Star 2.0的系统集成原理和方法。","阿拉伯文,兼容处理,系统集成"
1997,多字体印刷维吾尔文的切分,"哈力木拉提,丁晓青","在许多文字识别系统中, 字符切分是预处理阶段的一部分, 其目的是从文本图象中分离出字母图象。而后才能针对切分后的每个字母进行识别。在具有连体特征的文字中, 字符切分就显得特别重要, 因为字符切分的准确与否直接影响字符的识别。维吾尔文就具有这种明显的连体特点, 本文主要讨论了采用抽取投影特征的方法, 实现了多字体维吾尔文的行切分、字切分和字符切分。","维吾尔文,连体字符,字符切分,文字识别,投影,特征,草书"
1997,汉语受限语言的设计与应用,"孙健,张尧,王启祥","在机器翻译和自然语言理解等领域内, 受限语言的研究是一项有意义的工作。本文在分析考查现代汉语岐义短语的基础上, 设计了一个汉语受限语言, 籍以对存在汉语中的岐义进行受限处理, 并且给出了一个应用实例——面向受限汉语的机器翻译前编辑系统。","汉语,受限语言,歧义,机器翻译,前编辑"
1997,一种文本理解的知识表示方法,"麻志毅,姚天顺","对自然语言文本的理解, 应该把它与一定的情境联系起来。本文正是基于这样的思想, 讨论了一个文本所描述的事物及其有关情境是如何在机内表示的。","情境理论,复杂特征集,知识表示,自然语言理解"
1997,汉语文本压缩研究及其应用,王忠效,"汉语文本压缩至今很少受到重视, 然而, 作为许多计算机应用系统的支撑技术, 其重要性毋庸置疑。本文结合汉语文本的特征对现行文本压缩技术进行评述, 指出汉语文本理论上可能获得的平均压缩比率(〉3.9) 及现行压缩算法所能达到的水平(1.6左右)。此外, 讨论了汉语文本压缩的研究方向以及几种典型的应用。","汉语文本压缩,算术编码,Huffman 编码,Lempel-Ziv算法,熵"
1997,基于变换的汉语句法功能标注探讨,"周明,潘海华","本文尝试利用基于变换的方法标注中文句子词汇的句法功能。系统输入已分词并标注了词性的句子, 输出每个词的依存关系。我们首先设计了一个由44种依存关系组成的汉语依存体系, 然后以人-机互助的方式标注了1300句中文句子。其中1100句作为训练文本用来获取标注规则, 余下200句用做测试。设计了17类变换模板, 采用基于变换的算法获取了60条有序的依存关系标注规则。在测试时, 对新词标注以该词词性所对应的最高频的依存关系作为初始标注以提高鲁棒性。实验表明这种方法简单可行, 取得了初步满意的效果。","基于变换的学习算法,汉语,句法标注,依存关系"
1997,HNC理论概要,黄曾阳,"主编按语:《HNC理论概要》的作者黄曾阳先生创立的面向整个自然语言理解的理论框架, 在语义表达上有自己的特色, 在语义处理上走了一条新路。鉴于汉语语法研究尚有诸多困惑, HNC理论所走的以语义表达为基础的新路子对突破汉语理解问题尤其有实际意义。",
1997,中文机构名称的识别与分析,"张小衡,王玲玲","中文机构名称数目庞大, 层出不穷, 绝大多数未能收入词典, 给自然语言处理带来困扰。但是, 从语言学的角度来看, 机构名称是一种偏正复合式专有名词, 同时又是一类较为简单的偏正名词词组, 有自己的结构规律和形态标记。本文以高校名称为重点,以中国内地、香港和台湾三地实际语料为依据, 从语言学和计算机技术两方面对机构名称的识别与分析展开讨论, 并总结出相应的规则。根据这些规则, 对六百多万字的三地语料库作高校名称识别, 正确率(指前后界定位均正确) 达97.3 % , 召回率为96.9 %。这些规则还可应用于拼音-汉字智能转换和机器翻译等其它领域。","机构名称,专有名词,短语分析,自然语言处理"
1997,印刷表格文本分析识别系统的研究,"曾湘宁,沈兰生,任鲲鹏","本文介绍了一个印刷表格文本分析识别系统。提出了表格特征点分析方法。在表格图象处理的基础上, 对表格线进行分析, 在考虑表格线和字符块粘连的情况下提取字符块, 判别汉字串和数英串后分别识别, 生成表格。实验表明本方法的有效性。","表格分析,印刷文本识别,特征点"
1997,汉语树库的构建,"周强,张伟,俞士汶","本文讨论了汉语树库构建的若干基础问题, 包括一个适合于自动分析和人工标注的汉语句法标记集、汉语树库加工处理规范和人机互助的树库加工模型, 介绍了一个已经实现的汉语自动句法标注系统, 和在此基础上进行的一些树库构建实验, 最后提出了构建大规模汉语树库的设想。","树库,句法标记集,树库加工规范,语料加工模型,语料库语言学"
1997,基于变长码的面向字符文本处理方法,赵旭晟,"传统的以字节为对象的文本处理方法在今天越来越显出它的弊端。本文提出的基于变长码的面向字符的文本处理方法, 在统一编码的基础上, 将逻辑字符与它的存储方式独立开来, 并以此为单位进行文本处理。这种方式可以有效地解决汉字等多字节字符集的编辑、显示、检索的不便, 还可使字符集的容量无限扩充。本文陈述了变长码的概念与面向字符的文本处理方法的基本思想, 并以汉字处理为例给出了几个层次的实现方法。","面向字符的文本处理,变长码,逻辑字符,编码,字符集"
1997,机器辅助翻译中模糊查词典和快速录入单词,"刘小虎,李生,吴葳","本文介绍在单词记忆不准确的情况下, 如何查找词典以及如何只键入单词中的几个字母快速录入单词的算法。在辅助翻译和写作系统中, 词汇级的帮助是最基本的, 主要指词典查询。但很多情况下, 用户单词记忆不很准确, 只记住了几个字母, 本文解决这种情况下的模糊查询问题。这种模糊技术的核心是全文检索, 依赖于词典的特殊索引。在解决了模糊查询之后, 利用全文检索技术以及模糊二分查找技术进一步开发了写作系统中的快速录入功能。","机器辅助翻译,模糊查词典,单词的快速录入,全文检索"
1997,一种基于马尔可夫模型的汉语语音识别后处理中的音字转换方法,"梅勇,徐秉铮","为了提高汉语语音识别率, 本文根据一种基于马尔可夫模型的统计语言模型去实现汉语音字转换, 在实现过程中, 提出了它的简化模型, 该模型不仅保证了实时性,而且也为以后的工作打下伏笔; 同时对训练文本的稀疏问题提出了一种新的解决方案。利用以上模型的模拟实验表明, 前向-后向的马尔可夫模型具有较好的识别性能; 且以词为输出单元的模型识别性能优于以字符为输出单元的模型。","语音识别,后处理,马尔可夫模型"
1992,自然语言篇章理解及基于理解的自动文摘研究,"王建波,王开铸","我们对文章结构进行了介析, 研究语言单位间的意义相关性, 基于这种相关性,提出意义分析方法, 并给出篇章的意义表示。最后, 分析了自动文摘研究现状, 介绍了我们在这方面的研究情况。",
1992,基于Hypertext和多知识源的智能汉语教学系统,"徐海平,阮晓钢,陈增武,胡上序,任海波","ICTS 是关于汉语知识的 智能型计耳机辅助教学系统。本文 首先给出了ICTS的总体结构和各主 妥部分的 设计思想, 提出了基于多知识源的推理机制; 接着阐述了Hypertext技术在ICTS构造中的应用; 最后对ICTS 的实现作出了总结。",
1992,一种低点阵字库压缩算法,蒋贤春,"本文介绍一种低点阵汉字库不失真压缩还原算法。该算法在压缩时考虑了汉字本身笔划的特奴, 有较高的压缩比。在还原汉字、对前缀码进行译码时, 给出一种块射的方法,使得译码速度提高了三倍, 这种译码方法适用于所有计算机前缓码译码问题。",
1992,一个大陆汉字与台湾汉字文本文件间的智能转换系统,"杨道沅,扶良文","本文叙述了大陆和台湾计耳机汉字系统的内码结构, 在此基袖上, 提出了设计一个实现两种汉字文本文件转换系统的方法。",
1992,重要词语辖区和结构前后界——自然语言理解的谓框—功能优选试探法,黄自由,"白然语言理解的关键问题之一是确定复杂长句的结构前后界。划界有错, 理解必误。本文提出“ 框架一功能优选试探法” 能正确有效地解决这一难题。谓框的主体部分—认识结构的基干是用笔者多年前提出的“ 多重空间格态模型” 来设计的。这一模型象染色体, 对表层结构的今析和生成起着关键性作用。试探法的基本做法是, 利用辖区原则, 先由谓框预期辖区模式, 匹配实有语句后将该句划成若干语块, 然后按语义一功能合格原则先块内再块间地以优选关系试探各单位的搭配, 直至正确确定该句的结构前后界和句法结构关系。",
1992,计算机汉字I/O处理的数学模型,钱培德,"汉字输入与输出是计算机汉字信息处理系统中的最基本部分, 本文对汉字输入与输出的数字模型作了探索。文章首先给出了汉字输入过程及有关编码的数学描述, 并重点描述了汉字的编码输入过程。文章然后给出了汉字输出过程及有关编码的数学描述。",
1992,汉语音字转换中同音字(词)的概率后处理,"唐武,杨行峻,郭进","本文论述了一种新的汉语音字转换的概率后处理算法, 该算法用字( 词) 相对于前后向拼音的条件概率代替常见的字( 词) 相对于前后向汉字的条件概率, 极大地压缩了数据空间, 提高了查找效率, 使概率后处理的实付应用史趋于现实。该方法也可应用于汉字印刷体和手写体的图形识别的启处理过程。",
1992,汉字是什么类型的键盘文字?,"叶楚强,彭昌","不从理论简单性的角度看问题, 汉字键盘输入法有亿万种。认识到偶体文字和偶体隐现文字是汉字时健盘文字类型, 汉字健盘翰入全面解决的时刻就到来了, 震古耀今的汉字就整个地从笔写文字时代进入到电脑写作文字时代。",
2006-01-18,汉语分词中组合歧义字段的研究,"秦颖,王小捷,张素香","汉语自动分词中组合歧义是难点问题,难在两点: 组合歧义字段的发现和歧义的消解。本文研究了组合歧义字段在切开与不切时的词性变化规律,提出了一种新的组合歧义字段自动采集方法,实验结果表明该方法可以有效地自动发现组合歧义字段,在1998年1月《人民日报》中就检测到400多个组合歧义字段,远大于常规方法检测到的歧义字段数目。之后利用最大熵模型对60个组合歧义字段进行消歧,考察了六种特征及其组合对消歧性能的影响,消歧的平均准确度达88.05%。","计算机应用,中文信息处理,汉语切分,组合歧义,最大熵,特征"
2006-01-14,面向机器辅助翻译的汉语语块自动抽取研究,"姜柄圭,张秦龙,谌贻荣,常宝宝","本文提出了一种统计和规则相结合的语块抽取方法。本文使用Nagao串频统计算法进行基于词语的串频统计,进一步分别利用统计方法、语块边界过滤规则对2-gram到10-gram语块进行过滤,得到候选语块,取得了令人满意的结果。通过实验发现,在统计方法中互信息和信息熵相结合的方法较单一的互信息方法好;在语块边界规则过滤方法中语块左右边界规则和停用词对语块抽取的结果有较大影响。实验结果表明统计和过滤规则相结合的方法要优于纯粹的统计方法。应用本文方法,再辅以人工校对,可以方便地获取重复出现的多词语块。在机器辅助翻译系统中,使用现有的语块抽取方法抽取重复的语言单位,就可以方便地建设翻译记忆库,提高翻译的工作效率。","人工智能,机器翻译,语块抽取,串频统计,内部结合紧密度,信息熵,语块组合规则"
2006-01-10,基于本体的专业机器翻译术语词典研究,"黄河燕,,张克亮,张孝飞","在专业机器翻译系统的设计和实现中,要解决的一个关键问题是如何有效地组织面向不同专业领域的专业术语,以及如何根据当前所处理的文本选择相应的术语定义。本文首先分析现有专业机器翻译系统在术语词典组织和建设方面存在的主要问题,以及基于本体(Ontology)的领域知识概念体系的特点;其次,探讨面向专业机器翻译的术语词典研究的几个重要方面,包括通用领域本体的设计、专业术语的描述和向本体的映射、双语或多语MT专业词库的组织和应用等;最后,介绍我们初步已完成的工作,主要包括机器翻译专业领域分类系统设计、专业词典向专业分类系统的映射、ICS标准向专业领域分类系统的映射等。映射实验结果表明,专业领域分类系统对于机器翻译专业词典具有良好的覆盖性。","人工智能,机器翻译,本体,术语词典"
2005-12-20,利用音译和网络挖掘翻译命名实体,"蒋龙,周明,简立峰","本文提出了一种新颖的方法,综合利用音译和网络挖掘来提高命名实体翻译的效果。具体而言,首先利用音译模型生成一个候选翻译,然后利用音译信息配合网络挖掘获得更多的候选翻译。最后,使用最大熵(Maximum Entropy)模型综合考虑源词和候选翻译之间的各种特征,如发音相似度,上下文本特征,网页共现关系等,来排序得到的候选翻译,从而决定最终的翻译结果。实验结果显示我们的方法显著的提高了命名实体翻译的精确度。","人工智能,机器翻译,音译,命名实体翻译,网络挖掘"
2005-01-02,跨语言相似文档检索,"王洪俊,施水才,俞士汶,肖诗斌","检索一篇文档在其他语言中的译文对于双语平行语料库的建立是一件很有意义的工作。本文提出一种改进的跨语言相似文档检索算法,该算法使用双语词典或统计翻译模型作为双语知识库,查找两篇文档的共同翻译词对,把翻译词对的权重作为一种特征来进行相似度计算,用Dice方法的改进算法计算双语文档的相似度。在实验中,统计检索文档的译文排在检索结果前 N位的总次数来评价算法的性能,并使用了两个噪音数据集来评价算法的有效性。实验表明,在噪音数据干扰比较大的情况下,译文排在检索结果前5位的译文结果接近90%。实验证明,翻译词对的权重对于相似度计算有很大帮助,本算法可以有效地发现一种语言书写的文档在另一种语言中的译稿。","计算机应用,中文信息处理,跨语言相似文档检索,文档相似度,双语文档对齐"
2005-01-12,基于中文机构名简称的检索方法研究,"钟良伍,郑方","对于是否是中文机构名或机构名简称的自动判别,已经有广泛和深入的研究;但是对机构名简称和全称的匹配,目前鲜有研究成果。本文针对基于中文机构名简称的检索方法,研究了机构名的结构特征,总结出两种规则,定制了一个基于关键词类的分词工具,提出简称和全称匹配的一种算法,并且结合多级索引技术,实现了基于中文机构名简称的检索系统。实验结果表明,本文所提方法的准确性较好,首选准确率达到近95%,在全称机构名总数达到51万的情况下,检索平均耗时约0.21秒,达到实用要求。","计算机应用,中文信息处理,多级索引,模糊匹配,分词算法"
2005-10-09,基于段落匹配和分布密度的偏重摘要实现机制,"林鸿飞,杨志豪,赵晶","本文提出了基于段落匹配和分布密度的偏重文本摘要实现机制,旨在满足摘要的个性化要求。首先在关键字同义扩充的基础上,利用基于侧面相似度的段落匹配方法,获取相关的文本段落集合。然后通过计算文本窗口的分布密度函数,获取关键字集聚区域,依据覆盖区域的句子权重,输出的最终偏重摘要。最后进行了评价实验,通过问答测验和相似比较,效果良好,而且表明偏重摘要对于多主题文本更为有效。","计算机应用,中文信息处理,文本摘要,偏重摘要,同义扩充,段落匹配,分布密度"
2005-12-30,基于SDC特征和GMM-UBM模型的自动语种识别*,"姜洪臣,郑榕,张树武,徐波","本文提出了一种基于SDC特征和GMM-UBM模型的自动语种识别方法。SDC特征由许多语音帧的一阶差分谱连接扩展而成,与传统的MFCC特征相比,包含了更多的时序特征信息。UBM模型反映了所有待识别语种的特征分布特性,借助贝叶斯自适应算法可以快速得到每个语种的模型。与传统的GMM方法相比,该方法的训练和识别的速度更快。该方法对OGI电话语音库中11个语种进行了测试,其10秒、30秒和45秒句子的最佳识别正确率分别为72.38%、82.62%和85.23%,识别速度约为0.03倍实时。","计算机应用,中文信息处理,SDC特征, GMM-UBM模型, 贝叶斯自适应,自动语种识别"
2006-02-27,基于约束模型的韵律短语预测,"董宏辉,陶建华,徐波","本文提出了基于语法约束和长度约束的韵律短语预测模型。在语法约束模型中,我们引入了组块作为基本的节律分析单元。韵律短语的长度约束模型是利用隐马尔科夫模型对语句中韵律短语的长度规划进行建模,这个模型对短语的长度分布及韵律词与韵律短语的关系进行了描述。最后,利用一个称为k-候选的方法来融合这两个约束模型。整个方法充分利用了韵律短语的语法约束和长度约束,并将之有机地结合起来。试验表明,该预测模型达到了很好的效果,韵律短语边界识别的调和平均值达到82.9%。","计算机应用,中文信息处理,语法约束,长度约束,韵律短语"
2005-12-22,一种支持多语言文本布局方向的文档处理模型,"贾彦民,吴健","文档处理是文字处理的关键组成部分,针对多语言混合排版的需求,本文提出了基于“框”的支持不同方向的多语言文本布局的文档处理模型。该模型把对文本布局方向的处理封装在文档格式化模块中,将多文本布局方向的问题规约为文本布局方向为从左向右(水平)的文档格式化的问题,并设计了多文本布局方向文档格式化的递归算法。该模型可以很好支持包括我国民族文字蒙古文、维吾尔文、藏文在内的各种不同书写方向文字的文本布局。","计算机应用,中文信息处理,文档格式化,文本布局方向,文字处理"
2005-07-26,试论汉字数字输入法评价,"周克兰,吕强,张玉华,潘吉斯,钱培德","GB18031对如何科学评价数字输入法起到非常重要的指导作用。但是GB18031的部分性能指标在执行时存在一定的难度。数字输入法软件功能至今缺乏相应的国家标准。建立科学的数字输入法软件功能国家标准成为相当迫切的问题。本文讨论了GB18031中规定的易学性的可判定性,并对重码键选率的执行难点进行了定量分析。本文还分析了面向普及型汉字录入人员的数字输入法的特点,对进一步完善GB18031提出了具体的建议。本文还说明了建立数字输入法功能国家标准的必要性,对如何建立数字输入法功能国家标准进行了初步研究。","计算机应用,中文信息处理,数字输入法,重码键选率,输入法软件功能"
2006-02-17,汉语拼音的短韵母编码与汉字输入,方贵明,"《汉语拼音方案》在中文信息处理中具有重要地位,拼音输入法更是电脑汉字输入的大众化方法。由于韵母采用1～4个字母,显得长短不齐。本文提出短韵母编码方案,除原来单字母韵母外,其他韵母用{aoeiuv}中的两个字母来表示,使得拼音编码变短。由于韵母采用的字母与声母采用的20个字母不同,在键盘输入汉字时可以采用“声韵声”方式输入词组。此方案可用于字母键盘,在数字键盘更有优势。声调的4个键与短韵母编码的6个键互不相同,拼音串输入时容易切分各字拼音,即使省略了韵母。每对模糊音设有3个数字键盘编码,以方便部分字音需要模糊的用户。","计算机应用,中文信息处理,汉语拼音方案,声母韵母,数字键盘的汉字输入,拼音输入法"
2006-07-15,中文语义角色标注的特征工程,"刘怀军,车万翔,刘挺","基于统计机器学习的语义角色标注在自然语言处理领域越来越受到重视,丰富多样的特征直接决定语义角色标注系统的性能。本文针对中文的特点,在英文语义角色标注特征的基础上,提出了一些更有效的新特征和组合特征: 例如,句法成分后一个词、谓语动词和短语类型的组合、谓语动词类别信息和路径的组合等,并在Chinese Proposition Bank(CPB)语料数据上,使用最大熵分类器进行了实验,系统F-Score由89.76%增加到91.31%。结果表明,这些新特征和组合特征显著提高了系统的性能。因此,目前进行语义角色标注应集中精力寻找丰富有效的特征。","计算机应用,中文信息处理,语义分析,语义角色标注,特征工程,最大熵分类器"
2006-07-20,统计机器翻译中短语切分的新方法,"何中军,刘群,林守勋","基于短语的统计机器翻译是目前主流的一种统计机器翻译方法,但是目前基于短语的翻译系统都没有对短语切分作专门处理,认为一个句子的所有短语切分都是等概率的。本文提出了一种短语切分方法,将句子的短语切分概率化: 首先,识别出汉语语料库中所有出现次数大于2次的词语串,将其作为汉语短语; 其次,用最短路径方法进行短语切分,并利用Viterbi算法迭代统计短语的出现频率。在2005年863汉英机器翻译评测测试集上的实验结果(BLEU4)是: 0.1764(篇章),0.2231(对话)。实验表明,对于长句子(如篇章),短语切分模型的加入有助于提高翻译质量,比原来约提高了0.5个百分点。","人工智能,机器翻译,统计机器翻译,翻译模型,短语切分"
2006-07-30,基于知网的中文问题自动分类,"孙景广,蔡东风,吕德新,董燕举","问答系统应能用准确、简洁的答案回答用户用自然语言提出的问题。问题分类是问答系统所要处理的第一步,分类结果的正确率直接影响后续工作的进行。本文提出了一种使用知网作为语义资源选取分类特征,并使用最大熵模型进行分类的新方法。该方法以问题的疑问词、句法结构、疑问意向词、疑问意向词在知网中的首义原作为分类特征。实验结果表明,在知网中选取的首义原能很好的表达问题焦点词的语义信息,可作为问题分类的一个主要特征。该方法能显著地提高问题分类的精度,大类和小类的分类精度分别达到了92.18%和83.86%。","计算机应用,中文信息处理,问答系统,问题分类,知网,最大熵模型,分类特征"
2006-07-13,基于语义理解的文本倾向性识别机制,"徐琳宏,林鸿飞,杨志豪","文本倾向性识别在垃圾邮件过滤、信息安全和自动文摘等领域都有广泛的应用。本文提出了基于语义理解的文本倾向性识别机制。其主要思想是首先计算词汇与知网中已标注褒贬性的词汇间的相似度,获取词汇的倾向性;再选择倾向性明显的词汇作为特征值,用SVM分类器分析文本的褒贬性;最后采用否定规则匹配文本中的语义否定的策略提高分类效果,同时处理程度副词附近的褒义词和贬义词,以加强对文本褒贬义强度的识别。","计算机应用,中文信息处理,倾向性识别,知网, 语义相似度, 否定句,程度副词"
2006-07-28,基于非连续短语的统计翻译模型研究,"张大鲲,张玮,冯元勇,孙乐","目前统计机器翻译的主流方法仍然是基于短语的翻译模型。然而,该模型并没有考虑对非连续短语的处理。本文提出了一种基于非连续短语的统计翻译模型,利用该模型可以使翻译的基本单元从连续短语扩展到带有间隔的非连续短语,以更好地解决词语翻译时的上下文依赖问题。同时,由于该方法抽取的短语数量较少,也使得解码的效率得到了提高。实验表明,在效率提高的情况下,非连续短语模型可以取得与层次型短语模型相当的翻译结果。","人工智能,机器翻译,非连续短语,统计机器翻译,短语模型"
2006-07-26,基于大规模日志分析的搜索引擎用户行为分析,"余慧佳,刘奕群,张敏,茹立云,马少平","用户行为分析是网络信息检索技术得以前进的重要基石,也是能够在商用搜索引擎中发挥重要作用的各种算法的基本出发点之一。为了更好的理解中文搜索用户的检索行为,本文对搜狗搜索引擎在一个月内的近5 000万条查询日志进行了分析。我们从独立查询词分布、同一session内的用户查询习惯及用户是否使用高级检索功能等方面对用户行为进行了分析。分析结论对于改进中文搜索引擎的检索算法和更准确的评测检索效果都有较好的指导意义。","计算机应用,中文信息处理,网络信息检索,搜索引擎,用户行为分析,点击信息分析"
2006-07-22,中文Base NP识别: 错误驱动的组合分类器方法,"徐昉,宗成庆,王霞","本文采用一种新的错误驱动的组合分类器方法来实现中文Base NP识别。本文首先对中文和英文Base NP识别技术现状进行了简要分析和概述,明确了中文Base NP识别的任务,然后,基于前人的工作提出了错误驱动的组合分类器方法,其基本思路是: 通过对比两种不同类型的分类器—基于转化的方法和条件随机场方法的分类结果,再利用支持向量机学习其中的错误规律,对两分类器产生的不同结果进行纠错,从而达到提高系统整体性能的效果。我们在宾州中文树库转化得到的Base NP语料集上进行了Base NP识别交叉验证实验,与单独使用基于转化的方法、条件随机场方法以及支持向量机方法相比较,错误驱动的组合分类器方法的实验结果都有所提高,最佳结果F值达到了89.72%,相对于文中Base NP识别的其他方法,最大提高幅度为2.35%。","计算机应用,中文信息处理,错误驱动,中文Base NP 识别,组合分类器"
2006-07-22,词汇化句法分析与子语类框架获取的互动方法,"冀铁亮,穗志方","概率句法分析器(PCFG Parser)是基于概率规则集的上下文无关文法的句法分析器。规则集主要是针对词类和短语类。然而事实上,词性相同而词汇不同,其所常用的句法规则也通常不同。目前NLP研究的一个趋势和热点就是词汇化的句法分析。针对概率句法分析独立性假设中缺乏词汇化的缺陷,本文将谓语动词的子语类信息与概率句法分析结合起来,提出一种基于动词子语类信息的词汇化概率句法分析方法。论文建立了基于汉语动词子语类框架的统计句法分析模型,并且针对动词子语类框架难以获取的问题,提出一种词汇化概率句法分析与动词子语类框架获取的互动方法。实验利用这种互动的方法获取了汉语中十个常用高频动词的概率化子语类信息,并结合原有的概率句法分析器PCFG实现了一个基于动词子语类信息的概率句法分析器原型系统S-PCFG。实验证明了基于动词子语类信息的概率句法分析对自然语言句法分析的准确率和速度均有所提高。同时分析了新的概率句法分析器的不足之处,为进一步的改进提供条件。","计算机应用,中文信息处理,词汇化概率句法分析,子语类框架,词汇知识自动获取"
2006-08-24,COLING/ACL 2006会议报道,邹煜,,
2005-11-04,基于词典属性特征的粗粒度词义消歧,"吴云芳,金澎,郭涛","本文依据《现代汉语语法信息词典》中对词语多义的属性特征描述,对《人民日报》语料中155 个词语共 4 996 个同形实例进行了粗粒度词义自动消歧实验,同时用贝叶斯算法进行了比较测试。基于词典属性特征的消歧方法在同形层面上准确率达到 90%, 但召回率偏低。其优点在于两个方面: 1) 不受词义标注语料库规模的影响;2) 对特定词语意义的消歧准确率可达到100%。本文也讨论了适用于不同词类的消歧特征。","人工智能,自然语言处理,特征,词义,词义消歧,贝叶斯分类法"
2006-05-22,一种用于词性标注的相关投票融合策略,"郭永辉,吴保民,王炳锡","各种词性标注方法总是利用从某一侧面描述的语言学知识,当训练语料达到一定规模、训练模型完善到一定程度后,标注精度很难再有进一步的提高。本文在对TBED、DT、HMM和ME四种基于语料库的词性标注方法研究的基础上,提出了一种新的词性标注融合策略——相关投票法。从理论上分析了该方法的优越性,并与其他融合策略进行了对比实验。实验结果表明,应用融合策略可以更加全面地描述词性标注知识,从而更好地完成词性标注任务;在几种融合策略中,相关投票法是最优秀的,它使标注的平均错误率降低27.85%。","人工智能,自然语言处理,词性标注,融合策略,相关投票"
2006-04-25,实体提及的多层嵌套识别方法研究,"刘非凡,赵军,徐波","实体识别在许多自然语言处理应用系统中发挥着极其重要的作用。目前大部分研究集中在命名实体识别,且不考虑实体之间的嵌套,本文在自动内容抽取评测(Automatic Content Extraction, ACE)背景下,对汉语文本中各种实体提及(命名性,名词性,代词性)的多层嵌套识别进行了研究。我们将嵌套实体识别分成两个子任务: 嵌套实体边界检测和实体多层信息标注。首先,本文提出了一种层次结构信息编码方法,将多层嵌套边界检测问题转化为传统的序列标注问题,利用条件随机场模型融合多种特征进行统计决策。其次,将多层信息标注问题看作分类问题,从实现的角度设计了含有两个分类引擎的并行SVM分类器,避免了对每层信息标注都设计一个分类器,比采用单一分类器在性能上有明显提高。在标准ACE语料上的实验表明,基于条件随机场的多层实体边界检测模型正确率达到71％,融合特征选择策略的两个并行分类引擎的正确率也分别达到了89.05%和82.17%。","人工智能,自然语言处理,实体提及嵌套识别,条件随机场,支持向量机"
2006-04-08,基于混合模型的中国人名自动识别,"毛婷婷,李丽双,黄德根","本文提出了一种支持向量机(SVM)和概率统计模型相结合的中国人名自动识别方法。该方法首先按字抽取特征向量的属性得到训练集,采用多项式核函数建立SVM人名识别模型,然后在特征空间中计算测试样本到SVM最优超平面的距离,当该距离大于给定的阈值时使用SVM对测试样本进行分类,否则使用概率统计方法。实验表明,采用混合模型,对样本在空间的不同分布使用不同的方法可以取得比单独使用SVM或概率统计更好的分类效果,系统开式综合指标F-值比单纯使用支持向量机方法提高了1.51%。","计算机应用,中文信息处理,支持向量机,概率统计,混合模型,人名识别"
2006-06-08,基于标点符号分割的汉语句法分析算法,"毛奇,连乐新,周文翠,袁春风","目前大部分句法解析器都忽略标点符号这一重要的句法特征或者只进行非常简单的处理。本文根据标点符号的句法结构特性,提出单独解析块的概念,并且根据标点符号在句子中的特有特征和位置关系,给出了基于决策树算法(Id3)单独解析块识别方法,将标点融入汉语句法分析中。本文所用的实验数据(包括训练集和测试集)均来自中文宾州树库5.0。对句长大于40个词的汉语长句单独进行了实验,句法分析精度和召回率分别提高1.59%和0.93%,同时时间开销降低了近2/3。实验结果表明,标点对汉语长句句法分析非常有利, 系统性能获得了较大提高。","计算机应用,中文信息处理,句法解析器,单独解析块,决策树(Id3)"
2006-06-04,自然语言处理在信息检索中的应用综述,"王灿辉,张敏,马少平","在信息检索①发展的过程中,研究者们不断尝试着将自然语言处理应用到检索里,希望能够为检索效果提高带来帮助。然而这些尝试的结果大多和研究者们最初的设想相反,自然语言处理在大多数情况下没有改进信息检索效果,甚至反而起了负面作用。即便有一些帮助,也往往是微小的,远远不如自然语言处理所需要的计算消耗那么大。研究者们对这些现象进行了分析,认为: 自然语言处理更适合于应用在需要精确结果的任务中,例如问答系统、信息抽取等;自然语言处理需要针对信息检索进行优化才可能发挥积极作用。最新的一些进展(例如在语言模型中加入自然语言处理)在一定程度上印证了这一结论。","人工智能,自然语言处理,综述,信息检索"
2006-02-08,基于决策树和马尔可夫链的问答对自动提取,"刘佳宾,胡国平,陈超,邵正荣","问答系统能用准确、简洁的答案回答用户用自然语言提出的问题,很明显系统中问答对的规模是影响问答系统最终性能的主要因素。为了提高问答对的规模、充分利用互联网资源,本文提出了一种基于决策树和马尔科夫链的在互联网上自动抽取问答对的算法。先根据网页中的HTML标记把网页表示成一棵DOM树;然后利用树中每个节点的结构和文字信息,抽取相应的特征;最后将得到的节点特征通过由决策树和一阶马尔可夫链结合得出的分类模型进行分类。试验结果表明准确率达到了90.398%,召回率达到了86.032%。对大量网页抽取的结果表明该分类模型能够适应对各种各样的网页的抽取。","人工智能,模式识别,信息抽取,DOM树,决策树,马尔可夫链"
2006-05-10,基于统计抽词和格律的全宋词切分语料库建立,"苏劲松,周昌乐,李翼鸿","全宋词切分语料库的建立是计算机研究宋词的基础。本文对宋词中“词”的界定提出了自己的看法,并在综合考虑统计抽词方法和基于诗词格律切分方法各自优点的基础上,提出建立全宋词切分语料库的新方法。我们首先通过统计抽词来抽取结合程度较强的二字词,并结合相关资源建立词表;在此基础上,结合宋词的格律特点按照一定的规则来对全宋词进行了切分。实验证明,本文中的方法具有较好的效果。","计算机应用,中文信息处理,宋词,语料库,统计抽词,格律"
2006-07-12,一种基于主题的文本聚类方法,"赵世奇,刘挺,李生","现有的文本聚类方法难以正确识别和描述文本的主题,从而难以实现按照主题对文本进行聚类。本文提出了一种新的基于主题的文本聚类方法: LFIC。该方法能够准确识别文本主题并根据文本的主题对其进行聚类。本方法定义和抽取了“主题元素”,并利用其进行基本类索引。同时还整合利用了语言学特征。实验表明,LFIC的聚类准确率达到94.66%,优于几种传统聚类方法。","人工智能,模式识别,基于主题文本聚类,基本类索引,语言学特征"
2006-03-03,面向变异短文本的快速聚类算法,"黄永光,刘挺,车万翔,胡晓光","本文主要针对近些年来大量出现在聊天语言中和手机短信中的短文本,提出了一种快速有效的聚类算法。这些短文本由于具有不规范性和大量相似性等特点,我们称其为变异短文本。本文在原有的网页去重算法[1~3]的基础上,根据变异短文本的特点,采取了特定的特征串抽取方法,并融合了压缩编码的思想,从而加快了处理速度。实验表明,基于该算法的聚类系统对于大量的变异短文本处理速度可以达到每小时百万级以上,并且有比较高的准确率。","人工智能,模式识别,检索,特征串,聚类"
2006-04-05,基于无监督学习的问答模式抽取技术,"吴友政,赵军,徐波","本文提出了一种基于无监督学习算法的问答模式抽取技术从互联网上抽取应用于汉语问答系统的答案模式。该算法可以避免有监督学习算法的不足,它无需用户提供&lt;提问,答案&gt;对作为训练集,只需用户提供每种提问类型两个或以上的提问实例,算法即可通过Web检索、主题划分、模式提取、垂直聚类和水平聚类等步骤完成该类型提问的答案模式的学习。实验结果表明,论文提出的无监督问答模式学习方法是有效的,基于模式匹配的答案抽取技术能够较大幅度地提高汉语问答系统的性能。","人工智能,自然语言处理,汉语问答系统,问答模式,机器学习"
2006-07-18,一种基于图划分的无监督汉语指代消解算法,"周俊生,,黄书剑,陈家骏,曲维光","指代消解是自然语言处理领域中的一个重要问题。针对当前中文指代标注训练语料非常缺乏的现状,本文提出一种无监督聚类算法实现对名词短语的指代消解。引入图对名词短语的指代消解问题进行建模,将指代消解问题转化为图划分问题,并引入一个有效的模块函数实现对图的自动划分,使得指代消解过程并不是孤立地对每一对名词短语分别进行共指决策,而是充分考虑了多个待消解项之间的相关性,并且避免了阈值选择问题。通过在ACE中文语料上的人称代词消解和名词短语消解实验结果表明,该算法是一种有效可行的无监督指代消解算法。","人工智能,自然语言处理,聚类,指代消解,模块函数"
2006-06-14,古籍自动校勘的研究和实现,"常娥,侯汉清,曹玲","古籍自动校勘是指利用计算机自动发现并标记出古籍不同版本之间的文字差异,并提供各种校勘辅助工具帮助专家勘误。本文讨论了古籍自动校勘的意义,接着详细阐述了古籍自动校勘系统的总体设计及其实现,包括选题和资料收集、自动校勘的对象和方法,最深入讨论了古代官名表、人名表、地名表等自动校勘辅助工具的建设问题。最后,设计了实验检查校勘系统的效果。实验结果表明,本系统的召回率和精确率分别达到了92.3%、95.2%。","计算机应用,中文信息处理,古籍整理,自动校勘,校勘辅助工具"
2006-04-14,一种新的加权动态网格汉字特征抽取方法,"陈光,张洪刚,郭军","为了更有效地提取手写汉字的特征,提高识别精度,本文提出了一种利用非线性归一化过程产生的坐标变换信息来提取手写汉字有效特征的方法。该方法通过非线性归一化获得各有效像素点在原汉字图像及规整后汉字图像中的坐标变换关系,在原图像上抽取各点特征,在归一化图像上进行网格的均匀划分和特征统计并形成用于分类的特征向量。该方法有效克服了以往先进行归一化预处理方法和动态网格方法的一些不足,兼顾了与传统结构特征提取方法的有效结合。针对HCL2000脱机手写汉字库大字符集样本的实验结果表明,该特征提取方法可有效提高识别精度和特征抽取速度。","人工智能,模式识别,手写汉字识别,非线性归一化,加权动态网格,特征提取"
2006-05-18,汉语普通话语音合成语料库TH-CoSS的建设和分析,"蔡莲红,崔丹丹,蔡锐","本文介绍了汉语语音合成语料库TH-CoSS的建设和分析。本语料库包括男女声朗读语句约2万个。语料库分为四个部分: TTS系统建库用语句、TTS系统测试用语句、特殊语调语句和特殊音节组。语料设计考虑了语料的平衡和音段、韵律信息的丰富。语料库中除了文本、语音数据外,还带有音段切分标志,标注文件采用XML格式。为了方便语音分析与开发,特研制了标注软件。本文还给出了语境特征对语音韵律影响的分析结果。","计算机应用,中文信息处理,语音合成,汉语,语料库"
2006-04-12,基于支持向量机的音字转换模型,"姜维,关毅,王晓龙,刘秉权","针对N-gram在音字转换中不易融合更多特征,本文提出了一种基于支持向量机(SVM)的音字转换模型,有效提供可以融合多种知识源的音字转换框架。同时,SVM优越的泛化能力减轻了传统模型易于过度拟合的问题,而通过软间隔分类又在一定程度上克服小样本中噪声问题。此外,本文利用粗糙集理论提取复杂特征以及长距离特征,并将其融合于SVM模型中,克服了传统模型难于实现远距离约束的问题。实验结果表明,基于SVM音字转换模型比传统采用绝对平滑算法的Trigram模型精度提高了1.2%;增加远距离特征的SVM模型精度提高1.6%。","人工智能,自然语言处理,支持向量机, 音字转换, 粗糙集理论, 远距离特征"
2006-03-12,一种两层次无监督的音频分割算法,"张世磊,张树武,徐波","本文提出一种两层次无监督音频分割算法,它用于检测音频流中的说话人、环境、信道等声学特征变化点,该方法将音频分割过程分为两个层次: 区域层次和边界层次,通过固定检测窗移动,它快速定位到声学特征变化点存在的区域,然后在潜在变化区域内采用T2 统计值和贝叶斯信息准则(BIC)结合的方法快速确定片断边界。在区域检测层次,将修正的广义对数似然比准则应用于潜在的变化区域检测,它即无需设定阈值门限又可保证低的漏检率,在1997年Hub4中文广播语音数据库上的实验结果表明,同传统的混合分割算法比较,该算法在处理速度得到提高的同时,声学特征变化点的召回率提高10.5％。","人工智能,模式识别,两层次无监督音频分割,修正广义似然比,区域层次,边界层次"
2006-09-25,维文版Office设计中关键技术的研究与实现,"卢有飞,张伟,张岩,缪成,李春","维吾尔文,汉文和英文等多文种办公套件,对少数民族地区信息化的发展,起着重要作用。该文首先介绍了维吾尔文的特点,然后分析并实现了永中集成Office维吾尔文版设计中的自动选形、按音节断行和自动拉长等处理维吾尔文的关键技术。这些关键技术在维吾尔文版Office中应用后,通过测试能使维吾尔文排版非常规整。同时这些关键技术对维吾尔文文字处理,对其他维吾尔文软件的开发都有普遍地指导作用。","计算机应用,中文信息处理,维吾尔文处理,算法,自动选形,自动拉长,断行"
2006-06-27,蒙古文显示在OpenOffice.org办公套件中的实现,"孟凡强,吴健,贾彦民","蒙古文是一种复杂文字,目前操作系统和办公套件都还不支持蒙古文的显示。OpenOffice.org是可以运行在Linux和Windows上跨平台的办公套件,它分别使用ICU LayoutEngine和Uniscribe进行复杂文字处理。本文以支持蒙古文处理的Linux版本OpenOiffice.org为基础,详细分析了OpenOffice.org在Linux和Windows系统上的复杂文本处理过程,采用Uniscribe与ICU相结合的方案,实现了OpenOffice.org在Windows平台上对蒙古文的显示。","计算机应用,中文信息处理,复杂文字, 复杂文本布局引擎,ICU,Uniscribe"
2006-01-23,藏文编码字符集的扩充集在Linux上的实现,"张兴亮,芮建武,谢谦,程伟,吴健","国内藏文软件开发普遍使用的是基于垂直预组合字符的实现方案,但是缺乏统一的编码标准。藏文编码字符集扩充集的推出,对于国内藏文软件的标准化、国际化具有重要意义。本文通过分析ISO/IEC 10646藏文编码字符集基本集、藏文编码字符集扩充集国家标准,区分它们描述字丁的差异,分析由编码方案所导致的实现上的关键问题。最后,针对藏文扩充集B的特殊性,提出并实现了基于Linux国际化架构下支持藏文扩充集标准的解决方案。","计算机应用,中文信息处理,藏文字丁,扩充集,OpenType,扩充QT方案"
1999,基于语义知识的汉语句法结构排歧,"苑春法,黄锦辉,李文捷","汉语在词类这个语言层次上存在着许多歧义结构,这给汉语的自动句法分析带来了难以逾越的障碍。通过寻找汉语语义类之间可能存在的句法关系建立汉语语义关联网,这为用汉语语义知识来解决句法歧义开辟了道路。文章针对具体的汉语歧义结构研究具体的解决办法,从而减少了计算的复杂度。","语义关联网,依存语法,句法树"
1999,中文文本的关键词自动抽取和模糊分类,"何新贵,彭甫阳","本文提出了中文文本分类的两种模糊方法,一种基于模糊集间的语义距离,一种基于本文中提出的‘模糊分类网络’。两者都必须首先从文本中抽取关键词集合,本文给出了一种主要采用统计方法结合受限自然语言理解技术的模糊关键词集合提取方法,它与模糊分类方法结合,可望达到文本信息的自动分类。所提出的方法同样适合于模式识别之类问题的解决。","文本,分类,模糊方法,模糊分类网络,语义距离"
1999,智能型俄汉机器翻译系统的句法规则库的设计原则,"李向东,周清波,黄河燕,张昕,张新红",本文提出了建立智能型俄汉机器翻译系统句法规则库的6条原则: 以谓语动词作为句子结构关系的中心; 语法、语义信息一体化; 常见词序优先; 以函数形式反映俄语句法的制约关系; 强制性认定及源语言分析与目标语生成同步进行。,"机器翻译,句法规则库"
1999,汉字字形编辑器的面向对象实现技术,"黄源,王瑜,张福炎","本文介绍了一个实用的采用面向对象技术实现的汉字字形编辑器(C Glyph Editor) 。首先介绍了类与其实例的设计思路,接着阐明了典型编辑操作的实现即系统中各对象的通讯机制,最后对设计字形编辑器时采用传统设计方法与面向对象技术进行了比较与分析。","字形编辑器,面向对象技术,变换,游历"
1999,高频最大交集型歧义切分字段在汉语自动分词中的作用,"孙茂松,左正平,邹嘉彦","交集型歧义切分字段是影响汉语自动分词系统精度的一个重要因素。本文引入了最大交集型歧义切分字段的概念,并将之区分为真、伪两种主要类型。考察一个约1亿字的汉语语料库,我们发现,最大交集型歧义切分字段的高频部分表现出相当强的覆盖能力及稳定性:前4,619个的覆盖率为59.20% ,且覆盖率受领域变化的影响不大。而其中4,279个为伪歧义型,覆盖率高达53.35%。根据以上分析,我们提出了一种基于记忆的、高频最大交集型歧义切分字段的处理策略,可有效改善实用型非受限汉语自动分词系统的精度。","中文信息处理,汉语自动分词,高频最大交集型歧义切分字段,基于记忆的排歧策略"
1999,中国古代诗词格律自动检索与教学系统,"罗凤珠,李元萍,曹伟政","计算机辅助教学与研究的优点是可以突破时空的限制,辅以计算机准确、快速处理资料的能力,使教学或活动能发挥最好的效果,由计算机处理优于以人力整理的部分,设计辅助教材或教学辅助工具,在中国文学的教学上,有极大的发挥空间。","计算机辅助教学,词律,词谱,词韵,格律自动检索系统"
1999,一个基于C/S模式的汉字词属性分析和重组系统的设计,"钱培德,杨季文,吕强,朱巧明","随着ISO10646的出台,庞大的汉字词集的属性分析是一个迫切需要解决的难题,本文主要阐述了采用C/S模式的汉字词属性分析和重组系统的数据模型,并简单介绍了系统的设计要点。","C/S模式,汉字词,汉字词属性,分析和重组"
1999,藏文内码扩展体系,于洪志,"针对藏文编码字符集的基本集和辅助集建立在不同平面、编码体系不同所存在的问题,本文提出建立藏文内码扩展体系,给出了藏文合成、生成、分解的规则和方法:通过内码转换表合成藏文藏文内字,实现基本集与辅助集的信息交换;通过构件集,生成规范、标准的藏文外字,满足藏文编码字符集开放性的需要。并且,向上,在字汇一级,兼容UCS ;向下,与GB2312的事实上的内码标准兼容,是一个全藏文编码体系。","藏文内码扩展体系,内字、外字复合序列,组合用字符,合成内字,生成外字"
1999,中文运算中的语法分析,"张京媛,吴健,孙玉芳","在中文运算中,对汉语数词的语法分析是一个重要环节。本文研究设计了一个中文语法分析器,由主控分析器、知识库、管理器、错误信息表组成,设计步骤如下,根据汉语数词的BNF文法,推导出句法图,将句法图在分析程序运行之前送入主程序中,分析程序根据句法图的结构对汉语数词进行语法检查,若有错,给出错误信息,否则,进行下一步处理。",
1999,评《现代汉语语法信息词典详解》,"冯志伟,曹右琦","北京大学计算语言学研究所俞士汶、朱学锋、王惠、张芸芸著的《现代汉语语法信息词典详解》,最近作为《中文信息处理丛书》之一由清华大学出版社出版了。这本专著是根据电子版《现代汉语语法信息词典》编写的,整部词典收词5万多条,存储空间达16兆字节。这部词典对真实文本的覆盖率高,所采用的体系反映了汉语语法研究的最新成就,分类体系可操作性强,对语法属性的描述非常深入、非常丰富,具有较高的权威性和可靠性。在1995年11月电子部组织技术鉴定会上专家一致认为:“这部词典的规模、深度与质量,在我国语言工程实践中是前所未有的,达到了国内外领先水平。”《现代汉语语法信息词典详解》从这部具有国内外领先水平的词典中,选出1万词作为本书的大部分篇幅提供给读者,并对词典的内容、基本理论依据、词语的语法功能分类、词语的语法属性描述、以及词典的应用与发展详细地加以说明,使我们从中不难窥测到词典的全貌,并进一步了解到作者所依据的理论和方法。",
1999,基于转换的汉语基本名词短语识别模型,"赵军,黄昌宁","基本名词短语的识别在自然语言信息处理领域具有重要作用。本文首先从语言学的角度提出了汉语基本名词短语的概念,然后从语言信息处理的角度将用于基本名词短语识别的知识分为两部分,即表示基本名词短语句法组成的基本结构模板(静态知识)与表示基本名词短语出现的上下文环境特征的转换规则(动态知识)。在此基础上设计了一种基于转换的基本名词短语识别模型,该模型可同时结合这两类知识识别基本名词短语。实验结果显示了较高的识别正确率。","自然语言处理,知识获取,语料库,名词短语"
1999,现代汉语计算语言模型中语言单位的频度—频级关系,"关毅,王晓龙,张凯","Zipf定律是一个反映英文单词词频分布情况的普适性统计规律。我们通过实验发现,在现代汉语的字、词、二元对等等语言单位上,其频度与频级的关系也近似地遵循Zipf定律,说明了Zipf定律对于汉语的不同层次的语言单位也是普遍适用的。本文通过实验证实了Zipf定律所反映的汉语语言单位频度-频级关系,并进而深入讨论了它对于汉语自然语言处理的各项技术,尤其是建立现代汉语基于统计的计算语言模型所具有的重要指导意义。","Zipf定律,字频,词频,二元对频度"
1999,利用语料库技术的中文自动文摘系统,"姜贤塔,陈根才","本文着重介绍利用“后邻字符树”的方法在领域语料库中生成字符树库,用于自动文摘候选句子选取时提高精度,介绍了后邻字符树的构造、后邻字符树库的生成及优化和句子权值计算方法。","自动文摘,字符树,字频统计,语料库"
1999,基于单元合并的汉字切分算法的改进,"周嫔,马少平,姜哲","本文介绍了对基于单元合并的汉字切分算法作出的改进。该改进算法对原算法中的核心部分高级合并部分进行了修改,通过在所有的可合并单元中找最佳合并组合,来避免原来的算法在高级合并过程中可能导致的某些合并错误。经过多个实际样本的测试,所作的改进在不降低原算法各种性能的前提下,消除了原算法在某些情况下产生的错误,进一步有效地提高了切分的正确率。","单元合并,切分算法,高级合并,最佳合并组合"
1999,曲线轮廓汉字自动生成及其变形方法,"马小虎,刘玉龙,潘志庚,石教英","本文介绍一种基于曲线轮廓汉字和Fourier级数描述的汉字变形新方法。该方法利用曲线轮廓所提供的字形笔划结构特征信息,运用多级数学模型,通过计算机软件功能,对字形自动进行变形,生成一系列形式多样的新字形,是一种动态汉字库技术。","轮廓字库,Fourier级数,汉字变形,动态字库"
1999,从GB2312-80汉字到整型数的连续可逆映射,游荣彦,"本文讨论建立从GB2312-80汉字到整型数据的映射在中文信息处理中的意义,提出了一个连续可逆映射,并论证了该映射,并论证了该映射所具的优良性。","ASCII码,区位码,数据压缩,映射,取模"
1999,口语自动翻译系统技术评析,"宗成庆,黄泰翼,徐波","近几年来,随着信息技术的发展,口语自动翻译技术成为新的研究热点。目前国际上一些著名大学和研究机构甚至企业,都纷纷加入这一高技术的竞争行列,我国在相关技术方面也进行了卓有成效的研究。本文对目前自动口语翻译研究的技术现状进行了全面综述和分析,并对一些具体问题作了深入探讨。作者希望本文作出的分析和讨论的问题,能够对我国的自动口语翻译研究提供有益的参考。","口语翻译,语音翻译,对话处理,机器翻译,鲁棒性"
1999,一种自组织的汉语词义排歧方法,"李涓子,黄昌宁,杨尔弘","长期以来,词义排歧一直被认为是自然语言处理的难题之一。本文用机器可读词典《现代汉语辞海》提供的搭配实例作为多义词的初始搭配知识,采用适当的统计和自组织方法自动扩大搭配集;为保证学习质量,在学习过程中逐渐增大上下文窗口的长度;提出使用搭配统计表的多元最大对数似然比词义排歧算法。最后,对本文提出的方法进行了实验,实验表明这种算法具有较高的正确率。","自然语言处理,词义排歧,自组织方法,搭配"
1999,汉语短语结构定界歧义类型分析及分布统计,"詹卫东,常宝宝,俞士汶","本文对汉语短语结构的定界歧义做了全面考察,从歧义格式的组成成分,歧义对外造成的影响,模式歧义和实例歧义的对应关系三方面考察了短语结构定界歧义的不同类型,并对汉语短语结构定界歧义的不同类型进行了初步统计。希望能将计算机处理汉语时碰到的短语结构边界歧义问题进一步清晰化,供理论研究者和应用系统开发人员参考。","短语,短语定界歧义,自然语言处理"
1999,一种基于奇异值分解的双语信息过滤算法,"路海明,徐晋晖,卢增祥,李衍达","本文提出了一种基于SVD(奇异值分解)的双语信息过滤算法,将双语文档进行了统一的表示,使得适应于单语过滤的算法可以方便地用于双语过滤,同时对文档向量进行了压缩,滤去了噪声。在应用方面,将双语过滤算法用于互联网上的个性化主动信息过滤。","双语信息过滤,SVD,互联网,Bookmark服务"
1999,中文文档自动分类系统的设计与实现,"邹涛,王继成,黄源,张福炎","文档自动分类是信息处理领域中的一项重要研究课题。本文阐述了一个中文文档自动分类系统的设计与实现,并着重介绍了系统实现中的一些主要技术问题的处理,如文本分类模型、特征提取、词典构造等。","文本分类,分类模型,VSM,特征提取"
1999,基于统计与神经元方法相结合的手写体相似字识别,"张德喜,马少平,朱绍文,金奕江","本文提出了一种基于统计识别方法与人工神经元网络相结合的手写体相似汉字识别方法。该方法充分利用了统计识别方法和神经元网络识别方法的优点,不仅显著地提高了相似字的识别率,而且有效地提高了系统的整体性能。对相似字的识别率由79.02%提高到84.32% ,提高了五个百分点,整体识别率提高了1.3个百分点。","神经元网络,汉字识别,相似字识别"
1999,粤-普机器翻译中的词处理,张小衡,"粤语和普通话之间的机器翻译研究应首先考虑由粤语到普通话的书面语翻译,并以单词为突破口。本文重点讨论粤-普书面语机器翻译中的词处理,尤其是方言词处理,包括方言词的识别和方言词的翻译两方面,同时介绍一个已经初步实现了的单词级粤-普机器翻译试验系统。文章最后将给出结论和讨论。","机器翻译,粤语与普通话,词处理"
1999,计算机辅助汉语教学系统中语音评价体系初探,"郭巧,陆际联","本文探讨和研究计算机辅助汉语教学系统中语音评价体系的组成与实现方法。采用标准普通话语音示教数据库和非特定人大词汇量标准普通话汉语语料数据库,建立标准普通话示教语句特征模板库。采用Kohonen自组织神经网络进行学习者语音信号的分类与识别,经过汉语语音教学效果评价系统的处理,获得相应的量化评价结果。初步给出了计算机辅助汉语教学系统中语音评价体系的总体框架及其实现方法。通过实验验证了本语音评价体系的设计方案是合理的、可行的。它基本上能够满足计算机辅助汉语教学系统在线评价学生语音学习效果的需要。","语音信号处理,语音信号评价,计算机辅助教学"
1999,小学语文ICAI系统诊断模块中的造句错误类型分析,杨开城,"本文从句法分析的角度出发,全面分析了错误文本的技术表现,从信息处理的需要出发扩展了造词法的概念,提出把词语搭配错归入句法错的观点,并试着对“语义错”这种模糊的提法给出了明确的描述。本文在详细分析各种错误类型的特征的基础上,提出了相应的诊断策略。","造词法,搭配,语义"
1999,中文搜索引擎现状与展望,"都云程,卢献华","本文介绍了中文搜索引擎的发展现状,分析了中文搜索引擎中存在的问题,以及与国外先进的搜索引擎的差距,提出了中文搜索引擎的发展方向。","中文搜索引擎,全文检索,中文自动分词,相关排序"
2007-03-01,自然语言处理的计算模型,张钹,"本文讨论自然语言处理的计算模型。目前已经存在有各种类型的语言计算模型,如分析模型、概率统计模型、混合模型等,这些模型各具特色,并存在其自身的局限性。自然语言处理作为一个不适定问题,我们将讨论求解这类问题的本质困难,面临的挑战,以及解决这些困难的途径。","人工智能,自然语言处理,计算模型,分析模型,概念统计模型,混合模型,不适定问题"
2007-03-22,中文分词十年回顾,"黄昌宁,赵海","过去的十年间,尤其是2003年国际中文分词评测活动Bakeoff开展以来,中文自动分词技术有了可喜的进步。其主要表现为: (1)通过分词规范+词表+分词语料库的方法,使中文词语在真实文本中得到了可计算的定义,这是实现计算机自动分词和可比评测的基础;(2)实践证明,基于手工规则的分词系统在评测中不敌基于统计学习的分词系统;(3)在Bakeoff数据上的评估结果表明,未登录词造成的分词精度失落至少比分词歧义大5倍以上;(4)实验证明,能够大幅度提高未登录词识别性能的字标注统计学习方法优于以往的基于词(或词典)的方法,并使自动分词系统的精度达到了新高。","计算机应用, 中文信息处理,中文分词,词语定义,未登录词识别,字标注分词方法"
2007,有关“理解和分词孰先孰后”的反思,吴安迪,"“中文分词十年回顾”一文中有“理解和分词孰先孰后”这一节,专门讨论NLPwin中文系统的分词。作为该系统的开发者之一,我觉得有必要对这个问题作一反省。&nbsp;&nbsp;&nbsp;作为一门科学,语言学的目标之一是了解人类语言处理的机制。对我而言,计算语言学的最高境界是做出一套能反映人脑语言机制真实状况的, 具有心理学价值的电脑系统。从心理语言学的角度看,“理解和分词孰先孰后”这个问题是不存在的。人脑分析句子的过程显然是一个分词和理解互动的过程,理解依赖于分词,分词也依赖于理解。NLPwin中文系统的设计理念就是要反映人脑的这一分析过程。我们没有做一个专用于分词的系统,因为孤立的分词不是一种自然的人类语言行为。&nbsp;&nbsp;&nbsp;从工程的角度看,分词和理解是完全可以分开的。对于工程来说,切分一个汉语的字串和切分任何其他字串没有太大的区别。我们可以把最好的、具有通用性的切分技术用于汉语分词。在此过程中我们不需要知道所切汉语字串所表达的意义。这里所要解决的主要是一个数学问题,而不是语言理解问题。把理解插入分词过程会大大增加计算的复杂度,其结果往往是得不偿失。所以如果我们的目的仅仅是分词,理解是没有必要的。",
2006-10-27,汉语基本块描述体系,周强,"块分析是自然语言处理研究中的重要技术,其处理基础是设计一套合理有效的块描述体系。本文在吸收和总结前人研究成果和经验的基础上,提出了一套基于拓扑结构的汉语基本块描述体系。它通过引入词汇关联信息确定基本拓扑结构,形成了很好的基本块内聚性判定准则,建立了句法形式与语义内容的有机联系桥梁。这套描述体系大大简化了从现有的句法树库TCT中自动提取基本块标注语料库和相关词汇关联知识库的处理过程,为进一步进行汉语基本块自动分析和词汇关联知识获取互动进化研究打下了很好的基础。","计算机应用,中文信息处理,基本块,部分分析,语料库标注,词汇知识获取"
2006-09-04,单纯形算法在统计机器翻译Re-ranking中的应用,"付雷,刘群","近年来,discriminative re-ranking技术已经被应用到很多自然语言处理相关的分支中,像句法分析,词性标注,机器翻译等,并都取得了比较好的效果,在各自相应的评估标准下都有所提高。本文将以统计机器翻译为例,详细地讲解利用单纯形算法(Simplex Algorithm)对翻译结果进行re-rank的原理和过程,算法的实现和使用方法,以及re-rank实验中特征选择的方法,并给出该算法在NIST-2002(开发集)和NIST-2005(测试集)中英文机器翻译测试集合上的实验结果,在开发集和测试集上,BLEU分值分别获得了1.26%和1.16%的提高。","人工智能,机器翻译, discriminative re-ranking,单纯形算法,统计机器翻译"
2006-12-08,双语知识库中关联实例的多策略提取机制,"张桂平,,姚天顺,尹宝生,蔡东风,宋彦","双语库是翻译记忆系统最重要的组成部分之一。从有限规模的双语库中提取更多的符合用户当前翻译需要的关联实例是翻译记忆技术研究的主要内容,本文首先对当前基于单一方法的实例检索算法存在的局限性进行了分析,并在对双语库进行知识化表示的基础上,提出了基于多策略的关联实例提取机制,即综合运用句子句法结构匹配、句子编辑距离计算、句子短语片段匹配、词汇语义泛化、基于扩展信息(如: 句子来源、所属专业、应用频度等信息)的优选等策略进行关联实例提取。试验结果表明,该方法有效提高了关联实例的召回数量和质量,明显改善了对用户的辅助效果。","人工智能,机器翻译,双语知识库,关联实例,多策略提取机制,翻译记忆"
2006-09-29,汉语述语形容词机器词典机器学习词聚类研究,"王锦,陈群秀","本文提出了一个基于现代汉语述语形容词机器词典以及平衡语料库的形容词多信息聚类算法。聚类的过程根据形容词的语料提取了三重信息(所修饰的名词,同义近义词以及反义词),从而使形容词与形容词之间构成网络关系。本文重点描述了如何根据三重信息分别建模计算形容词的相似性并通过计算字面相似度以及路径权值这些辅助信息修正每两个形容词之间的相似度,从而在某种程度上缓解了数据稀疏的问题,实验结果显示该算法是有效的。","人工智能,机器翻译,机器学习,词聚类,搭配对,Kendall τ系数法,字面相似度,路径权值"
2006-08-24,基于层次聚类的自适应信息过滤学习算法,"洪宇,张宇,刘挺,郑伟,龚诚,李生","本文采用一种基于层次聚类的自适应学习策略,从系统反馈的信息流中,动态提取一类最优信息的质心更新用户模型,有效屏蔽了阈值失真和初始信息稀疏造成的大量反馈噪声,并且能够近似模仿人工反馈,完善自适应学习机制的智能性。","计算机应用,中文信息处理,自适应信息过滤,用户模型,相关反馈,阈值,层次聚类"
2006-05-27,一种基于紧密度的半监督文本分类方法,"郑海清,林琛,牛军钰","自动的文本分类已经成为一个重要的研究课题。在实际的应用情况下,很多训练语料都只有一个数目有限的正例集合,同时语料中的正例和未标注文档在数量上的分布通常也是不均衡的。因此这种文本分类任务有着不同于传统的文本分类任务的特点,传统的文本分类器如果直接应用到这类问题上,也难以取得令人满意的效果。因此,本文提出了一种基于紧密度衡量的方法来解决这一类问题。由于没有标注出来的负例文档,所以,本文先提取出一些可信的负例,然后再根据紧密度衡量对提取出的负例集合进行扩展,进而得到包含正负例的训练集合,从而提高分类器的性能。该方法不需要借助特别的外部知识库来对特征提取,因此能够比较好的应用到各个不同的分类环境中。在TREC’05(国际文本检索会议)的基因项目的文本分类任务语料上的实验表明,该算法在解决半监督文本分类问题中取得了优异的成绩。","计算机应用,中文信息处理,文本分类,半监督机器学习,支持向量机,紧密度"
2006-09-29,基于信息增益的中文文本关联分类,"陈志雄,陈健,闵华清","关联分类是一种通过挖掘训练集中的关联规则,并利用这些规则预测新数据类属性的分类技术。最近的研究表明,关联分类取得了比传统的分类方法如C4.5更高的准确率。现有的基于支持度-置信度架构的关联分类方法仅仅是选择频繁文字构建分类规则,忽略了文字的分类有效性。本文提出一种新的ACIG算法,结合信息增益与FoilGain在中文文本中选择规则的文字,以提高文字的分类有效性。实验结果表明,ACIG算法比其他关联分类算法(CPAR)有更高的准确率。","计算机应用,中文信息处理,信息增益,关联分类,文本分类"
2006-09-25,用于文本分类的改进KNN算法,"王煜,王正欧,白石","最近邻分类器是假定局部的类条件概率不变,而这个假定在高维特征空间中无效。因此在高维特征空间中使用k最近邻分类器,不对特征权重进行修正就会引起严重的偏差。本文采用灵敏度法,利用前馈神经网络获得初始特征权重并进行二次降维。在初始权重下,根据样本间相似度采用SS树方法将训练样本划分成若干小区域,以此寻找待分类样本的近似k0个最近邻,并根据近似k0个最近邻和Chi-square距离原理计算新权重,搜索出新的k个最近邻。此方法在付出较小时间代价的情况下,在文本分离中可获得较好的分类精度的提高。","计算机应用,中文信息处理,文本分类,神经网络,Chi-square距离,KNN算法"
2006-05-16,中文网络聊天语言的奇异性与动态性研究,"夏云庆,黄锦辉,张普",": 随着互联网走入社会生活,网络聊天逐渐成为一种新的沟通渠道,网络聊天语言便应运而生。这类语言的日益丰富,给语言信息处理带来了新的挑战。研究发现,困难主要来自网络聊天语言的奇异性和动态性。本文借助真实网络聊天语言文本,对网络聊天语言的奇异性和动态性进行详细分析和归纳,并设计了面向解决奇异性和动态性问题的网络聊天语言文本识别与转换方法。我们先以网络聊天语言语料库为基础建立网络聊天语言模型和语言转换模型,通过信源–信道模型实现网络聊天语言向标准语言的转换。但该方法过于依赖网络聊天语言语料库,虽然能较好解决奇异性问题,但不能处理动态性问题。因此,我们进而以标准汉语语料库为基础建立文字语音映射模型,对信源–信道模型进行改进,最终有效解决了网络聊天语言的动态性问题。","计算机应用,中文信息处理,网络聊天语言,奇异性,动态性,语言信息处理"
2006-08-28,面向中文陌生文本的人机交互式分词方法,"李斌,陈小荷","自动分词是中文信息处理的基础课题之一。为了克服传统分词方法在处理特殊领域文本时遇到的困难,本文提出了一种新的分词方法,在没有词表和训练语料的条件下,让用户参与到分词过程中,增加系统的语言知识,以适应于不同的语料和分词标准。系统采用改进的后缀数组算法,不断提取出候选词语,交给用户进行筛选,最后得到词表进行分词。四个不同语料的实验结果显示,不经过人工筛选,分词F值可以达到72%左右;而经过较少的人机交互,分词F值可以提高12%以上。随着用户工作量的增加,系统还能够进一步提高分词效果。","计算机应用,中文信息处理,自动分词,未登录词识别,陌生文本,人机交互"
2006-06-03,中文词语语义相似度计算——基于《知网》2000,"李峰,李芳","词语语义相似度的计算,一种比较常用的方法是使用分类体系的语义词典(如Wordnet)。本文首先利用Hownet中“义原”的树状层次结构,得到“义原”的相似度,再通过“义原”的相似度得到词语(“概念”)的相似度。本文通过引入事物信息量的思想,提出了自己的观点: 认为知网中的“义原”对“概念”描述的作用大小取决于其本身所含的语义信息量;“义原”对“概念”的描述划分为直接描述和间接描述两类,并据此计算中文词语语义相似度,在一定程度上得到了和人的直观更加符合的结果。","计算机应用,中文信息处理,词语语义相似度,知网,“义原”,语义信息量"
2005-08-30,基于广义置信度的样本选择算法,任俊玲,": 对模式识别系统而言,不同的训练样本在建立模式类模型时所起的作用不同,因此必须对训练样本进行选择。而在训练样本中,边界样本的判定方式以及训练样本中包含边界样本数量的多少对分类的精度起主要作用。为此,结合基于模板匹配的脱机手写汉字识别,定义了一种通过广义置信度判定边界样本的方法,并且在此基础上建立了基于广义置信度的训练样本选择算法。通过在脱机手写汉字数据库HCL2004上进行实验,由该算法选择出的训练样本集在训练样本数减少的同时,使得系统识别率有了较大的提高,从而证实了该算法的有效性。","人工智能,模式识别,广义置信度,样本选择,手写汉字识别,HCL2004"
2006-06-07,基于弹性网格模糊特征的手写体汉字识别方法,"刘伟,朱宁波,何浩智,李德鑫,孙发军","网格方向特征在手写体汉字识别系统中得到广泛应用,被认为是目前较成熟的手写体汉字特征之一。网格技术是网格方向特征的关键技术之一。根据汉字笔画分布特点及拓扑结构的相关性,提出了一种新的基于弹性网格及其相关模糊特征的提取方法。该方法使特征向量的信息量增加,特征更加稳定。对银行支票图像大写金额的识别率达到97.64%,实验结果证明本文方法比其他网格方向特征更有效。","人工智能,模式识别,弹性网格,相关模糊特征,手写体汉字识别"
2006-12-25,支持重音合成的汉语语音合成系统,朱维彬,"针对基于单元挑选的汉语语音合成系统中重音预测及实现,本文采用了知识指导下的数据驱动建模策略。首先,采用经过感知结果优化的重音检测器,实现了语音数据库的自动标注;其次,利用重音标注数据库,训练得到支持重音预测的韵律预测模型;用重音韵律预测模型替代原语音合成系统中的相应模型,从而构成了支持重音合成的语音合成系统。实验结果分析表明,基于感知结果优化的重音检测器的标注结果是可靠的;支持重音的韵律声学预测模型是合理的;新的合成系统能够合成出带有轻重变化的语音。","计算机应用,中文信息处理,重音,韵律模型,语音合成"
2007-04-10,分词规范亟需补充的三方面内容,"李玉梅,陈晓,姜自霞,易江燕,靳光瑾,黄昌宁","本文认为,为提高语料库的分词标注质量应在分词规范中补充三个内容: ①命名实体(人名、地名、机构名)标注细则;②表义字串(日期、时间、百分数等)标注细则;③歧义字串的消解细则。因为一方面命名实体和表义字串已被不少分词语料库视为分词单位,另一方面在以往的分词规范中几乎从不谈及歧义消解问题。其实人们对歧义字串的语感往往是不同的。因此有必要在规范中对典型的歧义字串予以说明。实践表明,在规范中交待清楚以上三方面内容,就可以在很大程度上避免标注的错误和不一致性。","计算机应用,中文信息处理,语料库,分词规范,分词歧义消解"
2007-04-25,基于有效子串标注的中文分词,"赵海,揭春雨","由于基于已切分语料的学习方法和体系的兴起,中文分词在本世纪的头几年取得了显著的突破。尤其是2003年国际中文分词评测活动Bakeoff开展以来,基于字标注的统计学习方法引起了广泛关注。本文探讨这一学习框架的推广问题,以一种更为可靠的算法寻找更长的标注单元来实现中文分词的大规模语料学习,同时改进已有工作的不足。我们提出子串标注的一般化框架,包括两个步骤,一是确定有效子串词典的迭代最大匹配过滤算法,二是在给定文本上实现子串单元识别的双词典最大匹配算法。该方法的有效性在Bakeoff-2005评测语料上获得了验证。","计算机应用,中文信息处理,中文分词,基于子串标注的分词"
2007-03-20,基于双字耦合度的中文分词交叉歧义处理方法,"王思力,王斌","本文提出了一种利用双字耦合度和t-测试差解决中文分词中交叉歧义的方法: 首先利用词典找出所有的交叉歧义,然后用双字耦合度和t-测试差的线性叠加值来判断各歧义位置是否该切分。实验结果表明,双字耦合度和t-测试差的结合要优于互信息和t-测试差的结合,因此,用双字耦合度和t-测试差的线性叠加值来消除交叉歧义是一种简单有效的方法。","计算机应用,中文信息处理,中文分词,双字耦合度,t-测试差"
2007-05-18,基于动作建模的中文依存句法分析,"段湘煜,赵军,徐波","决策式依存句法分析,也就是基于分析动作的句法分析方法,常常被认为是一种高效的分析算法,但是它的性能稍低于一些更复杂的句法分析模型。本文将决策式句法分析同产生式、判别式句法分析这些复杂模型做了比较,试验数据采用宾州中文树库。结果显示,对于中文依存句法分析,决策式句法分析在性能上好于产生式和判别式句法分析。更进一步,我们观察到决策式句法分析是一种贪婪的算法,它在每个分析步骤只挑选最有可能的分析动作而丢失了对整句话依存分析的全局视角。基于此,我们提出了两种模型用来对句法分析动作进行建模以避免原决策式依存分析方法的贪婪性。试验结果显示,基于动作建模的依存分析模型在性能上好于原决策式依存分析方法,同时保持了较低的时间复杂度。","计算机应用,中文信息处理,中文依存句法分析,决策式依存分析,动作建模"
2007-04-28,基于依存分析和错误驱动的中文时间表达式识别,"贺瑞芳,秦兵,刘挺,潘越群,李生","时间表达式识别是进行时间表达式归一化的基础,其识别结果的好坏直接影响归一化的效果。本文提出一种基于依存分析和错误驱动识别中文时间表达式的新方法。首先以时间触发词为切入点,据依存关系递归地识别时间表达式,大大地提高了识别效果;然后,采用错误驱动学习来进一步增强识别效果,根据错误识别结果和人工标注的差异自动地获取和改进规则,使系统的性能又提高了近3.5%。最终在封闭测试集和开放测试集上,F1值达到了76.38%和76.57%。","计算机应用,中文信息处理,时间表达式识别,触发词,依存分析,错误驱动学习"
2007-03-31,基于分解转移矩阵的PageRank迭代计算方法,"刘松彬,都云程,施水才","本文提出了一种基于分解转移矩阵的PageRank的迭代计算方法。该方法对PageRank理论模型进一步推导,把其Markov状态转移矩阵进行了分解,从而降低存储开销和计算复杂度,减少I/O需求,使得PageRank计算的工程化实现更为简单。实验表明1 700多万的网页2.8亿条链接,可以在30秒内完成一次迭代,内存需求峰值585MB,可以满足工程化应用的需求。","计算机应用,中文信息处理,PageRank,搜索引擎,Markov状态转移矩阵,矩阵分解"
2007-05-10,面向信息检索的概念关系自动构建,"胡熠,陆汝占,刘慧","概念之间的依存分析是提高信息检索性能的关键。相比概念关系的强弱而言,识别关系的类型更有意义。本文在Bootstrapping框架下,以“(地理)领属”,“(实体)功能”和“(动作)对象”三种语义关系类型为例,获得了构建上下文中两个概念特定关系的语义模板,并开发了一个名为SPG的系统。本文的工作: (1)引入了生物信息计算中序列比对的方法两两生成相似上下文的模板;(2)定义了新的模板评价机制估计模板的置信度。就这三种概念关系的识别而言,SPG获得的模板集合相比DIPRE系统获得了更高的正确率和覆盖能力。","计算机应用,中文信息处理,文本检索模型,概念关系构建,bootstrapping"
2007-04-13,基于多重冗余标记CRFs的句子情感分析研究,"王根,赵军","本文提出了一种基于多重冗余标记的CRFs并将其应用于情感分析任务。该方法不仅能够有效地解决有序标记的分类问题,还能够在保证情感分析中各子任务能够使用不同特征的前提下,将情感分析中的主客观分类、褒贬分类和褒贬强弱分类任务统一在一个模型之中,在多个子任务上寻求联合最优,制约分步完成时误差的传播。实验证明,该方法有效地提高了句子情感分析任务的准确率。在理论上,该方法也为基于最大似然训练的算法解决序回归问题提供了一条途径。","计算机应用,中文信息处理,句子情感分析,序回归,条件随机场,冗余标记"
2007-04-15,面向文本分类的基于最小冗余原则的特征选取,"张希娟,王会珍,朱靖波","在文本分类中,为了降低计算复杂度,常用的特征选取方法(如IG)都假设特征之间条件独立。该假设将引入严重的特征冗余现象。为了降低特征子集的冗余度,本文提出了一种基于最小冗余原则(minimal Redundancy Principle, MRP)的特征选取方法。通过考虑不同特征之间的相关性,选择较小冗余度的特征子集。实验结果显示基于最小冗余原则方法能够改善特征选取的效果,提高文本分类的性能。","计算机应用,中文信息处理,条件独立性假设,最小冗余原则,特征选取,文本分类"
2007-04-13,中文歌词的统计特征及其检索应用,"郑亚斌,刘知远,孙茂松","我们在歌词上做了一些传统的自然语言处理相关的实验。歌词是歌曲语义上的重要表达,因此,对歌词的分析可以作为歌曲音频处理的互补。我们利用齐夫定律对歌词语料库的字和词进行统计特征的考察,实验表明,其分布基本符合齐夫定律。利用向量空间模型的表示,我们可以找到比较相似的歌词集合。另外,我们探讨了如何利用歌词中的时间标注信息进行进一步的分析: 例如发现歌曲中重复片段,节奏划分,检索等。初步的实验表明,我们的方法具有一定的效果。","计算机应用,中文信息处理,歌词,齐夫定律,k-近邻,节奏"
2007-03-30,基于链接分析的重要Blog信息源发现,"杨宇航,赵铁军,郑德权,于浩","本文提出了一种基于链接分析的对Blog信息源进行量化评估的方法,在此基础之上发现重要Blog信息源,既体现了Blog信息的特点,又在一定程度上减小了作弊链接对链接分析结果的影响,能为用户阅读信息提供方便,并可望为Blog信息检索提供一种新的思路。为了证明该评估方法的有效性,本文还提出了Blog信息源重要性的评价指标,对比了重要Blog信息源量化评估方法和评价指标的评分结果,通过相关性分析,表明此方法和评价指标存在高度的一致性。","计算机应用,中文信息处理,重要Blog信息源,链接分析,评价指标,相关性分析"
2007-04-20,汉语语句主题语义倾向分析方法的研究,"姚天昉,娄德成","本文介绍了如何识别汉语语句主题和主题与情感描述项之间的关系以及如何计算主题的语义倾向(极性)。我们利用领域本体来抽取语句主题以及它的属性,然后在句法分析的基础上,识别主题和情感描述项之间的关系,从而最终决定语句中每个主题的极性。实验结果显示,与手工标注的语料作为金标准进行比较,用于识别主题和主题极性的改进后的SBV极性传递算法的F度量达到了72.41%。它比原来的SBV极性传递算法和VOB极性传递算法的F度量分别提高了7.6%和2.09%。因此,所建议的改进的SBV极性传递算法是合理和有效的。","计算机应用,中文信息处理,意见挖掘,主题,语义倾向"
2007-05-11,现代汉语语义资源用于短语歧义模式消歧研究,"王锦,陈群秀","现代汉语存在着许多歧义短语结构,仅依靠句中词性标记无法获得词与词之间正确的搭配关系。本文研究了大量包含歧义的短语实例,分析了计算机处理汉语结构时面临的定界歧义和结构关系歧义问题,在已有短语结构规则的基础上归纳出了七种结构歧义模式,提出了分析歧义模式的关键是四种基本搭配信息的判断,并实现了基于语义知识和搭配知识的消歧算法。对887处短语进行排歧的实验结果表明,处理短语结构的正确率由82.30%上升到87.18%。","计算机应用,中文信息处理,现代汉语语义知识库,搭配词典,短语歧义排歧"
2007-04-30,基于分层语块分析的统计翻译研究,"魏玮,杜金华,徐波","本文描述了一个基于分层语块分析的统计翻译模型。该模型在形式上不仅符合同步上下文无关文法,而且融合了基于条件随机场的英文语块分析知识,因此基于分层语块分析的统计翻译模型做到了将句法翻译模型和短语翻译模型有效地结合。该系统的解码算法改进了线图分析的CKY算法,融入了线性的N-gram语言模型。目前,本文主要针对中文－英文的口语翻译进行了一系列实验,并以国际口语评测IWSLT(International Workshop on Spoken Language Translation)为标准,在2005年的评测测试集上,BLEU和NIST得分均比统计短语翻译系统有所提高。","人工智能,机器翻译,基于分层语块分析的统计翻译模型,条件随机场,CKY算法"
2007-04-12,基于“松弛尺度”的短语翻译对抽取方法,"何彦青,周玉,宗成庆,王霞","短语对抽取是基于短语统计机器翻译方法的关键技术。当前广泛使用的Och提出的短语对抽取方法,过于依赖词对齐结果,因而只能抽取与词对齐完全相容的短语对。本文给出一种基于“松弛尺度”的短语抽取方法,对不能完全相容的短语对,结合词性标注信息和词典信息来判断是否进行抽取,放松“完全相容”的限制,可以保证为更多的源短语找到目标短语。实验表明,该抽取方法的性能比Och的方法有明显的改善和提高。","人工智能,机器翻译,短语对抽取,统计机器翻译,松弛尺度"
2007-04-10,汉语框架语义知识库及软件描述体系,"郝晓燕,刘伟,李茹,刘开瑛","汉语框架网络工程是以框架语义学为理论基础的基于语料库的计算词典编纂工程,用于语言学、计算语言学研究及自然语言处理研究。该工程的结果包括两部分: 汉语框架语义知识库(即词典资源)和相关软件。其中,汉语框架网络知识库包括框架库、句子库和词元库三部分,相关软件主要包括汉语框架语义知识库管理系统和基于Web的展示系统。本文介绍了汉语框架语义知识库的语义描述体系以及软件描述体系。","计算机应用,中文信息处理,汉语框架网络,框架语义,描述体系,软件"
2007-04-03,基于加权二部图的汉日词对齐,"吴宏林,刘绍明,于戈","高效的自动词对齐技术是词对齐语料库建设的关键所在。当前很多词对齐方法存在以下不足: 未登录词问题、灵活翻译问题和全局最优匹配问题。针对以上不足,该文提出加权二部图最大匹配词对齐模型,利用二部图为双语句对建模,利用词形、语义、词性和共现等信息计算单词间的相似度,利用加权二部图最大匹配获得最终对齐结果。在汉日词语对齐上的实验表明,该方法在一定程度上解决了以上三点不足,F-Score为80%,优于GIZA/sub>sub>++的72%。","计算机应用,中文信息处理,词对齐,二部图,匹配"
2007-04-15,现代汉语虚词知识库的研究,"昝红英,张坤丽,柴玉梅,俞士汶","现代汉语虚词在句法中所起的作用比较复杂,其个性较强,用法各异。目前已有的虚词研究成果大都是面向人用的,对虚词个性的描写难以避免主观性和模糊性,很难直接应用于自然语言处理的研究。本文从计算语言学的观点出发,根据目前已有的虚词研究成果以及对真实语料中虚词用法规律的考察,着力构建面向机器的现代汉语虚词用法信息词典和虚词用法规则库,旨在为现代汉语虚词用法的机器识别打下一定的数据基础。","计算机应用,中文信息处理,虚词,语言知识库,用法属性,规则库"
2007-04-10,基于双语语料库的短语复述实例获取研究,"李维刚,刘挺,李生","本文提出一种基于双语语料库的短语复述实例获取方法,尤其能够很好的抽取歧义短语的复述实例。该方法通过输入一个双语短语对约束短语的语义,利用词对齐的双语语料库,构造一个双向抽取模型从中抽取双语对的复述实例。双向抽取模型通过比较每一个候选复述短语和输入短语之间的语义一致性,来确定每个候选是否成为最终的复述实例。实验结果表明,本文短语复述实例获取方法的综合准确率达到了 60% ,获取了较好的性能。","计算机应用,中文信息处理,复述实例,复述获取,短语复述,双语语料库"
2007-04-05,语言学与统计方法结合建立汉语动词SCF类型集,"冀铁亮,孙薇薇,穗志方","动词子语类框架(Subcategorization Frame以下简称SCF)在句法分析、语义角色标注等方面的研究中具有不可或缺的重要作用。在子语类框架信息的获取过程中,首先要建立标准完备的子语类框架类型集。目前英语研究已经建立了获得普遍共识的子语类框架类型集。而汉语方面还没有标准的动词子类框架类型集。本文提出一种语言学知识与统计方法相结合的汉语动词子语类框架类型集的半自动获取方案。初步建立起既符合统计结果又基本符合语言学理论的汉语动词子语类框架类型集。实验证明,加入语言学理论的子语类框架类型集降低了对语料的依赖程度,比完全由分析语料产生的类型集更完备。","计算机应用,中文信息处理,动词子语类框架,类型集,语言学与统计方法结合"
2007-04-09,基于多层次特征集成的中文实体指代识别,"张海雷,曹菲菲,陈文亮,任飞亮,王会珍,朱靖波","实体指代识别(Entity Mention Detection, EMD)是识别文本中对实体的指代(Mention)的任务,包括专名、普通名词、代词指代的识别。本文提出一种基于多层次特征集成的中文实体指代识别方法,利用条件随机场模型的特征集成能力,综合使用字符、拼音、词及词性、各类专名列表、频次统计等各层次特征提高识别性能。本文利用流水线框架,分三个阶段标注实体指代的各项信息。基于本方法的指代识别系统参加了2007年自动内容抽取(ACE07)中文EMD评测,系统的ACE Value值名列第二。","计算机应用,中文信息处理,实体指代识别,多任务标注,条件随机场模型,ACE评测"
2007-04-30,否定词跨标点句管辖的判断,"张瑞朋,宋柔","现代汉语中基本否定词“不”以及扩充词“从不”、“很不”、“不能”、“不会”等的否定辖域受到学术界重视,但前人研究一般局限于句内,且主要局限于基本否定词,其实否定辖域也涉及多个标点句,否定词的管辖判断也涉及到扩充的否定词。跨标否定词跨标点句管辖的判断和否定词共享问题是整个跨标点句句法共享问题的一个重要组成部分。本文从形式上找到了一些否定词跨标点句的共享规律,即着重从形式角度讨论了否定词跨标点句的辖域问题,对现代汉语长句句法分析有重要作用,并对汉外机器翻译有实用价值。","计算机应用,中文信息处理,否定词,标点句,管辖,共享"
2007,ACL2007会议观感,"刘群,刘洋","每年一度的计算语言学学会年会(Annual Meeting of ACL)是计算语言学界的盛会,也是计算语言学和自然语言处理领域最有影响的学术会议,ACL每年发表的论文都反映了这一领域的最新研究进展和学术动向,受到研究工作者的广泛重视。今年的ACL2007是ACL的第45届年会,在美丽而又浪漫的东欧古城——捷克首都布拉格召开,同时召开的有EMNLP-CoNLL 2007和IWPT 2007等2个学术会议(Conference)、15个学术研讨会(Workshop)以及5个专题讲座(Tutorial)。会期从6月23日开始一直持续到6月30日结束,根据ACL2007网站公布的名单,参加会议的人数达到了创纪录的1",
2007-02-01,知网的理论发现,"董振东,董强,郝长伶","知网正式发布至今已经8年了。海内外很多人对它已不陌生了。现在该是我们为知网的理论发现做点小结的时候了。本文它们包括(1)知网的知识观,(2)关于知识的获取和表达,(3)事件类概念分类的双轴论,(4)关于语义角色,(5)知识数据描述语言(KDML)。本文还介绍了知网的计算意义的能力以及它最新发展。知网将成为一些新兴技术如自然语言搜索等的基础设施。","计算机应用,中文信息处理,知识系统, 本体论,义原,语义角色, WordNet"
2007-01-26,语义角色的精细等级及其在信息处理中的应用,袁毓林,"本文首先讨论语义角色的三种精细程度不一的分类层级,介绍它们各自在语言信息处理系统中的有关应用。接着,分别介绍三种为语言信息处理服务的语义资源对于语义角色的不同处理: (i)加州大学伯克利分校框架网的语义角色——基于场景的语义框架中的框架元素; (ii)宾州大学命题库的语义角色——基于特定动词的编了号的原型角色; (iii)北京大学中文网库的语义角色——基于特定谓词的各论元成分的论旨角色。最后,从建库目标、方法论、标注内容和系统构成等方面,比较这三个语义关系标注语料库的同异。","计算机应用,中文信息处理,语义角色,语义标注,框架网,命题库,网库,框架元素,论旨角色"
2006-07-07,一种结合SVM学习的产生式依存分析方法,"罗强,奚建清","本文提出了一种结合SVM学习和产生式模型的依存分析方法。该方法用产生式模型的分析错误对SVM分类器进行训练。为进一步提高分析精度,采用扩大寻优范围的动态规划算法对产生式模型的分析结果进行错误估计,同时引入范围参数,使得寻优范围可以根据实际情况进行调整。本方法在不牺牲分类性能的前提下,有效减少了训练SVM分类器所依赖的支撑向量数。在对哈工大中文树库语料上的对比测试结果表明,该方法的依存分析精度达到86.4%,具有很强的依存分析能力。","计算机应用,中文信息处理,中文依存分析,产生式概率模型, SVM学习,SMO,动态规划算法"
2006-08-11,中文本体映射研究与实现,"李佳,祝铭,刘辰,杨正球","本体间的异构是语义网建设亟待解决的问题,本体映射则是解决本体异构的有效手段。中文资源是信息网络的重要组成部分,实现中文本体间以及中文与其他本体的映射是实现知识共享重用的一个重要组成部分。本文从元素层的角度对中文本体映射进行了研究,提出利用知网,结合多种技术计算词汇相似度,利用词汇的相似度计算概念匹配的可信度,实现元素层本体映射的算法,并根据此算法实现了ELOMC(Element Level Ontology Matching for Chinese)系统。","计算机应用,中文信息处理,中文本体映射, 知网, 词汇相似度, 语义网"
2005-12-21,基于关键短语的文本分类研究,刘华,"文本分类的进一步改进除了算法方面,应该还立足于影响文本分类最底层、最根本的因素: 文本表示中的特征项,提高特征项的完整独立程度。关键短语是具有强文本表示功能的特征短语,在表示文本时,能将文本的内容特征(如主题类别)鲜明地表示出来。关键短语具有结构稳定、语义完整和较强统计意义的特点,能克服向量空间模型和贝叶斯假设的缺点,更适合作为文本表示的特征,有利于提高文本分类的效果。本文从语言学、认知心理学和言语习得、计算语言学等方面寻求关键短语优势的理论依据,对关键短语进行了界定,通过抽取网页上专家标引的关键词获得关键短语。在约3万篇测试集上(共15个大类,244个小类),与以词为特征的文本分类相比,以关键短语为特征的文本分类的大类微平均提高了3.1％,小类微平均提高了15％。","计算机应用,中文信息处理,文本分类,关键短语,文本表示,特征项"
2006-12-28,基于网页内容的广告推介研究,"施水才,程涛,王霞,吕学强","网页与广告关联是基于网页内容的网络广告的核心技术,本文提出了一种基于语义的、以实现网页和广告精确匹配为目标的广告推介方法。首先对一个Web网页进行主题信息提取,获得网页的主题词;然后再对这些主题词语作同义词扩展、上位词扩展、下位词扩展和相关词扩展,最后从待匹配的广告中选择匹配度最高的广告。对该方法进行了模型系统实现并进行了测试运行, 结果表明该方法是行之有效的。","计算机应用,中文信息处理,同义词词林,主题词,网页数据抽取,关联度"
2006-09-04,一种抗噪音的中文网页分类方法,"王小冷,王斌","网页分类可以看成是噪音环境下的文本分类问题。本文是在噪音环境下文本分类方法的一种探索: 把在传统文本分类中性能基本相当的基于N-gram模型的贝叶斯(NGBayes)、基于分词的朴素贝叶斯(NBayes)和基于分词的k近邻(kNN)分类方法应用到网页分类领域,在中文Web信息检索论坛提供的中文网页分类训练集——CCT2002-v1.1(Corp_1)和我们自己整理的中文网页集(Corp_2)进行了实验。验证了三种分类方法在非噪音环境下性能基本相当,而噪音环境下的实验结果表明,NGBayes的分类性能远远高于其他两种方法,这说明NGBayes对中文网页中的噪音不敏感。然后通过对特征的分析,探讨了NGBayes抗噪音的原因。从而得出结论: NGBayes是一种抗噪音的中文网页分类方法。","计算机应用,中文信息处理,N-gram模型,NBayes, kNN"
2006-12-28,基于混合并行遗传算法的文本聚类研究,"何婷婷,戴文华,焦翠珍","针对传统K-Means聚类算法对初始聚类中心的选择敏感,易陷入局部最优解的问题,提出一种基于混合并行遗传算法的文本聚类方法。该方法首先将文档集合表示成向量空间模型,并在文档向量中随机选择初始聚类中心形成染色体,然后结合K-Means算法的高效性和并行遗传算法的全局优化能力,通过种群内的遗传、变异和种群间的并行进化、联姻,有效地避免了局部最优解的出现。实验表明该算法相对于K-Means算法、简单遗传算法等文本聚类方法具有更高的精确度和全局寻优能力。","计算机应用,中文信息处理,并行遗传算法,K-Means聚类,文本聚类,向量空间模型,特征抽取"
2006-10-18,一种基于关键词的中文文档图像检索方法,"黄祥林,高芸,杨丽芳,王鹏鹏","本文提出了一种基于关键词的中文文档图像检索方法,能在不经OCR(Optical Character Recognition)识别的情况下,直接利用中文字符的图像特征进行关键词检索。首先将文档图像分割成单个中文字符图像,接着对字符图像进行汉字笔画的特征数据提取,然后在特征数据间进行基于WMHD(Weighted Modified Hausdorff Distance)的相似性测量。该方法不受字号的影响,也有一定的抗字体能力,实验证明其具有较高的检索效果。","计算机应用,中文信息处理,中文文档图像,关键词检索,加权的修正Hausdorff距离(WMHD)"
2007-01-08,一个面向广播语音识别的语言模型自适应框架,"王晓瑞,丁鹏,梁家恩,徐波","语言模型自适应的目的是减小模型与识别任务之间的语言差异。这些差异包括词典差异、风格和内容差异以及模型的概率分布差异。本文提出一种新的非迭代的中文新词提取方法和一种新的开放式词典的中文语言模型。基于这些技术,本文提出一个面向广播语音识别的语言模型自适应框架,该框架联合了以下技术: 一种新的非迭代的新词提取方法,一种新的中文开放式词典语言模型,一种基于困惑度( PPL) 的背景语料筛选方法和一个 N2gram 概率分布自适应模块。另外,本文还专门分析了在语言模型自适应过程中命名实体词的识别情况。实验表明,通过使用该框架,误识率相对下降了10 % ,实体词识别准确率提高了4 %。","计算机应用,中文信息处理,语言模型自适应,新词提取,开放式词典"
2006-04-14,普通话广播语音的多层次标注与检索,"章森,华绍和","广播语音的自动识别、标注、检索等是涉及到语音技术、自然语言处理、信息检索等多个领域的综合性课题。在介绍了广播语音的自动标注与检索的研究概况并分析了其中涉及的关键技术基础上,提出了面向普通话广播语音的多层次自动标注框架以及基于多层次标注的语音检索方案,对文档层、句子层和词语层的标注属性进行了探讨,采用了递归标注方法对属性逐层细化,并讨论了对语音自动标注至关重要的语音识别引擎和语音流分割等问题。基于本文提出的方法,对10 小时的普通话广播语音资料进行了标注和检索,得到了比较满意的实验结果。","计算机应用,中文信息处理,广播语音,自动标注,语音检索,声学模型,语言模型"
2007-03-23,中文实体关系抽取中的特征选择研究,"董静,孙乐,冯元勇,黄瑞红","命名实体关系抽取是信息抽取研究领域中的重要研究课题之一。通过分析,本文提出将中文实体关系划分为: 包含实体关系与非包含实体关系。针对同一种句法特征在识别它们时性能的明显差异,本文对这两种关系采用了不同的句法特征集,并提出了一些适合各自特点的新的句法特征。在CRF 模型框架下,以ACE2007 的语料作为实验数据,结果表明本文的划分方法和新特征有效的提高了汉语实体关系抽取任务的性能。关键词: 计算机应用;中文信息处理;实体关系抽取;包含关系;非包含关系;特征选择;ACE 评测",
2006-10-20,中文科技文档中的数学表达式定位,"张志伟,孔凡让,刘维来,龙潜,刘永斌","数学表达式定位是印刷体数学表达式识别的前提。针对中文科技文档,分别对独立表达式和内嵌表达式的定位问题提出了新的方法。采用自适应神经模糊推理系统(ANFIS) 对行特征进行分类,提取出独立表达式;采用模糊聚类和动态规划方法,从文档中依次提取出汉字、中文标点和英文字符,利用启发式规则合并剩余的数学符号而提取出内嵌表达式。实验表明,提出的表达式定位方法有很高的正确率。","人工智能,模式识别,数学表达式定位,自适应神经模糊推理系统,模糊聚类,中英文分离"
2007-01-22,基于语言学知识的发音质量评价算法改进,"刘庆升,魏思,胡郁,郭武,王仁华","随着普通话推广工作的深入,采用计算机进行普通话的辅助测试和学习的需求日益迫切。本文针对普通话发音特点,提出了一种改进的基于音素的自动发音质量评价算法。新算法在隐马尔科夫模型的对数后验概率算法基础上,引入普通话发音的语言学知识。与改进前相比,新算法不仅降低了运算量,而且在普通话水平测试的 303 人现场录音库上,使得机器打分与国家级评测员打分之间的相关度从0. 704 提升到0. 795 。","计算机应用,中文信息处理,语音识别,发音质量评价,对数后验概率,语言学知识"
2007-02-08,词汇搭配和用户模型在拼音输入法中的应用,"张玮,孙乐,冯元勇,李文波,黄瑞红","中文输入法是中文信息处理的难题之一。随着互联网上中文用户的不断增加,中文输入法的重要性也变得日益突出。本文在对句子中长距离词汇依赖现象观察的基础上,抽取出语料库中的词汇搭配来获取长距离特征,并以此构建基于词汇搭配关系的拼音输入法系统;同时将词汇搭配的思想应用到拼音输入法的用户模型中,从而使我们的输入法系统能够辅助用户更加有效的输入。实验表明基于词汇搭配关系的改进方法对提高输入法的准确率有积极的作用。","计算机应用,中文信息处理,中文输入法,中文信息处理,统计语言模型,词汇搭配,长距离特征,用户模型"
2007-01-10,中国在ISO/ IEC JTC1/ SC2 的活动与中文编码的国际标准化,陈壮,"标准化是实现技术产业化的基础。中文信息处理技术是我国特有的、具有国际领先水平的技术。我国自 20 世纪80 年代参与ISO/ IEC J TC1/ SC2 的活动以来,在中文编码技术的国际标准化工作中取得了显著成绩。本文介绍了ISO/ IEC J TC1/ SC2 的工作领域、工作方式和组织结构;我国参与ISO/ IEC J TC1/ SC2 及其下属该组织活动的方式;国际标准ISO/ IEC 10646 的大致情况和我国在参与此国际标准研制工作中取得的成绩、当前的工作和未来工作的计划。本文论述了我国参与本文ISO/ IEC J TC1/ SC2 活动的意义,以及我国在ISO/ IEC J TC1/ SC2 活动中的作用、地位和影响。作者还提出了对未来工作的建议。","计算机应用,中文信息处理, ISO/ IEC J TC1/ SC2 , ISO/ IEC 10646 ,中文编码"
2007-01-10,基于Open Type 的维哈柯文自动选形引擎的设计与实现,"苏国平,缪成,夏国平","本文面向维哈柯文自动选择显示字形研究,分析了新疆地区普遍使用的维吾尔、哈萨克文和柯尔克孜文变形显示的特点,简要介绍了最新Open Type 字体技术的结构与操作步骤,基于该字体技术设计了一种通用维哈柯文自动选形引擎,通过分析文字的连接类型,构建自动选形规则库,按照规则绑定字形标签,并应用Open Type 字体解释引擎按照字形标签完成字符替换与置位操作。并且在永中office 维哈柯文版本上实现了该自动选形引擎, 经实际应用测试证明,该字体引擎完全实现了维哈柯文变形显示的要求。","计算机应用,中文信息处理,维哈柯文,选形引擎,Open Type"
2007-02-01,藏语述说动词小句宾语及其标记3,江荻,"本文讨论藏语述说动词管控的句子性小句宾语。藏语述说动词包括说类动词、认知动词、思考动词、询问动词及其他语义相关的动词。从小句自身结构看,可以是完整的句子,带主语、谓语以及句末动词体貌标记和语气词,也可能只是单一的谓语动词。小句宾语自身具有谓词性,通常通过添加名词化标记使之名词化。小句宾语的标记来自古代述说类动词的类典型zer 的语法化,而在现代藏语中作为小句标记语音和书写形式上都有多个变体。小句宾语内部也有复杂的关系和层次,类似于英语的直接引语与间接引语。小句缺省主语的情况下,动作发出者可通过表示体貌、情态的语法词以及上下文来确定。小句的句类包括陈述、疑问、祈使和感叹,可带不同的句类语气词。最后应该指出,有一部分述说动词小句宾语经常不带名词化标记,这种现象会给句法处理算法带来一定的麻烦,相关原因和解决办法还须进一步研究。","计算机应用,中文信息处理,藏语,述说动词小句宾语,标记,语法化"
1999,英汉机器翻译中人称代词的处理,"梁茂成,李刚","人称代词处于一切自然语言的词汇核心层,机器翻译对其处理是否得当影响极大。本文运用对比分析的方法,揭示英汉人称代词的异同,同时针对机器翻译对人称代词处理的不足,提出解决问题的途径。","机器翻译,人称代词,回指,预指"
1999,文本层次分析与文本浏览,"林鸿飞,战学刚,姚天顺","本文简要描述了文本的物理结构和逻辑结构以及相应的向量空间模型。研制了具有导航机制的文本浏览系统。提出了文本结构分析中的层次分析方法,它采用有序划分层次的方法。并在此基础上,给出了文本结构中各单元的标记信息,由此形成了文本的可视化表示。利用文本、层次、段落的超文本连接,根据浏览的需要,逐级展现文本细节,帮助用户有目的、有选择地浏览文本。最后给出评价的结果。","向量空间模型,文本结构分析,文本浏览,文本可视化"
1999,文本分类中基于对数似然比测试的特征词选择方法,李国臣,"本文将对数似然比测试用于文本分类中的特征词选择。与传统的频度、集中度和分散度等多种统计指标的测试独立进行的方法相比较,这种方法利用协方差矩阵协调了各个统计指标之间的联系,从而将它们有机地统一为一个整体。实验显示,这种特征词选择方法优于传统的频度测试、集中度测试和分散度测试独立进行的特征词选择的方法。","文本分类,特征词选择,对数似然比测试"
1999,基于组合特征的中文版面分析方法,"田学东,郭宝兰","在对中、外文版面特点进行比较的基础上,指出了中文版面分析的困难所在,并有针对性地归纳出了相应的版面组合特征。利用这些特征,建立了一种以自底向上分析为主,同时融入自顶向下某些方法与结果的中文版面分析方法。实验结果表明,这种方法能够对比较规范的中文版面进行分析,具有较高的效率和较好的适应性。","版面分析,文字识别,组合特征,连通区域"
1999,基于HMM的汉语文本识别后处理研究,"李元祥,丁晓青,刘长松","本文用HMM(Hidden Markov Model)描述汉语文本识别后处理,将汉语语言和单字识别这两个概率模型结合起来,以充分利用单字识别器提供的信息。语言模型的参数由语料库统计得到;单字识别模型的参数为条件概率,经理论分析,它可转化为后验概率来求解。在分析训练样本集单字识别结果的基础上,提出一种统计方法估计候选字的后验概率。HMM在脱机手写体汉语文本识别中的实验表明,后处理性能除取决于语言模型外,还取决于后验概率的精确估计。","汉字识别,后处理,语言模型,隐马尔可夫模型,后验概率"
1999,利用层次图形元法识别工整手写汉字的研究,"陈伟,方应谦","本文提出了利用层次图形元识别工整手写汉字的算法。详细介绍了手写汉字图形元抽取规则、图形元隶属度计算、层次结构描述方法及模糊匹配算法。经对有代表性的800个高频汉字非限人识别实验,平均识别率达85%。","图形元,隶属度函数,层次结构"
1999,字符点相关技术与神经网络识别,"樊丽萍,陈健美,邹荣金","本文探讨一种新的在工程图扫描图象中自动识别字符、符号的方法,该研究是基于点相关的神经网络识别技术,这种方法考虑了各种退化的字符样本,训练一个字符时使用一种新的学习规则来自动完成训练过程,并从一组字符图象样本集中产生每个字符的理想的特征描述。这种方法使学习的复杂度呈常量,并在工程图字符识别中得到实际的应用。","图纸扫描,图象分割,神经网络,字符识别"
1999,中文Web文档库全文检索技术研究与实现,"杨文清,黄宜华,张福炎","全文检索是一种非常有效的信息检索技术,本文结合国家863项目《WWW文档协同写作系统》的设计与开发,研究对中文Web文档库实现全文检索的主要技术,着重讨论了字表法全文检索技术细节,最后介绍了一个实用的全文检索系统的实现。","全文检索,Web文档库,索引库"
1999,全国第五届计算语言学联合学术会议(JSCL-99),,"为促进国内计算语言学的研究和应用,加强同行间的学术交流与合作,中国中文信息学会、中国计算机学会、中国人工智能学会和北京市语言学会共同发起于1999年11月1-3日在北京龙泉宾馆联合举办“全国第五届计算语言学联合学术会议(JSCL - 99) ”,会议的正式语言为中文与英文。",
1999,蒙古文自动处理系统研究,"嘎日迪,赵小兵,马红旭,赛音,白小玲","本文主要介绍了蒙古文自动处理系统的构成;蒙古文语料库,蒙古文知识库,蒙古文数据库以及蒙古文自动处理系统的软件等问题的初步探索研究的过程。","蒙古文自动处理,蒙古文语料库,蒙古文知识库,蒙古文数据库"
1999,全国第五届计算语言学联合学术会议(JSCL-99),,"为促进国内计算语言学的研究和应用,加强同行间的学术交流与合作,中国中文信息学会、中国计算机学会、中国人工智能学会和北京市语言学会共同发起于1999年11月1-3日在北京龙泉宾馆联合举办“全国第五届计算语言学联合学术会议(JSCL - 99) ”,会议的正式语言为中文与英文。",
1999,“自然语言多语种文本生成系统”在上海交通大学研制成功,"盛焕烨,姚天昉",由上海交通大学计算机科学与工程系盛焕烨教授、姚天昉副教授领衔的课题组所承担的“自然语言多语种文本生成理论、技术与应用研究”项目(得到上海市科学技术发展基金和德国大众基金的资助)于1999年5月29日通过了上海市科学技术委员会组织的成果鉴定。鉴定委员会由复旦大学吴立德教授担任主任、东北大学姚天顺教授担任副主任。,
1999,“机器翻译与计算机语言信息处理国际会议”圆满成功,,"中国中文信息学会自然语言处理专业委员会和国家自然科学基金委员会联合主办的《机器翻译与计算机语言信息处理国际会议》(International Conference on Machine Translation & Language Information Processing) ,于1999年6月26至28日在北京国谊宾馆举行。",
1999,写在前面,董振东,"本刊编委会决定出一期机器翻译专集。其文章多选自今年六月在北京召开的“国际机器翻译与计算机语言信息处理会议”的论文集。这次会议可以看作是我国机器翻译研究与开发的总结和检阅。今年是我国第一次机器翻译试验成功展示的40周年。虽然那时的试验规模很小,没法跟今天的相比,但是在当时已经是世界水平了。当时世界上能进行这样试验的国家实在是屈指可数。就是那次成功,带动了更多的机构和人员的热情参与。后来我们的机译研究先驱者还写了关于机器翻译的小册子,题为《机器翻译浅说》。它对于后来人是很宝贵的。我记得在70年代国内很多初次步入这一领域的人们的文章都会引用这本小册子,我自己也是由它带进门的。在我们颇有点自豪地检阅我们今天的成绩的时候,我们不会忘记我们的先驱者当年的努力和成就。因此我想六月的会议和这一期专刊也应是对这个40周年纪念的庆贺。",
1999,汉英机器翻译源语分析中词的识别,傅爱平,"汉英MT源语分析首先遇到的问题是词的识别。汉语中的“词”没有明确的定义,语素和词、词和词组、词组和句子,相互之间也没有清楚的界限。按照先分词、再句法分析的办法,会在分词时遇到构词问题和句法问题相互交错的困难。作者认为,可以把字作为源语句法分析的起始点,使词和词组的识别与句法分析同时进行。本文叙述了这种观点及其实现过程,并且以处理离合词为例,说明了识别的基本方法。","机器翻译,汉语自动分析,汉语词的自动识别"
1999,英日机器翻译系统E-to-J原语分析中的兼类词消歧策略,冯志伟,"本文介绍了商品化英日机器翻译系统E-to-J中兼类词的消歧策略。作者根据机器翻译的实践,把英语中同形兼类词归纳为29种类型,提出了基于上下文环境的处理这些兼类词类型的消歧方法。","机器翻译,原语分析,兼类词,消歧"
1999,汉语的一种知识一体化表示方法,"孟洁,陈群秀","随着计算机自然语言处理研究的不断深入,人们越来越认识到知识在计算机语言处理系统中的重要性。本文结合语言心理学的一些相关结论,对照人在真实领域中的语言处理过程,提出了一种适合计算机存储和检索的知识一体化表示方法,并分析了它的实际应用前景。","自然语言处理,知识,知识表示,知识的一体化表示"
1999,一种人机互动的多策略机器翻译系统IHSMTS的设计与实现原理,"黄河燕,陈肇雄,宋继平","现有单一策略的机器翻译系统很难有效地解决机器翻译所面临的所有问题。本文,提出一种基于人机交互互动的多策略机器翻译系统设计方法,该方法把基于多知识一体化描述的规则推理、基于经验记忆的类比启发式搜索推理和基于统计知识的概率方法及适当程度的人机交互有机地结合起来,利用现有基于规则的智能机器翻译系统自动产生具有各种特征知识的特征事例模式库,从而既可以通过与以往翻译实例的类比启发式搜索有效地利用以往系统成功的句子分析经验解决相似句子的分析,同时对特征事例模式库中没有相似实例的句子,又可以利用原有基于规则的方法和统计概率方法进行翻译转换处理,并在系统本身的知识不足以解决所遇到的多义区分问题时适时由人介入,从而可以大提高系统的翻译速度和翻译准确率,增强系统的实用性。","智能型机器翻译,人机互动,基于事例的机器翻译,类比推理,事例特征"
1999,英汉机器翻译系统的建造—用于英语词典翻译出版的专用系统,"郑保山,刘群,张祥","本文从人工翻译和机器翻译的经验出发,引入数据仓库和数据挖掘技术建造语料库,提出一个采用模板技术的译语精确生成和机助人译结合的动态机器翻译系统,专门用于英语词典的翻译出版,促使机器翻译走向实用化,初步研究取得了较好效果。","机器翻译,数据仓库,数据挖掘,译文模板,机助人译,英语词典"
1999,人机接口的智能化设计,"尹宝生,张桂平","本文通过对计算机大众用户的水平和需求的分析,针对现有操作系统的人机交互方式的优缺点指出了未来操作系统的三个主要发展方向:可视化、自然语言化和主动服务。","人机交互,自然语言,主动服务"
1999,机器翻译系统及翻译工具的开发和使用,JohnHutchins,"本文综述了当前计算机翻译软件的需求和应用情况,着重讨论了如何设计翻译系统使其质量能达到可出版水平, 其中包括开发受限语言翻译系统, 翻译工作站, 及软件本地化。同时本文还涉及到如何开发非传统概念的翻译软件,特别是针对Web页面和其它应用在Internet上各类信息的翻译软件。本文还讨论了机器翻译未来的需要以及正在开发的一些系统。文中最后一部分比较了一下通过人译、机译,机助人译的方式达到的几种最合适的翻译方法。","机器翻译,机助翻译,翻译工作站,多语系统"
1999,机器翻译系统文法中语法和语义研究——《天语》英汉翻译系统的文法概要,王广义,"机器翻译是双语言背景下的言语行为,是双语言知识的计算机处理。一个机器翻译系统的性能优劣,取决于它所使用的文法(当然,算法对于系统的影响也不可忽视)对相关语言知识,特别是对语法、语义的描写的深度和广度。本文简要介绍了《天语》英汉机器翻译系统(TYECT)的文法的基本内容。","机器翻译,天语,文法,TYECT"
1999,中文文献的层次分类方法,"战学刚,林鸿飞,姚天顺","现有的分类系统通常忽略类别体系的层次结构,在对文献进行分类时,往往很难区分类别相近的文献属于哪一类。本文基于向量空间模型,提出根据类别体系的层次结构,自顶向下,逐层分类的方法。其目的是提高分类精度;并根据概念词典,将同义词或下位概念映射到单一的概念词上,由这些概念词构成一个规模很小的特征集,以缩小特征向量空间的维数,从而减少分类系统的计算量。此外,通过对类别层次体系的分析,压缩特征向量,从另一方面减少分类系统的计算量。","文献分类,向量空间模型,类别层次结构"
1999,数据库汉语查询接口WTCDIS系统的设计与实现,"李保利,周锡令,胡景凡","数据库自然语言查询接口可以使用户直接以日常生活中使用的自然语言提出查询请求,获取数据库中的信息。这是一个具有重要理论价值和巨大实用价值的研究领域。本文在总结数据库汉语查询的语言规律基础上,重点介绍了我们设计实现的一个数据库汉语查询接口系统WTCDIS,最后给出了对该系统进行初步测试的结果。","自然语言处理,数据库自然语言接口,中文信息处理"
1999,小类别数手写汉字建模,"薛炳如,杨静宇,娄震,胡钟山","在手写汉字识别的研究中,鲜有研究者提出建立手写汉字的数学模型,本文在这方面作了一些探讨。建模的目的通常有两个:一是手写汉字的表示或描述,二是手写汉字的识别。本文针对小类别数手写汉字,在骨架图形的基础上,把手写汉字看作孤枝、孤环和部件的集合,并定义三者之间的方位关系,从而建立手写汉字的数学模型。实验表明,该模型用于识别,效果良好。","OCR,字符识别,手写汉字识别,建模"
1999,基于结构助词驱动的韵律短语界定的研究,"应宏,蔡莲红","提高合成语音的自然度是汉语文语转换系统(CTTS)的核心任务,而韵律短语的界定扮演着重要的角色。本文通过分析虚词的特征,研究了结构助词在连续语流中的特点、地位,以及在韵律短语界定中的作用,得到了一组相应的规则和结论。","虚词,结构助词,韵律短语,韵律短语界定,汉语文语转换系统(CTTS)"
1999,面向中文移动通讯产品的点阵汉字压缩新技术及其压缩字形数据结构设计,"王智慧,王瑜,藤志猛,张福炎","本文介绍了一种新型点阵汉字压缩技术──“构件嵌套组合”技术的研究,该技术建立在统计学和汉字框架结构的基础上,可实现对汉字高效压缩,能够很好地满足中文移动通讯产品的需要。文中讨论了这种新型汉字压缩技术的主要设计思想及其压缩字形数据结构设计等。","点阵汉字,构件,压缩,矢量"
1999,字形技术及OpenType字体文件格式研究,"肖明,胡金柱,赵慧","随着Windows操作系统及TrueType字体(TTF)的流行,字形技术的重要性已越来越受到人们的关注。本文介绍了从TrueType、OpenType到Clear Type的最新发展,比较详细地分析了OpenType字体(OTF)文件的结构,解释了其中的重要描述表。用户若能正确掌握OTF字体文件格式,就可以建立自己的特殊字体,并可以在字体应用领域取得良好的应用效果。","TrueType字体,OpenType字体,Clear Type字体,曲线轮廓"
1999,关于汉字字符串排序算法,钟诚,"分析汉字字符串分组排序算法,在讨论基选择的基础上,给出将字符串映射成整数和处理映射冲突数据的改进的有效方法。","汉字字符串,排序算法,映射"
2000,汉语自动分词词典机制的实验研究,"孙茂松,左正平,黄昌宁","分词词典是汉语自动分词系统的一个基本组成部分。其查询速度直接影响到分词系统的处理速度。本文设计并通过实验考察了三种典型的分词词典机制:整词二分、TRIE索引树及逐字二分,着重比较了它们的时间、空间效率。实验显示:基于逐字二分的分词词典机制简洁、高效,较好地满足了实用型汉语自动分词系统的需要。","中文信息处理,汉语自动分词,汉语自动分词词典机制"
2000,基于语料库的中文姓名识别方法研究,"郑家恒,李鑫,谭红叶","本文在大规模语料基础上提取和分析了中文姓氏和名字用字的使用频率,研究了中文姓名识别的评价函数,动态地建立了姓名识别统计数据表和姓名阈值。提出了在不作分词处理的原始文本中进行中文姓名识别的方法。经开放测试,召回率为95.23%;精确率为87.31% 。","中文姓名识别,姓氏使用频率,自动分词"
2000,基于DOP的汉语句法分析技术,"张跃杰,朱靖波,张跃,姚天顺","本文提出一种以DOP技术作为基本框架,同时利用基于相似的概率评估技术,实现汉语句法分析的方法。其中,对于输入语句,首先需要经过词汇层与词性层两层初选。然后,基于已构建知识源,获取输入语句的片段组合形式。最后,对输入语句与初选结果进行相似性评估,完成输入语句的组合分析过程。为论证方法有效性,基于包含1000个语句的真实汉语语料构建知识源,并采用包含100个语句的真实汉语语料作为测试集。实验表明,句法分析的各项指标都比较令人满意,可有效地实现汉语句法分析。","面向数据的分析,汉语句法分析,相似性评估,树库,片段库,片段组合库"
2000,中文手写文稿的二值化与行列切分,"蔡樱,盛立东","灰度图象的二值化与行列切分是预处理中的重要环节,对识别系统有很大的影响。针对带有框线的文稿图象,本文提出了双重阈值法的二值化方法,有效地去除了框线。在字符分割部分。本文提出了先三行后单行列切分的方法,准确地提取了字符。","手写文稿识别,二值化,行列切分"
2000,基于伪MMX技术的并行识别算法及其应用,"黎明刚,郭军","本文提出了一种通用的并行算法模型。这种模型可以适用于许多多数据块处理系统。该算法可以成倍提高系统的处理速度。算法的核心采用了伪MMX技术,对机器硬件没有特殊要求,保证了程序的可移植性。本文对此做了详细论述。同时本文还讨论了该算法模型在余弦整形变换系统上的实现,其处理速度较原算法有了成倍提高。","并行算法,手写汉字识别,伪MMX技术,整形变换"
2000,用统计方法实现汉字输入的智能联想,"刘长松,伍振军,乔春雷,李元祥","联想是汉字输入法的重要补充手段,能够大大加快输入的速度。本文首次提出了智能联想的概念,分析了智能联想的原理和遇到的困难,使用汉语语料库的方法构造并分析比较了3种智能联想方案,充分利用汉语字词间的相关性,使平均联想成功率超过40%。","联想,语料库,语言模型,中文输入"
2000,汉语文本动态字母表0阶模型算术编码,"王忠效,范植华","本文探讨汉语文本的0阶统计模型的构造方法,提出了一个卓有成效的汉语文本压缩算法。仅仅凭借这一最初级的模型,汉语文本的编码效率已经超过LZ与Huffman编码的混合算法。由于0阶统计模型是各种高阶统计模型的基础,所以,本文对汉语以及其他大字符集文种(如日文、朝鲜文) 的文本压缩研究具有重要的参考意义。","数据压缩,汉语文本压缩,算术编码,统计模型"
2000,基于国产开放系统平台Java虚拟机的中文化,"丁宇新,梅嘉,程虎","本文在深入分析Java内部编码机制的基础上,指出了现存Java开发工具中中文化存在的问题,并提出了解决方案。我们采用了编码识别技术,将两种不同形式的中文字符编码","Java,虚拟机,中文化"
2000,论藏文的序性及排序方法,"江荻,周季文","为解决藏文排序问题,本文提出藏文的构造序和字符序概念,并在此基础上提出解决藏文词典序的计算机方案。文章对各类藏文构造及字符进行了分析和赋值,给出了藏文计算机排序的技术流程图。","藏文,词典序,构造序,字符序"
2007-07-22,建设综合型语言知识库的理念与成果的价值,俞士汶,"积20余年之努力与锤炼,北京大学计算语言学研究所完成的一项科研成果“综合型语言知识库”于2007年2月通过了教育部组织的技术鉴定。鉴定结论认为“其规模、深度、质量和应用效果在我国语言工程实践中是前所未有的。该成果是以汉语为核心的多语言知识库建设中最全面、最重要的研究成果,总体上达到了国际领先水平”。本文在介绍以《现代汉语语法信息词典》为基础的综合型语言知识库的规模、构成、内容、品质和发展历程之后,陈述建设综合型语言知识库的理念,期望与读者分享在计算语言学和自然语言处理这一交叉学科领域内治学的心得与研发的经验。同时也对这项成果的应用实例进行分析,评估它的应用潜力,期望它在以汉语为核心的多语言信息处理事业的发展中起到铺路填坑或者投石问路的作用。","计算机应用,中文信息处理,综合型语言知识库,多语言信息处理,计算语言学,自然语言处理,现代汉语语法信息词典,治学心得"
2005-11-15,基于语境信息的汉语组合型歧义消歧方法,"冯素琴,陈惠明","组合型歧义切分字段一直是汉语自动分词的难点,难点在于消歧依赖其上下文语境信息。本文采集、统计了组合型歧义字段的前后语境信息,应用对数似然比建立了语境计算模型,并考虑了语境信息的窗口大小、位置和频次对消歧的影响而设计了权值计算公式。在此基础上,1.使用语境信息中对数似然比的最大值进行消歧;2.使用语境信息中合、分两种情况下各自的对数似然比之和,取值大者进行消歧。对高频出现的14个组合型分词歧义进行实验,前者的平均准确率为84.93%,后者的平均准确率为95.60%。实验证明使用语境信息之和对消解组合型分词歧义具有良好效果。","计算机应用,中文信息处理,自然语言处理,汉语自动分词,组合型切分歧义,对数似然比,语境信息"
2006-09-14,中文组织机构名称与简称的识别,"沈嘉懿,李芳,徐飞玉,HansUszkoreit","本文提出了一种基于规则识别中文组织机构全称和简称的方法。全称的识别首先借助机构后缀词库获得其右边界,然后通过规则匹配并借助贝叶斯概率模型加以决策获得其左边界。简称的识别是在全称的基础上应用其对应的简称规则实现的。在开放性测试中,该方法的总体查全率为85.19%,查准率为83.03%,F Measure为84.10%;简称的查全率为67.18%,查准率为74.14%。目前该方法已应用于中文关系的抽取系统。","计算机应用,中文信息处理,组织机构名称识别,组织机构简称识别,规则匹配,贝叶斯概率模型"
2007-04-16,基于分类信心重排序的中文共指消解研究,"冯元勇,孙乐,董静,李文波","共指消解是自然语言处理的核心问题之一。本文针对分步消解中分类器全局信息的不足,依据分类信心对全体提及配对进行排序,优先根据可靠的分类结果对提及进行聚集或分离。实验表明,该算法在多个学习框架下显著地改善了系统的整体性能。","计算机应用,中文信息处理,中文共指消解,提及配对共指分类信心,信息抽取,自然语言处理,机器学习,聚类算法"
2007-03-12,中文阅读理解语料库构建技术研究,"郝晓燕,李济洪,由丽萍,刘开瑛","阅读理解问答系统指的是能够自动分析一个自然语言文章,并且根据文中的信息为每个问题生成一个答案的系统,具有很高的研究价值。然而,缺乏中文阅读理解语料库已经成为制约汉语阅读理解问答系统发展的主要障碍。本文对于中文阅读理解语料库的构建过程进行了详细的介绍,包括语料选材、编写问句,标注答案句、语料加工和评测机制,尤其是基于汉语框架语义知识库对语料进行了框架元素、短语类型和句法功能三个层面标注的深加工技术。","计算机应用,中文信息处理,阅读理解问答系统,中文阅读理解语料库,汉语框架语义知识库"
2007-03-19,基于配价的维吾尔语框架语义知识库的构建,"吾买尔江·库尔班,阿里甫·库尔班","本文阐述了以配价作为基本描写法、真实语料为事实依据的维吾尔语框架语义知识库(简称框架网FrameNet)的构建,该知识库在构建维吾尔语词汇及其所属框架的语义词典等诸多领域有着广阔的应用空间和发展前景。提出了研究维吾尔语中句法功能和概念结构(也就是语义结构) 之间的关系, 以及建立用于自然语言处理的维吾尔语网上词汇知识库的意义。在维吾尔语的研究中引入了框架语义知识库(框架网)。框架语义知识库作为一种网上词汇语料库, 包括对每个词位( lexeme)的各个涵义的句法、语义信息的详尽描述。本文为维吾尔语框架语义知识库中各个框架元素的句法、语义特征的说明等自然语言信息处理研究提出新的研究思路,对基于配价的维吾尔语框架语义知识库构建的方法进行了探讨。","计算机应用,中文信息处理,框架网, 维吾尔语,配价"
2007-03-20,信息检索中一种基于词语—主题词相关度的语言模型,"田萱,杜小勇,李海华","本文提出一种基于词语—主题词相关关系的语言模型TSA-LM(Term-Subject Association Based Language Model ),它的基本思想是把一篇文档分成两个文档块,一部分是由领域主题词表中的主题词构成的主题词文档块,另一部分是由非主题词构成的非主题词文档块,分别计算两个文档块和查询的似然程度。对非主题词文档块,假设词语间独立无关,沿用经典的语言模型计算;对主题词文档块,把查询词语和主题词相关关系引入语言模型中来估计该文档块和查询的似然程度。词语—主题词相关关系采用词语—主题词相关度来衡量。词语—主题词相关度的计算除了来源于对文档中词语—主题词共现性的观察外,还来源于宏观上对词语—文档—主题词归属关系的观察。公开数据集上的检索实验结果表明,基于词语—主题词相关关系的语言模型可以有效提高检索效果。","计算机应用,中文信息处理,语言模型, 主题词, 词语—主题词相关关系,词语—文档—主题词归属关系, 词语—主题词共现关系"
2007-03-15,汉语词同现网络的小世界效应和无标度特性,"刘知远,孙茂松","人类语言的某些重要方面可以通过复杂网络来刻画。本文基于不同规模和类型的语料库,建立了汉语词同现网络,并从复杂网络的角度对这些网络进行了系统的实验考察。实验结果表明汉语词同现网络具有复杂网络的两个基本性质: (1)网络的平均最短路径为2.63-2.75,聚合系数远大于相同参数下的随机网络,这揭示了汉语同现网络的小世界效应;(2)网络中词的度大体上呈幂律分布,表明汉语同现网络具有无标度特性。本文还对实验中所得到的汉语核心词典进行了定量分析。","计算机应用,中文信息处理,词的同现,复杂网络,小世界,无标度,核心词典"
2007-01-09,中文信息检索系统的模糊匹配算法研究和实现,"王静帆,邬晓钧,夏云庆,郑方","在现代中文信息检索系统中,用户输入的字符串和实际数据库中的条目往往存在局部偏差,而基于关键词匹配的检索技术不能很好地解决这一问题。本文参考并改进了Tarhio和Ukkonen提出的过滤算法[1],针对汉字拼音输入法中常出现的同音字/近音字混用现象,将算法进一步扩展到广义的Edit Distance上。实验表明,本文提出的算法能有效提高中文信息检索系统的召回率,在实际应用中可达到“子线性”的效率。","计算机应用,中文信息处理,模糊匹配,过滤算法,动态规划"
2007-03-12,基于COSA算法的中文文本聚类,"谷波,李济洪,刘开瑛","传统聚类算法在计算两个对象间的距离时,每个属性对距离的贡献相同。COSA(Clustering On Subsets of Attributes)算法[1]认为在不同的分组中,每个属性对计算距离所起的作用可能并不相等,因为不同分组中的对象可能在不同的属性子集上聚集。文献[1]在此基础上定义了新的距离,并提出了两种COSA算法: COSA1算法是一种分割的聚类算法;COSA2算法是一种层次聚类算法。为了对比COSA距离和传统的欧氏距离在文本聚类中的表现,本文对中文文本进行了分割聚类和层次聚类的实验。实验结果显示出COSA算法较基于欧氏距离的聚类算法有更好的性能,而且对于属性数的变化,COSA算法更加稳定。","计算机应用,中文信息处理,文本聚类,COSA算法,K-means算法"
2007,话题检测与跟踪的评测及研究综述,"洪宇,张宇,刘挺,李生","话题检测与跟踪是一项面向新闻媒体信息流进行未知话题识别和已知话题跟踪的信息处理技术。自从1996年前瞻性的探索以来,该领域进行的多次大规模评测为信息识别、采集和组织等相关技术提供了新的测试平台。由于话题检测与跟踪相对于信息检索、信息挖掘和信息抽取等自然语言处理技术具备很多共性,并面向具备突发性和延续性规律的新闻语料,因此逐渐成为当前信息处理领域的研究热点。本文简要介绍了话题检测与跟踪的研究背景、任务定义、评测方法以及相关技术,并通过分析目前TDT领域的研究现状展望未来的发展趋势。","计算机应用,中文信息处理,综述, 话题检测与跟踪,自然语言处理,事件,新闻报道"
2007,基于监督学习的中文情感分类技术比较研究,"唐慧丰,谭松波,程学旗","情感分类是一项具有较大实用价值的分类技术,它可以在一定程度上解决网络评论信息杂乱的现象,方便用户准确定位所需信息。目前针对中文情感分类的研究相对较少,其中各种有监督学习方法的分类效果以及文本特征表示方法和特征选择机制等因素对分类性能的影响更是亟待研究的问题。本文以n-gram以及名词、动词、形容词、副词作为不同的文本表示特征,以互信息、信息增益、CHI统计量和文档频率作为不同的特征选择方法,以中心向量法、KNN、Winnow、Nave Bayes和SVM作为不同的文本分类方法,在不同的特征数量和不同规模的训练集情况下,分别进行了中文情感分类实验,并对实验结果进行了比较,对比结果表明:  采用BiGrams特征表示方法、信息增益特征选择方法和SVM分类方法,在足够大训练集和选择适当数量特征的情况下,情感分类能取得较好的效果。","计算机应用,中文信息处理,情感分类,文本分类,语言模型,中文信息处理"
2007,使用机器学习方法进行新闻的情感自动分类,"徐军,丁宇新,王晓龙","本文主要研究机器学习方法在新闻文本的情感分类中的应用,判断其是正面还是负面。我们利用朴素贝叶斯和最大熵方法进行新闻及评论语料的情感分类研究。实验表明,机器学习方法在基于情感的文本分类中也能取得不错的分类性能,最高准确率能达到90%。同时我们也发现,对于基于情感的文本分类,选择具有语义倾向的词汇作为特征项、对否定词正确处理和采用二值作为特征项权重能提高分类的准确率。总之,基于情感的文本分类是一个更具挑战性的工作。","计算机应用,中文信息处理,文本分类,情感分析,贝叶斯,最大熵"
2007,基于HowNet的VSM模型扩展在文本分类中的应用研究,"孙宏纲,陆余良,刘金红,龚笔宏","在采用VSM模型进行文本分类时,如果特征向量维数相差悬殊,会给分类结果产生很大负面影响。为了解决这一问题,本文引入了特征向量扩展的思想,同时定义了有效原始信息浓度的概念。特征向量扩展以HowNet语义词典为依据,对高维和低维特征向量采用不同的扩展策略,从而减小了不同类别语料间有效原始信息浓度的差值,进而改善复杂语料的分类结果。实验表明该方法在复杂语料情况下,通过对特征向量进行HowNet语义扩展,可以较好的改善分类结果。","计算机应用,中文信息处理,HowNet, VSM模型, 文本分类"
2007,利用上下文提高文本聚类的效果,"丘志宏,宫雷光","传统文本聚类的向量空间模型中,认为词的权重只和词频有关,而与词语出现的上下文无关。本文介绍了如何借助按词语之间关系组织的本体论词典对文章进行上下文分析,得到文章中词语之间意义上的相互关系,进而用相关词语的词频以及关系的权重量化地给出一个词语受到上下文的支持程度,所以在衡量词语权重时不仅考虑其词频,而且考虑上下文的支持情况。文章还介绍了如何用自动构建的方法得到本文所需的词典,使得在本体论词典资源还不太丰富的汉语中也能应用上面的方法。实验数据表明,本文的方法能有效的消除噪音,提高文本聚类的效果。","计算机应用,中文信息处理,文本聚类,上下文,词语权重,本体论词典"
2007,普通话声调的客观评测,"汤霖,尹俊勋","声调在普通话中起着构词辨义的作用,声调的准确程度是决定普通话水平的重要因素。声调的客观评测是普通话水平客观评测系统的重要子系统之一。在分析普通话声调特点的基础上,提出了能消除语速影响和音节间相互影响的建模方法。选择了能反映声调特点的5个基频比值与归一化的基频共同作为声调评测参数,利用高斯混合模型对60人的实测语音数据进行了测试,结果表明:客观测试同主观测试的符合率达到88.24%。","计算机应用,中文信息处理,声调,客观评测, 语音处理,普通话水平测试"
2007,古维吾尔文(察合台文) 及转写符号的智能输入法研究,"地里木拉提·吐尔逊,瓦依提·阿不力孜,吐尔根·伊布拉音","作为古维吾尔文(察合台文)文献数字化整理系统的关键技术,本文研究了察合台文和转写符号的智能输入方法,此模块的功能最终直接影响到察合台文与现代维吾尔文之间的转写效果。通过测试,本文提出的察合台文智能输入法和察合台文转写符号的智能输入法,具有较高的准确性、稳定性、易用性等优点。","计算机应用,中文信息处理,古维吾尔文(察合台文),数字化整理,智能输入法,撰写符号,UNICODE"
2000,前言,张炘中,"中国中文信息学会基础理论专业委员会于1999年10月在昆明召开了“第七届全国汉字识别学术会议”。本刊编委会决定从会议论文集中优选代表性的文章,出一期“汉字识别”专集。",
2000,863手写汉字识别测试平台,"刘昌平,钱跃良,张永慧,宋东,李丰林","本文详细介绍了用于1998年4月在北京举行的全国863评测手写汉字识别测试平台的情况,如测试大纲、测试样本的选择与分类、测试结果等,并提出了作者的一些看法和建议。","汉字识别,识别率"
2000,基于组件的中文版面分析,"刘定强,张炘中","本文提出基于组件的中文版面分析方法。本方法是以自底向上为主,同时结合了自顶向下和基于组件的思想。基于组件的思想使得算法结构清晰、图象扫描次数被尽可能地减少;以自底向上为主、同时结合自顶向下的方法具有效率高、准确性高、文档适应性广的特点。二维有序树型结构的文档及其组件的组织形式,提高了频繁的查找操作速度,同时为版面描述和分析结果的应用提供了方便。","版面分析,自底向上,组件"
2000,大型中文古籍《四库全书》自动版面分析系统,"姜哲,马少平,夏莹","《四库全书》是中文古籍的经典和代表。对《四库全书》的整理,可以为其它古籍的整理积累和提供经验。本系统属于《四库全书》电子版专用OCR系统的预处理配套系统,主要功能是对《四库全书》的页面图象进行分析和理解,分离图象中的汉字用于识别和统计,获取版面结构以便于重编和出版。《四库全书》属于手写木版印刷,版面有一定规范,但形式多样、结构复杂、图象质量和字体大小有差异,版面分析的难度很大。本系统采用了自顶向下方法与自底向上方法相结合、自动处理与人工修正相结合的设计思想。从实用情况看,本系统已经能够自动采用相应算法,处理多种规范和准规范的版面,并提供方便的人工辅助纠错功能,保障了预处理工作的顺利进行,也为识别系统的学习建库和识别创造了良好的条件。","四库全书,中文古籍,版面分析,汉字识别"
2000,中文商务名片识别系统的实现,"张纯,张涛,黄笑","本文介绍了一个实际应用中的中文商务名片识别系统,分析了系统的结构。该系统首先结合实际中的一些具体问题对名片图象进行预处理,然后在传统版面分析技术的基础上针对名片版面的特点进行版面分析。在对分割区域的字符进行识别之后,根据识别结果中的语义知识和版面分析得到的位置信息对识别结果进行基于知识规则的理解,从而实现了名片信息的自动录入,整个系统在实际中表现出了良好的性能。","名片识别系统,图象处理,版面分析,字符识别,基于知识规则的理解"
2000,汉字识别中以词为分类单位的分类器研究,"方应谦,王鲁","汉字识别中,以往的分类器设计都是以字为单位的“字分类器”。字分类器的输出总是与待识字结构相似的一个侯选字集合。这是使后级识别容易产生误识的主要原因。为克服字分类器的缺点,本文给出了以词为单位的词分类器设计的策略与方法,并实验验证了词分类器在分类率及分类速度方面均优于字分类器。","汉字识别,分类,词分类器"
2000,一种多字体特大字符集字符识别系统,"高涛,李明敬,李志峰","多字体特大字符集字符识别是当前OCR技术研究的热点之一。本文利用一组在抗干扰和描述字符拓扑结构方面具有互补性的特征,其于Support Vector技术和可增长自组织神经网络模型,建立一种识别系统来处理该问题。其中包括一个利用Supprt Vector技术建立的Optimal Margin语言分类器,一个以可增长自组织神经网的粗分类器,结合统计和结构两种识别方法的三级汉字分类器,最后给出良好的实验结果,从而得到该识别系统为解决上述问题的有效方法之一的结论。","汉字识别,OCR技术,自组织神经网络,Support Vector技术"
2000,基于小波网络和多模块网络的数字识别,"宋红萍,刘宏超,全安寿,崔晋川,汤映杰","本文研究一种新的数字识别方法,这种方法用小波神经网络抽取特征、用多模块结构神经网络作模式分类器。小波分解的函数近似能力和人工神经网络的学习能力结合起来形成的小波神经网络,有着良好的特征描述性能,可用作特征抽取工具。多模块结构的神经网络将一个k类的模式分类问题转换为k个互相独立的2类分类问题。这种结构将一个复杂的分类问题化解为多个简单的分类问题,各个模块互相并联,各自负责一种模式的识别。用这种修改过的多模块结构网络的BP训练方法,可加速训练和提高训练精度,并且各模块可互相独立地进行训练。用美国NIST数字样本进行训练及测试,结果良好。这种方法可用于更广泛的平面图形识别。","数字识别,小波神经网络,多模块神经网络"
2000,一种地形图粘连汉字提取算法,"徐战武,刘肖琳","地形图中包含了大量的字体丰富的汉字注记,其中有一部分由于与其它图符对象相互粘连而使得对象的尺寸变大超过了预定的阈值或失去了原有的结构特性,大大增加了提取难度。本文提出了一种寻求最佳分割点的算法来去除粘连,提取汉字的算法,取得了良好的效果。首先,在已提取出的汉字周围确定一个局部搜索范围,当局部范围内存在大尺寸的对象时表明可能有潜在的粘连汉字;其次,以图象中的分枝点和端点为顶点,以其中的图段为边建立对应的图;然后,在图中寻找最佳分割点,将图符分割成不同的互相分离的几个部分;最后,用连通成分结构分析的方法来提取汉字。","地形图,汉字注记,连通成分,图,分割点"
2000,一种基于直线提取和补全的通用表格分析方法,"章海涛,李志峰","表格分析是表格自动处理过程中的第一步。本文充分利用表格的特点,给出一个基于直线提取和补全的通用表格分析方法。先使用一种矢量化的直线提取算法在游程连通图的基础上得到表格线,同时对表格进行倾斜校正。然后根据表格特性调整表格线,再从表格线得到表格特征点,最后建立规则通过对表格线的补全来求得表格结构的行单元描述。使用该方法对表格图象进行分析,能处理表格线断裂、文字表格线粘连等常见问题,正确得到表格结构。","表格分析,表格自动处理,直线提取"
2000,汉字的线性分类实验,"金奕江,马少平","本文通过实验研究了在汉字识别中应用线性分类器的可能性,考察了汉字之间的线性可分性。实验使用了两种主要的线性分类器: Fisher线性判别和感知器。实验检验每一对汉字的线性可分性。实验结果表明,汉字之间的线性分类性是相当好的。尤其是Fisher线性判别,不能成功线性分类的汉字仅占百万分之4.25 。这显示了在汉字识别中应用线性分类器是有着巨大的潜力的。同时,线性分类实验结果还可用来检验所选取特征的好坏,有利于客观的评价特征。","汉字识别,线性分类器,Fisher分类器,感知器"
2007-05-28,基于多向量和实体模糊匹配的话题关联识别,"张晓艳,王挺,陈火旺","本文在对新闻报道理论分析及实验验证的基础上,提出一种多向量表示模型,使其在尽量不丢失信息的情况下,对特征集合尽可能细地划分。基于该模型,本文设计了一种模糊匹配的方法用于计算命名实体子向量之间的关联度,它们和多个向量相似度一起用支持向量机进行整合,形成报道模型间的相似度。本文选用TDT4中文语料作为测试语料,将上述模型及模糊匹配技术用于话题关联识别。实验表明,多向量模型能够改进话题关联识别的性能,模糊匹配技术也在一定程度上弥补了精确匹配带来的性能损失。","计算机应用,中文信息处理,话题关联识别, 多向量表示模型, 命名实体模糊匹配"
2007-05-31,基于布局特征与语言特征的网页主要内容块发现,"韩先培,刘康,赵军","本文综合分析了网页内容块各方面的特征,提出了一个联合使用布局特征和语言特征的网页主要内容块发现方法,有效地解决了以往模型中通用性与高准确率不能共存的缺点。该方法使用网页视觉块树表示网页,对网页内容块的布局特征和语言特征分别建立了独立的分类器,然后组合这两个分类器来进行网页内容块分类。实验结果表明,在保持非噪音块召回率在90%以上的同时,组合分类器的准确率达到85%,比只使用布局特征的分类器提高5个百分点,比只使用语言特征的分类器提高15个百分点;在5个站点上的分类结果表明组合分类器在不同站点上性能稳定,具有良好的通用性。","计算机应用,中文信息处理,网页清理,主要内容块发现,网页切分,布局特征,语言特征"
2007-05-21,一种全自动生成网页信息抽取Wrapper的方法,"梅雪,程学旗,郭岩,张刚,丁国栋","Web网页信息抽取是近年来广泛关注的话题。如何最快最准地从大量Web网页中获取主要数据成为该领域的一个研究重点。文章中提出了一种全自动化生成网页信息抽取Wrapper的方法。该方法充分利用网页设计模版的结构化、层次化特点,运用网页链接分类算法和网页结构分离算法,抽取出网页中各个信息单元,并输出相应Wrapper。利用Wrapper能够对同类网页自动地进行信息抽取。实验结果表明,该方法同时实现了对网页中严格的结构化信息和松散的结构化信息的自动化抽取,抽取结果达到非常高的准确率。","计算机应用,中文信息处理,网页信息抽取,网页结构分离,包装器"
2007-05-29,中文网页信息检索测试集的构建、分析及应用,"李静静,闫宏飞","随着WWW的迅速发展,Web信息检索技术成为研究者广泛关注的话题,但缺少合适的测试评测机制制约了中文网页信息检索技术的发展。参考国外测试集的构建经验,我们构建了大规模中文网页信息检索测试集CWT,并组织了SEWM中文网页检索评测,希望在国内外各个研究小组的共同参与下建立并完善CWT,一起推动中文网页信息检索技术的发展。本文在调研和分析国内外现有研究进展的基础上,详细介绍了CWT的构建原则和方法,并对CWT进行了有效的统计分析和实验研究。本文提出的构建测试集的方法为以后的研究提供了参考。","计算机应用,中文信息处理,CWT,信息检索,评测,测试集,文档集"
2007-05-30,基于推拉策略的文本分类增量学习研究,"罗长升,段建国,郭莉","学习算法是否具有增量学习能力是衡量其是否适合于解决现实问题的一个重要方面。增量学习使学习算法的时间和空间资源消耗保持在可以管理和控制的水平,已被广泛应用于解决大规模数据集问题。针对文本分类问题,本文提出了增量学习算法的一般性问题。基于推拉策略的基本思想,本文提出了文本分类的增量学习算法ICCDP,并使用该算法对提出的一般性问题进行了分析。实验表明,该算法训练速度快,分类精度高,具有较高的实用价值。","计算机应用,中文信息处理,增量学习,推拉策略,文本分类,中心法"
2007-05-29,知识增益: 文本分类中一种新的特征选择方法,"徐燕,王斌,李锦涛,孙春明","特征选择在文本分类中起重要的作用。文档频率(DF)、信息增益(IG)和互信息(MI)等特征选择方法在文本分类中广泛应用。已有的实验结果表明,IG是最有效的特征选择算法之一,该方法基于申农提出的信息论。本文基于粗糙集理论,提出了一种新的特征选择方法(KG算法),该方法依据粗糙集理论关于知识的观点,即知识是分类事物的能力,将知识进行量化,提出知识增益的概念,得到基于知识增益的特征选择方法。在两个通用的语料集OHSUMED和NewsGroup上进行分类实验发现KG算法均超过IG的性能,特别是在特征空间的维数降到低维时尤其明显,可见KG算法有较好的性能;","计算机应用,中文信息处理,文本分类,特征选择,粗糙集,信息检索"
2007-05-02,改进的OPTICS算法及其在文本聚类中的应用,"曾依灵,许洪波,白硕","基于密度的OPTICS聚类算法以可视化的结果输出方式直观呈现语料结构,但由于其结果组织策略在处理稀疏点时的局限性,算法实际性能未能得到充分发挥。本文针对此缺陷提出一种有效的结果重组织策略以辅助稀疏点的重新定位,并针对文本领域的特点改变距离度量方法,形成了OPTICS-Plus文本聚类算法。在真实文本分类语料上的实验表明,我们的结果重组织策略能够辅助算法产生更为清晰反映语料结构的可达图,与K-means算法的比较则证实了OPTICS-Plus具有较为良好的聚类性能。","计算机应用,中文信息处理,OPTICS算法,密度聚类,文本挖掘"
2007-05-03,基于查询空间的分布式文档集合划分算法,"张刚,刘悦,程学旗","合理的文档集合划分能够有效的提高分布式信息检索的效果,本文针对分布式信息检索中的集合划分问题,提出了一种基于查询空间的文档集合划分算法。与传统的基于文档空间的划分算法相比,该算法从一种全新的角度看待和理解文档集合划分问题,给出了一种针对大规模海量信息的文档集合划分解决方案。实验表明该算法在算法效果和算法效率方面都有很大的提高。","计算机应用,中文信息处理,分布式信息检索,文档集合划分,聚类"
2007-05-26,基于聚类语言模型的生物文献检索技术研究,"文健,李舟军","近年来研究表明使用主题语言模型增强了信息检索的性能,但是仍然不能解决信息检索存在的一些难点问题,如数据稀疏问题,同义词问题,多义词问题,对文档中不可见项和可见项的平滑问题。这些问题在一些领域相关文献检索中显得尤其重要,比如大规模的生物文献检索。本文提出了一种新的基于聚类的主题语言模型方法进行生物文献检索,这主要包括两个方面工作,一是采用本体库中的概念表示文档,并在此基础上进行模糊聚类,把聚类的结果作为数据集中的主题,文档属于某个主题的概率由文档与聚类的模糊相似度决定。二是采用EM算法来估计主题产生项的概率。把上述方法集成到语言模型中就得到本文的语言模型。本文的语言模型能够准确描述项在不同主题中的分布概率,以及文档属于某个主题的概率,并且利用本体中概念部分地解决了同义词问题,而且项可以由不同的主题产生,这也能够部分解决词的多义问题。本文的方法在TREC 2004/05 Genomics Track数据集上进行了测试,与简单语言模型以及现有主题语言模型相比,检索性能得到一定的提高。","计算机应用,中文信息处理,主题语言模型,信息检索,聚类"
2007-05-22,基于多过滤器集成学习的在线垃圾邮件过滤,"刘伍颖,王挺","垃圾邮件过滤就是在线对邮件做出Spam(垃圾)或Ham(非垃圾)的判断,这是一种根据客户反馈不断自学习的过程。本文抽取邮件的语言特征和行为特征构建多个简单过滤器,然后采用集成学习方法组合这些简单过滤器,获得了比简单过滤器更高的性能。实验表明单一特征学习的计算复杂性低、速度较快,而集成学习的效果更好。本文提出的将SVM集成学习用于邮件过滤的方法,在各种集成学习方法中效果最好。","计算机应用,中文信息处理,垃圾邮件过滤,机器学习,集成学习,支持向量机"
2007-05-27,基于偏最小二乘特征抽取的垃圾邮件过滤,"王鹏鸣,吴水秀,王明文,黄国斌","随着垃圾邮件逐渐成为网络用户的一大困扰,垃圾邮件过滤技术的研究显得越来越重要。针对电子邮件存在数据极度稀疏性、高特征维数和多重相关性等特点,本文提出了一种基于偏最小二乘原理的特征抽取方法,可以通过对原始特征进行线性组合抽取出既可反映邮件内容又可反映邮件类型的潜在语义特征,并可解决多重相关性问题。在Enron-Spam邮件数据集上的实验结果表明,同χ2特征选择方法相比,该方法在较低维数上可以获取良好的邮件过滤性能。","计算机应用,中文信息处理,垃圾邮件过滤,偏最小二乘,特征抽取"
2007-05-26,基于浅层语义树核的阅读理解答案句抽取,"张志昌,张宇,刘挺,李生","阅读理解系统是通过对一篇自然语言文本的分析理解,对用户根据该文本所提的问题,自动抽取或者生成答案。本文提出一种利用浅层语义信息的英文阅读理解抽取方法,首先将问题和所有候选句的语义角色标注结果表示成树状结构,用树核(tree kernel)的方法计算问题和每个候选句之间的语义结构相似度,将该相似度值和词袋方法获得的词匹配数融合在一起,选择具有最高分值的候选句作为最终的答案句。在Remedia测试语料上,本文方法取得43.3%的HumSent准确率。","计算机应用,中文信息处理,阅读理解,答案句抽取,浅层语义,树核"
2007-05-31,一种新的层次化结构问题分类器,"李方涛,张显,孙建树,朱小燕","问题分类是自动问答系统中关键技术之一,而问题中的关键词语是问题分类的重要依据。本文主要探讨问题词和中心词在问题分类中所起的作用,提出一种基于问题词和中心词的层次化结构问题分类器。分类器首先利用问题词将句子集分为三类,然后对于每个类别分别建立相应的分类器,对于what型问题,本文构造了基于关联规则的中心词分类器。本文实现的层次化结构分类器在TREC 2007 QA问题集和UIUC数据集上精度分别达到了90.6%和84.0%,充分显示了问题词和中心词在问题分类中至关重要的作用。","计算机应用,中文信息处理,问题分类,自动问答系统,问题词,中心词"
2007-05-31,基于本体的数字图书馆个性化用户模型表示,"宋丽哲,詹赤兵,王胜海","本文针对当前个性化服务中基于关键词的用户兴趣表示方法在语义上的不足,结合本体语义信息丰富的特点,提出了一种基于本体的用户模型表示方法。在数字图书馆领域内,介绍了本体形式化描述并构建了数字图书馆领域本体,给出了用户模型的表示方法。并以个性化信息检索为例,说明了利用用户兴趣本体表示中的同义,上下位等关系给用户提供服务的方法。实验表明基于本体的表示方法能够给用户提供更加个性化的信息。","计算机应用,中文信息处理,本体,用户模型,个性化服务,数字图书馆"
2007-05-29,基于单字提示特征的中文命名实体识别快速算法,"冯元勇,孙乐,李文波,张大鲲","近年来条件随机场(CRF)模型在自然语言处理中的应用越来越广泛。标准的线性链(Linear-chain)模型一般采用L-BFGS参数估计方法,收敛速度慢。本文在分析模型复杂度的基础上提出了一种改进的快速CRF算法。该算法通过引入小规模单字特征降低特征的规模,并通过在推理过程中引入任务相关的人工知识压缩Viterbi和Baum-Welch格搜索空间,提高了训练的速度。在中文863命名实体识别评测语料和SIGHAN06语料集上进行的实验表明,该算法在不影响中文命名实体识别精度的同时,有效地降低了模型的训练代价。","计算机应用,中文信息处理,中文命名实体识别, 条件随机场,自然语言处理,机器学习"
2007-05-25,基于伪相关反馈模型的领域词典生成算法,"黄玉兰,龚才春,许洪波,程学旗","本文提出了一种基于伪相关反馈模型的领域词典自动生成算法。将领域词典生成过程视为领域术语的检索过程假设初始检索出来的前若干个字符串与领域相关,将这些字符串加到领域词典中,重新检索,如此迭代,直到生成的领域词典达到预先设定的规模。实验表明,本算法经过若干次迭代后生成的领域词典准确率高于已有领域词典生成算法。","计算机应用,中文信息处理,有意串,领域词典,大规模语料,伪相关反馈"
2007-05-20,情感语料库的构建和分析,"徐琳宏,林鸿飞,赵晶","本文介绍了情感语料库构建方面的一些经验,讨论了在设计和建设情感语料库中的几个基本问题制定标注规范、选择标注集、设计标注工具以及标注过程中的质量监控。目前已经标注完成近4万句,100万字的语料。在完成这些已标注语料的基础上,进一步给出了语料库的情感分布,情感迁移规律等统计数据,分析了情感语料库的特点及应用。它的建成将为文本情感计算提供更加强大的资源支持。","计算机应用,中文信息处理,情感语料库,文本编码规范,一致性检查,情感迁移"
2007-05-27,基于层叠CRFs模型的句子褒贬度分析研究,"刘康,赵军","本文研究句子的褒贬度分析问题。针对传统的基于分类的句子褒贬度分析方法不能考虑上下文信息的问题,以及基于单层模型的句子褒贬度分类方法中的由于标记冗余引起的分类精度不高问题,本文提出了基于层叠式CRFs模型的句子褒贬度分析方法。该方法利用多个CRFs模型从粗到细分步地判断句子的褒贬类别及其褒贬强度,其中层叠式框架可以考虑句子褒贬类别与褒贬强度类别之间的层级冗余关系,而CRFs模型可以利用上下文信息对于句子褒贬类别和强度的影响。该方法在有效识别句子褒贬度的同时,提高了句子褒贬强度判别的准确度。实验证明相对于传统分类方法和单层CRFs模型,本文的方法取得了良好的效果。","计算机应用,中文信息处理,句子褒贬度分析,褒贬分类,褒贬强度分析,冗余标记,层叠式条件随机场"
1999-06-30,机器学习在汉语关联词语识别中的应用,"高维君,姚天顺,黎邦洋,陈伟光,邹嘉彦","关联词语在一些汉语议论文章中占很大的比重,因而,对于此类汉语文章的分析,关联词可以起到非常重要的作用。本文主要讨论如何将机器学习应用于汉语关联词的歧义辨别——原因,方法和效果。我们在已经加工完毕的80篇汉语语料的基础上,抽取了用于机器学习的训练集和测试集,并使用C4.5进行了测试,识别正确率在80%以上。在文章的后面,我们还从语言学的角度对机器学习的结果进行了解释和分析。","关联词语,机器学习,C4.5,话语分析,语料库"
1999-09-16,基于参照的对词结构操作语义的归纳学习,危辉,"心理语言学的研究和认知发展过程证明在语言获得的早期经历了一个自主的归纳学习过程,本文的出发点是语言发展的规律,并将词结构形式语义的获得过程和表示基础放在一个具有统一的语言理解和语言产生机制的语言信息加工模型中来考虑。本文讨论了一个基于实例的机器学习系统,为了获得词结构的形式语义,采用了操作语义的定义,并设计了一个基于参照的发现学习算法,其目的是使语义能伴随例句样本的丰富而精密化。","归纳学习,操作语义,计算语言学"
1999-07-16,基于统计方法的中文姓名识别,"刘秉伟,黄萱菁,郭以昆,吴立德","本文介绍一个中文姓名的自动识别系统,该系统使用从姓名样本库和真实文本语料库中得到的大量统计数据,以提高系统识别性能。我们从1994年人民日报中随机抽取100篇文章作为测试样本,实验结果表明,准确率和召回率可同时达到90%以上。","自动分词,未登录词,中文姓名识别"
1999-08-20,文本自动分类中的词权重与分类算法,"刁倩,王永成,张惠惠,何骥","本文详细阐述了自动分类中的词与文献的相关权重的经典计算方法IDF(Inverse Document Frequency) ,进一步总结了两种典型的分类算法——Bayes判别准则与向量空间模型(VSM) ,并提出结合词权重和分类算法进行分类的具体公式以及相关实验结果。","自动分类,IDF,Bayes判别准则,向量空间模型(VSM)"
1999-06-16,手写文稿识别的一种后处理方法和系统集成,"蔡樱,盛立东","本文提出了一种词间匹配的后处理方法,利用汉语上下文中词和词之间有一定的联系来对识别结果进行纠错,并综合词匹配、词间匹配和Markov语言模型使各环节之间形成反馈,相互补偿,形成一个较好的组合,以此来提高后处理部分的纠错能力以及稳定性。","汉字识别,后处理,词间匹配,词匹配,Markov语言模型"
1999-07-21,基于支持向量机的手写体相似字识别,"田盛丰,黄厚宽,李洪波","本文提出对手写相似汉字进行识别的支持向量机方法。该方法与人工神经网络一样适用于小规模分类,但由于支持向量机依据结构风险最小化原则,因此泛化能力更强。并且,由于支持向量机算法是一个凸二次优化问题,能够保证找到的极值解就是全局最优解。本文用支持向量机算法对三组手写相似汉字进行了识别,取得了较好的结果。","汉字识别,相似字识别,支持向量机"
1999-11-09,“调素”论与普通话合成自然度的提高,"郑新春,柴佩琪","文献[1]中的“边缘调素脱落论”提出了音步内各音节合成时边缘调素脱落的一般规律,其规律较好地揭示了两字词和三字词的轻重音分布原则和连续变调的现象;本文在此基础上,进一步提出了音步间各音步合成时的“边缘调素脱落论”,该规律揭示了四字以上词的“调素”脱落规则以及轻重音分布原则,通过该规律的应用,提高了句子合成的自然度。","调素,音步,轻重音原则,自然度"
1999-06-28,一种基于句法语义特征的汉语句法分析器,杨开城,"句法分析不是简单地符号推理,而应该是一种实体推理。增加语义信息是实现句法分析实体推理的有效手段。本文所介绍的句法分析器有两个特色:一是利用基于词的兼类处理规则大大提高了句法分析的效率;二是利用词静态和动态的句法语义特征来限制句法规则过强的生成能力,取得了较好的效果。","句法语义特征,句法分析,兼类消解"
1999-07-19,CJK汉字字形文件处理器的实现,"程波,孙玉芳","X Window在汉字字形方面一直只支持点阵字形,另外字符集也只是针对GB2312 - 80 ,无法满足对大字符集汉字应用的需要。本文侧重于在X Window下“透明地”提供对CJK统一汉字字符集TrueType字形的支持。文章介绍了X Server的结构,详细描述了汉字字形文件处理器实现算法。","汉字,字形文件处理器,CJK统一汉字字符集,TrueType,字形管理库,GBK"
1999-07-30,Zipf定律与汉字字频分布,游荣彦,"本文证明了在以Zipf定律描述整个汉字字频分布时,不管如何精心挑选参数a和c ,一些累计拟合频率都有明显的误差。针对这一现象,本文提出了一个解决办法,那就是以Zipf定律仅描述汉字字频分布的尾部的方法。","计量语言模型,汉字字频分布,Zipf定律,拟合频率"
1999-07-19,一种层次化的LSD规则体系及其分析算法,"李沐,姚天顺","本文提出了一种基于词汇属性结构描述和规则继承的层次化LSD规则体系,讨论了该规则体系下的规则搜索策略和词汇化规则索引的实现方法,并在此基础上首次给出了LSD文法的非确定性分析算法。该规则系统具有从传统属性文法到现代词汇文法的可伸缩性,同时较好地解决了线性规则库中复杂的规则交互问题。","LSD方法,词汇语义驱动,规则继承,规则搜索策略,层次化规则体系"
1999-11-29,基于ER模型的数据库受限汉语查询界面RChiQL的文法分析系统研究,"崔宗军,唐世渭,杨冬青","RChiQL是一个基于受限汉语的关系数据库查询语言界面的计算模型,其中文法分析占有重要地位。本文引入了一种新的文法GWERSC(Grammar with ER Semantic Characteristics , ER语义特征文法) ,设计了分析算法,其内嵌的ER语义特征有利于排除语法分析的歧义并可简化语义分析。","关系数据库,ER,模型,自然语言接口,文法分析"
1999-09-02,句法评分和语义评分,刘颖,"本文使用句法评分和语义评分对句法分析和语义分析阶段进行消歧。句法评分和语义评分可以和传统的句法语义分析阶段结合起来,更有效地对自然语言进行分析。这是规则方法和统计方法相结合的一种行之有效的方法。对于句法语义评分,使用最大可能原理和K-best方法进行实验,实验结果表明:对于训练集和测试集,两种方法在考虑一个左上文或一个左右上下文时都比不考虑上下文的正确率高。所有训练集的正确率比测试集的正确率高。对于训练集,当语料规模越来越大时,正确率也逐渐在增加。","句法评分,语义评分,消歧"
2000-01-03,车牌识别(LPR)中的图像提取及分割,"刘智勇,刘迎建","在车牌识别(LPR)系统的实现过程中,最关键的部分就是车牌图像的提取以及车牌字符图像的分割。本文详细介绍了一种实际应用的车牌识别系统中的图像提取及分割的过程。针对车牌的固有特点,设计了一个变换函数突出其特点从而进行车牌的提取;对车牌字的图像分割提出并解决了一些在实际中应该注意的地方。理论分析及实验结果表明文章中提出的方法是非常有效的。在我们的实验中,在Pentium Ⅱ300 ,内存64M的环境,从图像输入到识别结果输出的平均时间大概为0.6秒。","车牌识别(LPR),图像复原,图像提取,图像分割"
1999-12-13,基于p范式模型的检索,"迟呈英,战学刚,姚天顺","随着电子文本的大量涌现,人们对信息检索工具提出了更高的要求。本文介绍一种扩展的布尔检索模型及其在中文信息检索系统中的应用,并利用相关反馈技术改善检索系统性能。","信息检索,向量空间模型,布尔模型,p范式模型"
1999-09-27,中文全文检索系统中的压缩模型和模式匹配技术,"刘祖斌,王永成,刘椿年","本文给出了一种适用中文全文检索系统的压缩模型,使传统的LZW模型能适用于大字符集语言源文本。方法的关键是通过引入切割标记控制字典多叉树的节点的无限扩大。对文件的检索直接在压缩文件上进行,因而可较大地提高检索效率。","数据压缩,模式匹配,全文检索"
1999-11-24,语用的通信协调模型,程显毅,"通信不仅仅是为了交换信息,它更主要的是,当系统出现冲突时,起协调作用;当系统需要合作时,起协商作用。用传统的协议机制实现这个目标是有局限的,为此本文基于语用预设讨论了语用在解决协调时的作用,并给出了一个语用的通信协调模型。","协调,语用预设,语用模型,Agent"
2000-01-03,面向置标文档的文档转换技术研究,"李景春,武港山,王强,张福炎","文档系统间的转换是文档内容共享和协作的必然途径,转换根据不同应用目的包括失真,不失真和增值三种方式。置标文档是用标签(Tag)进行文档结构描述的文档。本文介绍了一种面向置标文档的文档转换增值技术,给出了一种文档转换描述语言,用户可以利用它来定义转换信息从而实现文档间复杂的转换。","文档转换,失真,增值,置标文档"
1999-11-04,蒙古文多变体附加成分智能化处理研究,S·苏雅拉图,"本文以蒙古文整词复杂特征合一计算和并行处理为科学目的,依据算法学可计算性理论,基于蒙古文整词输入法编码原理及方法,提出了一种多结点连续判断推理求解蒙古文多变体附加成分值的智能化模糊输入生成处理方法与技术。","蒙古文,多变体附加成分,知识表示方法,模糊输入生成"
2007-12-23,网络时代的人工智能,"李德毅,肖俐平","五十多年来,人工智能在模式识别、知识工程、机器人等领域已经取得重大成就,但是离真正的人类智能还相差甚远。本文强调 在当今的网络时代,作为信息技术的先导,人工智能科学有着非常值得关注的研究方向,要在学科交叉研究中实现人工智能的发展与创新。要关注认知科学、脑科学、生物智能、物理学、复杂网络、计算机科学与人工智能之间的交叉渗透,尤其是重视认知物理学的研究;自然语言是人类思维活动的载体,是人工智能研究知识表示无法回避的直接对象,要对语言中的概念建立起能够定量表示的不确定性转换模型,发展不确定性人工智能;要利用现实生活中复杂网络的小世界模型和无尺度特性,把网络拓扑作为知识表示的一种新方法,研究网络拓扑的演化与网络动力学行为,研究网络化了的智能,从而适应信息时代数据挖掘的普遍要求,迎接人工智能科学与应用新的辉煌。","人工智能,不确定性人工智能,认知物理学,数据场,云模型,网络化智能"
2007-05-15,中文词法分析与句法分析融合策略研究,"米海涛,熊德意,刘群","利用外部资源是提升句法分析性能的一种有效方法。本文利用中文词法分析器这一外部资源,提出了一种通用转换方法将中文词法分析器与句法分析器有机地融合在一起。通过基于转换的错误驱动学习和条件随机场解决不同切词、词性标注标准间的转换问题。在句法分析方面,本文提出了多子模型句法分析器,将中心词驱动模型和结构上下文模型有效结合在一起。融合后的中文句法分析性能在宾州中文树库1.0版①测试集上F1值达到了82.5％的最好水平。","计算机应用,中文信息处理,中文句法分析,中文词法分析,融合策略,基于转换的错误驱动学习,条件随机场"
2007-03-18,基于加权投票K—近邻法的生物医学缩略语消歧,"于中华,陈蓉,胡俊锋,陈源","生物医学文献信息抽取对充分挖掘利用生物医学领域取得的重要成果,促进生物医学的进一步发展具有重要意义。本文针对生物医学缩略语的分析理解问题,提出了基于加权投票K—近邻法的生物医学缩略语消歧算法。该算法基于“One Sense Per Discourse”假设自动生成带类标实例数据,消歧特征选用能表达文本主题的全局特征词,分类算法采用加权投票K—近邻法。在包含177 762篇Medline摘要的真实语料上进行的实验表明,本文所提出的算法明显优于相关工作中的算法。此外,实验还表明,对于缩略语消歧,加权投票K—近邻法与经典K—近邻法相比,不但具有高的预测准确率,而且性能更加稳定。","计算机应用,中文信息处理,生物医学信息抽取,缩略语消歧,加权投票K—近邻法"
2006-12-01,基于最大熵模型的共指消解研究,"庞宁,杨尔弘","共指是突发事件新闻报道中的常见现象。良好的处理共指现象,是进行信息提取的基本必要过程。本文采用最大熵模型对汉语突发事件新闻报道中的共指现象进行消解,目的是提取出突发事件新闻报道中指向同一实体的名词、代词和名词短语。根据问题特点,算法选择了8类特征作为模型的特征,该模型在20万字的新闻语料上进行训练,在10万字规模的语料上进行测试,最终的测试得到系统的F值为64.5%。","计算机应用,中文信息处理,最大熵模型,共指消解"
2007-03-12,基于句法的统计机器翻译综述,"熊德意,刘群,林守勋","本文对基于句法的统计机器翻译进行了综述。按照模型所基于的语法不同,将基于句法的统计机器翻译分为两大类 基于形式化语法和基于语言学语法。对这两个不同类别,我们分别介绍它们代表性的工作,包括模型的构建、训练和解码器的设计等,并对比了各个模型的优点和缺点。最后我们对基于句法的统计机器翻译进行了总结,指出设计句法模型时要注意的问题,并对未来的发展趋势进行了预测。","人工智能,机器翻译,统计机器翻译,基于句法的统计机器翻译,树到串,树到树,依存语法"
2007-04-18,基于派生文法的日—蒙动词短语机器翻译研究,百顺,"本文探索了源语为日语,目标语为蒙古语的动词短语机器翻译系统的实现方式。基于主张日语不活用的派生文法,重新分析日语附加成分。将日语的词干和附加成分转换到蒙古语的词干和附加成分之后,运用蒙古语的语音规则来处理并生成动词短语。在此基础上试做了日—蒙动词短语机器翻译系统。对30篇日文报道的403个动词短语进行测试,取得了95.78% 的正确率。","人工智能,机器翻译,派生文法,日语附加成分的分析,语音规则,短语生成"
2007-05-28,中文观点挖掘中的主观性关系抽取,"章剑锋,张奇,吴立德,黄萱菁","本文所针对的具体任务是抽取评价词和目标对象之间的关联关系。所采用的方法是将同一句子中共现的评价词与评价对象作为候选集合,应用最大熵模型并结合词、词性、语义和位置等特征进行关系抽取。我们将关系抽取引入观点挖掘,所提出的方法一定程度上解决了指代消解以及评价对象遗漏的问题。实验结果表明该方法的F值比取最近评价对象的Baseline方法有了15%的提高,并且发现程度副词能够帮助提高主观性关系抽取的性能。","计算机应用,中文信息处理,观点挖掘,关系抽取,最大熵"
2007-05-15,一种快速说话人搜索算法,"朱磊,江杰,郑榕,徐波","随着音频数据的不断增加,说话人识别已经变得越来越困难。本文提出了一种新颖的方法,在已有的说话人识别系统(GMM-UBM系统)的基础上,综合利用Index和Simulation,以很小的代价,极大地提高了说话人识别的速度,从而使说话人搜索成为可能。具体而言,就是采用两遍搜索策略,首先通过建立索引,在索引空间,比较索引间的欧氏距离,粗略地筛选出一定量的候选说话人目标;然后在此基础上,通过更精细的Simulation模型匹配,找出最佳的识别结果。实验结果显示我们的方法能以很小的代价,显著地提高说话人识别的速度。","计算机应用,中文信息处理,说话人识别,说话人搜索,两遍搜索"
2007-05-15,半监督学习和主动学习相结合的浅层语义分析,"陈耀东,王挺,陈火旺","语义分析是基于内容的文本挖掘领域的重要技术和研究难点。有监督机器学习方法受限于标注语料的规模,在小规模标注样本中难以获取较高性能。本文面向浅层语义分析任务,采用一种新颖的半监督学习方法——直推式支持向量机,并结合其训练特点提出了基于主动学习的样本优化策略。实验表明,本文提出的浅层语义分析方法通过整合主动学习与半监督学习,在小规模标注样本环境中取得了良好的学习效果。","计算机应用,中文信息处理,浅层语义分析,半监督学习,直推式支持向量机,主动学习"
2007-05-03,企业内部邮件中话题讨论检索研究,"富羽鹏,张敏,马少平","随着信息技术的发展,企业检索已成为人们越来越关注的一个新的应用领域。作为企业检索的一个典型任务,企业内部的邮件检索是在企业中常常遇到的一个问题。企业内部存在着大量的可以公开访问的电子邮件,这些是企业重要的信息资源,如何高速有效地从这些邮件中检索到需要的信息具有很大意义。本文根据电子邮件本身具有的格式化特征和语义拓扑结构提出了基于电子邮件特征的检索模型。实验表明,该模型对电子邮件可以进行有效的检索,并且使用该模型在TREC2006电子邮件话题检索评测中取得了优异的性能成绩。","计算机应用,中文信息处理,企业信息检索,邮件检索,话题讨论检索"
2007-04-10,基于自动句对齐的相似古文句子检索,"郭锐,宋继华,廖敏","随着语料库语言学的兴起,基于实例的机器翻译(EBMT)得到越来越多的研究。如何快速准确地构建大规模古今汉语平行语料库,以及从大量的对齐实例(句子级)中检索和输入句子最相似的源句子是基于实例的古今汉语机器翻译必须解决的问题。本文综合考虑句子长度、汉字字形、标点符号三个因素提出了古今汉语句子互译模型,基于遗传算法、动态规划算法实现了古今汉语的自动句对齐。接着为古文句子建立全文索引,基于汉字的信息熵,本文设计与实现一种高效的最相似古文句子检索算法。最后给出了自动句对齐和最相似古文句子检索的实验结果。","计算机应用,中文信息处理,古今汉语平行语料库,句子对齐,相似句子,基于实例的机器翻译"
2007-04-10,基于字单元分析的中文辅助阅读系统,"方高林,于浩,孟遥,邹纲","辅助汉语学习研究作为一个重要的研究领域,已经在自然语言处理领域激发起越来越多人的兴趣。文中提出一个基于字分析单元的辅助阅读系统,它可以为汉语学习者提供即时的辅助翻译和学习功能。系统首先提出基于字信息的汉语词法分析方法,对汉语网页中文本进行分词处理,然后利用基于组成字结构信息的方法发现新词。对于通用词典未收录的新词(例如: 专业术语、专有名词和固定短语),系统提出了基于语义预测和反馈学习的方法在Web上挖掘出地道的译文。对于常用词,系统通过汉英(或汉日)词典提供即时的译文显示,用户也可通过词用法检索模块在网络上检索到该词的具体用法实例。该系统关键技术包括: 基于字信息的汉语词法分析,基于组成字结构信息的新词发现,基于语义预测和反馈学习的新词译文获取,这些模块均以字分析单元的方法为主线,并始终贯穿着整个系统。实验表明该系统在各方面都具有良好的性能。","计算机应用,中文信息处理,词法分析,新词发现,术语翻译,Web挖掘,辅助汉语学习"
2007-01-16,一种基于区分性准则的模型结构优化方法,"鄢志杰,胡郁,王仁华","本文提出了一种基于区分性准则的模型结构优化方法,用以调整HMM自动语音识别系统中声学模型各状态混合高斯核成分数量的分配。通过优化选定的准则,声学模型可以在使用相同参数数量的情况下得到更好的识别性能,也可以在保持相当性能的前提下降低所需要的模型参数。相对于传统的基于似然度及复杂度惩罚的模型结构优化准则来讲,基于区分性准则的优化方法能够更直接地提高模型的区分度和鉴别力,从而得到更好的识别效果。在一个面向嵌入式系统的中文连续数字串识别任务上的实验结果证明,基于最大互信息量准则的模型结构优化能够得到比传统的、基于模型似然度及复杂度的方法更好的识别效果。","计算机应用,中文信息处理,自动语音识别,声学模型,模型结构优化"
2007-05-30,中文语音确认中子词置信度性能的研究,"孙成立,刘刚,郭军","本文提出了一种基于最小分类错误准则(MCE)的子词权重参数估计算法,通过MCE训练得到子词的权重系数。子词对词级置信度贡献量的研究表明: 韵母的确认能力显著好于声母,在置信性能方面比声母更加稳定和可靠,区分能力优于声母。在130个关键词的关键词检测系统实验表明,采用不同子词贡献权重比等贡献权重时等错误率下降3.05%。","计算机应用,中文信息处理,语音确认, 置信度, 似然比检验, 最小分类错误"
2007-04-29,语音合成系统中高质量的韵律生成,"郭庆,片江伸之,于浩,岩见田均","本文对富士通中文语音合成系统尤其是其中的韵律生成部分进行了描述。该系统是一个以音节为基本合成单元,在韵律参数生成结果即音长和基频预测结果的指导下,从音库中搜寻全局最优的合成单元,然后采用PSOLA算法进行波形调整的拼接合成系统。从提高合成语音韵律的角度出发,本文围绕音长预测和基频预测部分对该系统进行了详细的描述。最后,给出了韵律评测和系统评测的结果。","计算机应用,中文信息处理,韵律参数生成,音长预测,基频预测,决策树"
2007-04-06,基于词图的音素识别及在语种识别中的应用,"王士进,郑榕,徐波","本文介绍了一种基于词图的并行音素识别方法的自动语种识别系统,基于词图的并行音素识别方法是并行音素识别方法的一个扩展,它用识别产生的词图来描述声学候选结果空间,比并行音素识别方法中用最佳路径音子序列包含更丰富的信息。通过真实环境广播语音测试表明,该方法比并行音素识别方法识别性能提升了约6%,在每个语种约4小时的训练数据下,跟其他的几种语种识别方法也有可比的性能。","计算机应用,中文信息处理,语种识别,基于词图的并行音素识别方法"
2000-02-02,基于语境的语义排歧方法,"郑杰,茅于杭,董清富","本文针对英汉机器翻译系统(ECMT)中的语义排歧问题,提出了一种根据单词与语境之间的关系以消除单词语义歧义的模型。该模型利用反映单词之间语义共现关系的知识库词典,对有歧义的单词作出排歧。为提高知识库的覆盖率,本模型在对大量语料进行分析的基础上建立起单词语义分类之间的相关程度矩阵,同时采用动态链表来表示和维护语境,给出了寻找歧义单词的最可能的语义的排歧算法。开放测试的实验结果表明本方法使语义排歧的正确率提高约10%。","自然语言处理,语义排歧,机器翻译"
2000-02-28,用说明模板改进基于配价的德汉机器翻译,"凌小鹏,柴佩琪","配价描述了德语句子中的必须出现的部分,即补足语的构成情况,因此利用基于配价的方法能够较好地解决补足语的翻译问题。但说明语和补足语不同,它不是句子中必须出现的成分,有很大的任意性。因此,配价的方法并不能实现说明语的翻译。这也是基于配价的翻译系统存在的一个主要的不足。本文提出了一种在配价基础上,通过引用说明模板实现说明语翻译的方案。","说明模板,配价,机器翻译,自然语言处理"
2000-02-21,I-Tree和LFG,吴蔚天,"I-Tree是基于黎锦熙先生的三中心词学说建立的具有语法普遍性的语法理论。自1989年提出以后得到了很快的发展、充实与应用。LFG文法是于1982年在国外提出的最具挑战性的文法。其特点是用功能结构表示句中组分及其间的关系,极具I-Tree的性质,但不如I-Tree简单、直接、易读、易懂、易用。LFG规则要用数理逻辑表达式书写,限制了LFG的发展与应用。I-Tree的规则是基于传统的语法知识书写的。说母语、有良好语感和语法知识的人都能使用。将I-Tree与LFG进行比较可以更深刻地了解I-Tree 的特点。有利于发展基于推理的语法分析器。","汉语形式语法,词汇功能语法,汉语分析,自然语言处理"
2000-01-10,应用分段辨认序列频度信息的说话人确认方法,"翁武斌,方棣棠","本文提出了一种基于语音分段辨认序列信息的与文本无关的说话人确认方法,并且着重分析了其中关键因素的变化,包括聚类数、阈值以及判定准则的变化,对确认效果的影响。通过实验证明了分段辨认序列频度信息是一种非常有效的说话人确认信息,对于确认结果起到很好的辅助作用。同时也指出了新方法的不足和今后的改进方向。","说话人识别,说话人辨认,说话人确认,文本无关,错误接受率,错误拒绝率,分段辨认序列"
2000-01-18,一个联机识别自然手写汉字的多分类器集成系统,"黄襄念,程萍,彭健,杨波","本文提出一种联机识别自然手写体汉字的多分类器集成模型。该模型中,我们把依照01、WB和SO特征码设计的不同分类器进行集成,综合模式多种全局和局部特征,从汉字的多个结构层进行识别。初步实验结果为,识别率98.6%。","联机识别,汉字识别,综合集成,集成"
2000-02-17,手写体汉字在特征空间的可视化分析,"陈津颖,金奕江,马少平","手写体汉字特征一般在几百维以上,在这样的高维空间中,汉字样本是如何分布的?本文从可视化的角度对这一问题进行了探讨。论文首先给出了所选用的汉字特征的定义,然后对一些具有代表性的汉字实例,从K-L变换法、线性投影法和非线性投影法三个方面,对汉字在特征空间的分布问题进行了可视化分析,结果表明,可视化分析可以帮助人们了解汉字在特征空间的分布情况,对改进识别器的性能具有指导意义。","汉字识别,可视化分析,特征空间"
1999-10-22,基于潜在语义索引的文本浏览机制,"林鸿飞,姚天顺","文本浏览是伴随着因特网上日益增多的在线文本而出现的辅助阅读机制,本文给出了基于潜在语义索引的文本浏览机制。它吸取了潜在语义索引和概念标注的优点,利用潜在语义索引,减少词汇间的“斜交”现象,在语义空间上进行项与项、文本与文本、项与文本之间的相似度计算。利用概念词典将文本特征项按语义分类,给予层次分类以确定的含义。最后,实现以分层概念为基础的信息导航。","文本浏览,潜在语义索引,概念标注,特征抽取"
2000-02-17,Java语言的中文处理问题完整解决方案,"余海燕,郑笑飞","本文从字符编码、编译器及运行环境三方面剖析了Java语言处理中文时出现乱码等现象的原因,并给出了一个适应各种平台的完整方案来解决中文的读写、网络传输和输入问题。","Java Unicode GB UTF - 8,JDK,Applet"
2000,“信息时代的文明与古籍数字化系列讲座”在北大举行,,"中国中文信息学会与北京大学计算语言学研究所联合于2000年5月18日至6月6日在北大举办了题为“信息时代的文明与古籍数字化”的系列讲座。具体内容有:信息时代的文明概说,信息的性质,信息科技的冲击,第四讲古诗中词汇隐喻意的计算机辅助分析及台湾典藏数位化计划简介。第一、二、三、五讲特邀台湾中研院资讯科学研究所文献处理室主任谢清俊教授担任主讲人。第四讲则由北大计算语言学研究所博士生胡俊峰汇报该所在古诗计算机辅助研究领域所取得的最新成果。",
2000-05-19,独立于语种的文本分类方法,"黄萱菁,吴立德,石崎洋之,徐国伟","文本分类是指在给定分类体系下,根据文本的内容自动确定文本类别的过程。本文提出了一个基于机器学习的、独立于语种的文本分类模型,并对模型中的特征抽取、分类器和评价方法进行了详细的介绍。该模型已经在中文和日文两个语种的新闻语料上得到实现,并获得了较好的分类性能。","文本分类,特征抽取,机器学习"
2000-05-22,基于Web中文检索系统SEARCH2000的设计与实现,"杜林,张毅波,孙玉芳","本文详细介绍Search 2000中文检索系统的设计思想及实现方法。与传统的全文检索系统相比,基于WEB的信息检索系统,具有许多全新的特征。页面为半结构化文档、页面通过超链接相互关联、页面的内容覆盖不同应用领域并且拥有大量专有名词和缩略词汇,这些特性成为影响查询精度的主要因素。针对Web的上述特性设计的Search2000全文检索系统,使用智能化的页面相关分析、评分技术,以及高效数据存取、压缩算法和知识库的支持,使其具有使用方便、查询时间短、查询精度高等特点。","信息检索,相关评分,中文信息处理"
2000-05-22,构建知网关系的网状表示,"周强,冯松岩","本文介绍了一个针对知网关系的网状表示结构及其实现方法。通过构建三张数据表:概念表、特征表和关系表,以及建立它们的记录项之间的双向多元联系,可以方便地把知网的所有知识(概念、特征以及它们之间的各种关系) 集成在一起,从而为进一步进行基于知网的信息检索和知识推理打下很好的基础。","知网,概念,特征,关系"
2000-05-03,基于统计的汉语组块分析,"刘芳,赵铁军,于浩,杨沐昀,方高林","组块分析是一种大大降低句法分析难度的有效手段。本文针对汉语普遍规律,提出了一套符合汉语语言特点的汉语组块体系,并在此基础上设计实现了一种统计与错误驱动相结合的、能够分析有限层次的组块自动识别算法。实验证明,该方法能够有效地处理真实文本中的浅层分析问题,具有较好的准确率和鲁棒性。","组块分析,汉语句法分析,统计方法"
2000-05-24,平行语料库中双语术语词典的自动抽取,"孙乐,金友兵,杜林,孙玉芳","本文提出了一种从英汉平行语料库中自动抽取术语词典的算法。首先采用基于字符长度的改进的统计方法对平行语料进行句子级的对齐,并对英文语料和中文语料分别进行词性标注和切分与词性标注。统计已对齐和标注的双语语料中的名词和名词短语生成候选术语集。然后对每个英文候选术语计算与其相关的中文翻译之间的翻译概率。最后通过设定随词频变化的阈值来选取中文翻译。在对真实语料的术语抽取实验中取得了较好的结果。","术语抽取,平行语料,句子对齐,翻译概率"
2000-05-08,基于未对齐汉英双语库的翻译对抽取,王斌,"本文主要研究基于未对齐的汉英双语库翻译对抽取。文章首先介绍了Pascale Fung在这方面设计的两个算法。在此基础上,文章对后一种算法进行了部分的改进,使得其更适合于真实双语文本的翻译对抽取。实现结果表明改进后算法的有效性。本方法可以用于基于大规模双语语料库的短语翻译抽取、词典编纂等应用,具有较高的应用价值。","双语库,对齐,翻译对,抽取,自然语言处理"
2000-05-28,基于搭配对的汉语形容词—名词聚类,"闻扬,苑春法,黄昌宁","本文提出了一个双向分级聚类的算法同时对不同词性的词进行聚类。在聚类过程中,不同词性的词的聚类交替进行,相互影响。我们以最小描述长度的原理为基础构造了目标函数。为了减小数据稀疏的影响,又提出了修饰度的与修正距离的概念。将此算法应用于汉语形容词- 名词的搭配对,对形容词与名词进行聚类,实验结果显示该算法是有效的。","双向分级聚类,搭配对,修饰度,最小描述长度"
2000-05-23,基于语料库的英语从句识别研究,"张晶,赵铁军,姚建民,李生","为改善英汉机译系统复杂句的翻译效果,针对英语复杂句中从句的边界界定问题,本文提出一种基于语料库的方法识别从句,该方法利用词性信息,将规则方法和统计方法结合用于识别从句的边界,获得良好的实验结果,封闭测试的精确率为92.69% ,召回率91.04%;开放测试的精确率为80.34% ,召回率83.93%。","从句,语料库,知识获取"
2000-05-18,大规模现代汉语标注语料库的加工规范,"俞士汶,朱学锋,段慧明","北京大学计算语言学研究所在开发了《现代汉语语法信息词典》等语言资源的基础上,又在实施另一项大型语言工程,即对大规模的现代汉语原始语料进行多级加工,目前的加工项目包括词语切分、词性标注(包括动词和形容词的特殊用法) ,并标出专有名词以及短语型的地名、机构名称等等。","现代汉语标注语料库,词语切分,词性标注,现代汉语语法信息词典,加工规范"
2008-01-28,语义资源建设的最新趋势和长远目标——通过映射对比、走向统一联合、实现自动推理,袁毓林,"本文首先介绍WordNet、VerbNet、PropBank和FrameNet这几个主流语义资源的结构,并分析其各自的缺陷;然后,介绍怎样在不同的资源之间建立起映射关系(包括义项映射和框架映射),达到语义资源的统一和整合,形成词汇语义资源的连接和互补;最后,介绍面向自动推理的更加深层的语义关系的表示和标注的趋向从动词的论元结构走向句子的命题结构,不同动词和句子所表达的事件及其关系,不同词类范畴(动词和事件名词等)所表示的相关事件及其关系,指代词、空语类跟有关表示事件的先行语之间的回指关系。","计算机应用,中文信息处理,语义资源,映射关系,论元结构,命题结构,事件关系,回指关系"
2007-06-30,分层次的汉语功能块描述库构建分析,"陈亿,周强,宇航","现有功能块分析器对于不同长度和不同结构功能块的分析性能研究表明,长的结构复杂的功能块正是功能块自动分析的难点所在。由此,我们设计了新的分层次的功能块体系,并从清华句法树库TCT中自动生成了新的功能块语料库。通过对新的功能块语料库长度分布、内部结构分布分析,以及与单层次功能块语料库的相互关系的研究,我们证实了新的分层次功能块描述体系具有结构简单、长度短且分布均匀的优良特点。这些性质对功能块分析器的性能提高将会有很大的帮助。","计算机应用,中文信息处理,部分分析,功能块,分层次描述"
2007-08-07,一种基于无监督学习的词变体识别方法,"王宝勋,王晓龙,刘秉权,李鹏","本文提出了一种生物医药领域词变体的识别策略。首先使用最小编辑距离算法和字符匹配算法从语料中分别获得特定目标词的形态学变体和缩略词,并将其作为候选词变体。本文采用系统相似模型获得每个词变体上下文语义的量化评价。本文的方法不需要任何语言学知识和精细加工的语料资源,实验表明,该方法可以在保证准确率的同时显著地提高词变体识别的召回率。","计算机应用,中文信息处理,词变体, 缩略词, 最小编辑距离, 系统相似模型"
2007-09-24,名词隐喻相似度及推理识别研究,王治敏,"本文考察了汉语名词隐喻的相似性特点,尝试利用隐喻相似度推理、词典信息等多种方法实现n＋n隐喻表达的发现和提取。隐喻相似度推理,首先运用人机互助方法对中文概念词典(CCD)进行合理剪裁,建立了一个词语对应一个语义类的词典格式,为后续的推理实验提供了保证。同时也验证了名词隐喻知识库的有效性。实验证明,最大熵方法、隐喻相似度、词典知识等多种方法大大提高了识别效果。","计算机应用,中文信息处理,隐喻识别,相似度推理,最大熵"
2007-07-19,语言学组合特征在语义关系抽取中的应用,"奚斌,钱龙华,周国栋,朱巧明,钱培德","语义关系抽取是信息抽取中的一个重要的研究领域。目前基于特征向量的语义关系抽取已经很难通过发掘新的特征来提高抽取的性能。本文提出了一种特征组合方法,通过在各种词法、语法、语义的基本特征内部及特征之间进行合理的组合形成组合特征,使用基于支持向量机的学习方法,使得关系抽取的准确率和召回率得到了提高。在ACE 2004语料库的7个关系大类和23个关系子类抽取实验中F值分别达到了66.6%和59.50%。实验结果表明通过对基本语言学特征进行组合所得到的组合特征能够显著地提高语义关系抽取的性能。","计算机应用,中文信息处理,语义关系抽取,支持向量机,组合特征"
2007-07-16,搜索引擎中的聚类浏览技术,"李红梅,丁振国,周水生,周利华","搜索引擎大多以文档列表的形式将搜索结果显示给用户,随着Web文档数量的剧增,使得用户查找相关信息变得越来越困难,一种解决方法是对搜索结果进行聚类提高其可浏览性。搜索引擎的聚类浏览技术能使用户在更高的主题层次上查看搜索结果,方便地找到感兴趣的信息。本文介绍了搜索引擎的聚类浏览技术对聚类算法的基本要求及其分类方法,研究分析了主要聚类算法及其改进方法的特点,讨论了对聚类质量的评价,最后指出了聚类浏览技术的发展趋势。","计算机应用,中文信息处理,搜索引擎,文档聚类,信息检索,聚类标识"
2007-06-11,文本意见挖掘综述,"姚天昉,程希文,徐飞玉,汉思·乌思克尔特,王睿","意见挖掘是针对主观性文本自动获取有用的意见信息和知识,它是一个新颖而且十分重要的研究课题。这种技术可以应用于现实生活中的许多方面,如电子商务、商业智能、信息监控、民意调查、电子学习、报刊编辑、企业管理等。本文首先对意见挖掘进行了定义,然后阐述了意见挖掘研究的目的,接着从主题的识别、意见持有者的识别、陈述的选择和情感的分析四个方面对意见挖掘的研究现状进行了综述,并介绍了几个成型的系统。此外,我们针对汉语的意见挖掘做了特别的分析。最后对整个领域的研究进行了总结。","计算机应用, 中文信息处理, 意见挖掘,主观性文本,综述"
2007-06-06,自动文摘评价方法综述,"张瑾,王小磊,许洪波","评价是自动文摘领域长期关注的焦点,对自动文摘技术的发展起着积极的促进作用。本文首先介绍了自动文摘评价方法的应用背景和面临的困难;然后对自动文摘评价方法进行了简单介绍和评价;接着在了解国内外研究现状的基础上详细分析了文摘评价方法的关键技术;最后对自动文摘评价方法未来的发展趋势进行了展望。","计算机应用, 中文信息处理, 文本挖掘,自动文摘,自然语言处理,多文档文摘, 文摘评价方法"
2007-05-28,生物医学文本挖掘技术的研究与进展,"王浩畅,赵铁军","生物医学研究是二十一世纪最受关注的研究领域之一,该领域发表了巨量的研究论文,已经达到年平均60万篇以上。如何在规模巨大的研究文献中有效地获取相关知识,是该领域研究者所面临的挑战。作为生物信息学分支之一的生物医学文本挖掘技术就是一项高效自动地获取相关知识的新探索,近年来取得了较大进展。这篇综述介绍了生物医学文本挖掘的主要研究方法和成果,即基于机器学习方法的生物医学命名实体识别、缩写词和同义词的识别、命名实体关系抽取,以及相关资源建设、相关评测会议和学术会议等。此外还简要介绍了国内研究现状,最后对该领域近期发展作了展望。","计算机应用,中文信息处理,生物信息学,文本挖掘,信息抽取,机器学习"
2007-05-09,采用支持向量机的说话者确认中的样本平衡,"龙艳花,郭武,戴礼荣","支持向量机在与文本无关的话者确认系统中已经取得了广泛的应用,但是在实际应用系统中获得的目标说话人样本与冒认者样本数量比一般在几千分之一,因此存在很严重的样本非平衡问题,冒认者样本选择的好坏直接影响到整个系统的性能。本文提出了两种挑选冒认者样本的方法。实验证明这些方法能有效地解决上述问题,性能比随机挑选冒认者样本的方法有了提升,经过在2004年NIST说话人识别数据库上进行测试,等错误率由9.3%降低到6.8%,错误率相对下降了26.9%。","计算机应用,中文信息处理,支持向量机,冒认者"
2007-10-18,维吾尔语动词附加语素的复杂特征研究,阿孜古丽·夏力甫,"本文以复杂特征理论为指导思想,对维吾尔语动词附加语素的多样性进行了初步的研究。维吾尔语附加语素可分为构词语素、构形语素和构词—构形语素等三种类型。这些附加语素在分类、语法形式、体、时、人称、数、附加条件等方面形成了不同的复杂特征。动词附加语素与词根或词干连接时有不同的附加规则。本文主要论述动词附加语素及其变体的多种分类、附加条件、动词附加语素的复杂特征的分类及表现形式,以动词直接陈述式一般过去时的附加语素为例进行特征结构之间的合一。","计算机应用,中文信息处理,动词,附加语素,复杂特征"
2007-08-03,基于最小编辑距离的维语词语检错与纠错研究,"玛依热·依布拉音,米吉提·阿不里米提,艾斯卡尔·艾木都拉","拼写错误的发现和候选词选取是文本分析中的一个重要的技术问题。本文结合维吾尔语的语音和词语结构特点,列出了文本中常见的拼写错误类型,详细分析了解决方法,利用最小编辑距离(minimum edit distance)算法实现了维吾尔语文本拼写错误分析中的查错和纠错功能,并以此为基础,结合维吾尔语构词规则,进一步提高了建议候选词的准确率和速度。该算法已被成功地应用到了维吾尔语文字自动校对和多文种文本检索等领域中。在以新疆高校学报为语料的测试中,词语查纠率达到 85%以上。","计算机应用,中文信息处理,维语尔语,词法分析,纠错,最小编辑距离"
2007-01-14,一种面向构形计算的汉字字形形式化描述方法,"林民,宋柔","目前汉字字形描述方法存在的主要问题是缺少能涵盖一切可能汉字的可计算的字形形式化描述体系,从而造成汉字处理应用中的一系列障碍。本文给出了一种汉字网格字形描述方法,实验表明,该方法具有描述一切可能汉字字形(包括错字)骨架的能力,支持不同颗粒度的构字元素、结构关系等字形特征的自动提取和计算,为字形特征的自动分析处理提供了一种有效的手段,从而也为基于字形计算的各种应用建立了可靠的基础。","计算机应用,中文信息处理,汉字字形,形式化描述,网格字形,特征计算"
2007-03-30,藏文字库标准符合性自动检测方案设计与实现,"刘瀚猛,芮建武,白真龙,吴健","软件产品的标准符合性测试是衡量产品质量与性能的重要方法。本文根据藏文字符集标准与字型标准,按软件产品的可用性原则,分析与定义了藏文字库标准符合性检测的含义与内容。本文提出了实施藏文字库标准符合性测试的方案与算法,实现了藏文字库测试程序。测试结果表明本文的检测方案可行、完整,也为其他文字的字库标准符合性检测提供了很好的途径。","计算机应用, 中文信息处理,藏文,字库,标准符合性,测试"
2007-09-19,中心词驱动的汉语统计句法分析模型的改进,"何亮,戴新宇,周俊生,陈家骏","在对Dan Bikel基于Collins中心词驱动概率句法分析模型实现的句法分析器进行深入研究分析的基础上,对其进行了两个方面的改进。一是通过提供N-best词性候选序列,改进原模型在词性方面的处理,改善了句法分析的结果;二是在该模型中引进单独的基本名词短语识别,从而降低句法分析的复杂度,提高了效率,其中,针对中文的特点,通过对BaseNP的概念进行一系列的扩展,深入研究了基于不同层次概念的BaseNP对句法分析的影响并探讨更适合中文句法分析的BaseNP定义。利用改进的句法分析模型进行中文句法分析实验,实验结果表明,改进模型可以缩短分析时间26%,提高F值4.4个百分点,交叉括号平均减少18%。","计算机应用,中文信息处理,中心词驱动PCFG概率模型,基本名词短语,  N-Best 词性序列, 汉语句法分析"
2007-12-03,汉语交集型歧义切分字段关于专业领域的统计特性,"乔维,孙茂松","交集型分词歧义是汉语自动分词中的主要歧义类型之一。现有的汉语自动分词系统对它的处理能力尚不能完全令人满意。针对交集型分词歧义,基于通用语料库的考察目前已有不少,但还没有基于专业领域语料库的相关考察。根据一个中等规模的汉语通用词表、一个规模约为9亿字的通用语料库和两个涵盖55个专业领域、总规模约为1.4亿字的专业领域语料库,对从通用语料库中抽取的高频交集型歧义切分字段在专业领域语料库中的统计特性,以及从专业领域语料库中抽取的交集型歧义切分字段关于专业领域的统计特性进行了穷尽式、多角度的考察。给出的观察结果对设计面向专业领域的汉语自动分词算法具有一定的参考价值。","计算机应用,中文信息处理,汉语自动分词,专业领域语料库,交集型歧义切分字段,伪歧义,真歧义"
2007-08-17,汉语篇章修辞结构的标注研究,乐明,"汉语篇章修辞结构标注项目CJPL采用大陆主要媒体的财经评论文章为语料,依据修辞结构理论(Rhetorical Structure Theory,RST),定义了以标点符号为边界的篇章修辞分析基本单元和47种区分核心性单元的汉语修辞关系集,并草拟了近60页的篇章结构标注工作守则。这一工作目前完成了对97篇财经评论文章的修辞结构标注,在较大规模数据的基础上检验了修辞结构理论及其形式化方法在汉语篇章分析中的可移用性。树库所带有的修辞关系信息以及三类篇章提示标记的篇章用法特征,可以为篇章层级的中文信息处理提供一些浅层语言形式标记的数据。","计算机应用,中文信息处理,汉语语料库,篇章标注,修辞结构理论"
2007-09-17,中医药古文献语料库设计与开发研究,"刘耀,段慧明,王惠临,周扬,王振国,李宏展","专业领域语料库是对专业领域文献进行自然语言处理的重要的不可或缺的基础,是对专业文本内容与意图进行深层把握的必由之路。通过对研究背景的分析,进一步明析了专业文献进行自然语言处理的必要性,并在对专业文献语料库的研究特点进行分析的基础上,深入探讨了专业语料库的设计思想及原理,同时,对语料库词类的标注信息进行了深入研究。成功地开发了针对专业领域语料库的辅助加工系统,为专业领域语料库建设提供了理论指导和技术支撑。","计算机应用,中文信息处理,自然语言处理,语料库,中医药古文献,知识工程"
2007-08-09,农业古籍断句标点模式研究,"黄建年,侯汉清","农业古籍的整理已经引起了众多学者和专家的注意,但是,对于农业古籍的自动断句、标点模式的研究仍付之阙如。本研究探索并总结出部分农业古籍断句、标点识别模式。首先采用句法特征词断句法、同义语标志词法进行初步断句;进而利用反义复合词、引书标志、时序、数量词、重叠字词、动名结构及比较句法进一步对子句进行断句、标点;最后使用农业用语和禁用模式表进一步提高断句、标点后农业古籍的可读性和准确性。经测试表明,断句、标点的平均准确率分别达到48%和35%,证明本方法具有一定的正确性和可行性。","计算机应用,中文信息处理,农业古籍,古农书,古籍整理,断句,标点,模式匹配"
2007-06-06,语义对立度及其计算模型的研究,"麦范金,王挺","人类的思维离不开语言,联想思维主要通过相关、相似和对立三种方式。现阶段有关语义的相关和相似的研究已比较多,而有关对立的研究却比较少。文章把负值引入到相似度计算中,提出对立度等概念和相关的计算模型,将它们运用到语义对立程度的计算中,并通过仿真试验论证了这些概念模型和计算方法的可行性和有效性。","计算机应用,中文信息处理,语义,相似度,对立度,反义词"
2007-08-24,基于混淆网络解码的机器翻译多系统融合,"杜金华,魏玮,徐波","在对当前几种较流行的统计机器翻译多系统融合方法分析的基础上,提出了一种改进的多系统融合框架,该框架集成了最小贝叶斯风险解码和多特征混淆网络解码两种技术。融合过程如下(1) 从多个翻译系统输出的 -best结果中,利用最小贝叶斯风险解码器选择一个风险最小的假设作为对齐参考;(2) 将其余的 -best假设结果与该参考对齐,从而构建混淆网络。多特征混淆网络基于对数线性模型,引入了更多有效的知识源参与最优路径选择,融合后的BLEU得分比融合前最好的单系统BLEU得分提高了2.19%。在对齐方法上,我们提出了一种改进的翻译错误率(Translation Error Rate, TER)准则——GIZA-TER准则,该准则可以对CN网络进行更有效的短语调序。实验中的显著性检验证明了本文方法的有效性。","人工智能,机器翻译,多系统融合,最小贝叶斯风险解码,多特征混淆网络,GIZA-TER"
2007-09-07,一种命名实体翻译等价对的抽取方法,"陈怀兴,尹存燕,陈家骏","有关命名实体的翻译等价对在多语言处理中有着非常重要的意义。在过去的几年里,双语字典查找,音译模型等方法先后被提出。另一种极具价值的方法是从平行语料库中自动抽取有关命名实体的翻译等价对,现有的方法要求预先对双语语料库的两种语言文本进行命名实体标注。提出了一种只要求对语料库中源语言进行命名实体标注,目标语言不需标注,然后利用训练得到的HMM词对齐结果来抽取有关命名实体翻译等价对的方法。在实验中,把中文作为源语言,英文作为目标语言。实验结果表明用该方法,即使在对齐模型只是部分准确的情况下,也得到了较高正确率的命名实体翻译等价对。","人工智能,机器翻译,命名实体,　翻译等价对,HMM,对齐模型"
2007-05-31,双向聚类迭代的协同过滤推荐算法,"王明文,陶红亮,熊小勇","协同过滤是电子商务推荐系统中广泛采用的技术,然而数据稀疏性会影响协同过滤的推荐质量。针对数据稀疏问题提出一种双向聚类迭代的协同过滤推荐算法,对初始得到的用户聚类和项目聚类进行交叉迭代调整,使得聚类簇达到较为稳定的状态。调整后聚类簇的内聚性更强,类之间的区分度更大。实验表明,在调整后的聚类簇中查找邻居将更加准确,可以有效解决数据稀疏问题的影响,有利于提高推荐的准确性。","计算机应用,中文信息处理,协同过滤, 聚类, 交叉迭代, 平均绝对偏差"
2007-05-20,文档检索中句法信息的有效利用研究,"丁凡,王斌,白硕,刘宜轩,李亚楠","利用词项依存关系来改进词袋模型,一直是文本检索中一个热门话题。已有的定义词项依存的方法中,有两类主要的方法一类是词汇层次的依存关系,利用统计近邻信息来定义词项依存关系,另一类是句法层次的依存关系,由句法结构来定义词项依存关系。虽然已有的研究表明,相对于词袋模型,利用词项依存关系能够显著地提高检索性能,但这两类词项依存关系却缺乏系统的比较在利用词项依存关系来改进文档和查询的表达上,如何有效地利用句法信息,哪些句法信息对文本检索比较有效,依然是个有待研究的问题。为此,在文档表达上,比较了利用近邻信息和句法信息定义的词项依存关系的性能;在查询表达上,对利用不同层次的句法信息所定义的词项依存关系的性能进行了比较。为了系统地比较这些词项依存关系对检索性能的影响,在语言模型基础上,以平滑为思路,提出了一个能方便融入这两类词项依存关系的检索模型。在TREC语料上的实验表明,对于文档表达来说,句法关系较统计近邻关系没有明显的差别。在查询表达上,基于名词/专有词短语的部分句法信息较其他的句法信息更加有效。","计算机应用,中文信息处理,信息检索,词项依存,句法分析,词项近邻"
2007-09-10,一种中文文档的数学公式定位方法,"郭育生,谭怒涛,黄磊,刘昌平","为了从中英文混排的中文文档中定位数学公式,提出了一种基于中文字符识别和公式符号识别的数学公式定位方法。该方法主要由中文字符提取、内嵌公式提取和独立公式定位三个部分组成。在中文字符提取中,首先提取字符块信息中文字符识别结果、公式符号识别结果和字符块的几何特征,然后使用决策树的方法区分中文字符和非中文字符。在内嵌公式提取中,使用公式符号的语义信息、符号间的角标关系和公式的语义信息等从非中文字符中定位内嵌公式。在独立数学公式定位中,对包含较多内嵌公式符号且不包含中文字符的文字行提取版式结构特征,并使用高斯混合模型区分独立公式和普通文字行。在148幅文档图像共包含3 690个公式组成的测试集上取得了91.19%的公式定位正确率。",":人工智能,模式识别,中文文档,字符识别,数学公式,高斯混合模型"
2007-12-29,基于韵律信息的连续语流调型评测研究,"潘逸倩,魏思,王仁华","汉语连续语流中的调型评测是汉语语音评测的一个重要环节,利用连续语流中韵律耦合效应和韵律结构紧密相关这一特性,以韵律词为基本建模单元,建立基于多空间概率分布的HMM调型模型(MSD-HMM),使得汉语普通话水平评测系统针对标准连续语流的调型识别率从82.0% 提升至84.6%;针对有方言背景的非标准发音,机器评分与专家评分的相关度绝对提升超过3.0%。","计算机应用,中文信息处理,语音评测,调型评测,调型识别,韵律词,MSD-HMM"
2007-10-18,一种结构受限的异方差线性判别分析,"陈思宝,胡郁,王仁华","异方差线性判别分析(HLDA)因在语音识别中起到了巨大的特征去相关作用而被广泛利用。然而在训练数据不足或特征维数较高时,HLDA易出现不稳定性和小样本问题。根据特征的矩阵表示形式,提出了一种结构受限的HLDA。首先用二维线性判别分析(2DLDA)压缩矩阵形式的特征,然后作一维的HLDA。通过分析我们指出,二维的特征变换实际上是一种结构受限的一维特征变换。在RM库上的实验,受限HLDA对常规HLDA的词识别错误相对下降12.39%;在TIMIT库上的实验,受限HLDA对常规HLDA的音素识别错误相对下降4.43%。",": 计算机应用,中文信息处理,语音识别,特征变换,HLDA,结构受限"
2007-08-15,基于音素及其特征参数的维吾尔语音合成技术,"姑丽加玛丽·麦麦提艾力,艾斯卡尔·艾木都拉","首先建立了由维吾尔语中的单音素、双音素所构成的小规模语音语料库,设计了相应的拼接单元挑选算法,利用参数调整算法对拼接单元语音信号的时长、基频和短时能量等特征参数进行调整,并利用时域平滑算法对拼接点处的语音参数进行调整,从而进一步提高了合成语音的自然度。用C Sharp 编程语言实现了上述算法,试验结果表明研究思路和技术方案的可行性。该系统具有语料库小,合成语音的可懂度和自然度较高等优势。",": 计算机应用,中文信息处理,语料库,参数调整,语音合成,时域平滑"
2007-06-29,在通用字符集中藏文编码模式的研究与应用,欧珠,"藏文软件开发者在现代计算机系统中处理藏文数据时必须所具备的知识之一是藏文在通用字符集(Universal Character Set, UCS)中是如何进行编码。在设计藏文网页内容时UCS藏文数据的整理、设计藏文应用软件时藏文文本的处理操作或者在设计藏文OpenType或AAT字库时、UCS藏文编码模式应用等都要首先去理解UCS藏文编码模式。因此,理解和掌握UCS藏文编码模式是软件制作商首选目标。详细介绍了UCS藏文编码模式的组织结构和设计方法,以便于使用OpenType来支持复杂藏文文本的显示。",": 计算机应用,中文信息处理,UCS,藏文编码,组合,排序,重排"
2007-10-18,基于DUCET的藏文排序方法,"黄鹤鸣,契嘎·德熙嘉措(赵晨星)","DUCET为每个藏文字符规定了排序码,但藏文音节的拼写复杂性使得藏文排序不能直接应用这些排序码,提出了基于DUCET的藏文音节排序方法,主要思想是首先,将二维的藏文音节转化成一维的字母串;其次,从DUCET中查出每个字母的排序码,得到藏文音节对应的排序码串;最后,通过比较排序码串实现藏文音节间的排序。还讨论了藏文音节与一般藏文字母串以及藏文字符串与外文字符串间的比较规则。",": 计算机应用,中文信息处理,藏文字符串,藏文音节,DUCET,排序"
2007-08-22,基于Web Service的数字化民俗博物馆的研究与实现,"郎丰珍,吐尔根·伊布拉音","为增强世界各族人民对新疆少数民族民俗文化的了解,并实现各个大学数字博物馆之间的无间访问,提出了基于Web Service的英、汉、维三语数字化民俗博物馆的建设方案,文中分析设计了数字化民俗博物馆的总体结构,讨论了Web Service关键技术与ASP.Net技术,并结合Web 服务与ASP.Net技术,初步实现了数字民俗博物馆的建设,利用这两种技术的优点,提高了客户端的浏览速度,为用户提供了更方便、更透明的信息服务,并为不同用户提供了英、汉、维三种语言的选择。",": 计算机应用,中文信息处理,数字民俗博物馆,Web Service,ASP.NET"
2007-07-09,藏文编码字符集的优化研究,"高定国,欧珠","《信息交换用藏文编码字符集 基本集》奠定了研究藏文信息处理技术的基础,非常重要,但随着藏文信息处理技术研究的深入,也逐渐发现了《基本集》没能反映藏文构件的基本特征,增加了研究有关藏文工作的难度,同时,在使用中还存在藏文编码歧义等缺陷。针对上述问题提出了增加三个上加字的编码到BMP中,使得藏文编码能正确地反应藏文的构件特征,还提出用“界定藏文编码的使用方法”来消除《基本集》应用中存在的歧义以及正确理解几个字符的属性等问题。",": 计算机应用,中文信息处理,藏文编码,基本集,上加字"
2006-12-20,基于字形拓扑结构的甲骨文输入编码研究,"顾绍通,马小虎,杨亦鸣","分析了甲骨文字形的拓扑结构特征,考虑了甲骨文字形、读音等因素,制作了甲骨文输入法的字形码表和拼音码表,设计了一种简便、有效的甲骨文输入编码方案,开发了甲骨文输入法程序,利用该程序可以通过两种途径来输入甲骨文字形,即拆分取码方法和现代汉字拼音方法,从而解决了从通用甲骨文字库中调出所需字形的问题。",": 计算机应用,中文信息处理, 拓扑结构, 甲骨文, 输入, 编码"
2008-07-20,基于知识管理和智能控制的协同翻译平台——知识管理和机器翻译的融合,"张桂平,蔡东风","在对机器翻译发展艰难历程总结和反思的基础上,提出了以用户模型为核心的知识管理与机器翻译技术融合的新思想。2008年7月该成果通过了中国中文信息学会在京组织的鉴定,鉴定委员会一致认为“研制单位基于其所承担的国家863课题机器翻译和知识管理技术的融合研发的基于知识管理和智能控制的协同翻译平台已圆满完成。该项研究在利用知识管理技术实现人机双向协同翻译方面达到国际领先水平。”本文对平台研制的思想与方法、设计与实现、分析与应用、历程与展望进行了阐述。","人工智能,机器翻译,知识管理,用户模型,协同翻译平台"
2008-05-02,《现代汉语语义分类词典》(TMC)研制中若干问题的思考,苏新春,"《现代汉语语义分类词典》继承了《同义词词林》概念分类的传统,以反映一个社会的生活全貌及认识观念的概念关系为目的,收录了8万余条现代汉语通用性较高的语文词语,建构出了一个五级语义分类体系,里面包括9个一级类,62个二级类,518个三级类,2 076个四级类,12 613个五级类。所建构的义类关系,注重上位语义层对下位语义层有较强控制力,下位语义层对上位语义层的义域能全面覆盖,左右语义类具有互补对应的功能。","计算机应用,中文信息处理,语义分类词典,词汇系统,主题词"
2008-03-11,现代汉语名词语法属性的计量研究初探,"王萌,俞士汶,段慧明,孙薇薇","以《现代汉语语法信息词典》中语法属性的概率化描述为目标,基于1998年上半年《人民日报》标注语料,对名词语法属性的概率化进行了初步的实验研究。首先,考察了名词与数词、名词与量词搭配的相关属性,引进“分散度”概念,利用它对“数名”结构进行了定量分析;其次,考察了名词受不同量词修饰的分布情况。最后,把实验结果与《现代汉语语法信息词典》的相应属性进行了比照和分析,在属性概率化的同时也对其正确性进行了验证。","计算机应用,中文信息处理,现代汉语,现代汉语语法信息词典,概率语法属性描述,基本标注语料库,“数名”结构,“数量名”短语"
2008-04-03,汉语比较句识别研究,"黄小江,万小军,杨建武,肖建国","比较是常见的表达方式,提取事物之间的比较关系是一项新颖而有实用价值的研究。识别自然语言中的比较句,是提取比较关系的一个重要步骤。目前还没有针对汉语比较句的自动识别研究,语言学上比较句的哪些特征能够应用到自动识别上来是一个亟待研究的问题。该文讨论了汉语比较句的范畴、外延和特征,定义了汉语比较句识别的任务,并提出用SVM分类器将汉语句子分为“比较”和“非比较”两类。该文比较了比较句的语言学特征和统计特征,包括特征词、序列模式等在分类中的作用。实验结果表明:基于类序列规则的SVM分类器能够有效地识别汉语比较句,效果优于传统基于词的文本分类。","计算机应用,中文信息处理,汉语比较句识别,比较挖掘,文本分类,序列模式"
2008-03-11,指代消解中距离特征的研究,"杨勇,李艳翠,周国栋,朱巧明","指代消解是自然语言处理中的一个重要问题,包括专有名词、普通名词、代词的指代识别。本文实现了一个基于机器学习的英语名词短语的指代消解平台,通过对原始语料进行命名实体识别和名词短语识别等一系列预处理,选取了多个有效特征及其组合,分别采用最大熵和SVM两种分类算法对名词短语进行分类,在此基础上着重研究了距离特征对指代消解的影响。在传统的基于机器学习的指代消解研究方法中,候选词和先行语的距离被定义为特征,而没有考虑距离在生成训练样例中的作用,本文通过把候选词和先行语的距离作为一个特征加入机器学习算法和作为限制条件用于指代关系候选实例的产生两方面进行详细研究,在MUC-6基准语料库上评测,实验结果表明,合理利用距离特征能够大大提高系统的性能。最终,本文采用最大熵和SVM两种分类器在测试集上分别获得了67.5和68.7的F1值,该结果优于同类型的其他系统。","计算机应用,中文信息处理,指代消解,机器学习,距离特征,最大熵分类器,SVM分类器"
2008-01-08,中文语音合成中的文本正则化研究,"贾玉祥,黄德智,刘武,俞士汶","中文文本正则化是把非汉字字符串转化为汉字串以确定其读音的过程。该工作的难点:一是正则化对象——非汉字串形式复杂多样,难于归纳;二是非汉字串有歧义,需要消歧处理。文章引入非标准词的概念对非汉字串进行有效归类,提出非标准词的识别、消歧及标准词生成的三层正则化模型。在非标准词的消歧中引入机器学习的方法,避免了复杂规则的书写。实验表明,此方法取得了很好的效果,并具有良好的推广性,开放测试的正确率达到98.64%。","计算机应用,中文信息处理,文本正则化,语音合成,最大熵模型"
2008-01-05,基于HTK的维吾尔语连续语音声学建模,"陶梅,吾守尔·斯拉木,那斯尔江·吐尔逊","维吾尔语属于阿勒泰语系突厥语族,是黏着性语言。本文根据维吾尔语的特点,分析设计了维吾尔语语音识别系统的总体结构,讨论了维吾尔语最佳识别基元的选择方法,提出建立基于决策树聚类的上下文相关模型,并采用混合高斯分布(GMD)拟合观测概率分布,优化维吾尔语连续语音中HMM模型系统以提高识别性能。最后给出实验对比,得出结论,为今后维吾尔语连续语音识别研究提供依据。","人工智能,自然语言处理,维吾尔语,混合高斯分布,HMM,连续语音识别"
2008-05-27,多策略汉日机器翻译系统中的核心技术研究,"杜伟,陈群秀","多策略的机器翻译是当今机器翻译系统的一个发展方向。该文论述了一个多策略的汉日机器翻译系统中各翻译核心子系统所使用的核心技术和算法,其中包含了使用词法分析、句法分析和语义角色标注的汉语分析子系统、利用双重索引技术的基于翻译记忆技术的机器翻译子系统、以句法树片段为模板的基于实例模式的机器翻译子系统以及综合了配价模式和断段分析的机器翻译子系统。翻译记忆子系统的测试结果表明其具有高效的特性;实例模式子系统在1 559个句子的封闭测试中达到99%的准确率,在1 500个句子的开放测试中达到85%的准确率;配价模式子系统在3 059个句子的测试中达到了89%的准确率。","人工智能,机器翻译,汉日机器翻译系统,汉语分析,翻译记忆技术,基于实例的机器翻译方法,基于配价的机器翻译方法"
2008-01-21,基于Web的双语平行句对自动获取,"叶莎妮,吕雅娟,黄赟,刘群","双语平行句对是机器翻译的重要资源,但是由于获取途径的限制,句子级平行语料库不仅数量有限而且经常集中在特定领域,很难适应真实应用的需求。该文介绍了一个基于Web的双语平行句对自动获取系统。该系统融合了现有系统的优点,对其中的关键技术进行了改进。文中提出了一种自动发现双语网站中URL命名规律的方法,改进了双语平行句对抽取技术。实验结果表明文中所提出的方法大大提高了候选双语网站发现的召回率,所获取双语平行句对的召回率为93%,准确率为96%,证明了该文方法的有效性。此外,该文还对存在于双语对照网页内部的双语平行句对的抽取方法进行了研究,取得了初步成果。","计算机应用,中文信息处理,双语句对,平行网页,网页挖掘"
2008-03-31,基于语义理解的垃圾邮件过滤处理研究,"麦范金,叶东海,史慧","文章基于统计和基于规则的垃圾邮件过滤技术,将语义理解的研究和垃圾邮件过滤算法的研究结合起来,构建一个通过语义理解对垃圾邮件进行过滤的模型,并提出一种改进的分词算法,提高了分词的效率、准确率和识别未登录词的能力。最后通过实验数据可知,基于语义理解的过滤模型,在一定程度上解决了邮件过滤中遇到的词的“拆解”的问题、分词后所遇到的未登录词的问题,为垃圾邮件过滤提供了有益的探索。","计算机应用,中文信息处理,中文分词,垃圾邮件过滤,语义理解"
2008-04-15,一种改进的基于《知网》的词语语义相似度计算,"江敏,肖诗斌,王弘蔚,施水才","中科院刘群的基于《知网》的词语相似度计算是当前比较有代表性的计算词语相似度的方法之一。在测试中我们发现对一些存在对义或反义的词语与同义、近义词语一样具有较高的相似度,一些明显相似的词反而相似度较低,如“美丽”与“贼眉鼠眼”的相似度为0.814 815,与“优雅”的相似度为0.788 360 ,“深红”与“粉红”的相似度仅为0.074 074,这将不利于进行词语的极性识别。基于文本情感色彩分析的需要,把词语相似度的取值范围规定为[-1,+1],在刘群论文的基础上,进一步考虑了义原的深度信息,并利用《知网》义原间的反义、对义关系和义原的定义信息来计算词语的相似度。在词语极性识别实验中,得到了较好的实验结果P值为99.07%,R值为99.11%。","计算机应用,中文信息处理,知网,词语相似度,义原,词语极性识别"
2008-04-11,《知网》语义关系图的自动构建,"王宏显,周强,邬晓钧","在真实语言环境中,词语间的联系普遍存在、错综复杂。为了更好融合和使用各种语义资源库中的语义关系,构建可计算的汉语词汇语义资源,该文提出了通过构建语义关系图整合各种语义资源的方法,并在《知网》上实现。《知网》作为一个知识库系统,对各个词语义项是以分条记录的形式存储的,各种词汇语义关系隐含在词典文件和义原描述文件中。为提取《知网》中语义间的关系,本文首先将《知网》中的概念以概念树的形式重新表示,并从概念树中提取适当的语义关系,构建语义关系图。经过处理,得到88种589 984条语义关系,图上各种节点具有广泛的联系,为基于语义关系图的进一步分析和计算打下了基础。","计算机应用,中文信息处理,语义关系图,概念树,《知网》"
2008-03-11,《知网》在命名实体识别中的应用研究,"郑逢强,林磊,刘秉权,孙承杰","命名实体识别是自然语言处理领域的一项基础研究,它对于语言的深层处理有重要意义。该文以最大熵模型为基础来进行名实体识别,提出了基于《知网》的两种改进策略来增强模型的泛化性能。第一种策略是将《知网》中词的义原作为特征加入到最大熵模型中;第二种策略是利用《知网》来计算最大熵模型中词特征之间的概念相似度。在北京大学《人民日报》语料上的实验结果表明第一种策略可以有效地提高名实体识别的性能,第二种策略的改进效果不明显。","计算机应用,中文信息处理,名实体识别,概念相似度,《知网》,最大熵模型"
2008-03-12,基于核方法的中文实体关系抽取研究,"黄瑞红,孙乐,冯元勇,黄云平","命名实体关系抽取是信息抽取领域中的重要研究课题之一。该文探讨了核方法在中文关系抽取上的有效性问题,主要分为三部分研究了在卷积树核中使用不同的语法树对关系抽取性能的影响;通过构造复合核检查了树核与平面核之间的互补效果;改进了最短路径依赖核,将核计算建立在原最短依赖路径的最长公共子序列上,以消除原始最短路径依赖核对依赖路径长度相同的过严要求。因为核方法开始被用于英文关系抽取时,F1值也只有40%左右,而我们在ACE2007标准语料集上的实验结果表明,只使用作用在语法树上的卷积核时,中文关系抽取的F1值达到了35%,可见卷积核方法对中文关系抽取也是有效的,同时实验也表明最短路径依赖核对中文关系抽取效果不明显。","计算机应用,中文信息处理,中文实体关系抽取,核方法,卷积树核,复合核,最短路径依赖核"
2007-12-03,水书水字类属码的研究,董芳,"在我国贵州南部至今还使用一种古老的民族宗教典籍“水书”。水书水字的字形复杂,按照汉字编码理论提取的部件或码元很难与水字字形对应。水字的发音较难,采用音码理论实现水字的有序性也较困难。本文提出了水字类属码的编码模式。按照水字内容给予归类,由四位编码组成,第一码位为水字的类别码,区分正体水字和异体水字。第二码位为水字的属性码,根据水字内容进行编码。第三四位编码为水字在各属性中的摆放顺序位置。概述了实现水字可视化输入法的思路。","计算机应用,中文信息处理,水书,水字,类属码"
2008-01-08,一个手机整句输入算法的研究与实现,"吴晓春,吴娴,李培峰,朱巧明","随着时代的发展,手机在人们生活中起着越来越重要的作用。目前,相对于其他丰富多采的手机应用,手机输入法研究相对较少。该文针对手机存储空间小、计算能力低的特点,提出了一种整句查找算法。文章首先介绍了使用倒序排列的文件结构,然后根据该文件结构设计出一种宽度为N的整句查找算法,并在手机S60平台上实现了整句输入系统。文章最后对该整句算法的输入正确率进行了测试。实验结果表明,该算法有着较好的效果。","计算机应用,中文信息处理,输入法,手机,倒序排列,整句输入,S60平台"
2008-04-07,线性化朝鲜文字的歧义性研究,"蔡京哲,崔荣一","该文研究了线性化朝鲜文字重构过程中存在的固有的歧义性问题,并讨论了歧义性消除方案。首先,研究了描述朝鲜文字结构的形式化方法,给出朝鲜文字组成的基本规则和相应的有限状态自动机;其次,给出文字线性化与重构的数学描述,论证了文字重构时存在歧义性的必要条件和充分条件,并分析了线性化文字序列歧义度的本质和歧义性发生的概率;最后,讨论了文字重构歧义性消除的方案,给出了基于基本字母的在线式朝鲜文字序列输入算法和核心步骤,通过仿真实验验证了该方案的可靠性和有效性。","计算机应用,中文信息处理,朝鲜文字,线性化文字序列,文字重构,文字序列歧义性"
2008-06-10,基于条件随机场的有标记联合结构自动识别,"王东波,陈小荷,年洪东","文章介绍了条件随机场的基本原理,基于识别有标记联合结构的需要,根据有标记联合结构的语言学特征结合条件随机场的特性确定了条件随机场的7词位标注集、具有18个复杂特征的模板、增加4个语言学特征的模板。使用北京大学《人民日报》语料和清华大学973树库语料,基于复杂特征的特征模板和增加语言学特征的特征模板在含有嵌套的联合结构、无嵌套联合结构和最长联合结构语料上进行了实验,开放测试的调和平均值最高分别达到了88.21%, 87.85%和84.42%。","计算机应用,中文信息处理,有标记联合结构,条件随机场,特征模板"
2008-06-05,基于最大熵原则的汉语语义角色分类,"丁伟伟,常宝宝","语义角色标注是近些年来兴起的自然语言处理的一个新的研究领域。与英语方面的研究相比,汉语方面的工作还不是很充分。该文在参考已有工作的基础上,基于最大熵原则,对汉语语义角色标注中的一个方面——语义角色分类进行了深入的研究。在提出了一些新的特征之后,该文还充分利用了语义角色之间的相关性,提取语义角色的上下文特征,从而提高标记的准确率;此外,通过对不同特征的单独研究,笔者发现了不同特征取得最优值时的窗口大小差别很大。发现这一现象后,笔者设计了一种基于贪心策略的选择算法,对不同的特征选择不同的窗口大小,使得标记结果进一步提高。在综合采用了以上的策略之后,笔者的汉语语义角色分类系统可以达到95.00%的准确率,比前人有较为显著的提升。从而证明了笔者的方法是有效的。","计算机应用,中文信息处理,语义角色分类,最大熵,特征,上下文,窗口,贪心策略"
2008-06-11,“像”的明喻计算,"李斌,于丽丽,石民,曲维光","汉语隐喻计算是一项难度很大的工作,明喻由于带有明显的标志(比喻词)成为计算机自动识别的基础类型。该文着力于典型的比喻词“像”的比喻义及相关比喻成分的自动识别。首先,人工标注了1 586句语料,分析了明喻句的基本特点。然后,使用最大熵模型对“像”的比喻义和非比喻义进行分类,开放测试F值达到了89%。最后,用条件随机场模型识别出比喻的本体、喻体和相似点,F值分别达到了73%、86%和83%。","计算机应用,中文信息处理,隐喻计算,明喻,明喻识别"
2008-06-03,基于句间关系的汉语语义块省略恢复,"贾宁,张全","语义块是句子的语义构成单位,句子内发生的省略现象可以归结为语义块的省略。该文在句类分析的基础上,从小句间语义块共享关系的角度分析语义块的省略。将语义块的省略分为语义块整块共享形成的省略和语义块部分共享形成的省略,分析了两种情况的特点,并给出了相应的处理算法。测试表明,该算法对于两种省略均有很好的处理效果。","计算机应用,中文信息处理,省略,语义块共享,句间关系"
2008-06-05,基于语言模型验证的词义消歧语料获取,"郭宇航,车万翔,刘挺","作为一种稀缺资源,人工标注语料的匮乏限制了有指导词义消歧系统的大规模应用。有人提出了利用目标词的单义同义词在生语料中自动获取词义消歧语料的方法,然而,在某些上下文当中,用目标词替换这些单义的同义词并不合适,从而带来噪声。为此,笔者使用语言模型过滤这些噪声,达到净化训练数据,提高系统性能的目的。笔者在Senseval-3国际评测中文采样词词义消歧数据集上进行了实验,结果表明经过语言模型过滤的词义消歧系统性能明显高于未经过滤的系统。","计算机应用,中文信息处理,词义消歧,语言模型,噪声过滤"
2008-06-11,《中国语言生活状况报告》中成语与习语的调查与思考,"曾小兵,张志平,刘荣,杨尔弘,张普","成语与习语的调查是《中国语言生活状况报告》在2007年的新增项目,这表明成语与习语使用情况引起了人们更多的关注。成语与习语的研究在语言应用中有广泛而深刻的意义。该文在基于大规模真实语料调查的基础之上,对成语与习语的使用情况做出了“单字差异”等比较,从中发现一些语言现象并提出了自己的思考,以期对汉语语言事实的发现、语言规律的总结、语言词汇的规范化等方面有所裨益。","计算机应用,中文信息处理,中国语言生活,成语与习语,语言规律,词汇规范"
2008-06-19,以关键词抽取为核心的文摘句选择策略,"马亮,,何婷婷,,李芳,,陈劲光,,邵伟","针对面向查询的多文档自动文摘,该文提出了一种以关键词抽取为核心的文摘句选择策略。通过查询扩展的相关技术得到相关多文档集中词语的查询相关性特征,利用最大似然估计法得到语料中词语的话题相关性特征,并将这两个特征值进行特征融合得到词语的重要度以确定关键词。然后通过关键词的重要度来给候选句打分,进一步利用改进的MMR(Maximal Marginal Relevance)技术来调整候选句的得分,最后生成文摘。该文将特征融合引入到词语层面,在DUC2005的语料中测试取得了较好的效果。","计算机应用,中文信息处理,多文档文摘,关键词抽取,文摘句选择"
2008-06-17,基于最大熵模型的中文阅读理解问题回答技术研究,"李济洪,王瑞波,王凯华,李国臣","该文基于山西大学自主开发的中文阅读理解语料库CRCC v1.1版,根据问句和候选答案句的对应关系,构建了词层面以及句法层面共计35个特征,基于最大熵模型对中文阅读理解问题回答进行了建模,在35个特征全部加入最大熵模型的情况下,测试集上得到了75.46%的HumSent准确率。考虑到特征取值之间的相关性对权重估计的影响,笔者先对35个特征观测值矩阵进行主成分降维,选择适当的主成分个数重构特征,然后再使用最大熵模型进行建模,在测试集上的HumSent准确率达到80.18%. 实验结果表明,在阅读理解问答系统中,采用特征的主成分降维方法,能有效融合全部特征信息,回避了最大熵模型中特征筛选的过程,并且提高了阅读理解系统的准确率。","计算机应用,中文信息处理,阅读理解,问答系统,最大熵模型,主成分"
2008-06-03,汉语意见型主观性文本类型体系的研究,"刘全升,姚天昉,黄高辉,刘军,宋鸿彦","主观性文本是一种描述个人想法、情感和意见等的非约束性文本。它与主要描述以事实为主的客观性文本在内容和结构上有很大的不同。意见型文本是包含有意见元素(意见持有者、意见陈述范围、意见主题和意见情感)的一种主观性文本,它大量出现在网上的电子公告板、论坛和博客等媒介中,受到广泛的关注,并成为研究意见挖掘方法和技术的语料。该文介绍了主观性文本的定义及其与客观性文本的差异,同时着重讨论了意见型文本的定义、特点、类型体系及其在意见挖掘技术中的应用。","计算机应用,中文信息处理,主观性文本,类型体系,意见挖掘"
2008-06-15,一种基于WWW的Ontology属性值自动提取方法,"赵庆亮,穗志方","属性值是描述Ontology中类的重要信息,但是当前关于属性值的自动提取的研究并不多。该文提出一种基于WWW的Ontology属性值自动提取方法。论文首先提出了一种在小规模属性值种子集的基础上,包含属性值的句子的选择与属性值提取互动的方法。这种方法利用互联网信息的冗余性,自动抽取并扩充目标属性值集合。然后,为避免人工构造属性值种子集,提出种子集自动生成的方法。我们设计实验来计算提取结果的正确率和召回率,此外,我们还通过将填充后的Ontology信息用于网页正文提取任务来展示Ontology自动扩充结果的有效性。","计算机应用,中文信息处理,因特网,互动方法,属性值提取"
2008-06-11,基于联合权重的多文档关键词抽取技术,"杨洁,季铎,蔡东风,林晓庆,白宇","该文提出一种多文档关键词抽取方法,该方法提出ATF×PDF(Average Term Frequency×Proportional Document Frequency)来计算词语权重,并根据候选关键词之间的语义相似度,采用联合权重方法重新计算候选关键词的权重来抽取关键词。该方法综合考虑了词语的频率,词性以及词语之间的语义相似性等信息,实验表明,该方法能有效抽取多个文档的关键词,同基于关键词的聚类标记方法相比,其准确率提高3%,召回率提高7%,F-measure提高4.4%。","计算机应用,中文信息处理,ATF×PDF,联合权重,多文档,语义相似度"
2008-06-10,网络热点事件发现系统的设计,"刘星星,何婷婷,龚海军,陈龙","该文设计了一个热点事件发现系统。该系统面向互联网新闻报道流,能自动发现任意一段时间内网络上的热点事件,并给出描述事件发展过程的曲线图。针对网络新闻语料具有数据规模大和时间特征明显两个特性,系统将语料按时间(天)分组,对每天的语料采用凝聚聚类得到微类,选取某段时间内的所有微类,再做Single-pass聚类得到事件列表,利用事件热度计算公式,把候选事件按热度进行排序。采用该系统对2007年新闻语料进行实验,结果表明该系统能取得较好的效果。","计算机应用,中文信息处理,事件发现,凝聚聚类,Single-pass聚类,热度计算"
2008-06-01,基于统计特征的垃圾博客过滤,"刘玮,廖祥文,许洪波,王丽宏","该文根据垃圾博客和正常博客在统计特征上的差异,对多种针对博客分类有效的统计特征进行了分析,提出基于博客页面统计特征的过滤方法。在Blog06数据集上的实验表明,该方法的过滤准确性达到97%,比基于词频特征的过滤方法提高了约7%,在不同规模训练集上的准确性保持在95%左右,具有更好的泛化能力。","计算机应用,中文信息处理,内容分析,垃圾博客过滤,统计特征,词频特征,泛化能力"
2008-06-11,基于用户日志挖掘的搜索引擎广告效果分析,"陈磊,刘奕群,茹立云,马少平","随着搜索引擎市场的飞速发展,竞价排名广告以其有效、低风险、灵活等特点逐渐受到中小企业用户的青睐,成为搜索引擎稳定的收益增长点。然而竞价排名广告是否会影响用户体验,从而削弱其宣传效果并且影响用户对于搜索引擎的忠实度成为了企业及搜索引擎所担忧的问题。该文从网络用户日志中挖掘出网络用户对于广告的实际交互行为,并给出了各大搜索引擎竞价排名广告方面的统计数据。对于企业用户如何更有效地利用竞价排名广告以及搜索引擎如何平衡广告的经济效益和用户体验之间的关系都有较高的指导意义。","计算机应用,中文信息处理,搜索引擎,用户行为分析,竞价排名广告"
2008-06-05,机器学习的查询扩展在博客检索中的应用,"王秉卿,张奇,吴立德,黄萱菁","该文介绍一种新的查询扩展方法,该方法结合了查询扩展技术和机器学习理论。通过机器学习的方法挑选出查询扩展词,以此提高检索结果的性能。对于输入的查询项,首先通过伪反馈技术生成候选扩展词集合,然后使用支持向量机对输入的候选词评分,挑选得分较高的候选词和原始查询项组成一个新的查询项。由于训练这个支持向量机的训练数据较难获得,我们利用评测会议的检索结果和检索工具自动地生成训练数据。这套查询扩展方法的优点在于通过对训练语料的学习,能够对候选扩展词作出更合理的选择。在TREC评测会议组织的观点检索任务中,相对于不采用任何扩展技术的基准系统,该方法提高了MAP指标33.1%。","计算机应用,中文信息处理,信息检索,查询扩展,机器学习"
2008-06-05,一种有效的基于Web的双语翻译对获取方法,"郭稷,吕雅娟,刘群","命名实体和新词、术语的翻译对机器翻译、跨语言检索、自动问答等系统的性能有着重要的影响,但是这些翻译很难从现有的翻译词典中获得。该文提出了一种从中文网页中自动获取高质量双语翻译对的方法。该方法利用网页中双语翻译对的特点,使用统计判别模型,融合多种识别特征自动挖掘网站中存在的双语翻译对。实验结果表明,采用该模型构建的双语翻译词表,TOP1的正确率达到82.1%,TOP3的正确率达到94.5%。文中还提出了一种利用搜索引擎验证候选翻译的方法,经过验证,TOP1的正确率可以提高到84.3%。","计算机应用,中文信息处理,双语翻译对,统计判别模型,网络挖掘"
2008-06-01,统计和规则相结合的汉语最长名词短语自动识别,"代翠,周俏丽,蔡东风,杨洁","在分析汉语最长名词短语特点的基础上,提出了一种统计和规则相结合的汉语最长名词短语自动识别方法 通过实验词及词性的不同组合选择特征集合,基于该特征训练得到条件随机场(CRF)识别模型;分析错误识别结果,结合最长名词短语的边界信息和内部结构信息构建规则库对识别结果进行后处理,弥补了机器学习模型获取知识不够全面的不足。实验结果表明,用统计和规则相结合的方法识别最长名词短语是有效的,系统开放测试结果F值达到了90.2%。","计算机应用,中文信息处理,条件随机场,最长名词短语,基于规则的后处理"
2008-06-10,基于条件随机场的冠词选择研究,"宁伟,蔡东风,张桂平,季铎,苗雪雷","冠词选择需要综合考虑语言知识、语义知识以及世界知识,是汉英翻译中的一个难点。针对传统的基于规则和机器学习的方法,只考虑名词短语前冠词选择的问题,该文将冠词看作一种标记,将该问题形式化地描述为一个序列标注任务,提出一种基于条件随机场的解决策略,选取特征时充分利用词、词性等多层次资源,并引入前后词的互信息。实验采用包含91 106个冠词的专利摘要做测试语料,F值达到80%。","人工智能,机器翻译,冠词选择,条件随机场,序列标注,互信息"
2000-05-22,利用平行网页建立中英文统计翻译模型,"聂建云,陈江","建立翻译模型的目的是试图从平行文本(或翻译例句)中自动抽取翻译关系。本文将描述我们在建立中英文统计翻译模型上的尝试。我们所用的平行文本是从万维网上自动获得的半结构性平行文本。在训练过程中,我们尽量利用文本中的HTML结构信息。实验表明,所训练的翻译模型能达到80%的准确率。对于象跨语言信息检索这样的应用,这样的准确率已经能大致满足需要。这一工作表明,对于检索引擎上的问句的翻译可以使用比机器翻译成本更低的工具。","中英问句翻译,平行网页,句对齐,统计翻译模型,跨语言信息检索"
2000-05-23,提高汉语自动分词精度的多步处理策略,"赵铁军,吕雅娟,于浩,杨沐昀,刘芳","汉语自动分词在面向大规模真实文本进行分词时仍然存在很多困难。其中两个关键问题是未登录词的识别和切分歧义的消除。本文描述了一种旨在降低分词难度和提高分词精度的多步处理策略,整个处理步骤包括7个部分,即消除伪歧义、句子的全切分、部分确定性切分、数词串处理、重叠词处理、基于统计的未登录词识别以及使用词性信息消除切分歧义的一体化处理。开放测试结果表明分词精确率可达98%以上。","汉语自动分词,歧义,多步处理"
2000-05-23,利用遗传算法实现词类标记集的优化,"孙宏林,陆勤,俞士汶","过去词类标记集的选择主要基于专家的经验知识,缺乏自动或半自动的方法来辅助这一过程。本文提出了一种利用遗传算法来搜索优化的标记集的新方法。这种方法可以在一个候选标记集集合中自动搜索一个最优或较优的标记集,并可根据应用的需求调整参数以适应特定任务的需求。实验表明:遗传算法为标记集的选择提供了一种系统的有效的辅助手段。","词性标注,词类,标记集,遗传算法"
2001,中科院自动化所模式识别国家重点实验室正式成为国际语音翻译研究协会核心成员,宗成庆,"语音翻译(Speech-to-speech Translation)是近几年来国际上发展迅速的热点研究领域,为了推动语音翻译技术研究的快速发展,由美国CMU(Carnegie Mellon University)、日本ATR、德国Karlsruhe大学等单位联合发起,于1991年正式成立了国际语音翻译研究协会(Consortium for Speech Translation Advanced Research,简称C-STAR)。到目前为止C-STAR已经历了三个发展阶段,今年10月正式转为第三阶段C-STAR Ⅲ。C-STAR发展阶段的提升,标志着国际上语音翻译技术的不断进展。",
2000-05-23,基于分解与动态规划策略的汉语未登录词识别,"吕雅娟,赵铁军,杨沐昀,于浩,李生","未登录词的识别是汉语自动分词中的主要问题。本文以对中国人名,中国地名和外国译名进行整体识别为目标,采用分解处理策略降低了整体处理难度,并使用动态规划方法实现了最佳路径的搜索,较好地解决了未登录词之间的冲突问题。通过对真实语料识别的测试,证明该方法可以全面提高未登录词识别的正确率和召回率。","未登录词识别,分解处理,动态规划"
2000-03-16,基于N-gram信息的中文文档分类研究,"周水庚,关佶红,俞红奇,胡运发","传统文档分类系统都是基于文档的词属性,分类过程需要庞大的词典支持和复杂的切词处理。本文研究基于N-gram信息的中文文档分类,使中文文档分类系统摆脱对词典和切词处理的依赖,从而实现中文文档分类的领域无关性和时间无关性。利用kNN分类方法,实现了一个基于N-gram信息的中文文档分类系统。测试结果表明该文档分类系统具有和其它同类文档分类系统相当的性能。","文档分类,N-gram信息,属性选择,kNN法"
2000-03-09,基于变帧率训练的HMM汉语人名识别,"刘刚,张洪刚,郭军","本文针对语音识别中HMM模型需要大量训练,而在某些实际应用中不可能训练多次的问题,提出一种基于余弦整形变换的变帧率训练方法,并在人名声控拨号系统中进行实验,在训练一次的条件下,系统识别率提高4.2%。实验表明,该方法对解决语音识别系统中训练数据少的问题具有明显效果。","语音识别,HMM,余弦整形变换,变帧率"
2001,全国第六届计算语言学联合学术会议(JSCL-2001)2001年8月4日—6日太原:山西大学征文通知,,"为促进国内计算语言学的研究和应用,加强同行间的学术交流与合作,中国中文信息学会、中国计算机学会、中国人工智能学会和北京市语言学会等四个单位决定于2001年8月4日-6日在太原山西大学联合举办“全国第六届计算语言学联合学术会议(JSCL - 2001) ”。会议论文的正式语言为中文与英文。会议向全国征集有关计算语言学、自然语言理解和机器翻译方面的论文。来稿要求在理论或应用技术上确有创见、叙述清楚、行文流畅。全文不超过8000字,每篇论文均应有中英文两种文字标题、作者、姓名、单位和不超过200字的摘要,来稿要求全文一式三份。作者请自留底稿,会议概不退稿。大会将正式出版会议论文集。",
2000-03-14,一种利用校对信息的汉字识别自适应后处理方法,"李元祥,刘长松,丁晓青","后处理技术是汉字识别系统的重要组成部分。传统的识别后处理技术在很大程度上依赖于所训练的统计语言模型,没有考虑所处理文本的特殊性;而且没有利用识别器的动态识别特性。本文利用部分校对过的正确本文信息,一方面可以构建自适应语言模型,及时发现所处理文本的语言特点;另一方面可以利用识别器的动态识别特性,以修正候选字集;从而使得后续文本的识别后处理具有自适应性。40 万字的数据测试表明:这种方法的文本平均错误率较传统的后处理方法下降35.24%了,可以大大减轻数据录入人员的工作量,具有较高的实用价值。","汉字识别,后处理,语言模型,自适应,修正候选字集"
2000-02-15,用过滤器实现Web网站汉字简繁体自动转换,"张震,张曾科","本文对网络上汉字的显示与传输进行了研究,提出一种新的在Web服务器端直接解决汉字繁简体内码转换的方案,使得只有一种内码的中文主页也可以自动地对不同内码浏览器提供支持,而不必要求客户端安装软件。这种思想在Windows NT下用IIS里的ISAPI过滤器得以实现。","Web,汉字内码,GB2312,BIG5,ISAPI"
2015-11-23,《汉字规范码应用于基础教育教学实践》成果发布会在京召开,,二○○○年十二月九日中国中文信息学会与北京教科院基础教育教学研究中心在北京人民大会堂召开了《汉字规范码应用于基础教育教学实践》成果发布会。,
2000-03-07,《现代汉语语法信息词典》的新进展,"俞士汶,朱学锋,王惠","《现代汉语语法信息词典》是面向汉语信息处理的基本语言知识库。1995年11月底通过技术鉴定。5年来,北大计算语言学研究所在应用、推广的同时,仍把重要的力量投入词典本身的发展。至目前为止,词典收词已由5万条增加到7.3万条,并且全部完成了归类;为了处理未定义词,还开发了一个全新的语素库;词语语法属性描述中的瑕疵得到了进一步的修正,新增了20多个语法属性项目和大量的实例。整个词典的规模和质量有了显著的提高。","中文信息处理,现代汉语,语法属性描述,语法词典"
2001,现状和设想——试论中文信息处理与现代汉语研究,许嘉璐,"本文介绍了中文信息处理技术发展的现状及面临的主要困难,指出:关键在于对现代汉语研究的滞后。到目前为止,中文信息处理主要依赖于对大规模语料的统计,根据概率,对词与词的关系作出界定。多年来中文信息处理技术徘徊难进的现实说明,这一方法已经难以突破“瓶颈”,要使计算机对现代汉语进行自动化的处理,即使之真正“智能化”,就必须把人的语言知识“教”给计算机。这就需要根据计算机的要求加强对现代汉语的研究,特别是对语义的研究。文中介绍了当前朝此方向努力并已有较大进展的三个流派,并分别指出其不足;参考作者主持国家“九五”重点项目“信息处理用现代汉语词汇研究”的经验,提出了统一使用资源、携手并进、共同攻关的设想。","中文信息处理,现代汉语研究,战略性设想"
2000-01-20,句法范畴的代数结构与演绎系统,于江生,"本文给出了建立在含幺半群基础上的范畴语法的代数结构,定义了范畴方程和它的解并对范畴方程的解作了分类:相容性的相关性。定理“对于范畴方程的任意一个解X ,都存在唯一的本质解Y使得Y?X”使得我们可以通过一定的演绎规则对词w的本质范畴作扩张以得到w的所有句法范畴。最后,作者从范畴理论的角度给出了句法范畴演绎系统的数学描述。","句法范畴,范畴方程,本质解,类型提升"
2000-05-08,无词典高频字串快速提取和统计算法研究,"韩客松,王永成,陈桂林","本文提出了一种快速的高频字串提取和统计方法。使用Hash技术,该方法不需要词典,也不需要语料库的训练,不进行分词操作,依靠统计信息,提取高频字串。用语言学知识进行前缀后缀等处理后,得到的高频字串可以作为未登录词处理、歧义消解和加权处理等的辅助信息。实验显示了该方法速度较快且不受文章本身的限制,在处理小说等真实文本时体现了较高的可用性。","Hash技术,高频字串,统计,算法"
2000-05-26,一种基于EM非监督训练的自组织分词歧义解决方案,"王伟,钟义信,孙建,杨力","本文旨在提供一种基于非监督训练的分词歧义解决方案和一种分词算法。基于EM的思想,每个句子所对应的所有(或一定范围内)的分词结果构成训练集,通过这个训练集和初始的语言模型可以估计出一个新的语言模型。最终的语言模型通过多次迭代而得到。通过一种基于该最终语言模型的统计分词算法,对于每个句子至少带有一个歧义的测试集的正确切分精度达到85.36%(以句子为单位) 。","EM算法,分词歧义,非监督"
2000-03-02,汉语声调识别中的基音平滑新方法,"朱小燕,王昱,刘俊","汉语普通话是一种带声调的语言。声调可以用基音的轮廓信息进行描述。传统基音的平滑方法:线性平滑、中值平滑和一般的线性插值方法都不能很好地处理连续的基音频率有随机错误点的情况。本文提出了一种通过搜索来得到更精确的基音轮廓的新的基音平滑方法。这种方法具有简单可靠,快速高效的特点。实验表明这种方法比传统的方法识别错误率降低约40%。","基音检测,平滑,声调识别,语音识别"
2000-03-30,地形图数字注记的自动提取与识别,"徐战武,张涛,刘肖琳","地形图的自动扫描矢量化是GIS领域亟待解决的一个重要难题。地形图中包含了大量的字体丰富的数字注记,用以表示地物地貌的属性等特征,正确提取并识别这些数字是图纸处理中的重要组成部分。本文分析了现有的提取方法的不足,提出了一种新的数字注记自动提取与识别算法,首先根据先验的尺寸大小确定候选数字,再采用OCON结构的BP神经网络识别出真正的数字,然后利用近邻关系提取出扩展数字。实验表明,该算法是快速、高效、可靠的。","地形图,数字注记,BP网络"
2000-04-17,蒙古文整词编码研究,S·苏雅拉图,"作者基于蒙古文黏着记录其词汇方式和按书面音节拼读书写整词规则,提出了蒙古文整词编码方法。本文依据可计算性理论,提出了拼音文字非键盘映射编码方法,将整词编码分为输写码与计算码。整词输写码设计模仿传统蒙古文整词固有拼读书写规则,达到了最佳人机键盘交互目的。整词计算码既可载荷整词复杂特征知识信息、又可保证信息的可计算性,从而为蒙古文整词复杂特征合一计算和并行处理奠定了可行性科学基础。","蒙古文整词,输写码,计算码,可计算性,复杂特征载荷"
2000-06-27,基于词性和语义知识的汉语句法规则学习,"苑春法,陈刚,黄昌宁","本文提出了一种汉语句法规则学习的新方法。本方法的特点是:在规则的学习和表示上都利用了词性、语义以及上下文相关的信息。它不仅能自动学习上下文无关的二元规则,而且还能自动发现词类搭配中的歧义结构,并利用语义和上下文相关信息将歧义规则在句法分析之前进行排除。实验结果表明,该方法较好地解决了汉语句法规则的自动获取及排歧问题并极大地降低了句法分析的难度,显示了很好的应用前景。","句法分析,二元语义规则,二元词性规则,禁止规则"
2000-07-03,面向范畴语法分析的汉语词库的构造及实现,"秦莉娟,周昌乐","在蒙太鸠语法理论的基础上,利用范畴语法对汉语进行句法分析,并针对汉语范畴动态标注的不确定性进行跨层次松弛关联的计算研究,需要相应地构造范畴化机器词库。本文采用基本词库加扩展生成的思想构建生成的面向范畴语法分析的汉语词库,除具有一般词库的特点外,还对词语的范畴归属、词谓、词用等相关信息给出说明,以供范畴句法分析时选用。实验结果表明,在假设完备的前提下,测试该词库取得了较好的效果。","范畴语法,汉语词库,自然语言理解"
2000-07-11,基于实例的汉语句法结构分析歧义消解,"杨晓峰,李堂秋,洪青阳","本文论述了一种基于实例的汉语句法结构分析的消歧方法。本文首先提出了这种方法的总体思路,并对其语义知识资源—《知网》作了简要的介绍。然后详细地描述了基于实例的排歧法的主要算法。最后给出的算法实验结果例子证明,这种方法是对汉语的结构分析排歧是有效的。","歧义消解,基于实例,相似度,知网,依存关系树"
2000-05-08,汉语名物性短语句法位置语料库的设计,王家钺,"汉语句物性短语(NP)在汉语信息检索中有重要价值。本文以非统计的信息处理方法为出发点,介绍一个汉语名物性短语句法位置语料库的设计思想、所使用的句法位置标记集以及标记加工规范,并指出了这样一个语料库的潜在价值。目前正在以此为出发点建立一个汉语名物性短语句法位置语料库。","句法位置,语料库,名物性短语,非统计信息检索,汉语信息检索"
2000-06-01,基于二元接续关系检查的字词级自动查错方法,"张仰森,丁冰青","本文探讨了基于字字同现、词性二元接续和语义二元接续的中文文本的自动查错原理和查错算法;给出了字词接续判断模型,并讨论了与接续判断模型相关的查错知识库的构造方法。通过对实验结果的分析和评测,证明本文所述方法是可行的。","中文文本自动校对,自动查错,二元接续关系"
2000-08-30,基于PATRICIA tree的汉语自动分词词典机制,"杨文峰,陈光英,李星","分词词典是汉语信息处理系统的一个基本组成部分,其查询和更新效率将直接影响汉语信息处理系统的性能。本文采用PATRICIA tree的数据结构,设计了一种可以对词典词条进行快速查询、更新的分词词典机制,并从理论上初步分析了它的性能。最后通过实验,在时间效率上与逐字二分的分词词典机制进行了比较。结果表明,基于PATRICIA tree的分词词典机制具有更高的查询速度和更新效率,能满足大规模、开放文本处理系统的需求。","信息检索,PATRICIA tree,汉语自动分词"
2000-08-16,Hough变换在中文名片图像倾斜校正中的应用,"潘武模,焦扬,王庆人","近来,文档图像的计算机自动理解已取得很多进展。但是,对于具有倾斜的图像的理解仍然存在许多困难。这种困难在中文名片图像自动识别与理解系统中尤为突出。必须在系统的输入端对图像作有效的倾斜校正以保证系统的性能。由于中文名片版面复杂,名片中文字行以及每行字符较少,使得现有的倾斜校正算法在处理名片图像时效果很不理想。Hough变换可用于一般文档图像的倾斜校正。但是,Hough变换在名片图像中的应用还有待研究。本文提出一种二级Hough变换算法,并应用于名片图像理解系统,利用名片图像自身的特点提高Hough变换的精确度和速度。这一方法的效果已被实验结果所证实。","文档分析,版面理解,倾斜校正,Hough变换,中文名片"
2000-08-05,用网络[定向图]实现形态素之间的接续提高假名汉字转换的用户操作性能,"中岛晃,河野胜也","本文描述了把扩展句节数最小法进行形态素解析的结果登录在网络[定向图]上,并把这个网络[定向图]保留在计算机的内存中直到修改操作结束,实现了在不同句节切分的各种选择候补中取出所需候补。再根据所选候补去检索网络[定向图]上的路径,就可以得到符合操作者意图的全句的转换结果,从而使日文汉字输入时所需的假名汉字转换操作简单易行,提高了操作性能。","网络定向图,扩展句节数最小法,假名汉字转换,汉字输入法"
2000-11-15,健壮性学习算法,刘颖,"使用统计方法可以对汉英机器翻译的词性标注和句法语义分析阶段产生的歧义进行消歧,在估计过程中往往使用最大可能方法,但是并不是在所有的情况下取最大值都是正确的。为了从所有候选结果中取到正确的结果,本文使用健壮性学习算法。使用这个算法,当正确的候选结果评分不是最高时,仍能通过健壮性算法来调整正确结果的评分,使之最大,并且降低不正确候选的评分。而且,由于训练集与测试集存在不同,使训练集中的错误率最小不能保证测试集中的错误率也最小。因此当考虑训练语料库和测试语料库存在统计变化时,应该使用健壮性学习算法。","健壮性学习算法,机器翻译,评分"
2000-10-16,基于ER模型和受限汉语的数据库中文查询语言研究,"崔宗军,唐世渭,杨冬青","本文给出了一个基于ER模型和受限汉语的关系数据库汉语查询语言的计算模型RChiQL(Restrictive Chinese Query Language)及其实现方案,系统模拟人脑对语言处理的并行机制,将中文查询句的处理分为四个相互依存、相互交织的步骤(词的切分,文法分析,语义分析和SQL转换) ,其中引入了一种新的文法GWERSC(Grammar with ER Semantic Characteristics ,ER语义特征文法) ,其内嵌的ER模型语义既有利于语法分析又简化了语义分析,取得了很好的效果。","关系数据库,自然语言接口,ER模型,受限汉语"
2000-11-09,基于单汉字索引的全文检索系统的优化研究,"余海燕,张仲义","对于按照单汉字建立倒排索引的全文检索系统,最需要解决的问题是如何提高其存储效率和运算速度。本文针对此问题提出了以下优化方法:一是利用参数化的Golomb编码对倒排文件进行压缩;二是对求集合交集的逻辑乘算法进行改进;三是运用并行计算和双缓冲技术。实验结果表明,经过优化后的单汉字全文检索系统已达到实用化的程度。","全文检索,单汉字标引,倒排文件,Golomb编码"
2000-07-29,三个层面的中文文本主题自动提取研究,"韩客松,王永成,沈洲,吴芳芳","为适应Internet时代和大规模文献处理的需要,以中文文本为处理对象,研究了从主题词、主题概念和主题句三个不同层面自动抽取文本主题的方法,着重讨论了加权体系和一些经验值的获取方法。对新闻类文献做了实验,并简单进行了性能分析。","主题词,主题概念,主题句,加权"
2000-09-06,问答篇章生成系统中的用户模型和文本规划,"吴华,黄泰翼","在问答生成系统中,如果系统首先了解用户对问题所涉及的领域知识的掌握程度,系统则能根据这些知识组织文本,生成符合用户需要的内容,更好地进行人机交互。本文以花卉知识查询系统为基础,探索了用户知识对生成结果的影响,以及用户模型与文本规划之间的相互作用。实验结果表明:用户知识模型不但影响生成的内容,而且影响生成内容的风格。在此系统中,我们采取两种基本生成策略:Schema方法和Process方法,并探讨这两种生成方法的相互结合过程。","用户模型,文本规划,汉语生成"
2000-09-25,校园导航系统Easy Nav的设计与实现,"黄寅飞,郑方,燕鹏举,徐明星,吴文虎","本文介绍了校园导航口语对话系统EasyNav的设计与实现。在分析了口语对话系统的特点和要求之后,我们提出了适合于对话系统的基于规则的语言理解流程。在这一流程中,句法分析使用GLR分析器处理上下文无关文法(CFG),获取句子结构特征以便为语义分析服务,句法规则照顾到覆盖率和准确率间的平衡。语义分析使用考虑句法约束条件的模板匹配方法,以获取话者意图为目标,并消除句法分析引入的歧义。这一设计的优点是系统容易搭建,也容易扩展。","口语对话系统,语音理解,句法分析,模板匹配"
2000-10-09,一种现代藏文笔段提取算法,"王浩军,赵南元,邓钢轶","针对藏文字符笔段的几何特征和拓扑结构,本文提出了一种基于字符轮廓信息的藏文笔段提取算法:通过链码跟踪的方法得到笔段轮廓的点列,然后从点列中提取特征点并利用特征点切分出笔段,最后用笔段的轮廓线代替骨架线来表征藏文的笔段。本算法用于印刷体藏文笔段提取,取得了良好的效果,避免了传统细化算法所造成的畸变,提高了笔段提取的抗干扰能力,并减小了计算量,加快了特征提取的速度。","文字识别,藏文识别,笔段提取,轮廓信息"
2000-10-13,基于HMM的联机汉字识别系统及其改进的训练方法,"刘家锋,黄健华,唐降龙","本文描述了一个基于HMM模型的联机汉字识别系统的设计思想与实现方法。系统以联机汉字的笔段序列作为观察序列,采用带有多跨越的模型结构消除自由书写汉字笔段序列的冗余与丢失问题。HMM模型的训练是本系统设计的一个重要问题,针对复杂HMM模型参数训练容易收敛于局部最小的情况,本文结合联机汉字识别的特点,提出了一种利用“引导模型”进行训练的改进方法,避免了训练过程收敛于局部最小点的发生。经过大量样本的训练,本系统对规范书写汉字和自由书写汉字均取得了比较令人满意的结果。","隐含Markov模型,联机汉字识别"
2000-02-19,“炎黄”中文平台结构设计,"吴健,孙玉芳,李国华,李祥凯","随着我国计算机应用水平的提高, Internet的迅速普及, GB2312 - 80中的6763个汉字已不能满足应用的需要。ISO 10646标准的制定,使得为开发支持大汉字字符集的中文平台提供了宽阔的代码空间。","ISO 10646标准,大字符集,GBK,中文平台,跨平台"
2000-07-03,蒙古文整词计算机生成理论研究,S·苏雅拉图,"采用面向对象方法,模拟传统蒙古文整词各种形式构成机理,提出了几种蒙古语整词计算机生成数据模型。文章主要依据整词计算机生成三种模型,探讨了传统蒙古文整词计算机最优化生成理论所涉及的精确度、时间复杂度、空间复杂度三项基本要素以及最优化生成必须考虑的整词复杂特征载荷与一体化合一计算知识表示方法和计算结构,证明了“B - J - T= W”数据模型是传统蒙古文整词计算与生成最优化对象模型。","拼音文字整词,生成模型,精确度,时间度,空间度,优化"
2015-11-24,中国中文信息学会第五次全国会员代表大会暨学会成立二十周年学术年会征文通知,,"今年是中国中文信息学会成立二十周年, 初步定在2001年11月11-13日召开中国中文信息学会第五次全国会员代表大会暨学会成立二十周年学术年会。(征文内容详见《中文信息学报》2001年第2期p30)",
2000-10-30,现代汉语分词系统通用接口设计与实现,"娄珽,宋柔,李卫亮,罗智勇","现代汉语文本自动分词是中文信息处理的重要基石,为此提供一个通用的分词接口是非常重要的。本文提出了通用分词接口的目标,论述了它的原理和设计方案。该系统已经初步实现。","中文信息处理,汉语分词,通用接口"
2001-02-19,书面汉语分词连写的合理性与紧迫性及其实现,"李辉阳,韩忠愿,周经野","本文结合信息处理技术的发展,指出在书面语中采用分词连写的合理性和紧迫性,提出应将这一思想纳入相应的中文信息处理标准中,并在一些未来的信息平台(如eBook、WWW)上加以体现。同时对分词连写在具体实施时所面临的如何适应人们长期以来形成的读写习惯问题,给出了一个可行的解决办法。","分词连写,中文信息平台,中文信息处理"
2001-03-21,基于HowNet的事件角色语义特征提取,"郝秀兰,杨尔弘,舒鑫柱","本文提出了一种将HowNet中事件的主要特征与实体的主要特征联系起来的方法——为事件类定义角色语义表,从而将HowNet的事件类与语义解释联接起来。文中给出了角色语义表的形式描述、一个角色语义表获取算法,并举例说明了角色语义表的应用。","HowNet,角色语义表,事件类,实体类,特征提取"
2001-03-07,数据库自然语言查询系统Nchiql中语义依存树向SQL的转换,"孟小峰,王珊","本文介绍了关系数据库受限自然语言查询系统NChiql中语义依存树向SQL的转换算法。文章首先介绍了集合块的概念、划分方法以及集合块向SQL的转换算法,然后再给出最大集合块的再次划分方法,最后形成完整的转化算法。","语义依存树,SQL语言,自然语言接口"
2001-04-19,一种新的基于统计的词典扩展方法,"周正宇,李宗葛","在建立统计语言模型时,往往会遇到词典的词汇量不够的问题。对于医学等专业领域的语料,这一问题尤为严重。针对这一问题,本文提出了一种新的基于统计的识别新词方法——右边缘扩展法。该方法对分词后的语料中产生的连续单字词进行关联范数估计,利用右边缘扩展的方法判断词的边界。在实验中,我们将右边缘扩展法与基于Witten-Bell back off方法的两两合并法相结合,循环地调整词典,优化语言模型。实验结果表明,该算法具有很高的识别正确率与检出率,可以有效地识别出语料中出现的新词汇,尤其是专业术语。","词典,关联范数估计,右边缘扩展法,语言模型"
2001-02-26,文本数字水印,"黄华,齐春,李俊,朱伟芳","目前数字水印技术的研究和文献主要集中在静止图像和视频的保护等方面,文本数字水印研究的很少,国内甚至还未见到文本数字水印的相关文献。而实际上,一些文本文档比图像、视频等更需要得到保护;数字文本的保护对互联网时代的政府工作和电子商务等也具有重要意义。本文主要介绍文本数字水印技术的基本思想和目前的研究状况,首先介绍了文本数字水印的嵌入与检测方法,然后分析了用于中文的文本数字水印的研究方向以及可能的应用前景。","文本数字水印,版权保护"
2000-07-23,适用于信息设备的汉字输入法研究,"倪小东,李人厚,余克艰,庞宣明","当前,小电器产品和移动通讯产品都朝着数字化和网络化方向发展,特征之一是允许企业和用户之间、用户和用户之间可以进行交互式的信息交换,汉字输入对于这类产品在中国的推广应用是非常重要的。本文介绍了一种适用于数字键盘上使用的汉字输入技术,它由基于数字键盘的英文、全拼和前导拼音输入法组成,能够用于各类信息设备进行大量中英文混合信息的方便、快速输入。本文首先描述了输入法设计思想,然后分析了其性能和特点。","汉字输入法,数字键盘,信息设备,中文信息处理"
2001-04-20,二字词词义组合推理方法的研究,"郑家恒,钱揖丽,李竞","汉字是表义文字,具有丰富的语义内容,汉字是一个有限的封闭集,它的数目是有限的,而汉语的词是一个开放系统,它是无限的。本文以“字义基元化、词义组合化”为基本思想,从字义着手,研究二字词词义组合。首先以经过整理的《现代汉语规范字典》、《现代汉语词典》和《同义词词林》为资源,从中自动搜索、抽取出二字词词义组合,建立汉字字义、词义知识库,然后再采用《同义词词林》的语义体系,通过语义相关度等的计算确定它们的组合类型,为研究二字词词义的组合提供一定的参考价值。","词义,语义相关度,二字词词义组合,词汇学"
2001-04-29,一种词义与词的混合语言模型及其应用,"侯珺,王作英","本文提出了一种基于词和词义混合的统计语言模型,研究了这个模型在词义标注和汉语普通话语音识别中的性能,并且与传统的词义模型和基于词的语言模型进行了对比。这个模型比传统词义模型更准确地描述了词义和词的关系,在词义标注中具有较小的混淆度;在汉语普通话连续音识别中,这个词义模型的性能优于基于词的三元文法模型,并且需要较小的存储空间。","统计语言模型,词义模型,词义标注,语音识别"
2001-07-02,基于SVM和k-NN结合的汉语交集型歧义切分方法,"李蓉,刘少辉,叶世伟,史忠植","本文提出了基于支持向量机(SVM)和k-近邻(k-NN)相结合的一种分类方法,用于解决交集型伪歧义字段。首先将交集型伪歧义字段的歧义切分过程形式化为一个分类过程并给出一种歧义字段的表示方法。求解过程是一个有教师学习过程,从歧义字段中挑选出一些高频伪歧义字段,人工将其正确切分并代入SVM训练。对于待识别歧义字段通过使用SVM和k-NN相结合的分类算法即可得到切分结果。实验结果显示使用此方法可以正确处理91.6%的交集歧义字段,而且该算法具有一定的稳定性。","支持向量,类代表点,交集型歧义,汉语自动分词"
2001-04-06,汉英翻译系统英文生成中选词模型的设计,"陈毅东,李堂秋,洪青阳,郑旭玲","本文描述了一种基于实例比较,辅以语义模式匹配的英文选词模型的设计。首先,我们讨论了汉英翻译系统英文生成中选词的重要性,然后比较了几种可能的选词策略并提出我们的选词模型,接着我们较详细地描述了生成词典的结构以及选词算法。文中,我们还简要介绍了我们所使用的语义知识资源——《知网》。","基于实例,语义模式,相似度,知网"
2001-05-31,双语交叉分类模型的设计与实现,"林鸿飞,王剑峰","利用交叉分类机制共享因特网上各种语言的信息资源是知识挖掘的重要方法,本文给出了双语交叉分类的模型以及实现方法。其主要思想是不需要进行机器翻译和人工标注,利用文本特征抽取机制提取类别特征项和文本特征项,通过基于概念扩充的对译映射规则自动生成类别和文本特征向量,在此基础上利用潜在语义分析,将双语文本在语义层面上统一起来,通过类别与文本的语义相似度进行分类。从而获取较高的精度。","双语交叉文本分类,概念扩充,潜在语义分析,空间向量模型"
2001-06-18,距离加权统计语言模型及其应用,"金凌,吴文虎,郑方,吴根清","本文在统计语言模型构造中,提出了将词间距离信息结合到N-gram统计语言模型中的思路,并称之为距离加权的关联词统计语言模型。该模型可以考虑一个句子中非相邻词之间的关系,基于“词距越近关系越密切”的原则,通过距离加权函数来引入距离信息,提高模型的预测能力。本文还将其应用到一个中文整句拼音输入法系统中。实验表明,该模型与传统的N-gram统计语言模型相比,汉字误识率有所降低,模型性能有了一定提高。","N-gram,关联词模型,距离加权,数据平滑"
2001-01-17,自然语言处理中逻辑词的知识图分析,"张蕾,李学良,刘小冬","知识图是一种新的知识表示方法。本文从本体论的角度出发,将知识图的本体论分别与Aristotle、Kant和Peirce的三种知识表示的本体论进行了比较,表明知识图方法的有效性以及本原性,说明知识图是一种更为一般的知识表示方法。从知识图本体论的观点,研究了各类逻辑词的知识图表示。本文结合汉语的特点,从结构的角度,研究并揭示了逻辑词的共性和规律性。进一步阐明知识图“结构就是含义”的思想。逻辑词的知识图分析将为自然语言分析中词典的建立奠定基础。","自然语言处理,知识图,本体论,逻辑词,词图"
2001-06-14,IBM大型机与小型机间汉字转换解决方案,"翟凌慧,马少平","本文描述了在IBM的大型机ES/9000(基于MVS/VSE操作系统)与小型机RS/6000(基于AIX操作系统)间通过CICS传输中文数据存在的数据转换问题,分析了汉字EBCDIC码与汉字ASCII码单纯通过CICS配置不能正确转换的原因,给出了两种解决方案:第一种方案通过CICS程序、JAVA程序、CICS配置结合实现汉字转换;第二种方案只通过JAVA程序、CICS配置实现汉字转换。","汉字转换,CICS,Java,EBCDIC,IBM ES/9000,RS/6000,1381,935"
2002,基于特征加权的应力影响下顽健语音识别方法,"张磊,韩纪庆,王承发,张文祥","通过对应力影响下语音数据的分析,发现不同的特征维对变异的敏感程度不同。一般低维特征对变异比较敏感,相应的高维特征敏感程度差些。在此基础上,提出一种新的基于特征加权的变异语音识别方法。该方法通过对不同维特征加不同的权值来消除变异因素对语音特征的影响,从而提高系统的识别性能。文中提出对线性权值用最大相对熵估计方法获得权值。对航空模拟飞行器中采集的特定话者小词表孤立词的实验,最大相对熵估计方法的识别率可达到89.9% ,与多重风格训练方法相比,识别率提高了13.1%。","语音识别,应力影响,特征加权,最大熵相对估计"
2002,朗读语料与自然口语的差异分析,"刘亚斌,李爱军","本文通过对朗读语音语料库ASCCD、自然口语独白语音语料库CASS和自然口语对话语音语料库CADCC的统计分析,试图说明朗读语料与自然口语的主要差异。文章主要对二者在音节、声韵、副语言学和非语言学现象、语篇话题、话轮转换、基频变化以及音段音变现象等几个方面作了一些统计分析,并由此归纳出朗读语料与自然口语的几点不同。","语音语料库,自然口语,朗读语篇,韵律,音段"
2002,基于子带信息的鲁棒语音特征提取框架,"张欣研,王帆,郑方,徐明星,吴文虎","本文提出一种鲁棒语音特征提取框架。通过使用一种基于子带能量分布的噪声估计方法,无需静音段,就可以估计出带噪语音的子带噪声,同时提出结合谱减和谱加权方法对特征进行处理,最终生成具有较高鲁棒性的特征。","语言识别,噪声估计,鲁棒语音特征"
2002,基于相空间重构的语音特征研究,"陈亮,张雄伟","本文通过重构语音信号相空间。研究语音的相似序列重复度及其熵信息,分析比较了语音信号在相空间中的非线性特征。根据清音和浊音在多维相空间中的不同空间分布特性,对语音音素进行了分类。利用语音信号在相空间中的非线性特征可以为语音识别研究提供一个新的方向。","混沌,相空间重构,相似序列重复度,熵信息"
2002,唇读中序列口型的分类,"单卫,姚鸿勋,高文","本文针对汉语中所有声韵母发音序列中的连续口型提出了一种口型分类的思路。在建立了覆盖所有声韵母的汉语双模态语料库的基础之上,本文提出了一种两次分类的方法,对语料库中的图像进行唇的分割、定位及特征提取,并依靠选择的特征,将声韵母的发音序列中的口型聚为15类。本文的目的是在此分类的基础上,明确唇读识别阶段的状态数,减小搜索的空间,提高收敛速度。","唇读,双模态语料库,口型聚类,语音识别"
2002,广播语音的音频分割,"贾磊,穆向禺,徐波","本文的广播电视新闻的分割系统分为三部分:分割、分类和聚类。分割部分是采用本文提出的基于检测熵变化趋势的分割算法来检测连续语音音频信号的声学特征跳变点,从而实现不同性质的音频信号的分割。这种检测方法不同于传统的需要门限的跳变点检测方法,它是以检测一定窗长的信号内部的每一个可能的分割点所分割的两段信号的信号熵的变化趋势来检测音频信号声学特征跳变点的,可以避免由于门限的选择不当所带来的分割错误。分类部分是采用传统的基于高斯混合模型(GMM)的高斯分类器进行分类,聚类部分采用基于矢量量化(VQ)的说话人聚类算法进行说话人聚类。应用此系统分割三段30分钟的新闻,成功的实现了连续音频信号的分割,去除掉了所有的背景音乐,以较高的精度把属于同一个人的说话语音划归为一类,为广播语音的分类识别打下了良好的基础。","广播语音的音频分割,声学特征跳变点检测,基于BIC准则的声学特征跳变点检测,熵变化趋势"
2002,汉语韵律边界的声学实验研究,"胡伟湘,徐波,黄泰翼","本文以带有韵律标注的语料库ASCCD为基础,从语音信号分析的角度,研究了汉语普通话韵律间断模式在语音的时长、基频和音强等三个方面的表现特征,并在大量统计分析的基础上建立了识别分类的决策树模型,实验证明,这些特征能较好地描述朗读话语的韵律间断模式。","韵律边界,韵律结构,决策树"
2002,基于最大似然模型插值的快速说话人自适应算法,"吕萍,王作英,陆大金","本文提出了一种新的说话人自适应算法——最大似然模型插值。其基本思想是,利用语音单元间的相关性,根据最大似然准则由一组说话人相关模型的线性组合得到测试者的说话人自适应模型。接着介绍了此插值框架下的两种具体自适应算法:均值线性插值算法和矩阵线性插值算法。实验证明上述算法有良好的收敛性,在只有3句自适应数据时便能使识别系统的性能有较大提高。","连续语音识别,说话人自适应,最大似然模型插值,均值线性插值算法,矩阵线性插值"
2002,基于词图树扩展的语音命令理解及其容错算法的研究,"陈俊燕,李涓子,王作英","本文对计算机语音命令理解的算法作了一些探索性的研究。首先针对词图结构的特点提出了一种词图树扩展理解算法,通过分析与实验比较,发现该算法在保证精确率的下降很小的条件下可获得比传统的Nbest路径理解算法高得多的召回率,而计算效率仅相当于Nbest路径理解算法中句子候选数取值很小时的情况;其次根据对实验结果的分析与观察,给出了一种行之有效的命令理解容错算法,使得理解召回率提高到91.7% ,精确率仍保持在90%以上,而理解错误率降低了13.5% ,同时计算复杂度的上升几乎可以忽略。","语音命令,N-best路径理解算法,词图扩展,自顶向下的图表句法分析方法,容错"
2002,一种在线递增式语言模型自适应方法,"吴根清,郑方,金凌,吴文虎","本文针对传统统计语言模型的离线自适应方法,提出了一种在线实时的递增式自适应方法。该自适应方法需要解决几个问题。第一是要设计一种语言模型结构以适应在线的自适应;第二是如何利用在线收集到的语料对语言模型进行实时的参数修改;在我们设计的中文音转字平台中,将语言模型分成两个部分,分别是通用模型和用户模型。对于通用模型,采用高效的存储结构结合参数预取技术,提高了模型的速度;对于用户模型,使用动态的加权方法结合MAP 动态调整参数。本文所做的实验证明使用该方法能较大程度的降低中文音转字的错误率。","统计语言模型,N-gram,自适应,语音识别"
2008-07-07,基于树核函数的实体语义关系抽取方法研究,"庄成龙,钱龙华,周国栋","该文描述了一种改进的基于树核函数的实体语义关系抽取方法,通过在原有关系实例的结构化信息中加入实体语义信息和去除冗余信息的方法来提高关系抽取的性能。该方法在最短路径包含树的基础上,首先加入实体类型、引用类型等与实体相关的语义信息,然后对树进行裁剪,去掉修饰语冗余和并列冗余信息,并扩充所有格结构,最后生成实体语义关系实例。在ACE RDC 2004基准语料上进行的关系检测和7个关系大类抽取的实验表明,该方法在较大程度上提高了实体语义关系识别和分类的效果,F值分别达到了79.1%和71.9%。","计算机应用,中文信息处理,实体关系抽取,树核函数, 语义信息"
2008-05-22,基于条件随机场的汉语动宾搭配自动识别,"程月,陈小荷","该文提出一种基于机器自动学习的统计模型条件随机场的方法用于汉语动宾搭配的自动识别。实验比较了两种分词与词性标记集下的识别效果,并增加了词性筛选准则作为优化处理。在特征选择上,考察了动词次范畴特征、上下文特征以及它们之间的组合特征的不同实验结果。综合实验结果,基于树库分词和词性标记的最好结果F值是87.40%,基于北京大学标准的分词和词性标记的最好结果F值是74.70%。实验表明,条件随机场模型在词语搭配实例自动识别方面有效可行。","计算机应用,中文信息处理,动宾搭配,自动识别,条件随机场,特征模板"
2008-04-21,分布式策略与CRFs相结合识别汉语组块,"黄德根,于静","该文提出了一种基于CRFs的分布式策略及错误驱动的方法识别汉语组块。该方法首先将11种类型的汉语组块进行分组,结合CRFs构建不同的组块识别模型来识别组块;之后利用基于CRFs的错误驱动技术自动对分组组块进行二次识别;最后依据各分组F值大小顺序处理类型冲突。实验结果表明,基于CRFs的分布式策略及错误驱动方法识别汉语组块是有效的,系统开放式测试的精确率、召回率、F值分别达到94.90%、91.00%和92.91%,好于单独的CRFs方法、分布式策略方法及其他组合方法。","计算机应用,中文信息处理,组块识别,条件随机域(CRFs),分布式策略,基于CRFs的错误驱动,浅层句法分析"
2008-04-19,指代消解中语义角色特征的研究,"王海东,胡乃全,孔芳,周国栋","该文实现了一个基于机器学习的指代消解平台,并在此基础上着重研究了语义角色特征对指代消解的影响。该文使用ASSERT①语义角色标注系统得到语义角色标注信息,然后在原型系统的基础上加入语义角色特征。为了分析语义角色特征对指代消解的影响,该文还分析了语义角色特征和指代链特征以及代词细化特征的结合对系统的影响。通过把先行语和照应语在句子中所作的语义角色特征加入机器学习系统中进行研究,该文发现语义角色特征能够显著提高系统的性能,特别是对代词的消解有很好的效果。在ACE 2003 NWIRE基准语料上的所有类型名词短语的指代消解测试表明,召回率提高了3.4%,F值提高了1.8%。","计算机应用,中文信息处理,指代消解,语义角色,指代链,机器学习"
2007-11-07,一种改进的Wu-Manber多关键字匹配算法,"莫德敏,刘耀军","针对Wu-Manber算法在处理公共子后缀模式情况下的不足,该文提出了一种基于非空公共子后缀模式的处理算法。该算法把有非空公共子后缀的模式汇集在一起,进一步减小了next链表的平均长度。在匹配过程中减少了字符比较的次数,从而提高算法的运行效率。该文对搜狗实验室给出的相关文档进行全文检索实验,并和原Wu-Manber算法、孙晓山等提出的改进算法进行比较。实验结果表明,该文提出的改进算法有效地减少了匹配过程中字符比较的次数,从而提高匹配的速度和效率。","计算机应用,中文信息处理,Wu-Manber算法,多关键字匹配,模式匹配,字符串匹配"
2008-03-12,藏文自动分词系统中紧缩词的识别,才智杰,"在藏文信息处理中,涉及句法、语义都需要以词为基本单位,句法分析、语句理解、自动文摘、自动分类和机器翻译等,都是在切词之后基于词的层面来完成各项处理。因此,藏文分词是藏文信息处理的基础。该文通过研究藏文自动分词中的紧缩词,首次提出了它的一种识别方案,即还原法,并给出了还原算法。其基本思想是利用藏文紧缩词的添接规则还原藏文原文,以达到进行分词的目的。该还原算法已应用到笔者承担的国家语委项目中。经测试,在85万字节的藏文语料中紧缩词的识别准确率达99.83%。","计算机应用,中文信息处理,紧缩词,藏文分词,还原法,格助词"
2008-06-10,基于序列相交的短语译文获取,"王辰,宋国龙,吴宏林,张俐,刘绍明","短语译文获取技术是基于实例的机器翻译(EBMT)中的核心技术之一,其准确率直接影响到EBMT系统的性能。该文提出了一种基于序列相交的短语译文获取方法,该方法将句子视为词的序列,利用对中日句对齐语料库中包含待译短语的所有源语句子对应的目标语句子进行序列相交的方式,在不需要词对齐、句法分析及词典等资源的情况下,通过充分挖掘句对齐双语语料库的信息,获得高质量的短语译文。实验表明,该方法获得的短语译文准确率超过80%。","计算机应用,中文信息处理,EBMT,短语译文获取,序列相交"
2008-03-19,基于多模型融合的人名翻译系统,"庞薇,徐波","该文提出了一种基于加权有限状态转化器(WFST)的多模型融合人名翻译框架。该框架以两个基于字符的转换模型和两个基于发音的转换模型为核心,通过加权有限状态转换器将多模型进行融合实现对人名的翻译。与单个模型相比,该文提出的方法的优势在于通过从各种信息源得到的数据价值的最大化。实验结果表明,基于多模型融合方法的人名翻译的错误率比单一模型的人名翻译的错误率降低了7.14%。","计算机应用,中文信息处理,多模型融合,音译,命名实体,加权有限状态转换器"
2008-05-25,基于锚信息的生物医学文献双语摘要句子对齐,"陈相,林鸿飞","双语句子对齐在双语语料库的处理中有着非常重要的地位,是构建双语词典的第一步工作。该文利用基于带权二部图的最大权重匹配模型为生物医学文献双语摘要建模。在无双语词典的情况下,将基于长度的句子对齐方法和句子的位置信息相结合,充分利用医学文献双语摘要语料中的锚信息,将生物医学摘要段落和句子进行分类计算相似度,实现了生物医学文献双语摘要的句子对齐,取得了较好的实验结果。","计算机应用,中文信息处理,句子对齐,二部图,双语语料,相似度"
2008-03-24,基于团模型的文档重排算法研究,"付剑波,王明文,罗远胜,张华伟","为了满足用户对信息检索结果准确不断提高的需求,尽可能应用那些与查询及检索结果有关的信息进行查询结果优化是一种有效的手段。查询扩展和结果重排就是利用附加信息进行检索结果优化的方法。该文提出了基于文档团的文档重排模型(DCRM模型),此模型通过对文档集的学习,构造文档与文档关系的Markov网络,提取出文档Markov网络中的“文档团”,应用文档团信息进行文档重排。在adi、cacm、med、cisi和cran五个数据集上的实验结果表明,本文提出的基于文档团的文档重排模型较BM25模型性能得到有效提高。","计算机应用,中文信息处理,Markov网络,文档团,文档重排"
2008-05-06,一种基于时间流特性的垃圾邮件过滤方法,"徐隽,郑佳谦,姚静,牛军钰","垃圾邮件过滤具有处理规模巨大,数据无限递增、动态变化等流数据特征,传统的垃圾邮件过滤方法利用静态的文本特征提取方法,无法体现流数据特征随时间动态变化的特点。该文提出一种基于时间流特性来实时调整有效特征的垃圾邮件过滤方法,在TREC Spam Track语料集上的测试结果表明,该方法在保证垃圾邮件过滤高准确率的同时,使垃圾邮件过滤计算的时间性能和空间性能更加优化。","计算机应用,中文信息处理,垃圾邮件,流数据,时间流,文本分类,特征选择"
2008-01-12,面向事实性问题的答案选择技术研究综述,"董燕举,蔡东风,白宇","答案选择是问答系统的一个关键步骤,它的任务是从候选答案集中选择出最佳答案返给用户,其主要研究内容包括答案选择的标准、方法及评价。该文首先介绍了主要的答案选择标准,分析了答案选择标准与问答系统评测之间的关系。然后将答案选择策略分为基于冗余的策略、基于相似性的策略和基于推理的策略,分别对每种策略的主要答案选择方法和特点进行了概述。随后又介绍了答案选择的评价指标及答案验证评测。最后讨论了答案选择所面临的主要问题,并对其未来的发展方向进行了展望。","计算机应用,中文信息处理,综述,自然语言处理,问答系统,答案选择,答案验证,答案选择标准"
2008-05-31,倾向性分析用于金融市场波动率的研究,"王超,李楠,李欣丽,梁循","互联网金融信息对于金融市场的影响在当代已经越来越不可忽视。面对海量的信息,其中大部分为非结构化的文本数据,该论文结合目前已有的文本倾向性算法,把信息的褒贬值作为外部变量加入到针对股价波动率建立的时间序列模型中去,对金融市场的股价波动率进行预测。实验揭示出金融市场波动率与互联网上金融新闻的相关性,并且提出了一种有效的股市预测方法。","计算机应用,中文信息处理,文本倾向性,SVM,金融预测"
2008-06-11,统计与规则相结合的古文对联应对模型,"张开旭,孙茂松","该文将古文对联规则区分为硬规则与软规则,将软规则区分为字相对与上下文相对。并在软规则指导下建立对联应对的有向概率图模型,使用EM(Expectation-Maximization)算法估计模型参数,在求解的搜索过程中加入硬规则,从而给出了一种完整的对联自动应对方法。实验结果表明参数学习后的候选字列表由于一定程度上不考虑上下文相对的影响,比仅用频次统计的候选字列表更为合理。该方法还能够对训练语料库中工整与不工整的对联区分学习。基于该方法所实现的古文对联应对程序达到了一定水平。","计算机应用,中文信息处理,对联应对,最大熵马尔可夫模型"
2008-01-12,无存贮音码汉字字符串的音码反查实现技术,李永平,"该文用无存贮音码汉字字符串的音码反查实现技术,解决以往汉字型字符串在通过首音码或拼音全码进行反查时需要事先存贮汉字型字符串的音码作为辅助记忆码;解决一个汉字型字符串在转换音码时产生多个编码时需要选择问题;解决一个汉字型字符串由于转换时重码选择建立者与使用者的歧义问题。该文在GBK汉字库范围内,以首音码为例,使用汉字首音码库作为字典,实现无存贮音码汉字字符串的音码反查实现技术。","计算机应用,中文信息处理,首音码,重码,汉字,字符串,歧义"
2008-02-10,汉语大词汇量连续语音识别系统研究进展,"倪崇嘉,刘文举,徐波","大词汇量连续语音识别(LVCSR)技术近年来发展迅速,并在许多领域得到了广泛的应用,国内外许多大公司加大了对语音识别技术的研究,不少商业化的语音识别系统已经面世,并得到较为广泛的使用。该文综述了近年来大词汇量连续语音识别技术的研究进展,描述了汉语大词汇量连续语音识别系统,主要是基于统计方法的语音识别系统的框架与设计方法,对语音识别系统的一些关键技术和原理进行了分析,并对近年来国内外对语音识别研究发展动向进行了讨论。","计算机应用,中文信息处理,综述,语音识别,模型自适应,搜索技术"
2008-04-19,计算机模拟汉字字形认知发展过程的研究,"陈静,穆志纯","对汉字的认知研究不仅是认知科学,也是计算机科学特别是人工智能领域中的一个研究热点。但是,目前汉字认知的计算机模拟研究还相对滞后。该文采用自组织特征映射网络(Self-organizing Feature Map, SOFM)和自适应谐振理论(Adaptive Resonance Theory, ART)相结合的方法,构建汉字认知过程的发展模型,对汉字字形认知的发展过程(学习发展历程)进行了计算机模拟,以便研究汉字字形学习过程中的某些认知发展规律。模型通过训练,显示出了汉字认知发展过程中的某些规律。","计算机应用,中文信息处理,认知科学,人工智能,汉字认知发展,计算机模拟,自组织模型,自适应谐振理论"
2009-01-15,命名实体识别、排歧和跨语言关联,赵军,"命名实体是文本中承载信息的重要语言单位,命名实体的识别和分析在网络信息抽取、网络内容管理和知识工程等领域都占有非常重要的地位。有关命名实体的研究任务包括实体识别、实体排歧、实体跨语言关联、实体属性抽取、实体关系检测等,该文重点介绍命名实体识别、排歧和跨语言关联等任务的研究现状,包括难点、评测、现有方法和技术水平,并对下一步需要重点解决的问题进行分析和讨论。该文认为,命名实体识别、排歧和跨语言关联目前的技术水平还远远不能满足大规模真实应用的需求,需要更加深入的研究。在研究方法上,要突破自然语言文本的限制,直接面向海量、冗余、异构、不规范、含有大量噪声的网页信息处理。","计算机应用,中文信息处理,命名实体识别,命名实体排歧,命名实体跨语言关联"
2008-08-16,基于最大熵的依存句法分析,"辛霄,范士喜,王轩,王晓龙","该文提出并比较了三种基于最大熵模型的依存句法分析算法,其中最大生成树(MST)算法取得了最好的效果。MST算法的目标是在一个带有权重的有向图中寻找一棵最大的生成树。有向图的每条边都对应于一个句法依存关系,边的权重通过最大熵模型获得。训练和测试数据来源于CoNLL2008 Share Task的公用语料。预测的F1值在WSJ和Brown两个测试集上分别达到87.42%和80.8%,在参加评测单位中排名第6。","计算机应用,中文信息处理,句法分析,最大生成树,最大熵"
2008-08-11,基于语法分析和统计方法的答案排序模型,"李波,高文君,邱锡鹏","该文描述了一种构建问答式检索系统中答案排序模型的新方法。该方法结合了基于密度方法的度量特征和外部知识库,并且引入了基于语法分析方法的语法关键路径的新特征,使用支持向量机回归模型训练评估函数。实验证明,引入了上述语法关键路径特征后的新答案排序模型的排序性能有了明显提高。","计算机应用,中文信息处理,自动问题回答,语法关键路径,答案排序,支持向量机"
2008-08-16,面向协作式问答的问题理解技术研究,"张宇,赵鑫,刘挺","问题理解是问答系统中的重要组成部分,尤其对于协作式问答。在协作式问答中用户对所提出的问题进行了详细的说明和描述。如何利用这些描述信息来提高系统的性能,是一个很重要的问题。该文提出了一种基于词典和句法分析的方法,来对用户的问题进行分析,从中提取出有价值的关键词,以提高包含候选答案网页的召回率。通过实验对比分析,该方法的MPP值和MAP值都有了较大的提高。","计算机应用,中文信息处理,协作式问答,问题理解,句法分析"
2008-08-30,基于汉语框架网的旅游信息问答系统设计,"李茹,王文晶,梁吉业,宋小香,刘海静,由丽萍","该文借助汉语框架网(Chinese FrameNet,简称CFN)在语义表达方面的独特优势,探讨用本体描述语言建立面向特定领域的汉语框架语义知识库,并且以旅游交通领域中问答系统设计为例分析方法的有效性。方法中首先利用TREC分类与本体分类相结合的方式为查询问句分类,然后提出基于CFN的问句分析策略,通过CFN语义分析得到问句中三元组语义谓词、语义主体和语义客体,在问句分析的基础上从旅游本体知识库中对答案进行抽取并对答案处理,同时用本体编辑工具Protégé编码,实验证实方法是有效的。","计算机应用,中文信息处理,汉语框架网,本体,问答系统"
2008-08-31,基于字符语言模型的垃圾邮件过滤,"苏绥,林鸿飞,叶正","基于内容的过滤是当前解决垃圾邮件问题的主流技术之一。该文先简单综述了当前基于内容的垃圾邮件过滤中采用的各种技术,在此基础上提出将基于字符的语言模型应用于垃圾邮件过滤任务中,并通过实验对比了该方法与Nave Bayes、SVM和基于词的语言模型方法的性能差异,以及不同n值、不同特征选择方式对过滤结果的影响。实验结果表明,基于字符的语言模型实现简单且具有很高的性能,能较好地满足大规模在线邮件系统的需要,具有很高的实用价值。","计算机应用,中文信息处理,垃圾邮件过滤,语言模型,朴素贝叶斯,支撑向量机,n-Gram"
2008-08-31,基于核偏最小二乘分类的垃圾邮件过滤,"岑芳明,王明文,王鹏鸣,戴玉娟","垃圾邮件是Internet上亟待解决的问题,目前许多垃圾邮件过滤技术已经被使用。基于偏最小二乘的方法可以解决垃圾邮件的内容中普遍存在的数据稀疏性、高特征维数和多重相关性问题。但邮件内容之间的内在联系往往不是线性的,该文通过在偏最小二乘方法上引入核函数,去解决这一类的非线性问题。Enron-Spam垃圾数据集实验表明,同PLSR等方法比较,模型表现出了较好的过滤性能。","计算机应用,中文信息处理,垃圾邮件过滤,非线性,核偏最小二乘,回归,分类,潜在语义"
2008-08-27,网页搜索引擎查询日志的Session划分研究,"张磊,李亚楠,王斌,李鹏,蒋在帆","搜索引擎查询日志中的session (以下简称session)是指某特定用户为得到某个信息需求而在一段时间内的搜索行为的连续序列。Session的正确划分是进行用户搜索行为分析等一系列工作的重要基础,目前尚没有关于session的系统研究工作。本文针对相关研究工作的问题重新统一定义了session的概念并进行探索和比较研究,得出结论(1)统计语言模型因数据稀疏问题不适合做session划分;(2)利用多种属性的决策树方法可以得到比较理想的结果,以session为单位进行评价,F值达到了78.6%。","计算机应用,中文信息处理,网络信息检索,查询日志,session划分"
2008-08-21,基于人工标注的个性化检索系统评测的研究,"张宇,范基礼,郑伟,邹博伟,刘挺","个性化信息检索可以根据用户的检索兴趣返回个性化的检索结果。该文构建了个性化检索标注系统和个性化检索评测系统,生成个性化检索系统所需的语料集;并提出了以用户为中心的基于人工标注的个性化检索评价方法。个性化检索评测系统采用了NIST所建立的评价体系,根据用户的标注结果对个性化检索系统的性能进行自动评价,并给出量化、直观的性能指标。","计算机应用,中文信息处理,个性化信息检索,以用户为中心,评价方法"
2008-08-30,潜在语义索引中特征优化技术的研究,"季铎,郑伟,蔡东风","潜在语义索引被广泛应用于信息检索、文本分类、自动问答等领域中。潜在语义索引是一种降维方法,它把共现特征映射到同一维空间上,而非共现特征映射到不同的空间上。在潜在语义索引的语义空间中,共现特征通过文档内部以及文档之间的特征传递关系获得。该文认为这种特征传递关系会引入一些不存在的共现特征,从而降低潜在语义索引的性能,应该对这种特征传递关系进行一些选择,削除不存在的共现特征信息。该文采用文档频率对文档集合进行特征选择,用Complete-Link聚类算法在两个公开语料上进行三个实验,实验结果显示,保留文档频度的10%～15%时,其F1值分别提高了6.577 0%,1.992 8%和3.361 4%。","计算机应用,中文信息处理,潜在语义索引,共现特征,奇异值分解,特征选择"
2008-09-01,一种新的基于中间语义的跨语言信息检索模型,"黄国斌,王明文,叶浩","目前的跨语言信息检索能够使用的方法有四种查询词翻译的方法、文档翻译的方法、中间语言翻译方法和非翻译的方法。该文对这四种方法进行了简要介绍,提出它们的优缺点,并且提出了一种新的非翻译的方法——基于中间语义的方法。我们对提出来的方法进行了TREC跨语言语料库的试验,并且与单语言的信息检索模型进行了比较。试验证明我们的方法具有很好的性能和健壮性。","计算机应用,中文信息处理,跨语言信息检索,中间语义,潜在语义对,偏最小二乘,TREC"
2008-08-29,基于后缀树的Web检索结果聚类标签生成方法,"骆雄武,万小军,杨建武,吴於茜","对检索结果进行聚类能够方便用户从搜索结果中快速地找到自己需要的信息,当前已有各种聚类方法和系统被广泛使用,但是,现有大部分方法由于聚类标签的可读性和描述性较差,难以达到预期效果。该文提出了一种新的思路,注重于如何在聚类之前就产生好的标签,在生成了标签的基础上,再进行检索结果聚类。对于搜索引擎返回的结果,我们先统一建立一棵后缀树,然后计算后缀树中各个短语的得分,选取得分最高的若干短语作为候选标签。得到标签后,将搜索引擎返回的各个结果项分配到它所包含的标签对应的分类中,形成最后的聚类。实验表明,我们的方法是比较有效的。","计算机应用,中文信息处理,检索结果聚类,聚类标签生成,后缀树"
2008-08-28,基于用户兴趣的寻找虚拟社区核心成员的方法,"陈海强,程学旗,刘悦","发现虚拟社区中的核心成员对于社区数据挖掘等应用问题有着相当重要的应用价值。为解决该问题,作者首先分析了一些虚拟社区中成员的兴趣相似性分布情况,从中发现核心成员间的兴趣存在相对较高的相似性。据此,作者提出了基于兴趣集中性的核心成员求解算法,并在豆瓣网的虚拟社区中进行了实验分析,实验结果证明了算法的有效性。","计算机应用,中文信息处理,虚拟社区,核心成员,兴趣相似性"
2008-08-31,基于目的分析的作弊页面分类,"余慧佳,刘奕群,张敏,马少平,茹立云",随着互联网的飞速发展，因网络作弊而产生的垃圾页面越来越多，严重影响了搜索引擎的检索效率和用户体验。反作弊已经成为搜索引擎所面临的最重要挑战之一。但目前的反作弊研究大都是基于页面内容或链接特征的，没有一个通用可行的识别方法。本文主要基于作弊目的的分析，给出作弊页面另一种体系的分类，为基于目的的作弊页面识别起到良好的导向作用。,"计算机应用,中文信息处理,命名实体识别,命名实体排歧,命名实体跨语言关联"
2008-08-23,中文比较句识别及比较关系抽取,"宋锐,林鸿飞,常富洋","比较是一种具有一定说服力的评估方式,利用机器进行比较句的识别以及比较关系的抽取可以对观点挖掘、信息推荐等应用提供重要的依据。该文通过构建中文比较模式库以实现中文比较句的自动识别。在此基础上,该文通过选取比较主体、比较客体及其上下文的词、词性、位置、语义以及比较属性的领域知识等特征,利用条件随机域模型进行中文比较关系抽取。实验结果表明,中文比较模式库的构建有助于比较句的自动识别,而在词、词性、位置等Baseline特征中融入语义、领域知识及启发式规则特征后,基于条件随机域的比较关系抽取结果有了显著的提高。","计算机应用,中文信息处理,中文比较句识别,比较关系抽取,中文比较模式库,条件随机域"
2008-08-26,基于Wikipedia的语义元数据生成,"韩先培,赵军","语义元数据提供数据的语义信息,在数据的理解、管理、发现和交换中起着极为重要的作用。随着互联网上数据爆炸式的增长,对自动元数据生成技术的需求也就变得更加迫切。获得目标语义元数据及得到足够的训练语料是使用自动生成技术的两个基本问题。由于获得目标语义元数据需要专家知识,而获得足够的训练语料需要大量的手工工作,这也就使得这两个问题在构建一个成功的系统时至关重要。该文基于Wikipedia来解决这两个问题通过分析一个类别中条目的目录表(table-of-contents)来抽取目标语义元数据,通过对分析文档结构和赋予目标结构正确的语义元数据来构建训练语料库。实验结果表明,该文的方法能够有效地解决这两个问题,为进一步的大规模的语义元数据应用系统打下了坚实的基础。","计算机应用,中文信息处理,元数据,语义元数据,数据处理,语料库构建,语义标注"
2008-09-01,词汇间语义相关关系量化计算方法,"钟茂生,刘慧,刘磊","词汇间语义关系的定量化研究是自然语言处理任务中一个重要的基础性工作。词汇间语义关系总体上分为等同关系、上下位关系、相关关系,现有的语义关系定量化工作主要集中于词汇间语义的等同关系(相似性)量化研究。该文研究和提出了量化词汇间语义相关关系的基本思路和新方法,即构造词汇相关关系二分图来求解和量化词汇间间接相关关系,该方法能够解决在统计语料中没有出现的词汇对的相关关系量化求解问题。实验结果表明,该文提出的方法比单纯用互信息来计算和量化词汇间语义相关关系更为可行。同时,对于一个特定词汇而言,该文的方法能够得到一个相关关系量化的相对合理的趋势性结果。","计算机应用,中文信息处理,词汇间语义关系,相关关系,互信息,二分图,量化方法"
2008-09-01,一种基于谱聚类的共指消解方法,"谢永康,周雅倩,黄萱菁","本文针对中文共指消解的具体任务,提出采用谱聚类的方法进行共指消解。首先,在待消解项对上抽取特征,使用最大熵模型判断两个待消解项存在共指关系的概率;然后,以此概率值作为相似度进行谱聚类;最后,得到若干实体,实现共指消解。该方法能从全局的角度进行实体划分,有效的提高准确率。在ACE2007标准数据集上的Diagnostic实验结果表明该方法的ACE Value比baseline方法有了2.5%的提高,Unweighted Precision值有5.4%的提高。","共指消解,谱聚类,最大熵模型"
2001-11-20,中文金融新闻中公司名的识别,"王宁,葛瑞芳,苑春法,黄锦辉,李文捷","在金融领域信息抽取中,公司名扮演着非常重要的角色;因此如何正确识别文本中出现的公司名是一个非常重要的研究课题。在对金融新闻文本进行了深入地分析和研究的基础上,总结出了公司名的结构特征及其上下文信息,建立了六个用于识别公司名的知识库,并提出了一个基于两次扫描过程的识别策略。初步实验结果表明,在封闭测试中实验系统公司名识别的精确率可以达到97.3% ,召回率可达89.3%;在开放测试中精确率可以达到62.8% ,召回率可达62.1%。","公司名,金融领域,专名识别,信息抽取"
2001-09-17,运用文本领域的常识改善基于支撑向量机的文本分类器性能,"李辉,史忠植,许卓群","本文提出了一种提高中文文本分类器推广性能的方法。一般而言,采用机器学习的方法对文本集合进行训练,可以获得文本分类器。本文引入了文本语义不变性常识,并将其融合到文本分类器中,提出了改进文本分类器的方法。与支撑向量机相结合,设计并实现了改进的文本分类器。对中文文本分类的实验表明,文本语义不变性常识的运用有效地改善了分类器的推广性能。","文本分类,同语义文档子段替换,人工文档样本,相容性条件,支撑向量机"
2001-08-06,汉语文本形式结构分析及其标引算法,单永明,"本文从形式化的角度讨论了汉语文本的形式结构及有关的基本概念,给出了文本的标题、子标题、段落及其层次结构的一种划分与标记方法,提出了规范的与准规范的文本等概念,并以此为基础讨论了文本形式结构的标引问题,给出了两个标引算法。本文阐明的方法和结果对汉语文本的全文文本标引及结构化分析具有直接的现实意义。","中文信息处理,文本结构分析,标引树,自动标引算法"
2001-05-16,基于神经元网络的汉语短语边界识别,"奚晨海,孙茂松","短语边界的识别是浅层句法分析或组块分析的基础,对真实文本的处理具有重要意义。在一个含有64426词的汉语树库的支持下,本文设计并实现了基于神经元网络的汉语短语边界自动识别模型。初步实验结果显示,该模型的界定准确率为93.24%(封闭测试)和92.56%(开放测试)。","汉语短语边界自动识别,神经元网络,中文信息处理"
2001-08-23,汉语情感意义的机器标注研究初探,"应英,周锋,周昌乐","本文将情感计算引入到汉语的机器理解中,在已有的汉语机器理解研究的基础上,采用多重松弛迭代计算方法,对汉语情感意义的标注问题进行了研究,通过语境信息的利用,构建了一个实验性系统并取得了较准确的词语情感标注,为后续的句子情感意义的理解提供了基础,拓宽了汉语机器理解的研究范围。","汉语机器理解,情感标注,多重松驰迭代算法,学习及纠错机制"
2001-10-22,一种基于日语格语法表示的英语生成,"沈逸海,陈家骏,戴新宇,王启祥","本文在已开发的一个具有一定规模的基于转换翻译的日汉机器翻译系统的基础上,为了检验该系统的日语分析结果的表达能力,设计一个基于日语格语法表示的英语生成系统。文章首先描述了一种基于格语法的日语分析及其表示;然后给出了从该格语法表示的日语生成英语的生成过程,重点对生成规则的设计进行描述;最后对英语生成中的一些问题进行探讨。","机器翻译,日英,格语法,生成,规则"
2001-10-15,“是”字句主语和宾语的自动界定,"吴云芳,段慧明,俞士汶","“是”字句是现代汉语中比较特殊的、又是比较常见的一种句子形式,对其主语部分和宾语部分的自动界定和标注将有助于机器翻译、信息检索、信息提取等的研究。本文通过考察语料中“是”字句的句法表现,总结、提取了“是”字句的自动标注规则,对《人民日报》一个月语料中的“是”字句进行了自动标注。实验结果表明,对没有逗号的句子,标注正确率可达到99%以上;对有逗号的句子,标注正确率为89%。","“是”字句,自动标注,浅层分析"
2001-08-21,一种面向口语的译文质量自动评价方法,"程葳,徐波","译文质量的自动评价对机器翻译研究具有十分重要的意义。但现有方法主要是针对书面语翻译,没有考虑到口语翻译的特征。因此,本文提出了一种面向口语的新型的自动评价方法,通过定义信息段、标注权重和设计多种匹配策略等方法,使自动评价结果与人工打分更为接近,同时也提高了评价过程对不同输出译文的适应能力。各项实验表明,该算法对译文质量变化具有较高的敏感度,而且可以对输出译文质量作出与手工评判较为接近的评价结果。","机器翻译评测,口语翻译,自动评价"
2001-06-27,基于数学形态学的自适应文字版面分析方法,"刘飞,罗予频,胡东成","随着光学字符识别能力的提高,处理日渐复杂的版面成为文件处理系统中的关键部分。针对中文版面的特点,在基于组件的版面分析方法基础上,本文提出了一种具有自适应能力的基于数学形态学中膨胀变换的版面分析方法。该方法对基于组件版面分析的核心部分- 文字合并进行了自适应扩展,使其具有对不同字体大小、间距等样张更大范围的适应能力。","数学形态学,膨胀变换,组件,版面分析"
2001-09-17,基于知识的银行票据二值化方法,"徐蔚然,张洪刚,刘刚,郭军","本文结合银行票据OCR系统的开发,提出一种基于知识进行银行票据二值化的新思路,并针对各类识别域具体构造了一整套二值化方法。通过在银行票据OCR系统中的应用,验证了本文二值化方法的效果。","知识,二值化,文字识别,金融票据"
2001-12-30,基于多策略分析的复杂长句翻译处理算法,"黄河燕,陈肇雄","在实用机器翻译系统的研究开发中,复杂长句的翻译处理是其面临的一个主要难题。本文提出一种多语种通用的基于多策略分析的复杂长句翻译处理算法,该算法通过基于实例模式匹配和规则分析相结合的方法,综合利用源语言句子中多种相关的语言特征,包括语法语义特征、句子长度、标点符号、功能词以及上下文语境条件等对复杂长句进行切分简化处理和译文的复合生成。另一方面,通过对不同语种设计相同的知识表示形式,实现该算法对不同语种翻译系统的通用性。","机器翻译,多策略分析,长句切分简化处理"
2001-11-23,向量空间模型中特征词的区分度的定量研究,"游荣彦,邓志才,李传宏","本文提出了关于一个词的文本类间频率的概念,给出一个词在文本分类中的区分度的定义,讨论了区分度的性质,提出了选择特词新的方法,定义了特征词的权重,建立了向量空间模型的一套加权距离分类规则。实验结果表明,本文的方法是有效和有用的。","文本分类,向量空间模型,Bayes后验概率,加权距离"
2001-08-27,基于FIFA算法的文本分类,"朱靖波,姚天顺","本文提出了一种简单有效的文本分类方法,其中采用基于FIFA算法的内容主题分析技术,实现文本的自动分类过程。文中详细论述了文本自动分类的基本过程和FIFA算法描述,最后给出了文本自动分类的实验结果和评价。","FIFA算法,主题识别,文本分类,自然语言处理"
2001-12-04,基于粗集的汉语词语义项知识的获取,"杨尔弘,郝秀兰,李盛","由于自然语言语序的灵活性,使得自然语言知识的自动获取很困难。本文基于粗糙集理论的属性值约简方法,结合基于记忆的学习(Memory Based Learning,简称MBL) ,提出了一种汉语多义动词义项知识的获取方法,用该方法获得的知识可用于词义消歧。","粗集,记忆学习,汉语多义动词,知识获取,自然语言处理"
2001-10-17,基于News ML的大规模个性化新闻定制研究,"朱友芹,祁宁,夏国平","本文分析了多媒体新闻信息新标准NewsML的组成及用法,将知识经济时代工业产品大规模个性化定制思想引入新闻信息发布系统中,应用智能计算、元数据、多维数据立方体等先进技术,提出了一个基于NewsML的大规模个性化新闻定制原型系统。","NewsML,大规模定制,新闻定制"
2001-12-28,基于拼音模型的声学层识别的研究,"黄顺珍,方棣棠","本文介绍拼音模型的原理及应用。拼音模型是累加语言模型中同音字的相关数据后得到的3元模型,是在原来的声学模型和语言模型之间增加的一个新环节,可用来求取相关拼音串的先验概率,实验结果表明,用它作为声学层识别的后处理,可使第1名的识别率提高13个百分点,可使前5名的识别率与原来声学模型输出前10 的识别率相当。","声学模型,拼音模型,语言模型,连续语音识别"
2001-06-27,Outline字体结构式压缩算法及其实现,"宋晓丹,罗予频","针对CJK Outline字体在存储量上存在的不足,本文提出一种结构式压缩算法。算法对CJK字体进行集合变换,得到笔划集合元素;并利用聚类算法得到模板笔划;对相似数据进行统一存储与调用。同时,本文还提出了一种基于笔划段的笔划抽取算法,从图论角度实现了集合变换。结果显示,算法取得了较好的效果,而且适用于多种字体。","结构式Outline字体,字体压缩,笔划抽取"
2001-11-14,《全衡》词典的设计与建设,"张小衡,张群显","《全衡》是第一个较全面考虑香港和国际的需求的网上汉字输入系统,其核心部件是词典。《全衡》使用的是一部拥有六万余词条的词典,每一词条讲述一个词语,信息包括该词语的简体字形式、繁体字形式、汉语拼音表达式、粤语拼音表达式、仓颉输入法代码、速成输入法代码等。由其中任何一项入手,借助于系统中的检索程序可以方便地查找其它各项信息。这不仅有力地支持了汉字输入,对于汉语学习也很有帮助。本文简要介绍《全衡》的词典建设。","网上汉字输入,词典编辑,汉语拼音,粤语拼音,简体字,繁体字"
2015-11-25,古文字信息化处理国际学术研讨会,张再兴,由教育部人文社科重点研究基地华东师范大学中国文字研究与应用中心主办的“古文字信息化处理国际学术研讨会”于2001年10月26-29日在上海召开。这是迄今为止在汉字古文字计算机信息化处理研究领域中首次国际范围的重大学术聚会。来自美国、法国、加拿大、韩国等国家和中国香港、中国台湾地区的十余位专家与中国大陆从事古文字及古文字信息化处理研究的六十余位学者一起就古文字信息化处理各个方面的问题进行了广泛而深入的研讨。,
2002,电子词典与词汇知识表达,陈克健,"词汇知识的表达与取得是自然语言处理极须克服的问题,本论文提出一个初步的架构与常识的抽取机制。语言处理系统是以词为讯息处理单元,登录在词项下的讯息可以包括统计、语法、语义、常识等。语言分析系统利用〈词〉为引得取得输入语句中相关词汇的语法、语义、常识等信息,让语言处理系统有更好的聚焦能力,可以藉以解决分词歧义、结构的歧义。对于不易以人工整理取得的常识,本论文也提出计算机自动学习的策略,以渐进式的方式累积概念与概念之间的语义关系,来增进语言系统的分析能力。这个策略可行的几个关键技术,包括(1)未登录词判别及语法语义自动分类, (2)词义分析, (3)应用语法语义及常识的剖析系统。","词汇知识表达,知识抽取"
2002,中文概念词典的结构,"于江生,俞士汶","中文概念词典(Chinese Concept Dictionary ,简称CCD)是北京大学计算语言学研究所开发的与WordNet兼容的汉语语义词典。本文着重描述了CCD的结构:CCD中的“概念”用同义词的集合定义,CCD的主关系——概念之间的继承关系(即上下位关系)和一些附加关系使得CCD形成一个概念网络,其上的演绎规则是严格形式化了的,可应用于中文的语义分析。","概念,同义词集合,CCD,WordNet,计算词典学"
2002,中文词义关系的定义与判定原则,"蔡柏生,黄居仁,曾淑娟,林贞仪,陈克健,庄元珣","在英语及其它的欧洲语言里,词汇语意关系已有相当充分的研究。例如,欧语词网( EuroWordNet ,Vossen 1998) 就是一个以语意关系来勾勒词汇词义的数据库。也就是说,词汇意义的掌握是透与其它词汇语意的关连来获致的。为了确保数据库建立的品质与一致性,欧语词网计画就每一个处理的语言其词汇间的词义关系是否成立提出相应的语言测试。实际经验显示,利用这些语言测试,人们可以更容易且更一致地辨识是否一对词义之间确实具有某种词义关系。而且,每一个使用数据库的人也可以据以检验其中关系连结的正确性。换句话说,对一个可检验且独立于语言的词汇语意学理论而言,这些测试提供了一个基石。本文中,我们探究为中文词义关系建立中文语言测试的可能性。尝试为一些重要的语意关系提供测试的句式和规则来评估其可行性。这项研究除了建构中文词汇语意学的理论基础,也对Miller的词汇网络架构(WordNet ,Fellbaum 1998) 提供了一个有力的支持,这个架构在词汇表征和语言本体架构研究上开拓了关系为本的进路。","词义,语意关系,语言(为本的)测试"
2002,以词汇知识驱动的词网自动对映,柯淑津,"机读字典蕴藏着非常丰富的词汇语意知识,这些知识可由自动化方式粹取出来,有效地利用在各种自然语言处理相关研究上。本研究提出一套方法,以英文版的WordNet 作为基本骨架,结合比对属类词与比对定义内容两种技巧,将WordNet同义词集对映到朗文当代英汉双语词典之词条。并藉由这个对映将WordNet同义词集冠上中文翻译词汇。在实验部分,我们依岐义程度将词汇分为单一语意与语意岐义两部分进行。在单一语意部分的实验结果,以100%的涵盖率计算,可获得97.7%的精准率。而在语意岐义部分,我们得到85.4%精准率,以及63.4%涵盖率的实验结果。","词汇网络,机读字典,统计处理,属类词,自动对映"
2001-12-24,一种基于混合分析的汉语文本句法语义分析方法,"尹凌,姚天昉,张冬茉,李芳","本文提出了一种领域相关的汉语文本句法语义分析方法。根据领域文本的特点,该方法将浅层句法分析和深层句法语义分析结合在了一起。其浅层句法分析部分采用有限状态层叠的方法,将文本中的命名实体识别出来,从而大大减轻了深层分析部分的负担。其深层句法语义分析部分将语义分析和语法分析结合起来,主要依靠词汇搭配信息来决定句子的结构。该方法在解决领域相关文本的短语结构歧义方面取得了较好的试验结果。","浅层句法分析,深层句法分析,有限状态层叠,分语义场"
2002-01-18,汉英法律文献的子条级自动索引和对齐,"吕学强,李清隐,陈文亮,姚天顺","本文提出了基于结构标识的法律文献层次结构模型,该模型描述了汉英法律文献的层次结构特征及章、条、子条的连续性和对应性。根据该模型实现了汉英法律文献的子条级自动索引和对齐,系统具有纠错和容错能力。实验结果表明每篇文献的平均索引时间为3.31ms ,对齐准确率为98.6%。与基于词汇的方法结合后,对齐准确率为99.3%。","文本索引,文本对齐,汉英法律文献,结构标识,层次结构模型"
2002-01-25,藏文识别中相似字丁的区分研究,"王维兰,丁晓青,祁坤钰","相似字丁多是藏文识别中的一大难点。本文通过对相似字丁类型的研究,以及印刷体藏文识别结果的统计分析,得到图形结构的分析与识别结果相吻合的结论。说明必须根据藏文字丁的结构特点,在字符归一化、特征选择方面进行特殊的处理,以实现藏文识别中相似字丁的区分。","藏文识别,相似字丁,归一化,特征选择"
2008-08-28,一种基于谱聚类的共指消解方法,"谢永康,周雅倩,黄萱菁","该文针对中文共指消解的具体任务,提出采用谱聚类的方法进行共指消解。首先,在待消解项对上抽取特征,使用最大熵模型判断两个待消解项存在共指关系的概率;然后,以此概率值作为相似度进行谱聚类;最后,得到若干实体,实现共指消解。该方法能从全局的角度进行实体划分,有效地提高准确率。在ACE 2007标准数据集上的Diagnostic实验结果表明该方法的ACE Value比baseline方法有了2.5%的提高,Unweighted Precision值有5.4%的提高。","计算机应用,中文信息处理,共指消解,谱聚类,最大熵模型"
2008-08-19,基于可信度的中文完整词自动识别,"王芳,万常选","中文自动分词是中文信息检索中预处理工作的一部分,也是中文信息检索技术中的重要问题之一。针对在信息检索中完整词整体表达更有意义、更能体现用户查询目的的问题,结合完整词的成词特点,将互信息和完整词前后缀的计算,与组成完整词的可信度相关联,提出基于可信度的三种中文完整词自动识别方法,分别构成基于全信度、偏信度,以及前两者加权平均的混信度的完整词识别方法,设计及实现了基于可信度的三种完整词自动识别中文分词原型系统。最后给出了对第二届SIGHAN(2005)北京大学测试集语料的各项实验测试结果和分析,结果表明该原型系统的识别性能良好,且能同时满足多种性能的需求。","计算机应用,中文信息处理,中文分词,互信息,可信度,自动识别"
2008-08-28,基于统计信息的未登录词的扩展识别方法,"韩艳,林煜熙,姚建民","该文提出一种基于网络资源的未登录词的扩展识别方法。该方法以左右邻信息判断未登录词边界为基础对已识别出的二元候选未登录词种子进行扩展,从而得到不限长度的语义更完整的未登录词。实验证明该文方法可行有效。","计算机应用,中文信息处理,未登录词识别,左右邻信息,最频繁左邻比,最频繁右邻比,候选OOV扩展"
2008-09-03,面向主题爬取的多粒度URLs优先级计算方法,"陈竹敏,马军,韩晓晖,雷景生","垂直检索系统中主题爬虫的性能对整个系统至关重要。在设计主题爬虫时需要解决两个问题一是计算当前页面与给定主题的相关度, 二是计算待爬取URLs的访问优先级。对第一个问题,给出利用页面的主题文本块和相关链接块的相关度计算方法; 对第二个问题, 给出基于主题上下文和四种不同的粒度(即站点级、页面级、块级和链接级)的优先级计算方法。在此基础上, 提出基于上述方法的主题爬取算法。实验证明, 新算法在不增加时间复杂度的前提下, 在查准率和信息量总和方面明显优于其他三种经典的爬取算法。","计算机应用,中文信息处理,主题爬取,优先级计算,网页分块,相关度计算"
2008-08-30,基于“VASE”特征词的网络查询分类研究,"王俞霖,孙乐,李文波","网络查询分类对提高搜索引擎的搜索质量有重要的意义。该文通过对真实用户查询日志的分析和标注,发现四种特征词(称之为“VASE”特征词)对查询分类起决定性作用。我们提取特征词并构造了一个特征词倒排索引,用于对查询进行主题分类。在此基础之上,提出了基于网络扩展和加权特征词的方法改善分类的效果。实验结果显示,基于此分类方法的正确率和召回率分别达到78.2%和77.3%。","计算机应用,中文信息处理,网络查询分类,“VASE”特征词,网络扩展,加权特征词"
2008-08-26,基于向量距离的词序相似度算法,"董刊生,方金云","手机POI搜索已经成为手机搜索的主要应用之一。该文结合手机搜索的特点以及POI数据的结构性特征采用简拼进行POI搜索。由于词序相似度是影响简拼搜索排序结果的主要因素,该文提出了基于向量距离计算词序相似度的算法。该算法采用空间向量模型作为简拼的表示方法,将提取的公共简拼映射为位置向量,进而利用位置向量间的距离计算词序相似度。通过理论分析,该算法相比基于逆序数的词序相似度算法,将时间复杂度由O(nlogn)降为O(n),空间复杂度由O(n)降为O(1)。实验结果表明,基于向量距离的词序相似度算法有效地保证了准确性,可以满足手机POI简拼搜索的应用需求,并在性能上将词序相似度的计算效率提高16.88%。","计算机应用,中文信息处理,手机POI搜索,简拼搜索,词序相似度,向量距离"
2008-08-31,一种面向流分类的特征选择算法,"李文法,段洣毅,刘悦,孙春来","流分类技术在网络安全监控,QoS,入侵检测等方面起着重要的作用。流分类器处理的数据含有大量的相关与冗余特征,这不仅增加了分类器的计算复杂性,同时也影响了分类器的分类效果。针对高维特征空间,特征选择一方面可以提高分类精度与效率,另一方面可以找出富含信息的特征子集。该文提出一种wrapper型特征选择算法VFSA-C4.5来构建轻量级的流分类器。该算法采用快速模拟退火VFSA搜索策略对特征子集空间进行随机搜索,然后以提供的数据在C4.5上的分类正确率作为特征子集的评价标准,来获取最优特征子集。在流数据集上进行的大量实验结果表明,基于VFSA-C4.5的流分类器在不影响分类性能的情况下能够提高分类速度。","计算机应用,中文信息处理,流分类,特征选择,快速模拟退火,决策树"
2008-08-16,人机互助的交互式口语翻译方法,"刘鹏,宗成庆","基于短语的统计翻译模型是目前机器翻译领域广泛使用的模型之一。但是,由于在解码时采用短语精确匹配的策略,造成了严重的数据稀疏问题,短语表中的大量短语无法得到充分利用。为此,该文提出了人机互助的交互式翻译方法。对于翻译短语表中找不到的短语,首先通过模糊匹配的方法,在短语表中寻找与其相似的短语。然后利用组合分类器,判断哪些相似短语可能提高句子的翻译质量。最后,通过人机交互的方法,选择可能提高翻译质量且保持原句语义的短语。在口语语料上的实验结果证明,这种方法可以有效地提高翻译系统的译文质量。","人工智能,机器翻译,口语翻译,基于短语的统计机器翻译,人机交互,模糊匹配"
2008-11-24,汉英词语对齐规范,"赵红梅,刘群,张瑞强,吕雅娟,隅田英一郎,吴翠玲","该文介绍了一个新的汉英词语对齐规范。该规范以现有的LDC汉英词语对齐规范为基础,对其进行了较大的改进和扩展,特别是提出了一种全新的对齐标注方法 —— 将词语对齐区分为真对齐和伪对齐,真对齐又分为强对齐和弱对齐。这种细化的标注方法能够更好地刻画词语对齐的特点。该规范已经实际应用于大规模的人工词语对齐标注中。我们对对齐标注的一致性进行了评价。结果表明,在该规范的指导下,标注者内部和标注者间的对齐都取得了比较理想的一致性,两组强、弱、伪三种对齐的Kappa值分别为0.99、0.98、0.93 和0.96、0.83、0.68。最后,一个简单的实验初步证实了该规范在统计机器翻译中的有效性。","人工智能,机器翻译,汉英词语对齐规范,手工词语对齐,真对齐,伪对齐,强对齐,弱对齐,对齐和标注一致性"
2008-08-24,一种错误敏感的词对齐评价方法,"黄书剑,奚宁,赵迎功,戴新宇,陈家骏","对齐错误率(Alignment Error Rate,AER)是目前通用的词对齐评价标准。近年来的研究表明,AER虽然在一定程度上能够反映词对齐的质量,但它与机器翻译最终结果BLEU得分的相关性并不好。该文针对基于短语的机器翻译系统(PBSMT)分析了AER可能存在的一些问题,并根据词对齐结果中存在的不同类型的错误,提出了一种错误敏感的词对齐评测方法ESAER(Error-Sensitive Alignment Error Rate)。实验表明,该文提出的ESAER与BLEU的相关性要远远好于AER。","人工智能,机器翻译,统计机器翻译,词对齐,评价标准,AER,错误敏感"
2008-07-21,隐喻字面语义表示与生成,"王金锦,杨芸,周昌乐","在隐喻理解中,隐喻字面语义表示是隐喻深层语义表示的前提;确切地说,隐喻字面语义表示语言作为隐喻计算的输入语言直接影响到隐喻的最终释义,因此隐喻字面语义表示对隐喻的机器理解有着重要的影响作用。但在国内学术界,还鲜有这方面的研究。鉴于此,该文结合汉语隐喻特点,从隐喻字面语义表示的角度出发,将汉语隐喻分为无嵌套隐喻和嵌套隐喻两种。并在分析隐喻字面语义(浅层语义信息和隐喻信息)的基础上,提出了隐喻角色依存表示语言作为隐喻字面表示语言,最后给出隐喻角色依存表示语言生成算法。实验表明,该方法引入到汉语隐喻解释机制中是富有成效的。","计算机应用,中文信息处理,隐喻字面语义,隐喻计算,无嵌套隐喻,嵌套隐喻,隐喻角色依存表示语言"
2008-08-11,具有焦点标记作用的“是”字句重音分布研究,"贾媛,李爱军,马秋武,熊子瑜","本研究以汉语中标记焦点的结构“[是[…XP…]]”为研究对象,通过声学和感知实验,系统地考察了这一句式所标记的焦点成分的重音位置及其声学表现。实验结果显示,该句式所标记的焦点位置,以韵律词为单位,音高的音域被整体拉大,后面的成分的音阶被陆续压低,被标记的焦点成分后面通常存在中间短语边界,而在第二个焦点标记前面,通常有语调短语边界。以实验结果为基础,本研究进一步讨论了语法学界争论较多的,关于焦点和重音关系的问题,研究指出,汉语中有标记的焦点位置通常有重音分布,有重音的位置一般伴有语调(音高音域)的变化,但重音和语调的变化不是确定焦点位置的依据。","计算机应用,中文信息处理,“是”字句,焦点,重音,焦点和重音的关系"
2007-10-16,基于小世界模型的复合关键词提取方法研究,"马力,焦李成,白琳,周雅夫,董洛兵","该文提出了一种新的基于小世界网络特性的关键词提取算法。首先,利用K最邻近耦合图构成方式,将文档表示成为词语网络。引入词语聚类系数变化量和平均最短路径变化量来度量词语的重要性,选择重要性大的词语组成候选关键词集。利用侯选关键词集词语位置关系和汉语词性搭配关系,提取出复合关键词。 实验结果表明该方法是可行和有效的,获取复合关键词比一般关键词所表达的含义更便于人们对文本的理解。","计算机应用,中文信息处理,小世界网络,词语网络 ,平均最短路径变化量,聚类系数变化量,复合关键词"
2009-01-22,基于规则的中文阅读理解问题回答技术研究,"李济洪,杨杏丽,王瑞波,张娜,李国臣","该文针对中文阅读理解问答中的时间、人物、地点、数值、实体、描述六类问题,制定了各类问题回答的启发式规则集。对规则集中每条规则赋予一个相应权值,利用正交表对各规则所对应的权值进行了调优选取,给出了各候选答案句基于相应规则的得分计算方法。该文方法在山西大学自主开发的中文阅读理解语料库CRCC v1.1 上进行了实验,在整个语料库上得到了83.09%的HumSent准确率。为了与文献[10]中的最大熵方法比较,该文在与文献[10]中完全相同的训练集上调优规则的权值,在相同的测试集上测试,最终得到HumSent准确率81.13%,比最大熵的方法高大约1%, 且在全部的六类问题上,该文方法的HumSent准确率都不低于最大熵方法。","计算机应用,中文信息处理,阅读理解,问答系统,规则,正交表"
2008-06-25,汉语零形回指研究综述,"黄娴,张克亮","回指研究一直是语言学研究的一个热点,回指解析则是文本信息处理中亟待解决的问题之一。传统语言学从句法、语用、篇章、认知角度出发对汉语零形回指进行了广泛的研究。在自然语言处理领域,针对汉语零形回指也有一些颇有影响的研究,如基于向心理论的零形回指解析算法,基于HNC理论的零形回指处理方法,以及基于DRT理论和语义分析等方法提出的汉语零形回指解析方法。该文从语言学角度对这些理论研究进行介绍,旨在指出语言信息工作者在注重工程实践的同时,应关注并借鉴语言学基础理论研究的成果,而从事中文信息处理的语言学家也应加强语言形式化的研究。","计算机应用,中文信息处理,零形回指,语言学,语言信息处理"
2008-06-23,基于SVMTool的中文词性标注,"王丽杰,车万翔,刘挺","SVMTool是建立在支持向量机(SVM)原理上的序列标注工具,具有简单、灵活、高效的特点,可以融入大量的语言特征。该文将SVMTool应用于中文词性标注任务,将基于隐马尔科夫模型的基线系统准确率提升了2.07%。针对未登录词准确率不高的问题,该文加入了中文字、词的特征,包括构成汉字的部首特征和词重叠特征,并从理论上分析了这两个特征的可行性,实验显示加入这些特征后,未登录词标注的准确率提升了1.16%,平均错误率下降了7.40%。","计算机应用,中文信息处理,词性标注,SVMTool,未登录词,偏旁部首"
2008-08-29,SMS-2008标注中文短信息库,"马旭,徐蔚然,郭军,胡日勒","随着短信息应用的普及,用户、运营商及政府管理部门均迫切需要智能短信处理工具。语料库是研究算法,开发系统,测试性能等必不可少的基础资源。但受到技术、版权保护、隐私权利等种种原因,目前还没有公开的标准短信息语料库。SMS-2008标注短信息库是本项目组在国内外率先建立的多用途中文短信息语料库,它包括原始语料库、预处理语料库、隐私标注语料库、内容标注语料库、错误标注语料库等。该语料库可用于短信语言现象研究、短信分类过滤算法研究、隐私保护算法研究、自动纠错算法研究等。","计算机应用,中文信息处理,中文短信息,标注语料库"
2008-08-09,多文档文摘中基于时间信息的句子排序策略研究,"徐永东,王亚东,刘杨,王伟,权光日","文摘句排序是多文档自动文摘中的一个关键技术,直接影响到文摘的流畅程度和可读性。文本时间信息处理是影响排序算法质量的瓶颈技术,由于无法获得准确的时间信息,传统的句子排序策略均回避了这一问题,而且均无法获得稳定的高质量的排序效果。对此该文从文本时间信息处理入手,首先提出了中文文本时间信息抽取、语义计算以及时序推理算法,并在此算法基础上,借鉴传统的主成分排列的思想和句子相关度计算方法,提出了基于时间信息的句子排序算法。实验表明该算法的质量要明显好于传统的主成分排列算法和时序排列算法。","计算机应用,中文信息处理,多文档自动文摘,句子排序,中文时间信息处理"
2008-08-03,话语标记的语体特征研究及应用,"孟晓亮,侯敏","话语标记作为一种常见的话语现象,已成为话语分析研究的重要课题。由于研究角度不同,人们对于话语标记的认识和分类至今仍存在较大差异。该文从语体的角度提出假设,认为话语标记具有一定的语体特征。为准确描写话语标记的语体特征,提出了“语体度”的概念。通过对采样话语标记在不同语体的语料中分布情况进行定量分析,证实了相当一部分话语标记具有明显的语体特征,并根据分析结果选择特征向量,采用Rocchio分类法对开放文本进行自动语体分类实验,正确率达到82.9%。事实证明话语标记的语体特征对文本分类具有一定的参考价值。","计算机应用,中文信息处理,话语标记,语体特征,语体度,相似度,文本分类"
2008-04-02,中文搜索引擎查询与反馈词语特征研究,"赖茂生,屈鹏","查询式是网络用户搜索时表达其信息需求的主要方式,系统提示的相关词则是用户改善查询的有效工具,该文以这二者为研究对象,从用户的使用行为入手对这二者的特征进行刻画和分析。首先使用日志挖掘的方法,对查询式进行总体的定量描述;进而通过定性分类将查询式中的高频词分为主体词和辅助词两大类,并比照问卷调查的研究结果,发现网络用户在搜索时大量地使用辅助词,主体词的内容相对集中,查询式的长度较短,结构相对简单。在对相关词的研究中,综合问卷调查和对比实验研究结果,发现被试者对搜索引擎提示的相关词认同程度高而应用程度低。该文为理解网络用户搜索时的语言使用提供了实证研究结果,并对搜索引擎索引的改善有一定的参考意义。","计算机应用,中文信息处理,中文搜索引擎,用户搜索行为,语言使用,日志挖掘,问卷调查,对比实验"
2008-06-26,一种基于随机森林的多视角文本分类方法,"田宝明,戴新宇,陈家骏","基于词的向量空间模型是文本分类中的传统的表示文本的方法。这种表示方法的一个缺点是忽略了词之间的关系。最近一些使用潜在主题文本表示的方法,如隐含狄利克雷分配LDA (Latent Dirichlet Allocation)引起了人们的注意,这种表示方法可以处理词之间的关系。但是,只使用基于潜在主题的文本表示可能造成词信息的损失。我们使用改进的随机森林方法结合基于词的和基于LDA主题的两种文本表示方法。 对于两类特征分别构造随机森林,最终分类结果通过投票机制决定。在标准数据集上的实验结果表明,相比只使用一种文本特征的方法,我们的方法可以有效地结合两类特征,提高文本分类的性能。","计算机应用,中文信息处理,文本分类,向量空间模型,隐含狄利克雷分配,集成分类,随机森林"
2008-05-27,用宋词实现高嵌入率文本信息隐藏,"余振山,黄刘生,陈志立,李凌君,杨威,赵欣欣","文本信息隐藏是将秘密信息隐藏到文本中的一种技术。与加密后的密文通常是无意义的一串编码不同,文本隐藏生成的隐写文本看起来与普通文本无异,不容易引人怀疑。但是因为文本本身的冗余度低,与图像、视频等载体相比,文本隐藏算法较少且容量偏低。该文提出了一个新的利用宋词的文本隐藏算法,并设计实现了由编码器、解码器、词典和词牌模板组成的系统。秘密信息被隐藏到在字数、行数、句子形式、格律和韵脚等方面符合某个词牌的隐写宋词中。系统在保证良好安全性的同时,嵌入率达到了16%。据我们所知,这是第一个利用特殊体裁的文本信息隐藏算法。","计算机应用,中文信息处理,信息隐藏,文本隐写,嵌入率,语义安全,宋词,词牌"
2008-08-05,WNCT:一种WordNet概念自动翻译方法,"王石,曹存根","WordNet是在自然语言处理领域有重要作用的英语词汇知识库,该文提出了一种将WordNet中词汇概念自动翻译为中文的方法。首先,利用电子词典和术语翻译工具将英语词汇在义项的粒度上翻译为中文;其次,将特定概念中词汇的正确义项选择看作分类问题,归纳出基于翻译唯一性、概念内和概念间翻译交集、中文短语结构规则,以及基于PMI的翻译相关性共12个特征,训练分类模型实现正确义项的选择。实验结果表明,该方法对WordNet 3.0中概念翻译的覆盖率为85.21%,准确率为81.37%。","人工智能,机器翻译,WordNet翻译,词汇翻译,翻译消歧,中文词汇知识库,中文信息处理"
2008-06-19,基于Level Set方法的西夏字轮廓提取,柳长青,"随着国内外对西夏研究的不断深入,收藏于世界各地的大批西夏古籍文献通过影印方式陆续出版。如何将这些西夏古籍文献进行数字化、文本化则有着极其重要的意义。首先利用平滑和细化算法对西夏影印文献进行了预处理,然后利用Level set方法对影印文献中的西夏字进行了轮廓提取。Level Set演化函数在空间方向上采用了四阶紧致差分逼近式离散,计算过程中加入了窄带算法及全局优化方法。实验表明,算法在不增加计算时间的基础上可以得到较精确的西夏字轮廓。","人工智能,模式识别,西夏文信息处理,Level Set方法,西夏字,轮廓提取,紧致差分"
2008-05-10,因子分析在基于GMM的自动语种识别中的应用,"付强,宋彦,戴礼荣","在自动语种识别中,测试语音中说话人和信道的差异,会对系统性能产生很大的影响。针对于此,该文通过引入因子分析技术,根据语种识别的特点,建立了描述该差异 (说话人差异和信道差异)的子空间的数学模型,并分别从特征域和模型域两个方面尝试消除该差异的影响。在最新的NIST LRE2007的测试任务中,相对于GMM-UBM基线系统,该文方法有效地提高了系统识别性能。在30s时长的测试中,等错误率(EER)相对降低36.5%。","计算机应用,中文信息处理,自动语种识别,高斯混合模型,因子分析"
2008-06-25,汉语韵律短语的时长与音高研究,"倪崇嘉,刘文举,徐波","语句和篇章的韵律结构和信息结构的分析及模型化是提高语音合成的自然度、降低自然语言识别错误率的关键。该文在带有韵律标注ASCCD语料库的基础上对韵律短语的时长和音高特性进行了研究,得到并验证了如下一些结论:(1)韵律短语边界对音节时长有明显的延长作用,不同声调对音节的时长延长作用不同,并且不同的重音级别对音节时长的延长作用也不同。(2)韵律短语边界处中断的时长在较小的韵律边界表现的更为明显。韵律短语的边界处发生了明显的音高重置现象,韵律短语的音高低线总是下降的,而音高高线只是在重音后下降,并且重音处的音域大而且音高高线的位置高。","计算机应用,中文信息处理,主要韵律短语,次要韵律短语,时长,音高"
2008-08-12,基于自适应频率规整的鲁棒说话人辨认研究,"李燕萍,唐振民,张燕,丁辉","该文提出了一种基于自适应频率规整的鉴别性特征提取算法。该方法通过对语音频谱的各个频带的鉴别性分析及其量化结果对各个频域进行自适应的频率规整,进行非均匀子带滤波设计提取鉴别性特征;同时在噪声环境下,在特征提取前端进行了预增强处理,解决了测试语音与训练语音失配的问题,保证了特征的正确提取。实验证明,该特征原理简单,稳定性好,对语音内容不存在依赖性,有良好的抗噪性能,并且结合预增强处理是有效的,能够进一步提高辨认系统的识别率和鲁棒性。","计算机应用,中文信息处理,说话人辨认,自适应频率规整,鉴别性特征,鲁棒性"
2008-11-03,错音检测及其在语音教学中的应用综述,"万济萍,肖云鹏,叶卫平","在学习语音的过程中,找出学习者发音的错误并加以改进是非常重要的。错音检测技术就是自动诊断语流中错误发音的技术,也是计算机辅助发音训练研究的主要内容之一。该文总结了错音检测技术的研究和应用现状,分别介绍了基于语音识别、基于错音网络和基于声学语音学的错音检测技术。在此基础上又介绍了错音检测技术在计算机辅助发音训练系统中的应用,以及汉语自动发音评估技术的发展。文章最后给出了作者的分析和建议。","计算机应用,中文信息处理,自动发音错误检测,计算机辅助语言学习,计算机辅助发音训练,发音评估,语音识别"
2008-07-20,维吾尔语单音节词复辅音声学分析,"哈妮克孜·伊拉洪,祖丽皮亚·阿曼,艾斯卡尔·艾木都拉","为了提高语音合成的自然度该文从文本分析模块入手,利用“维吾尔语语音声学参数库”,选择了带复辅音的63 个单音节词的声学参数,包括辅音时长和辅音强度,通过语音分析软件研究了维吾尔语复辅音的组合规律和声学规律,复辅音中两个辅音声学特征之间的声学区别等问题。从语言类型学的角度看,在现代维吾尔语带复辅音的单音节词中前辅音比后辅音短且前辅音比后辅音强是固定声学特征。可是复辅音的组合不是固定的,因为组成复辅音的音素有可能再增加。","计算机应用,中文信息处理,维吾尔语,复辅音,声学分析,声学参数"
2008-06-23,面向信息处理的藏文分词规范研究,"扎西加,珠杰","自动分词是藏文信息处理领域的一项基础课题,也是智能化藏文信息处理的关键所在。 在藏文信息处理“字词处理”层面上,需要解决词的切分问题,而词类划分的标准和词的正确切分是进行藏文文本处理的必要条件。为了便于计算机对自动分词、词性标注的辨认,该文首先要确定满足藏文信息处理中词类的需求,并根据藏文自身的词汇特点与构词规律,提出了较为系统、适用的分词规范。","计算机应用,中文信息处理,分词规范,藏文,信息处理"
2008-05-05,基于ISO/IEC 10646标准的藏文编码转换的设计与实现,"张青,黄鹤鸣,章登义","目前,国内少数民族地区的书报印刷行业大多使用北大方正、华光藏文排版系统。这些软件的编码各异,致使有限的藏文资源无法实现交换和共享,造成这种现象的原因是各种软件编码体系不一致。解决这个问题的根本途径是将各种不同体系的藏文编码转换为符合国际标准的编码。该文以华光Windows藏文字符编码为例,首先对每个藏文字符进行构字分析,然后采用分表分组技术构造出每个字符符合ISO/IEC 10646标准的编码序列,最后采用hash技术优化查询算法,实现非标准的藏文字符编码向标准编码序列转换。","计算机应用, 中文信息处理,藏文,字符集标准,编码转换,分表分组技术"
2008-08-11,EBMT中高效的维吾尔语单词散列表构造算法,"田生伟,吐尔根·依布拉音,禹龙","基于实例的机器翻译(EBMT)是一种高效的机器翻译方法,如何快速地从海量实例模式库中找出与待翻译句子相似的候选实例,是EBMT研究的关键技术之一。统计分析维吾尔语单词字母的分布特征,构造了基于维吾尔语单词的倒排索引散列表,在等概率条件下,平均查找长度为1.59;依据散列冲突的同义词在维吾尔语料中出现的频率作为权值,提出了一种新颖的解决散列冲突的算法同义词次优树算法。实验显示,算法的性能比传统的顺序查找和二分查找算法分别高出了27.5 %,21.8%,证明了该算法在EBMT中有较高的检索效率。","计算机应用,中文信息处理,EBMT,散列,平均查找长度,次优树"
2008-10-17,基于CRFs边缘概率的中文分词,"罗彦彦,黄德根","将分词问题转化为序列标注问题,使用CRFs标注器进行序列标注是近年来广泛采用的分词方法。针对这一方法中CRFs的标记错误问题,该文提出基于CRFs边缘概率的分词方法。该方法从标注结果中发掘边缘概率高的候选词,重组边缘概率低的候选词,提出FMM的奖励机制修正重组后的子串。在第四届SIGHAN Bakeoff 中文简体语料SXU和NCC上进行闭式测试,分别在F-1值上达到了96.41%和94.30%的精度。","计算机应用,中文信息处理,中文分词,条件随机场(CRFs),边缘概率,最大向前匹配(FMM),全局特征"
2008-09-22,形容词与名词的语义组合模型研究,"赵春利,石定栩","该文首先针对传统方法研究形名组合的不足,提出了理解形名组合的基本语义模式,即事物、属性值和属性域;其次,根据形名组合的理解模式和语料库的调查,从哲学理论和语言事实角度,把名词各自分成了主体、事体、物体、时空、逻辑五个次类,把形容词分成了主体、事体、物体、时空和评价五个次类;最后,借助于计算语言学的研究思想和语义语法的理论原则,构建了形容词次类与名词次类间语义匹配的形名语义组合模型。研究结果表明该形名语义组合模型能深入细致地揭示形容词与名词的组合规律。","计算机应用,中文信息处理,形名组合,属性域,语义组合模型,语义语法"
2008-09-25,基于SVM融合多特征的介词结构自动识别,"温苗苗,吴云芳","介词结构在汉语文本中出现频率很高,正确识别介词结构边界对句法分析、语音合成中的韵律短语划分有着重要意义。该文较为系统地探讨了汉语中常用介词的边界识别问题。利用支持向量机SVM模型,基于输出概率而不是简单的二分法来选择正确的后边界。探讨了不同的特征选择,并尝试加入语义信息等不同特征组合以提高识别准确率。对常用的68个介词进行边界识别实验,5折交叉验证的准确率达到90.95%,优于前人的识别结果。","计算机应用,中文信息处理,介词结构识别,支持向量机,语义类"
2008-10-13,Dirichlet 过程及其在自然语言处理中的应用,"徐谦,周俊生,陈家骏","Dirichlet过程是一种典型的变参数贝叶斯模型,其优点是参数的个数和性质灵活可变,可通过模型和数据来自主地计算,近年来它已成为机器学习和自然语言处理研究领域中的一个研究热点。该文较为系统的介绍了Dirichlet过程的产生、发展,并重点介绍了其模型计算,同时结合自然语言处理中的具体应用问题进行了详细分析。最后讨论了Dirichlet过程未来的研究方向和发展趋势。","计算机应用,中文信息处理,变参数贝叶斯模型,Dirichlet过程,Dirichlet过程混合模型,马尔可夫链蒙特卡罗 "
2008-09-18,基于树核函数的英文代词消解研究,"王海东,胡乃全,孔芳,周国栋","该文提出了一种基于树核的英文代词消解方法。针对结构化信息在指代消解中的重要作用,该文使用SVM提供的卷积树核函数自动获取句法结构信息,将句法树作为一个特征,和其他基本特征相结合。该文系统的分析了训练用例的过滤及不同的剪枝策略对模型性能的影响,同时还分析了树核函数对于几句之内的代词消解有比较好的结果。在ACE2004 NWIRE基准数据上进行实验的结果说明树核能显著地提高代词消解系统的性能,并且对一句之内的代词消解有较好的效果。","计算机应用,中文信息处理,指代消解,句法结构,树核函数,修剪策略"
2008-08-16,基于短语模糊匹配和句子扩展的统计翻译方法,"刘鹏,宗成庆","近几年来,基于短语的统计翻译模型在机器翻译研究中受到普遍关注,并取得了较好的翻译性能。但是,由于目前基于短语的翻译系统在解码时采用精确匹配的策略,常常导致数据稀疏,一方面,有些短语在训练获得的短语表中找不到精确的匹配,使其成为未知短语;另一方面,短语表中大量的短语无法得到充分的利用。为此,我们提出了基于短语模糊匹配和句子扩展的翻译方法。对于不存在于短语表中的短语,通过模糊匹配的办法,寻找与其相似的短语,然后将所有相似短语用于替换原短语,从而生成扩展句子,在此基础上对所有扩展的句子进行翻译。由于并不是所有扩展后的句子都能提高原始句子的翻译效果,因此,我们在句子翻译完成后设置了组合分类器用于选择最优翻译结果。实验证明,这种方法可以有效地提高翻译系统的译文质量。","人工智能,机器翻译,基于短语的统计机器翻译,模糊匹配,组合分类器"
2008-09-19,基于层叠条件随机场的旅游领域命名实体识别,"郭剑毅,薛征山,余正涛,张志坤,张宜浩,姚贤明","针对旅游领域,提出了一种基于层叠条件随机场模型的旅游领域命名实体识别方法。该方法在低层条件随机场中以字为切分粒度,结合旅游景点常用字表、景点常用后缀表、地名常用字表等特征词典,实现简单旅游命名实体的识别;其识别结果传递到高层模型,以词为切分粒度,结合复杂特征,实现嵌套景点、特产风味、地点的识别。最后进行了两组相关实验,结果表明,在开放测试中,层叠条件随机场模型相比于单层模型,F值提高了8个百分点;相比于HMM模型,正确率提高了8个百分点,召回率提高了22个百分点,F值提高了15个百分点。","计算机应用,中文信息处理,旅游领域,命名实体识别,层叠条件随机场,特征模板"
2008-08-28,基于语义组块分析的汉语语义角色标注,"丁伟伟,常宝宝","近些年来,中文语义角色标注得到了大家的关注,不过大多是传统的基于句法树的系统,即对句法树上的节点进行语义角色识别和分类。该文提出了一种与传统方法不同的处理策略,我们称之为基于语义组块分析的语义角色标注。在新的方法中,语义角色标注的流程不再是传统的“句法分析——语义角色识别——语义角色分类”,而是一种简化的“语义组块识别——语义组块分类”流程。这一方法将汉语语义角色标注从一个节点的分类问题转化为序列标注问题,我们使用了条件随机域这一模型,取得了较好的结果。同时由于避开了句法分析这个阶段,使得语义角色标注摆脱了对句法分析的依赖,从而突破了汉语语法分析器的时间和性能限制。通过实验我们可以看出,新的方法可以取得较高的准确率,并且大大节省了分析的时间。通过对比,我们可以发现在自动切分和词性标注上的结果与在完全正确的切分和词性标注上的结果相比,还有较大差距。","计算机应用,中文信息处理,语义角色标注,语义组块分析,条件随机域,序列标注"
2008-08-26,汉语时间关系抽取与计算,"林静,苑春法","时间关系普遍存在于时间和事件概念之间,为信息组织提供了一条天然的线索。该文在信息抽取和时间信息标注的基础上,研究汉语中时间与时间、事件与时间和事件与事件之间的时间关系。一方面考虑汉语文本的特点,充分抽取蕴含于语法语义层面中的时间关系;另一方面定义了与文本无关的规则,实现了不同来源信息之间的时间关系的计算。这为信息抽取结果的组织、积累和共享打下了基础,对于事件追踪、多文本摘要等方面的研究也有一定的借鉴意义。","计算机应用,中文信息处理,时间关系抽取,时间关系计算,信息组织"
2008-10-15,基于同义词的词汇情感倾向判别方法,"王素格,李德玉,魏英杰,宋晓雷","词汇的情感倾向直接影响短语、句子、段落、篇章等更高层次语言粒度的情感倾向。对于基准词选取问题,该文提出了基于类别区分能力与情感词词表相结合的方法。考虑到词汇与其同义词很大程度上具有相同的情感倾向,我们提出了基于同义词的词汇情感倾向判别方法,这种方法一定程度上避免了数据稀疏问题。实验结果表明,基于同义词的词汇情感倾向判别方法优于仅采用目标词与基准词的词汇情感倾向判别方法。","计算机应用,中文信息处理,词汇情感倾向,基准词,关联强度,同义词"
2008-09-22,基于信息推理的网络新闻在线评论情绪分类,"李成伟,彭勤科,徐涛","网络评论数据的情绪倾向性信息对于企业商业智能系统、政府舆情分析等诸多领域有着广阔的应用空间和发展前景。该文基于语言类比超空间(HAL空间),利用信息推理方法,给出了一种短语级别的评论数据情绪倾向分类模型。该模型首先从评论文本中抽取符合预定义模式的短语,然后运用基于HAL空间的概念组合算法,将短语组合为概念C,最后使用信息推理算法,对概念C按情绪分类。实验表明,与SVM算法和Term-Count算法相比,该文的模型对于网络在线新闻评论数据分类效果较好。","计算机应用,中文信息处理,信息推理,情绪分类,HAL,语义倾向性"
2008-09-04,基于统计与正文特征的中文网页正文抽取研究,"周佳颖,朱珍民,高晓芳.","该文提出了一种基于统计与正文特征的网页正文抽取方法。该方法继承了统计方法的优点,同时利用正文特征克服了原有基于统计的方法无法抽取多正文体网页的缺陷。源于多正文体在网页的DOM树中对应着正文区域下的多棵具有相似特征的正文子树,该文首先基于统计的方法获取一条正文路径,然后学习该路径的正文特征识别正文区域和子树主干,最后根据区域及该主干具有的正文特征进而得到完整的正文。实验表明该方法抽取单正文和多正文的精确率分别为94%和91%。","计算机应用,中文信息处理,正文抽取,单正文体,多正文体"
2008-10-11,基于改进的LBP的低分辨率车牌汉字识别,"王叶,张洪刚,方旭,郭军","低分辩率的车牌汉字识别是字符识别中的一个难题。随着智能交通和模式识别技术的发展,传统的基于二值图的识别方法已不能满足实际要求。该文采用基于灰度图的汉字识别方法,避免了在传统二值化过程中不必要的结构信息丢失。该文将局域二值模式(Local Binary Patterns,LBP)算子运用于字符识别,使得车牌汉字的识别率由过去的74.25%提高到98.80%;并在已有的局域二值模式算子的基础上提出了一种改进的局部二值模式(Advanced Local Binary Pattern, ALBP)算法,使得汉字的识别时间大幅度缩短。实验结果表明,该文提出的方法对于低质量的车牌灰度汉字具有较强的鲁棒性,与传统识别方法相比,识别准确率和识别速度都有了较大的改进。","人工智能,模式识别,汉字识别,ALBP,识别准确率,识别速度"
2008-10-16,一个用于OCR输出的中文文本的拼写校对系统,李蓉,"该文描述了一个处理OCR输出的中文文本的拼写校正系统。使用一个大的正负语料库来建立错误模式库;负语料库中包含OCR识别错误,而正语料库中为对错误进行了编改后的正确文本。首先应用句子匹配算法从正负语料库中提取匹配的句子;然后使用比较算法从匹配的两个句子中提取不同的字符;若两个句子存在不同,则使用错词提取算法来获得错误词和对应的校正词,并以如下三元组的形式保存(校正词, 错词, 出现次数)。用上述算法运行整个正负语料库之后,可获得错误模式的集合,由此建立错误模式库。错误模式可看作是校正规则,用于校正文本中和模式中与“错词”相同形式的错误。根据“错词”的长度将错误模式分为两类,一类为“错词”的长度大于两个字符,可直接应用错误模式规则进行校正;另一类为“错词”的长度等于两个字符,需使用验证算法确定是否当前的模式需要被校正。以上方法是为同方光盘公司开发的THOCR中文校对系统的核心算法,其中正负语料库来自公司在期刊网建设中的积累。由于算法所获得的错误模式均来自真实的OCR识别文本,所以校对效果较好。结尾部分给出了本校对系统的实验结果。","计算机应用,中文信息处理,错误校对,正负语料,学习算法"
2008-09-16,基于视频三音子的汉语双模态语料库的建立,"赵晖,林成龙,唐朝京","为实现可视语音合成和双模态语音识别,需要建立符合条件的双模态语料库。该文提出了一种汉语双模态语料库的建立方法。根据视频中唇部发音特征,对已有的三音子模型聚类,形成视频三音子。在视频三音子的基础上,利用评估函数对原始语料中的句子打分,并实现语料的自动选取。与其他双模态语料库相比,该文所建立的语料库在覆盖率、覆盖效率和高频词分布律有了较大改进,能够更加真实反映汉语中的双模态语言现象。","计算机应用,中文信息处理,可视语音合成,双模态语料,视频三音子,评估函数"
2008-08-16,维吾尔语双音节词韵律特征声学分析,"祖丽皮亚·阿曼,艾斯卡尔·艾木都拉","该文从文本分析模块入手,利用“维吾尔语语音声学参数库”,选择了以开音节和闭音节结尾的969个双音节词的韵律参数,包括元音时长、音高和音强进行了统计分析,归纳了其元音时长、音高和音强分布模式,探讨了维吾尔语双音节词的韵律节奏模式与双音节词重音之间的关系问题,其目的是为了提高语音合成的自然度。我们相信本项研究对维吾尔语语言乃至整个阿尔泰语系语言的韵律研究具有较高的参考价值。","计算机应用,中文信息处理,语音合成,中文信息处理,维吾尔语,韵律特征,声学分析 "
2008-10-16,一种基于遗传优化和汉字声调的文本水印算法,"赵理,崔杜武","该文提出了一种基于遗传算法优化和汉字声调的中文文本水印算法。该算法基于统计特征来动态确定嵌入标志代码。在由标志代码确定的水印插入区,通过改变汉字集合声调的特征值来嵌入文本水印。该方法的水印容量由标志代码的数量动态确定,可自主的提高水印容量。整篇文档可以分割成若干个嵌入部分,各部分可单独进行插入、提取计算,极大的降低了计算的复杂性。","计算机应用,中文信息处理,文本水印,鲁棒性,水印容量,遗传算法"
2008-10-15,维吾尔语词首音节元音声学分析,"孜丽卡木·哈斯木,那斯尔江·吐尔逊,吾守尔·斯拉木","该文利用“维吾尔语语音声学参数数据库”,统计分析和归纳了维吾尔语词首音节元音的共振峰模式及其分布格局。声学元音图有多种画法,该文采用JOOS型声学元音图。这种元音图的特点是以F1为纵坐标,以F2为横坐标。这样绘制成的声学元音图与元音舌位图有很好的对应性。维语标准音词首音节中有[y, i, e, O, u, o, ;, A]等8个元音。舌位前后的分布特点是[u, o, A]为后元音,[y, i, e, O, ;]为前元音;开口度(舌位高低)分布特点是[y, i, u]为高元音,[e, O, o]为次高元音,[;]为次低元音, [A]为低元音.","计算机应用,中文信息处理,维吾尔语,词首音节元音,声学分析"
2008-09-01,基于有限状态机的智能手机输入模型设计,"刁红军,李培峰,钱培德","该文通过对现有智能手机上的输入方式进行分析,把输入法分解为中文、英文和数字三种不同的输入状态,再结合GOF一书中的状态设计模式,给出了一个基于有限状态机的智能手机输入模型,这种输入模型可以用于Windows mobile系统, Symbian的S60系统等多种智能手机系统上的输入法开发。这样不但能简化智能手机上输入法的开发工作,而且也为多种智能平台上的输入法维护和升级提供了方便。","计算机应用,中文信息处理,有限状态机,手机输入法,智能手机开发,设计模式"
2008-10-21,基于词汇语义特征的中文语义角色标注研究,"邵艳秋,穗志方,吴云芳","语义角色除了受句法结构限制之外,同词汇的语义特征也有着紧密的内在联系。对于一些仅依靠句法分析不能很好解决的角色标注问题,如句法结构相同的两个成分所对应的角色分别为完全不同的施事、受事角色的情况,可以通过引入一些词汇语义特征来进行处理。该文基于北京大学的语义词典CSD,引入了配价数、主客体语义类等词汇语义特征来进行语义角色标注研究。10折交叉验证的结果显示,通过引用词汇语义特征,所有角色标注的总体评价F值比单纯使用句法特征上升了1.11%,而其中Arg0和Arg1角色标注的F值达到93.85%和90.60%,比仅使用句法特征进行角色标注分别提高了1.10%和1.26%。","人工智能,自然语言处理,语义分析,语义角色标注,句法分析,语义词典,词汇语义特征"
2008-11-11,语义角色标注中句法特征的研究,"李军辉,王红玲,周国栋,朱巧明,钱培德","描述了一个基于特征向量的语义角色标注系统,该系统以单一句法分析树作为输入。首先进行预处理,过滤掉极不可能是角色的成分,然后进行角色分类(包括NULL类),最后处理嵌套情况及对中心语义角色去重处理。在优化组合已有特征的基础上,从语法、句型以及搭配角度出发,制定了新的有效的特征;实验表明了新特征的有效性及健壮性。最终在CoNLL-2005 Shared Task开发集和WSJ测试集上分别获得了77.54%和78.75%的F1值,是目前已知的基于单一句法分析中取得的最好性能。","人工智能,自然语言处理,语义角色标注,语法驱动特征,句型特征,搭配特征"
2008-12-19,汉语形容词的自动词义区分研究,"朱虹,刘扬,俞士汶","词义知识获取是词义知识库建设、词义消歧等任务的基础和起点,目前该工作基本依赖人类专家的智慧和洞察力,在大规模文本处理上缺乏意义计算的客观性和一致性。该文以汉语的中高频形容词为样本,深入挖掘词义特征并采用有参数初始化过程的EM迭代算法,实现了从真实文本中自动发现并区分词语词义的过程。该词义区分算法选取易获取的词形特征、基于大规模语料的搭配特征、基于网络语料的属性—宿主关系特征,替代以往难以获取的句法结构特征,并进一步利用HowNet优化了词形特征的选择。该工作可以应用于信息检索等领域,能够对现有词典起到修改和补充的作用,该思路亦可扩展到其他汉语词类上去。","计算机应用,中文信息处理,知识获取,词义区分,特征选择,EM算法"
2008-12-16,基于混合策略的高精度长术语自动抽取,"梁颖红,张文静,周德富","在目前的术语自动抽取中,双字词的精度已经达到了90.36%,但是三字以上的词的抽取精度只有66.63%,多字词的抽取成为了术语自动抽取的一个难点。该文提出了NC-value参数和互信息相结合的混合策略来识别三字以上的长术语的方法。该方法充分发挥了NC-value参数在利用词语上下文信息和互信息参数在词语结合强度两方面的优势,两者相互约束和配合,更有利于找到准确的长术语边界。采用生物信息领域Yapex语料进行实验,结果表明,三字以上长术语抽取正确率和召回率分别达到88.5%和76.6%,F测量值达到82.2%,稍高于其他方法的结果。","计算机应用,中文信息处理,术语抽取,NC-value,互信息"
2008-11-22,伪实例与人工标注实例相结合的词义消歧方法,"车超,滕弘飞","知识获取是制约基于语料库的词义消歧方法性能提高的瓶颈,使用等价伪词的自动语料标注方法是近年来解决该问题的有效方法。等价伪词是用来代替歧义词在语料中查找消歧实例的词。但使用等价伪词获得的部分伪实例质量太差,且无法为没有或很少同义词的歧义词确定等价伪词。基于此,该文提出一种将等价伪词获得的伪实例和人工标注实例相结合的词义消歧方法。该方法通过计算伪实例与歧义词上下文的句子相似度,删除质量低下的伪实例。并借助人工标注语料为某些无等价伪词的歧义词提供消歧实例,计算各义项的分布概率。在Senseval-3汉语消歧任务上的实验中,该文方法取得了平均F-值为0.79的成绩。","计算机应用,中文信息处理,词义消歧,知网,等价伪词,贝叶斯分类器,自动标注语料"
2008-10-11,一种基于LDA的CRF自动文摘方法,"吴晓锋,宗成庆","浅层狄利赫雷分配(Latent Dirichlet Allocation,LDA)方法近年来被广泛应用于文本聚类、分类、段落切分等等,并且也有人将其应用于基于提问的无监督的多文档自动摘要。该方法被认为能较好地对文本进行浅层语义建模。该文在前人工作基础上提出了基于LDA的条件随机场(Conditional Random Field, CRF)自动文摘(LCAS)方法,研究了LDA在有监督的单文档自动文摘中的作用,提出了将LDA提取的主题(Topic)作为特征加入CRF模型中进行训练的方法,并分析研究了在不同Topic下LDA对摘要结果的影响。实验结果表明,加入LDA特征后,能够有效地提高以传统特征为输入的CRF文摘系统的质量。","计算机应用,中文信息处理,自然语言处理,自动文摘, 狄利赫雷分布, 条件随机场"
2008-12-24,隐喻自动处理研究进展,"贾玉祥,俞士汶,朱学锋","隐喻在人类语言中普遍存在,是自然语言理解必须面对的问题。该文首先探讨了对隐喻的认识及语言中隐喻表达的分类。把隐喻自动处理分为隐喻识别、隐喻理解和隐喻生成三个子任务,对以往的研究成果进行梳理,着重介绍近几年来隐喻自动处理研究的新成果、新特点。隐喻自动处理离不开隐喻知识库的支持,文章也介绍了国内外隐喻知识库建设的主要成果。隐喻自动处理的目的是为了提高自然语言处理的智能化水平,文章探讨了隐喻处理在自然语言处理任务中的应用。最后展望了汉语隐喻自动处理研究的前景。","人工智能,机器翻译,隐喻自动处理,自然语言处理,机器学习,知识获取"
2008-11-12,事件关系表示模型,"仲兆满,刘宗田,周文,付剑锋","事件关系的表示及事件推理是基于事件的知识处理的核心内容。文章提出了事件影响因子的概念来刻画事件间相互影响的强弱,给出了一种事件影响因子的计算方法。在此基础上,建立了事件关系图ERM(Event Relationship Map)来描述领域中事件之间的关系。依据事件关系和事件要素可以进行事件推理,重点阐述了ERM上基于关系的事件推理算法。最后,做了一个事件关系推理的实验,结果证明所提模型及算法与人的主观判断相一致,是合理可行的。","人工智能,自然语言处理,事件,事件关系,事件影响因子,事件推理"
2008-10-20,基于词共现模型的垃圾邮件过滤方法研究,"张燕平,史科,徐庆鹏,谢飞","垃圾邮件过滤就是对邮件做出是垃圾或非垃圾的判断。传统的表示邮件的方法是在向量空间模型基础上通过信息增益等特征选择方法提取一部分词来表示邮件内容,存在语义信息不足的问题。该文提出一种将传统方法和词共现模型结合起来表示邮件特征的新方法,再采用交叉覆盖算法对邮件进行分类得到邮件分类器。实验表明,该文提出的邮件过滤算法与传统方法相比提高了过滤性能,词共现选择的维度要比传统方法选择的维度更具有代表性。","计算机应用,中文信息处理,向量空间模型,垃圾邮件过滤,词共现模型,交叉覆盖算法"
2008-08-19,印刷体汉字识别后处理方法的研究,"张宏涛,龙翀,朱小燕,孙俊","高阶N-gram语言模型在OCR后处理方面有着广泛的应用,但也面临着因模型复杂度大导致的数据稀疏,以及耗费较多的时空资源等问题。该文针对印刷体汉字识别的后处理,提出了一种基于字节的语言模型的后处理算法。通过采用字节作为语言模型的基本表示单位,模型的复杂度大大降低,从而数据稀疏问题得到很大程度上缓解。实验证明,采用基于字节的语言模型的后处理系统能够以极少的时空开销获取很好的识别性能。在有部分分割错误的测试集上,正确率从88.67%提高到了98.32%,错误率下降了85.18%,运行速度较基于字以及基于词的系统有了大幅的提升,提高了后处理系统的综合性能;与目前常用的基于词的语言模型后处理系统相比,新系统能够节省95%的运行时间和98%的内存资源,但系统识别率仅降低了1.11%。","计算机应用,中文信息处理,汉字识别,OCR,语言模型,后处理"
2009-01-19,一种基于使用差异的词语领域性分析方法,"李素建,宋涛,高杰,幺鹏跃,李文捷","领域知识的表达形式最终体现在词汇的领域性上,因此对领域词及其部件的领域度分析是一个关键。该文在分词的基础上,对各个领域语料进行分析,利用词语之间的关系,引入链接分析方法分析词语在各个领域中的使用重要性,并通过词语在各个领域中的使用差异性计算其领域度,从而达到领域分析的目的,获取某个领域的领域部件词。该文采用以上方法在军事、娱乐等领域进行了实验,实验结果表明该方法相对于当前常用的tf×idf方法和Bootstrapping方法,可以更有效地进行领域分析获取领域部件词。","人工智能,自然语言处理,领域性分析,领域词,领域部件词,链接分析,使用差异"
2009-01-04,音字转换中分层解码模型的研究与改进,"张顺昌,孙乐","音字转换是中文信息处理领域的一个重要研究方向,在语音识别、中文拼音输入中都有广泛应用。该文对音字转换中的拼音流切分歧义问题做了分析与研究,发现传统的分层隐马尔可夫解码模型在解决这个问题时存在缺陷,提出了利用语言模型知识辅助拼音流切分来改进已有的分层模型的思想。实验表明,与传统方法相比,该文的方法可以将首字准确率提高3%。","人工智能,自然语言处理,音字转换,HMM模型,中文信息处理,切分歧义"
2009-02-03,基于RFC模型的基频曲线导数域编码方法研究,"王磊,刘加","基频是发浊音时声带振动频率,通常用F0表示。在一个音节或连续的语音段中,F0是随时间变化的,这种变化的轨迹形成了基频曲线。基频曲线的走势可以反映出语句的重音、语调等韵律信息,所以对基频曲线的描述和研究就显得尤为重要。该文首先提出了一种基频曲线描述方法,即导数域编码方法,同时探讨了该编码方法在语音发音质量评价中对韵律的作用。实验结果表明基于该描述方法能够提高英语发音语调质量评价的性能,主观和客观评价的相关性由原来的基于基音极值差的0.38提高到0.49。","人工智能,模式识别,基音频率,导数,编码,应用"
2008-12-01,汉语语音检索的集外词问题与两阶段检索方法,"孟莎,刘加","该文针对大规模汉语语音检索任务提出汉语语音检索中的集外词问题和针对集外查询词的两阶段检索方法。汉语语音识别和检索中,集外词可以以词表词序列的形式被识别和检索到,因此被认为不存在集外词问题;该文发现集外查询词性能远远低于集内查询词,将此问题定义为汉语语音检索任务的集外词问题,并提出两阶段的检索方法,第一阶段通过模糊音素匹配的方法提高查全率,第二阶段通过词格修正的方法提高查准率。实验表明,两阶段的检索方法极大的提高了典型集外查询词的检索性能,FOM指标相对基线系统提高了24.1%。","计算机应用,中文信息处理,汉语语音检索,集外词,词格,大词汇量连续语音识别"
2008-11-17,汉蒙翻译模型中的依存语法与形态信息应用研究,"骆凯,李淼,乌达巴拉,杨攀,朱海","该文提出将源语言句法信息和目标语言形态信息引入汉蒙机器翻译的模型构造中,以降低译文的词形错误率等问题。在源语言端,利用汉语依存句法分析器获取依存树,将依存句法信息以标注形式记在每个词上;在目标语言端,分析并获取蒙古语形态信息;利用LOP思想将源语言依存句法信息和目标语言形态信息引入翻译模型构造中。实验表明,其BLEU评分比传统的短语统计翻译模型有明显提高。该方法通过词、短语、句法三层面信息的结合,实现了汉蒙两种语言语法结构的平衡,特别适合于源语言形态信息贫乏而目标语言形态信息丰富的统计机器翻译系统。","人工智能,机器翻译,依存语法,形态信息,汉蒙翻译模型,LOP-Factored模型,统计机器翻译"
2008-10-24,基于trigger对的蒙古语语言模型的三种实现方法比较,"刘志文,侯宏旭,李沙茹拉,柳林","基于trigger对的长距离蒙古语语言模型采用统计方法进行自然语言建模。该文简要介绍了基于trigger对的长距离蒙古语语言模型的三种实现方法,并在汉语-蒙古语机器翻译系统测试了这三种方法的性能。该文旨在通过对三种模型的比较研究,为基于trigger对的长距离蒙古语语言模型的具体应用提供参考和依据。","人工智能,自然语言处理,trigger对,蒙古语,语言模型"
2008-10-24,基于不确定有限自动机的蒙古文校对算法,斯·劳格劳,"该文首先分析了蒙古文电子文本中存在的错误类型、出错原因以及常用的查错纠错方法,然后根据蒙古文特有的书写习惯和编码特点提出一种基于不确定有限自动机的校对算法。该算法采用有限自动机的方法对校对算法所依据的知识词典进行描述,大大提高了文本查错和纠错速度。","人工智能,自然语言处理,蒙古文,校对,自动机,词法分析 "
2008-10-22,维吾尔语名词构形词缀有限状态自动机的构造,"早克热·卡德尔,艾山·吾买尔,吐尔根·依布拉音,艾斯卡尔·艾木都拉","该文主要阐述维吾尔语词干提取中使用的名词构形词缀分析DFA的构造过程。维吾尔语属于黏着语,所以维吾尔语自然语言处理系统必须实现词干提取。词干提取的主要任务从单词提取词干和连接词干词尾的构形词缀。维吾尔语单词的构形词缀按照一定的规则连接到词干词尾,这使得维吾尔语构形词缀的连接规则可用有限状态自动机形式化描述。该文首先介绍维吾尔语名词的形态结构,然后根据规则构造从右向左的有限状态自动机,最后对这个自动机进行方向翻转和转换确定自动机操作。","人工智能,自然语言处理,维吾尔语,黏着语,构形词缀,有限自动机,语音和谐,词干提取"
2002-03-18,信息抽取的语义知识资源研究,袁毓林,"本文讨论支持信息抽取的语义资源的建设问题, 举例说明了信息抽取至少需要三种层面的语义知识:(i)宏观的话语篇章知识, 籍此可以约束信息抽取的匹配模板的类型, 预测关键性的信息项目在文本中的分布位置；(ii)中观的论元结构知识, 籍此可以建立动词的论元成分跟事件模板的传递与继承关系, 帮助确定代词或空语类跟其先行语的回指关系, 进而确定其语义所指；(iii)微观的逻辑结构知识, 籍此可以确定否定词、量化词、模态词等逻辑算子跟其所约束的成分之间的逻辑关系（比如, 哪些成分处于否定的辖城之中, 其中哪个成分是否定的焦点, 在哪些语法条件下否定词是冗余的, 等等）。最后, 指出研究这三种语义知识所可利用的几种理论和方法。","信息抽取,语义资源,话语篇章,论元结构,逻辑结构"
2002-04-05,模糊语义模式及其在汉英机译系统英文生成选词中的应用,"陈毅东,李堂秋,郑旭玲","研究生成选词问题对改善机翻系统的翻译质量有重要意义, 基于语义模式的选词方法是常用的选词方法, 在混合选词模型也扮演了重要角色。本文针对该方法的不足, 提出了语义模式自动获取的思路和模糊语义模式的概念, 对其进行了改进。采用语义模式自动获取的思路可以克服传统手工方法需要巨大工作量的问题, 而模糊语义模式概念的提出则使语义模式能表示语言现象的量化差别。文中首先讨论该研究的重要性, 然后介绍了模糊语义模式的概念, 接着给出了构建模糊语义模式库时使用的一个训练算法, 最后给出了应用模糊语义模式进行选词的具体算法并将它与传统算法进行了比较。","选词,语义模式,模糊"
2002-03-06,基于规则学习的韵律结构预测,"赵晟,陶建华,蔡莲红","韵律结构的分析和预测作为提高语音合成系统自然度的一个重要核心组成, 日益受到重视。本文提出了一种基于规则学习的汉语韵律结构预测方法, 该方法从人工韵律标注的语料库中抽取语言学特征和两级韵律结构标记, 构建了实例数据库（example database), 再利用规则学习(rule learning)算法从实例中自动归纳韵律短语预测规则。本文通过大量的实验挑选出对于汉语韵律结构预测最有效的特征, 采用和比较了两种典型的规则学习算法。同时, 对于实验结果给出了较为系统的评价参数。实践表明, 规则学习算法用于韵律结构预侧达到了90%以上的正确率, 优于目前其他方法的结果, 是一种行之有效的办法。","韵律结构预测,规则学习,韵律词,韵律短语,转换规则"
2002-03-11,语音识别音字转换中的快速容错算法,"李明琴,王作英,陆大?","本文研究了汉语连续语音识别音字转换中的容错算法, 以纠正声学识别的替代、插入、删除错误。为了解决容错算法的计算量问题, 本文提出了两种快速算法。一是针对单独出现错误的快速容错算法；二是针对关键词的快速容错算法。快速算法有效地限制了容错算法的搜索空间, 提高了计算效率。快速容错算法应用在电话对话系统中, 字正确率从78.97%提高到86.68%, 关键词检测正确率从80.56%提高到88.52%,并且算法运算时间满足实时性要求。","容错算法,稳健语音识别,对话系统,关键词检测"
2002-02-25,一种相似汉字的识别算法,"蔺志青,郭军","本文提出了一种通用的基于部分空间方法的相似汉字识别算法, 该算法无须事先确定相似字组, 也不必人工选择各个相似字组的部分空间, 能够自动决定待识别字是否需要进入相似字识别过程, 以及怎样选择部分空间。实验结果证明了本算法的有效性。","相似汉字,文字识别,部分空间"
2002-02-20,北京大学现代汉语语料库基本加工规范,"俞士汶,段慧明,朱学锋,孙斌","北京大学计算语言学研究所已经完成了一个有2700万汉字的现代汉语语料库的基本加工。加工项目除词语切分和词性标注外, 还包括专有名词(人名、地名、团体机构名称等)标注、语素子类标注以及动词、形容词的特殊用法标注。这项大规模语言工程的顺利完成得益于事先制订并不断完善的规范。发表《北京大学现代汉语语料库墓本加工规范》是为了抛砖引玉, 更广泛地向专家、同行征询意见, 以便进一步修订。","现代汉语,语料库,词语切分,词性标注,规范"
2002-05-08,汉语基本短语的自动识别,"张昱琪,周强","本文应用基于实例的MBL(Memory-Based Learning)学习方法,对汉语中较常见的9种基本短语的边界及类别进行识别,并利用短语内部构成结构和词汇信息对预测中出现的边界歧义和短语类型歧义进行了排歧处理。实验中还比较了在特征向量中加入词汇信息与否对实验结果的影响。实验取得了比较令人满意的结果:对这9种基本短语的识别正确率达到95.2%;召回率达到93.7%。","部分分析,基本短语,基于实例学习,短语结构,词汇排歧"
2002-05-27,指代消解的基本方法和实现技术,王厚峰,"指代是自然语言中常见的语言现象,大量出现在篇章或对话中。随着篇章处理相关应用日益广泛,指代消解也显示出前所未有的重要性,并成为自然语言处理上热门的研究问题。针对指代和指代消解的有关问题,本文对基本概念作了说明,分析了语言中典型的指代现象和指代消解所需的基本语言知识;同时,介绍了指代消解中有代表性的几种计算模型和近10年来采用的若干实现技术。","指代消解,先行语,突显性"
2002-05-07,基于统计分词的中文网页分类,"黄科,马少平","本文将基于统计的二元分词方法应用于中文网页分类,实现了在事先没有词表的情况下通过统计构造二字词词表,从而根据网页中的文本进行分词,进而进行网页的分类。因特网上不同类型和来源的文本内容用词风格和类型存在相当的差别,新词不断出现,而且易于获得大量的同类型文本作为训练语料。这些都为实现统计分词提供了条件。本文通过试验测试了统计分词构造二字词表用于中文网页分类的效果。试验表明,在统计阈值选择合适的时候,通过构建的词表进行分词进而进行网页分类,能有效地提高网页分类的分类精度。此外,本文还分析了单字和分词对于文本分类的不同影响及其原因。","文本分类,统计分词,机器学习,计算机网络"
2002-06-24,一种基于上下文的中文信息检索查询扩展,"贺宏朝,何丕廉,高剑峰,黄昌宁","在中文信息检索的研究和实践中,由于查询中所使用的词可能与文件集中使用的词不匹配而导致一些相关的文件不能被成功地检索出来,这是影响检索效果的一个很关键的问题。查询扩展可以在一定程度上解决这种词的不匹配现象,然而,实验表明,通常简单的查询扩展并不能稳定地提高中文信息检索的检索效果。本论文中提出并实现了一种基于上下文的查询扩展方法,可以根据查询的上下文对扩展词进行选择,是一种相对“智能”的查询扩展方法。在TREC - 9 中文信息检索测试集上进行的实验表明,相对于通常简单的查询扩展,基于上下文的查询扩展方法取得了具有统计意义提高的检索效果。","查询扩展,基于上下文,中文信息检索"
2002-05-20,“CAU”词及其知识图分析,"刘小冬,张蕾","专家系统是人工智能研究领域的一个重要研究分支。专家系统主要由两部分组成:知识库和推理机。知识库中的知识主要由“IF-THEN”这样的知识组成。知识图是一种新的知识表示方法。在知识图中,含有“IF-THEN”结构的句子是由起因操作符(causal operator)或起因关系(CAU-relation)表示的。本文挑选了一些具有一定代表性的起因意义的汉语“CAU”操作符,并且基于知识图理论分析了这些操作符,并进行了分类,目的是为专家系统中知识库的建立做准备。","专家系统,知识图,知识库,起因单词"
2002-05-10,自动问答综述,"郑实福,刘挺,秦兵,李生","自动问答技术是自然语言处理领域中一个非常热门的研究方向,它综合运用了各种自然语言处理技术。本文介绍了自动问答技术的发展现状和自动问答系统中常用的技术。自动问答系统一般包括三个主要组成部分:问题分析、信息检索和答案抽取。本文分别介绍了这三个主要组成部分的主要功能和常用的方法。最后还介绍了自动问答系统的评价问题。","自动问答,问题分类,信息检索,答案抽取"
2002-03-29,汉字输入法码本自动更正设计研究,"陆剑江,钱培德","本文主要研究了在汉字输入法设计中的码本自动更正的设计与实现,提出了码本规则库的概念及设计思想,阐述了更正系统的工作原理,详细讨论了基于规则库的自动更正设计方案及工作流程,最后从实际应用的角度出发,提出了如何将输入法更正系统与输入法的集成策略。","输入法,码本,规则库,自动更正"
2002-02-20,北京大学现代汉语语料库基本加工规范(续),"俞士汶,段慧明,朱学锋,孙斌","北京大学计算语言学研究所已经完成了一个有2700万汉字的现代汉语语料库的基本加工。加工项目除词语切分和词性标注外,还包括专有名词(人名、地名、团体机构名称等)标注、语素子类标注以及动词、形容词的特殊用法标注。这项大规模语言工程的顺利完成得益于事先制订并不断完善的规范。发表《北京大学现代汉语语料库基本加工规范》是为了抛砖引玉,更广泛地向专家、同行征询意见,以便进一步修订。","现代汉语,语料库,词语切分,词性标注,规范"
2015-11-26,附录按代码的字母顺序排列的标记集,,,
2002-05-15,面向中间语义表示格式的汉语口语解析方法,"解国栋,宗成庆,徐波","口语解析在人机对话系统和口语翻译系统中的作用是十分关键的。本文提出了一种统计和规则相结合的汉语口语解析方法,解析结果是一种中间语义表示格式。该方法分为两个阶段。首先,采用统计方法,解析出输入句子的语义信息,然后,利用规则,将这些语义信息映射到中间语义表示格式。试验证明,此方法具有较强的鲁棒性,而且避免了完全用规则方法解析的一些弊端,达到较高的解析正确率。","人工智能,机器翻译,口语解析,统计解析模型,中间语义表示格式(IF)"
2002-04-09,日汉机器翻译系统中的多Agent研究,"张捷,陈群秀","机器翻译系统提高译文质量是一个关键性的难题。本文探讨如何在多方法的机器翻译系统中引入多Agent组织结构,并提出一种多层次多Agent组织结构 - 类工程组织结构,使用登记表通讯策略。该组织结构应用在多翻译方法的日汉MTS中,使翻译质量有了较大的改善。","人工智能,机器翻译,多Agent,类工程组织结构,登记表通讯策略,日汉机器翻译"
2002-05-07,基于双语语料的单个源语词汇和目标语多词单元的对齐,"陈博兴,杜利民","多词单元包括固定搭配、多词习语和多词术语等。本文提供了一个基于双语口语语料库的自动对齐单个源语词汇和目标语多词单元的算法,算法一方面通过计算对应于同一个源语词汇,多个目标语词汇之间的互信息和t值的归一化差值的大小来衡量目标语多个词语之间的关联程度以提取多词单元,另一方面通过计算互信息和t值的平均值作为多词单元和单个源语词汇之间互为相互翻译的衡量程度,用局部最优、首尾禁用词过滤以及长词优先等策略很好地解决了这个问题。另外,对短语翻译词典的分级,有效地减少了高级别词典中非正确翻译项的数目,使得翻译词典具有更好的实用性。","人工智能,机器翻译,双语对齐,多词单元,翻译词典,平均关联值,关联值归一化差值"
2002-04-17,基于链接的方法进行Web信息检索的TREC实验研究,"张敏,马少平,高剑锋","本文通过TREC实验研究基于链接信息的检索对Web信息检索的影响,包括使用链接描述文本,链接结构以及将基于链接的方法和传统基于内容检索的方法合并。得到如下结论:首先,链接描述文档对网页主题的概括有高度的精确性,但是对网页内容的描述有极大的不完全性;其次,与传统检索方法相比,使用链接文本在网页定位的任务上能够使系统性能提高96% ,但是在信息查询任务上没有帮助;最后,将基于链","计算机应用,中文信息处理,基于链接检索,基于链接的方法,Web信息检索,信息查询,网页定位"
2002-03-18,基于遗传算法的定题信息搜索策略,"许欢庆,王永成,孙强","定题检索将信息检索限定在特定主题领域,提供主题领域内信息的检索服务。它是新一代搜索引擎的发展方向之一。定题检索的关键技术是主题相关信息的搜索。本文提出了基于遗传算法的定题信息搜索策略,提高链接于内容相似度不高的网页之后的页面被搜索的机会,扩大了相关网页的搜索范围。同时,借助超链Metadata的提示信息预测链接页面的主题相关度,加快了搜索速度。对比搜索试验证明了算法具有较好的性能。","计算机应用,中文信息处理,定题检索,定题信息搜索,遗传算法,Hub,authority"
2002-04-02,“才”字句的句法语义分析,王楠,"本文结合“才”字句的基本句式,考察了副词“才”的句法组合功能,并着重分析了“才”的四种基本语义。指出这四种基本语义可以两两地归并为“表示事物的量”和“表示限定/排他”两种语义。并在此基础上进一步地归纳出副词“才”的深层语法意义——表示说话者对比客观事实与主观标准后作出的倾向性评判。","计算机应用,中文信息处理,“才”字句,句法语义,副词"
2002-04-08,基于语义依存关系的汉语语料库的构建,"尤昉,李涓子,王作英","语料库是自然语言处理中用于知识获取的重要资源。本文以句子理解为出发点,讨论了在设计和建设一个基于语义依存关系的汉语大规模语料库过程中的几个基础问题,包括:标注体系的选择、标注关系集的确定,标注工具的设计,以及标注过程中的质量控制。该语料库设计规模100万词次,利用70个语义、句法依存关系,在已具有语义类标记的语料上进一步标注句子的语义结构。其突出特点在于将《知网》语义关系体系的研究成果和具体语言应用相结合,对实际语言环境中词与词之间的依存关系进行了有效的描述,它的建成将为句子理解或基于内容的信息检索等应用提供更强大的知识库支持。","计算机应用,中文信息处理,语料库,语义依存关系,《知网》,动态角色与属性"
2002-06-14,一种文本相似度及其在语音识别中的应用,"李红莲,何伟,袁保宗","随着语音识别研究的深入,提高通用识别引擎的精度变得越来越困难。但对具体的语音识别任务,结合相应的背景,采取相应的措施,有可能达到很理想的识别精度。在已知语音输入为某有限集元素之一的情形,利用文本在发音上的相似度可以大大提高识别的精度。本文对原有文本相似度的定义进行了改进与完善,并就其在语音识别任务中的作用进行了深入的研究。","计算机应用,中文信息处理,相似度,语音识别,web语音浏览,语音拨号"
2003,搭建中华字符集大平台,李宇明,"为使中华文献有一个可进行文字加工的永久性本面目保存本,为满足数字化图书馆、博物馆、档案馆的建设要求,为促进用于知识发掘数据库的建设,为保证中华文化信息在国际互联网上的无障碍交际,必须尽快构建中华字符集。本文主要讨论中华字符集的内容及需要解决的技术问题。","计算机应用,中文信息处理,综述,中华字符集,文献保存,数字化,知识发掘,互联网"
2002-07-04,汉语句子谓语中心词的自动识别,"龚小谨,罗振声,骆卫华","谓语中心词的识别是句法成分分析中的一个非常重要的部分。本文提出了一种规则和特征学习相结合的谓语识别方法,将整个谓语识别的过程分为语片捆绑、谓语粗筛选和谓语精筛选三个阶段。在谓语粗筛选中,利用规则过滤掉明显不能充当谓语的词,得到一个准谓语集;在精筛选阶段,选择谓语的支持特征,根据统计计算得到每个特征对谓语的支持度,然后利用准谓语在句子中的上下文出现的特征对准谓语集中的词进行再次筛选,从而确定出句子的谓语中心词。经过测试表明,该方法是有效可行的。","计算机应用,中文信息处理,谓语中心词的识别,基于规则,特征选择,粗筛选,精筛选"
2003,古文字字库建设与古文字研究手段现代化学术研讨会,张德劭,"“古文字字库建设与古文字研究手段现代化学术研讨会”2002年12月28至29日在上海华东师范大学召开。会议是由教育部人文社会科学重点研究基地华东师大中国文字研究与应用中心组织召开的。与会的有国家教育部语信司的领导,学校领导,中外专家学者,信息产业界及出版界的人士等共40余人。",
2002-07-05,基于概念统计和语义层次分析的英文自动文摘研究,"季姮,罗振声,万敏,高小云","传统的自动文摘方法基于词语统计抽取文摘句,未进行文本的语义分析,导致文摘精度不高。为了克服传统方法的缺点,本文提出了一种基于主题概念的自动文摘方法,以概念统计和层次分析为基础设计并实现了一个英文自动文摘系统。系统利用WordNet以概念统计代替传统的词频统计,基于主题概念构建向量空间模型,计算句子重要度。并且根据主题概念在概念层次树上的分布进行文本结构分析划分意义块,以意义块为单元抽取文摘,初步解决了多主题文章的文摘结构不平衡问题。本文主要介绍了概念层次树的构造,主题概念的抽取步骤,基于主题概念的句子重要度的计算和意义块的划分算法。测试表明,通过概念统计和语义层次分析的方法,我们设计了更理想的向量空间模型,系统生成的文摘精度较高,并更全面地反映了原文的主要内容。","计算机应用,中文信息处理,概念统计,主题概念,向量空间模型,句子重要度,意义块划分"
2002-08-16,一种面向汉英口语翻译的双语语块处理方法,"程葳,赵军,徐波,刘非凡","基于语块的处理方法是近年来自然语言处理领域兴起的一条新思路。但是,要将其应用于口语翻译当中,还需按照口语特点对涉及双语的语块概念做出合理界定。本文在已有单语语块定义的基础上,根据中、英文差异和口语翻译特性,从句法和语义两个层次提出了一种汉英双语语块概念,并对其特点进行了分析。同时,针对中、英文并行语料库,建立了一套计算机自动划分与人工校对相结合的双语语块加工方法。应用该方法,对汉英句子级对齐的口语语料进行双语语块划分和对整,并以此为基础进行了基于双语语块的口语统计机器翻译实验。结果表明,本文提出的双语语块定义符合口语翻译的实际需要,使用基于双语语块的语料处理方法,能有效地提高口语系统的翻译性能。","人工智能,机器翻译,统计机器翻译,口语翻译,语料库,语块"
2002-09-13,基于特征串的大规模中文网页快速去重算法研究,"吴平博,陈群秀,马亮","网页检索结果中,用户经常会得到内容相同的冗余页面,其中大量是由于网站之间的转载造成。它们不但浪费了存储资源,并给用户的检索带来诸多不便。本文依据冗余网页的特点引入模糊匹配的思想,利用网页文本的内容、结构信息,提出了基于特征串的中文网页的快速去重算法,同时对算法进行了优化处理。实验结果表明该算法是有效的,大规模开放测试的重复网页召回率达97.3% ,去重正确率达99.5%。","计算机应用,中文信息处理,特征串,模糊匹配,去重算法,冗余网页"
2002-07-22,基于统计的中文地名识别,"黄德根,岳广玲,杨元生","本文针对有特征词的中文地名识别进行了研究。该系统使用从大规模地名词典和真实文本语料库得到的统计信息以及针对地名特点总结出来的规则,通过计算地名的构词可信度和接续可信度从而识别中文地名。该模型对自动分词的切分作了有效的调整,系统闭式召回率和精确率分别为90.24%和93.14% ,开式召回率和精确率分别达86.86%和91.48%。","计算机应用,中文信息处理,中文地名识别,构词可信度,接续可信度,自动分词"
2002-07-17,银行支票中小写金额图像的提取,"张重阳,娄震,杨静宇","支票图像的分割与识别是目前文档自动处理领域中讨论的一个热点问题。其中字符图像的分割是预处理过程中的一个重要环节,对识别系统有很大的影响。在我国的支票图像中常含有较深的印章图像叠加在被分割的字符上,增加了字符分割的难度。本文以支票中小写金额的图像为例,提出了字符的逐层分割方法以及用于判断印章图像是否去除的评判准则。首先去除图像中的底纹和定位格线;然后通过迭代的方法选取阈值去除印章图像;最后采用基于连通区的区域增长算法提取字符图像,去除碎块。在2725张实地采集的我国现行支票上的实验结果表明,本文的方法能够有效的去除印章图像,分割出字符。","计算机应用,中文信息处理,图像分割,二值化,阈值,文档图像分析"
2002-05-31,论汉字码本数据库管理技术,"吴娴,吕强,杨涛,杨季文,钱培德","任何一种中文输入法的研究中都会遇到码本的处理问题。在不同的时期,由于应用需求的不同,使得码本呈现出不同的表现形式。本文首先提出了汉字码本数据库的概念,它是指能够实现汉字字符信息到其相应属性的对应关系的数据结构。之后,本文讨论了不同层次上的两种码本:数据库码本和二进制码本。根据实践的经验,文中将不同阶段的汉字码本数据库分成文本文件形式、数据库码本形式和二进制文件形式,并且分别讨论了对这些码本的管理技术。","计算机应用,中文信息处理,码本,数据库,汉字码本数据库,管理技术"
2002-08-28,汉字键盘输入智能处理软件综述,"陈一凡,朱亮","作为输入编码的后处理,各种类型输入软件智能化的共同目标是由软件来识别和选定上屏的重码字、词与缩短平均码长,并促使编码简单化和规范化。本文简要地论述了基于理解的智能输入、基于语用统计的智能输入、基于模板匹配的智能输入和基于上下文关联的智能输入等四种类型的汉字键盘输入智能处理软件的原理、优点和有待解决的问题,并列举了每种类型的典型作品。","计算机应用,中文信息处理,综述,自然语言理解,语用统计,模板匹配,上下文关联,后处理"
2002-10-25,四种基本统计句法分析模型在汉语句法分析中的性能比较,"孟遥,李生,赵铁军,曹海龙","统计模型的选择是统计句法分析的关键。目前句法分析常用的有四种经典统计模型—PCFG模型,基于历史模型、分层渐近式模型和头驱动模型。本文通过实验,在已有的10000句汉语树库基础上,测试了这四种经典模型在现有数据规模下各自的性能,并论述了这四种经典模型的各自特点。本文旨在通过对四种基本模型的比较研究,为具体应用中句法分析模型的选择提供参考和依据。","计算机应用,中文信息处理,统计句法分析,基本模型,汉语分析"
2003-01-04,基于字串内部结合紧密度的汉语自动抽词实验研究,"罗盛芬,孙茂松","自动抽词是文本信息处理中的重要课题之一。当前比较通行的解决策略是通过评估候选字串内部结合紧密度来判断该串成词与否。本文分别考察了九种常用统计量在汉语自动抽词中的表现,进而尝试将它们组合在一起,以期提高性能。为了达到尽可能好的组合效果,采用了遗传算法来自动调整组合权重。对二字词的自动抽词实验结果表明,这九种常用统计量中,互信息的抽词能力最强,F-measure可达54.77% ,而组合后的F-measure为55.47% ,仅比互信息提高了0.70% ,效果并不显著。我们的结论是: (1) 上述统计量并不具备良好的互补性; (2) 通常情况下,建议直接选用互信息进行自动抽词,简单有效。","计算机应用,中文信息处理,自动抽词,统计量的组合,遗传算法"
2002-12-31,藏文自动分词系统的设计与实现,"陈玉忠,李保利,俞士汶","藏文自动分词系统的研制目前在国内仍是空白。本文从四个方面详细报告了书面藏文自动分词系统的具体实现过程,内容包括系统结构、分词知识库的组织与实现以及分词策略、算法设计及其详细的自动分词过程实例。文章最后给出了实验结果,结果表明系统具有较高的切分精度和较好的通用性。","计算机应用,中文信息处理,格助词,接续特征,藏文,自动分词"
2003-01-20,基于主题的Web文档聚类研究,"孙学刚,陈群秀,马亮","网络资源的不断膨胀和新旧信息的迅速更迭,使传统的手工分检的方法难以适应对海量电子数据的管理需要。Web文档聚类可以快速地将文档进行自动归类,并能够发现新的信息资源。针对Web文档数据的复杂性,本文提出了通过二次特征提取和聚类的方法,将Web文档按照主题进行自动聚类。在主题特征被有效提取的同时,实现了较高质量的Web文档聚类。","计算机应用,中文信息处理,Web文档聚类,OPTICS算法,特征提取,K近邻准则,二次特征提取和聚类的方法"
2002-11-04,自组织中文语义映射网络的优化特征编码方法,"张敏,马青,马少平","本文介绍自组织中文语义映射网络,并分别基于集合论、代数理论和概率论研究和提出六种不同的特征编码方法,这对自组织语义映射效果有很重要的影响。通过性能评价得出如下结论:使用TFIDF修正的频率密度编码能得到最佳效果,其语义映射的精确度和召回率分别为94.4%和90.7% ,而基于向量模型的方法则都不适用于中文自组织语义映射。文中给出结果分析。另外比较实验结果表明文中的最好方法其系统性能好于目前广泛采用的分层聚类技术,并远好于多元统计分析技术,例如主成分分析的特征降维编码。","计算机应用,中文信息处理,中文语义映射,自组织映射,特征编码,相似度计算,Kohonen网络"
2002-07-22,基于“相同与差异”的机译单元的自动提取研究,"陈博兴,杜利民","从双语语料库中提取的机译单元能更好地覆盖真实语言文本,本文提供了一个通过找出两个双语句对之间非全部为高频功能词的“相同和差异”部分,并且利用翻译词典和动态规划算法对齐“相同和差异”部分来获取机译单元的算法。对于获取的候选机译单元,本算法设计了三个过滤器来考察其正确性:双语词串相似度过滤考察其语义对应性,词性相似度过滤考察其语法对应性,首尾禁用词过滤考察其搭配正确性。通过抽样检验,最后提取的机译单元的正确率为86% ,召回率约为61.34% ,该算法对于获取机译单元提供了一种新的实用的方法。","人工智能,机器翻译,双语语料库,机译单元,相同和差异"
2002-12-04,基于语法信息的汉语韵律结构预测,曹剑芬,"韵律结构的预测, 主要包括短语的自动切分和重音的等级分布两个大的方面。本文在概述汉语韵律结构的基础上, 根据从自然话语中获得的韵律结构与句法结构和词性的关系, 用一种新的方法,通过文本分析,全面地预测韵律边界的位置分布及其等级差异,并进一步预测重音的位置分布及其等级差异。","计算机应用,中文信息处理,韵律结构,语法信息,韵律边界,重音"
2003-02-17,汉语基调的调模与语音合成的质量提高,"吴禀雅,周昌乐,吴洁敏","本文根据输入的汉语语篇中各个语词的感情色彩属性和语体色彩属性,通过一种语词属性文法及其合一运算,来得到整个语篇的调模。并通过调模得到相对应的音高和音长的基准值,来调整机器合成语音的语阶和语速,从而使机器合成的语音更加自然、流畅,丰富了机器合成语音的表现力,提高了语音合成的质量。","人工智能,机器翻译,调模,属性文法和合一运算,语速和语阶,质量提高"
2002-07-15,正易全:一个动态结构笔组汉字编码输入法,张小衡,"“正易全”是一个以“正”、“易”和“全”为基本指导思想的笔组型汉字编码输入法。在“正”方面,采用国际标准汉字集ISO10646 CJK, 并以《GB13000.1字符集汉字字序(笔画序)规范》和《信息处理用GB13000.1字符集汉字部件规范》指导编码;在“易”方面,以单双笔笔组和十来个常用部件为码元,按笔顺和音托等简单原则映射到26个英文字母建元上,从而避免了传统的繁复字根-键元对应表;在“全”方面,支持CJK中的所有20902字符,包括简体字、繁体字、日韩字和偏旁部首等,而且可以在不改变编码方案的前提下进一步扩充字集。正易全的单字最大码长为5个字母,平均码长4.315,键选率16.4%。该输入法的笔组-键元设计和取码模式是在对整个CJK字集作了全字编码以后多次试验、统计和优化后确定下来的。","计算机应用,中文信息处理,动态结构笔组,字形码,汉字输入"
2009-05-30,基于词边界分类的中文分词方法,"李寿山,黄居仁","该文研究和探讨一种新的分词方法 基于词边界分类的方法。该方法直接对字符与字符之间的边界进行分类,判断其是否为两个词之间的边界,从而达到分词的目的。相对于目前主流的基于字标注的分词方法,该方法的实现和训练更加快速、简单和直接,但却能获得比较接近的分词效果。更显著的是我们可以很容易地从词边界分类方法获得在线分词学习方法,该方法能够使我们的分词系统非常迅速地学习新的标注样本。","计算机应用,中文信息处理,中文分词,WBD方法,在线学习"
2009-05-26,基于最大间隔马尔可夫网模型的汉语分词方法,"李月伦,常宝宝","分词是汉语自然语言处理研究中非常重要的一个环节,在早先的研究中,最大熵模型和条件随机场(CRF)模型已经广泛运用到汉语自动分词的工作中。最大间隔马尔可夫网(M3N)模型是近年来由B.Taskar等[1]人提出的一种新型结构学习模型。该文尝试将这一模型用于汉语分词建模并进行实验,实验结果显示,基于给定的训练语料与测试语料,分词精度可以达到95％,表明基于最大间隔马尔科夫网的汉语分词方法可以取得较高的分词精度,是一种有效的汉语分词方法。","计算机应用,中文信息处理,最大间隔马尔可夫网模型,汉语分词,机器学习"
2009-05-27,SSD模型及其在汉语词性标注中的应用,"邢富坤1 2,宋 柔1,罗智勇1","该文提出了一种以符号解码与数值解码并举的SSD(Symbol-and-Statistics Decoding Model)模型,该模型被用于汉语词性标注任务,其标注正确率在封闭测试中达到97.08%,开放测试中达到95.67%,较二阶HMM的95.56%和94.70%都有较为显著提高。SSD模型的正确率虽然不及最大熵模型和CRF模型,但它的训练时间远少于后者,说明SSD模型在处理自然语言中的特定任务时是一种较强的实用模型。","计算机应用,中文信息处理,SSD模型,HMM,词性标注"
2009-05-30,基于依存句法分析的中文语义角色标注,"王步康,王红玲,袁晓虹,周国栋","依存句法是句法分析的一种,相比于短语结构句法分析,依存句法具有更简洁的表达方式。该文采用英文语义角色标注的研究方法,实现了一个基于中文依存句法分析的语义角色标注系统。该系统针对中文依存关系树,采用有效的剪枝算法和特征,使用最大熵分类器进行语义角色的识别和分类。系统使用了两种不同的语料,一种是由标准短语结构句法分析(CTB5.0)转换而来,另一种是CoNLL2009公布的中文语料。系统分别在两种语料的标准谓词和自动谓词的基础上进行实验,在标准谓词上取得的F1值分别为84.30％和81.68％,在自动谓词上的F1值为81.02％和81.33％。","计算机应用,中文信息处理,语义角色标注,依存关系,最大熵分类器"
2009-05-15,基于多词块的框架元素语义核心词自动识别研究,"李双红,李 茹,钟立军,郭伟昱","抽取一个句子的核心依存图是对句子进行语义理解的有效途径。在CFN自动标注的基础上,只能得到框架依存图,为了把框架依存图转换成框架核心依存图需要提取每个框架元素的语义核心词。该文提出了基于多词块标注的框架元素语义核心词识别和提取方法,通过对比分析,给出了多词块和框架元素的融合策略,并建立了在多词块标注基础上提取框架元素语义核心词的规则集。在6 771个框架元素上的实验结果显示,采用该文的方法和规则集提取框架元素核心词的平均准确率和覆盖率分别为95.58%和82.91%。","计算机应用,中文信息处理,框架元素,语义核心词,多词块"
2009-05-31,基于柱搜索的高阶依存句法分析,"李正华,车万翔,刘 挺","该文提出使用所有的孙子节点构成祖孙特征的高阶依存模型,并且使用柱搜索策略限制搜索空间,最终找到近似最优依存树。另外,该文以较小的时间复杂度为代价,使用了丰富的依存关系特征,并且允许模型在解码的过程中进行依存关系选择。作者参加了CoNLL 2009年多语依存句法分析和语义角色标注国际评测,最终获得联合任务总成绩第一名,依存句法分析总成绩第三名。","计算机应用,中文信息处理,柱搜索,高阶特征,依存分析"
2009-05-24,中文核心领域本体构建的一种改进方法,"谌贻荣,陆 勤,李文捷,崔高颖","核心本体对最基本的领域知识建模,并在上位本体和领域本体之间建立联系。上位本体是领域无关的而核心本体是领域相关的,因此在自动创建中文核心本体过程中,映射中文核心术语到上位本体概念有很多的错误。本文提出的改进方法首先找到共享后缀术语集内被共享的术语条数更多、与各术语的意义更接近的上位概念; 然后用其来改进词集中的核心术语和概念之间的映射。实验证明,该方法有效的提高了核心本体自动创建的精确度。","计算机应用,中文信息处理,本体构建,领域核心本体,上位本体,领域本体,上位关系"
2009-05-26,基于Web弱指导的本体概念实例及属性的同步提取,"康 为,穗志方","该文提出了一种基于Web弱指导的本体概念实例和属性的同步提取方法,利用小规模的种子实例和属性集,该文从Web上自动获取实例和属性共现的上下文模式,并利用种子实例和属性的关联性来评价这些模式。进一步,根据上下文模式提取候选概念实例和属性后,该文提出两种方法来评价提取的候选实例和属性。第一,利用概念实例和属性的关联性来互相评价对方的准确度;第二,利用候选实例或候选属性与种子实例或属性在上下文模式分布上的相似度来评价准确度。在疾病类实验结果表明,人工确认候选实例的准确率在前500个结果达到94%,前1 000个结果的准确率也高达93%。","计算机应用,中文信息处理,Web,概念实例提取,属性提取,弱指导,上下文模式"
2009-06-17,基于知网的中文结构排歧工具——VXY,"董 强,郝长伶,董振东","该文介绍了基于知网的中文结构排歧工具系列中的一种 — VXY。VXY采取了一种独到的排歧技术,对于语言难点采取“定点清除”的策略。它用来解决“V+N+的+N”类型的结构性歧义。VXY是一个自足的、可以现场考核检验的并可以真正付诸实用的系统,而不是仅仅某种方法论的表演或举例性的“游戏”。该文简要地介绍了VXY的组成部分,说明了它的意义计算的原理。同时,该文就如何更有效地利用知网进行结构和语义排歧,如何开辟不同于当前语言信息处理中的“三部曲”(语料标注、现成的计算、应试性的评测)的语言技术等问题进行讨论。","计算机应用,中文信息处理,语义,排歧工具,强支配,中文句法结构,知网"
2009-05-20,语法信息与韵律结构的分析与预测,"王永鑫,蔡莲红","韵律结构的自动预测是高自然度文语转换(TTS)系统的关键组成部分,直接影响到合成语音的自然度和表现力。该文建立了一个同时具有语法信息与韵律结构标注的汉语语料库。在这一语料库的基础上,对汉语的韵律结构组成、韵律结构与语法语义之间的关系进行了分析,并进行了预测试验。研究发现,汉语的韵律结构虽与语法结构不同,但是有着密切的联系,韵律结构可以通过语法结构进行预测。韵律结构除与语法结构有关之外,还要受到语句语义的制约。","计算机应用,中文信息处理,TTS,韵律结构,语法结构,语义"
2009-05-24,基于用户查询日志的命名实体挖掘,"翟海军,郭嘉丰,王小磊,许洪波","针对大规模查询日志中丰富的命名实体的挖掘是数据挖掘领域中的重要研究课题。已有的研究工作提出了一种基于种子实体的抽取框架,利用实体间的分布相似度进行挖掘。然而该工作只有当种子实体仅属于单个语义类别时才能取得好的结果,实际上命名实体往往可能从属于多个类别。该文通过引入一个弱指导话题模型,利用少量的人工指导信息,很好地解决了实体的类别模糊性,提高了挖掘的有效性。实验表明该文提出的方法在实体挖掘性能上显著优于已有的方法。","计算机应用,中文信息处理,分开命名实体,用户查询日志,话题模型"
2009-06-03,跨领域倾向性分析相关技术研究,"吴 琼,谭松波,张 刚,段洣毅,程学旗","该文主要研究文本的倾向性分析问题,即判断文本中的论断是正面还是负面的。已有的研究表明,监督分类方法对倾向性分析很有效。但是,多数情况下,已有的标注数据与待判断倾向性的数据不属于同一个领域,此时监督分类算法的性能明显下降。为解决此问题,该文提出一个算法,将文本的情感倾向性与图排序算法结合起来进行跨领域倾向性分析,该算法在图排序算法基础上,利用训练域文本的准确标签与测试域文本的伪标签来迭代进行倾向性分析。得到迭代最终结果后,为充分利用其中倾向性判断较为准确的测试文本来提高整个测试集倾向性分析的精度,将这些较准确的测试文本作为“种子”,进一步通过EM算法迭代进行跨领域倾向性分析。实验结果表明,该文提出的方法能大幅度提高跨领域倾向性分析的精度。","计算机应用,中文信息处理,跨领域,倾向性分析,图排序,EM算法"
2009-05-26,评价对象抽取及其倾向性分析,"刘鸿宇,赵妍妍,秦 兵,刘 挺","情感分析近年来已经成为自然语言处理领域的热点问题,该文对情感分析中的两项关键技术——评价对象抽取和倾向性判断进行了深入研究。在评价对象抽取阶段,首先使用句法分析结果获取候选评价对象,继而结合基于网络挖掘的PMI算法和名词剪枝算法对候选评价对象进行筛选。在倾向性判断阶段,通过分析情感句句型,归纳相应的分析规则,使用无指导的方法完成评价对象在情感句中的倾向性判断。该系统参加了COAE2008任务三的评测,取得了较好成绩。","计算机应用,中文信息处理,情感分析,评价对象,倾向性判断,句法分析"
2009-06-01,面向特定领域的产品评价对象自动识别研究,"宋晓雷,王素格,李红霞","产品评价对象的自动识别是文本观点信息抽取和倾向性分析中的重要研究课题之一。该文针对汽车评论,提出了一种不依赖外部资源的无指导评价对象自动识别方法。该方法首先综合使用词形模板和词性模板,采用模糊匹配方法和剪枝法抽取候选评价对象。然后,从候选对象集中,采用双向Bootstrapping方法识别出产品评价对象。最后,通过采用K均值聚类方法对产品评价对象进行聚类,实现从评价对象中自动抽取产品名称和产品属性。实验结果表明,该方法对产品评价对象识别的F值达到58.5%,产品名称识别的F值达到69.48%。","计算机应用,中文信息处理,产品评价对象,产品名称,产品属性,模板,K均值聚类,双向Bootstrapping方法"
2009-05-25,基于领域类别信息C-value的多词串自动抽取,"李 超,王会珍,朱慕华,张 俐,朱靖波","该本的多词串抽取是自然语言处理领域一项重要的研究内容。该文提出了一种多类别C-value(Multi-Class C-value)方法,利用多词串在不同领域的分布信息改善领域相关的多词串抽取的性能。在汽车、科技和旅行三个领域的数据上进行实验,评价多词串的准确率,在top-100级别上,较传统的C-value方法在三个领域中分别提高了12、12和13个百分点。实验结果验证了方法的有效性。","计算机应用,中文信息处理,多词串抽取,多类别C-value,领域信息"
2009-06-04,基于情感向量空间模型的歌词情感分析,"夏云庆,杨 莹,张鹏洲,刘宇飞","音频信号在歌曲情感分析中难以奏效,所以该文提出以歌词作为歌曲情感分析的依据,采取基于情感单元的情感向量空间模型(s-VSM)进行歌词情感分析。该模型较好地解决了基于词汇的向量空间模型(w-VSM)在文本表示效率、歧义、情感功能和数据稀疏性等方面的不足。同时,该文将情感词词频与Thayer二维情感压力模型相结合,提出了“轻松”、“压抑”之外的“复杂”、“含蓄”两类新的情感压力类别。实验证明 (1)s-VSM模型在歌词情感分类中优于传统方法;(2)四类情感压力模型对歌词情感分析很有帮助。","计算机应用,中文信息处理,文本情感分析,情感向量空间模型,情绪压力"
2009-06-03,统计机器翻译中多分词结果的融合,"马永亮,赵铁军","汉英统计机器翻译中,汉语语料通常需要使用中文分词将句子切分成词序列。然而中文分词不是为统计机器翻译而开发的技术,它的分词结果不能保证对统计机器翻译的优化。近些年,一些研究试图改进中文分词方法从而达到对统计机器翻译的优化。在该文中,从另外的角度研究中文分词对统计机器翻译的影响。基本思想是利用多分词结果作为额外的语言知识,提出一种简单而有效的方法使这些知识为统计机器翻译所用,使用了一系列策略融合多分词结果,并将融合结果应用在统计机器翻译系统中。实验结果表明这种方法比没有使用多分词结果融合的系统提高1.89个BLEU分数。","人工智能,机器翻译,统计机器翻译,中文分词,翻译模型特征插值,多策略特征融合"
2009-05-31,面向统计机器翻译的重对齐方法研究,"肖 桐,李天宁,陈如山,朱靖波,王会珍","词对齐是统计机器翻译中的重要技术之一。该文提出了一种重对齐方法,它在IBM models获得的正反双向词对齐的基础上,确定出正反双向对齐不一致的部分。之后,对双向词对齐不一致的部分进行重新对齐以得到更好的对称化的词对齐结果。此外,该文提出的方法还可以利用大规模单语语料来强化对齐结果。实验结果表明,相比在统计机器翻译中广泛使用的基于启发信息的词对齐对称化方法,该文提出的方法可以使统计机器翻译系统得到更高的翻译准确率。","人工智能,机器翻译,统计机器翻译,词对齐,重对齐,IBM models"
2009-05-30,基于规则和统计的日语分词和词性标注的研究,"姜尚仆,陈群秀","日语分词和词性标注是以日语为源语言的机器翻译等自然语言处理工作的第一步。该文提出了一种基于规则和统计的日语分词和词性标注方法,使用基于单一感知器的联合分词和词性标注算法作为基本框架,在其中加入了基于规则的词语的邻接属性作为特征。在小规模测试集上的实验结果表明,这种方法分词的F值达到了98.2%,分词加词性标注的F值达到了94.8%。该文所采用的方法已经成功应用到日汉机器翻译系统中。","人工智能,机器翻译,日汉机器翻译系统,日语分词,日语词性标注,联合分词"
2009-05-25,汉语块分析评测任务设计,"周 强,李玉梅","该文主要介绍了目前中文信息学会句法分析评测CIPS-ParsEval-2009中的三项块分析评测任务 基本块分析、功能块分析和事件描述小句识别的设计理念、判定标准和相关资源构建方法。然后给出了这三项目前的主要评测结果并对相关内容进行了简要分析。最后通过相关统计数据分析和国内外相关研究评述,总结了这三项评测任务的主要特色。","计算机应用,中文信息处理,基本块,功能块,事件描述小句,块标注库"
2009-03-23,中文词汇网络:跨语言知识处理基础架构的设计理念与实践,"黄居仁,谢舒凯,洪嘉馡,陈韵竹,苏依莉,陈永祥5,黄胜伟","中文词汇网络(Chinese WordNet, 简称CWN)的设计理念,是在完整的知识系统下兼顾词义与词义关系的精确表达与语言科技应用。中文词义的区分与词义间关系的精确表征必须建立在语言学理论,特别是词汇语义学的基础上。而词义内容与词义关系的发掘与验证,则必须源自实际语料。我们采用的方法是分析与语料结合。结合的方式则除了验证与举例外,主要是在大量语料上平行进行词义标记,以反向回馈验证。完整、强健知识系统的建立,是兼顾知识本体(ontology)的完备规范(formal integrity)和人类语言系统内部的完整知识。我们采用了上层共享知识本体(SUMO)来提供知识的规范系统表征。","计算机应用,中文信息处理,中文词汇网络,全球词汇网络网格,知识本体,多语处理,跨语言整合"
2009-01-02,基于篇章的中文地名识别研究,"唐旭日,陈小荷,许 超,李 斌","该文介绍了以篇章为单位的中文地名识别方法和系统实现。地名识别包括简单地名识别和复杂地名识别两个阶段。简单地名识别由基于条件随机场的识别模块和基于篇章地名关系的识别模块顺序构成,以原始文本为输入,直接利用地名内部结构和相邻字信息进行地名识别和文本分词,然后利用篇章地名关系和地名性判断进一步处理。复杂地名识别以简单地名识别结果为输入,采用条件随机场识别。系统在封闭测试和开放测试中F-1值分别达到92.87%和89.76%。研究发现,在地名性判断中地名确信度低的字串对于地名识别干扰性较大,篇章地名关系能够在不降低识别精确度的情况下有效提高召回率,综合利用地名短距离和长距离依存关系可以有效提高地名识别效果。","计算机应用,中文信息处理,篇章地名关系,条件随机场,地名性判断"
2009-03-04,《资治通鉴》历史领域本体构建及其应用研究,"彭炜明,宋继华","该文分析了目前领域本体构建过程中存在的一些问题,特别是“实例”在本体框架中的定位问题。在此基础上提出了《资治通鉴》历史领域本体工程和与之相适应的领域本体构建方法,即采用模式驱动的方式,自底向上构建领域本体,并用此方法实现了工程的先秦部分——先秦史本体。最后从SPARQL语言的本体查询和TouchGraph可视化两方面的应用评价了这个本体。","计算机应用,中文信息处理,领域本体,构建方法,本体工程"
2009-08-30,文本中人物性别识别研究,"唐 琴,林鸿飞","对文本中人物进行性别识别时除了利用其人名本身的用字特征外,可以从整个篇章出发,考虑篇章中描述不同性别时的两性特征差异。该文根据描述男女人物不同方面时存在的两性差异自动获取大量具有明显性别差异的性别倾向性特征词:性别倾向性描述词和性别倾向性称谓词。通过性别识别实验发现,性别倾向性描述词相对于性别倾向性称谓词具有更好的性别指示作用。另外,性别倾向性描述词结合性别倾向性称谓词和姓名的用字特征相对于仅利用人名进行性别识别的效果更好。","计算机应用,中文信息处理,性别倾向性特征词,性别倾向性描述词,性别倾向性称谓词,性别识别"
2009-04-02,基于贝叶斯估计的概念语义相似度算法,"吴 奎,周献中,王建宇,赵佳宝","传统的基于语义距离的概念语义相似度算法不能兼顾客观统计数据,基于信息量的相似度算法又难以获得权威统计样本,针对这些不足,该文提出一种基于贝叶斯估计的概念语义相似度算法。该算法首先假定概念出现概率是符合Beta分布的随机变量,然后基于语义距离的相似度算法计算先验参数,并根据统计样本计算该先验分布下基于最小风险的贝叶斯估计后验参数。随后利用基于信息量的语义相似度算法,便可获得主观经验与客观事实相结合的概念语义相似度。结合WordNet的实验分析表明,该算法与人为主观经验之间具有最大的相关系数。","计算机应用,中文信息处理,本体,语义相似度,贝叶斯估计,Beta分布"
2009-04-08,基于最大频繁项集的搜索引擎查询结果聚类算法,"苏 冲,陈清才,王晓龙,孟宪军","现有的搜索引擎查询结果聚类算法大多针对用户查询生成的网页摘要进行聚类,由于网页摘要篇幅较短,质量良莠不齐,聚类效果难以有较大的提高(比如后缀树算法,Lingo算法);而传统的基于全文的聚类算法运算复杂度较高,且难以生成高质量的类别标签,无法满足在线聚类的需求(比如KMeans算法)。该文提出一种基于全文最大频繁项集的网页在线聚类算法MFIC (Maximal Frequent Itemset Clustering)。算法首先基于全文挖掘最大频繁项集,然后依据网页集合之间最大频繁项集的共享关系进行聚类,最后依据类别包含的频繁项生成类别标签。实验结果表明MFIC算法降低了基于网页全文聚类的时间,聚类精度提高15%左右,且能生成可读性较好的类别标签。","计算机应用,中文信息处理,搜索引擎,网页聚类,频繁项集"
2009-06-04,基于网页布局相似度的Web论坛数据抽取,"王 允,李弼程,林 琛","Web论坛中蕴含着丰富的信息资源,充分利用这些信息资源依赖于论坛数据抽取技术。该文解决了从Web论坛抽取什么数据和如何抽取的问题,提出了一种基于网页布局相似度的Web论坛数据抽取方法,有效弥补了目前方法的自动化程度低,或准确率低的不足。该方法充分利用Web论坛网页布局结构上的特点,采用分级处理的方式,先识别出主题信息块、再利用待抽取数据的统计规律在主题信息块中完成抽取,整个过程不需要任何人工干预。实验结果表明,新方法对不同的BBS站点有很好的通用性,且具有较高的准确率和召回率。","计算机应用,中文信息处理,Web论坛,数据抽取,相似度"
2008-12-30,高性能中文垃圾邮件过滤器,"齐浩亮,程晓龙,杨沐昀 ,何晓宁,李 生,雷国华","设计并实现了基于在线过滤模式高性能中文垃圾邮件过滤器,能够较好地识别不断变化的垃圾邮件。以逻辑回归模型为基础,该文提出了字节级n元文法提取邮件特征,并采用TONE(Train On or Near Error)方法训练过滤器。在多个大规模中文垃圾邮件过滤公开评测数据上的实验结果表明,该文过滤器的性能在TREC 06C数据上优于当年评测的最好成绩,在SEWM 07立即反馈上1-ROCA值达到了0.000 0%,并明显优于SEWM 08评测在线过滤任务中的所有其他方法。","计算机应用,中文信息处理,中文垃圾邮件过滤,在线学习,逻辑回归模型,字节级n元文法,TONE"
2009-04-14,动词次范畴英汉论元对应关系获取,"朱聪慧,赵铁军,韩习武,郑德权","动词次范畴是根据句法行为对动词的进一步划分,它是由核心动词和一系列论元组成。其相关研究在英汉等多种语言方面都取得了较好的成果,但跨语言之间的研究还很少。该文提出了一种基于主动学习策略的英汉动词次范畴论元对应关系自动获取方法,这种方法可以在双语平行语料上,几乎不需要任何先验的语言学知识的情况下,自动获取英汉论元的对应关系。然后我们将这些对应关系加入了统计机器翻译系统。实验结果表明,融合了英汉动词次范畴论元对应关系的SMT系统在性能上有明显的提升,证明了自动抽取的对应关系的有效性,也为SMT提供了新的研究方向。","人工智能,机器翻译,动词次范畴化,跨语言论元对应关系,自动获取,统计机器翻译"
2009-08-30,基于统计的汉语格律诗生成研究,"何 晶,周 明,蒋 龙","古代中文诗歌的巅峰——中文格律诗,包括律诗和绝句,是中国古典诗词的奇葩。该文从已有的古今名诗中自动学习作诗知识,实现了一个中文格律诗的自动生成系统。该系统接收用户选择的表达其思路的若干个关键词作为输入,首先,利用相关词汇数据库和语言模型,实现了根据用户选定的关键词自动生成诗歌的第一句。其次,我们独创性地将格律诗的上下句关系映射为源语言到目标语言的翻译关系,设计了一个基于短语的统计机器翻译模型,从而把诗歌的第N-1句作为输入用以生成第N句。并提供了一个用户交互式的系统,使得用户可以在每一步都选择一个最佳诗句。最后,我们还精心设计了一套翔实的格律诗评测标准,并通过单句实验和全诗实验证明,该方法是诗歌产生的一个较好的方法。","人工智能,机器翻译,统计机器翻译,诗歌生成,绝句评测"
2009-02-04,>倒谱形状规整在噪声鲁棒性语音识别中的应用,"杜 俊,戴礼荣,王仁华","该文提出了一种新的用于鲁棒性语音识别的特征规整方法。我们观察到在噪声环境下语音特征分布的形状相比于干净环境变化很大,因此提出了一种称为倒谱形状规整的新方法,它是利用引入一个指数因子来达到对倒谱分布形状进行规整的目的。这种方法被证明在噪声环境下非常有效,特别是在低信噪比情况下。实验结果表明此新方法在aurora2和aurora3两个标准数据库上比经典的均值方差规整算法在词错误率方面分别有38%和25%的相对降低,并且倒谱形状规整也好于其它传统方法,比如直方图均衡和高阶倒谱矩规整方法。","计算机应用,中文信息处理,鲁棒性语音识别,形状规整"
2008-12-30,普通话发音错误自动检测技术,"张 峰,黄 超,戴礼荣","统计语音识别框架是现在发音错误检测系统的主流框架,而声学模型则是统计语音识别的基础。 该文一方面为了获得对于发音错误检测更好的声学模型,引入了说话人自适应训练(SAT)和选择性最大似然线性回归(SMLLR)技术;另一方面,由于字发音检错中存在严重的信息量不足问题和专家对于不同水平说话人的评价标注不一样,在后端上加入了话者得分归一化技术。在包含40个不同水平说话人的8 000个字的数据库上的实验结果表明,文中提出的方法有效的提高了系统性能,召回率为30%时,正确率从45.8％升到了53.6%,召回率为10%时,正确率从64.6%升到了79.9%。","计算机应用,中文信息处理,发音错误自动检错,说话人自适应训练,选择性最大似然线性回归,话者归一化"
2009-01-18,甲骨拓片字形图像复原方法,顾绍通,"提出了一种基于自适应阈值和分形几何的甲骨拓片字形图像复原方法。文章分析了甲骨拓片噪声的特点以及字形图像边缘的分形特征,通过计算自适应阈值对噪声区域进行填充。采用统计的方法计算甲骨拓片字形图像边缘的分形维数特征,对字形图像边缘进行压缩变换,进而对甲骨拓片字形图像边缘进行平滑。实验结果显示,这一方法的图像复原效果是比较明显的。","计算机应用,中文信息处理,甲骨拓片,自适应阈值,统计分形,分形维数,压缩变换,字形图像复原"
2009-09-12,基于加权SimRank的中文查询推荐研究,"李亚楠,许 晟,王 斌","查询推荐是搜索引擎系统中的一项重要技术,其通过推荐更合适的查询以提高用户的搜索体验。现有方法能够找到直接通过某种属性关联的相似查询,却忽略了具有间接关联的语义相关查询。该文将用户查询及查询间直接联系建模为查询关系图,并在图结构相似度算法SimRank的基础上提出了加权SimRank (简称WSimRank)用于查询推荐。WSimRank综合考虑了查询关系图的全局信息,因而能挖掘出查询间的间接关联和语义关系。然而,WSimRank复杂度太高而难以实用,该文将WSimRank转换为一个状态层次图的遍历和计算过程,进而采用动态规划、剪枝等策略对其进行优化从而可以实际应用。在大规模真实Web搜索日志上的实验表明, WSimRank在各项评价指标上均优于SimRank和传统查询推荐方法,其MAP指标接近0.9。","计算机应用,中文信息处理,搜索引擎,查询推荐,SimRank,WSimRank"
2009-09-14,中文交互式问答用户问题相关检测研究,"伍大勇,张 宇,刘 挺","交互式问答是具备处理系列相关问题以及与用户进行对话式交互的问答技术,是近年来国际上问答技术研究的一个热门方向,但是目前在中文问答领域几乎没有开展相关的研究。实现交互式问答系统首先要判别用户系列问题之间的相关性。该文探讨了提取问题中不同特征对中文交互式问答问题相关检测的作用,并且根据识别出的有效特征采用基于二元分类方法分别对翻译成中文的TREC QA问题集语料和真实的交互式问答语料进行问题相关检测实验,实验结果显示该文的方法获得了较好的问题相关检测效果。","计算机应用,中文信息处理,交互式问答,问题, 相关检测, 二元分类"
2009-09-03,一种基于文档相似度的检索结果重排序方法,"周 博,岑荣伟,刘奕群,张 敏,金奕江,马少平","对相关反馈问题的研究已有近30年的历史,相关反馈也被证明可以大程度稳定地提升检索系统的性能。当前网络环境下相关反馈的应用以及用户提供反馈信息的方式已经发生了明显的变化,因此相关反馈研究又一次引起了研究界的注意。该文提出了一种基于文档相似度的搜索结果重排序方法,该方法同时利用了反馈信息中的相关文档与不相关文档。在大规模网络信息检索标准实验数据上的实验结果表明:该方法不仅可以稳定地提高系统的检索性能,并且相较于经典的查询扩展方法有着明显的优势。","计算机应用,中文信息处理,相关反馈,文档重排序,信息检索"
2009-08-12,一种基于语境的词语相似度计算方法,"蔡东风,白 宇,于 水,叶 娜,任晓娜","词语相似度计算是机器翻译、信息检索等自然语言处理领域的关键问题之一。传统的词语相似度计算方法,未能很好地考虑上下文信息对词语语义的约束,从而不能对语境变换带来的词语间相似度的差异进行有效的区分。该文引入模糊数学中隶属函数的概念计算词语上下文信息的模糊重要度,并结合基于《知网》的语义相似度计算方法,提出一种基于语境的词语相似度计算方法。实验表明,该算法可以根据语境有效地区分语义相近的词语。","计算机应用,中文信息处理,语境,模糊重要度,词语相似度,隶属函数"
2009-09-11,面向网络论坛的突发话题发现,"陈 友,程学旗,杨 森","每天有大量的信息涌现在论坛上,用户可以通过论坛获知目前国际国内正在发生的一些突发事件。如何使用机器自动化的方法检测论坛中的突发话题已经成为搜索引擎以及网络挖掘系统的一项基础任务。话题检测与跟踪模型(TDT)可以很好的解决话题发现问题,但是TDT处理的对象是新闻语料,与论坛内容相比,新闻语料更准确、严谨、规范。TDT中使用的方法不适合用语随意的论坛。因此在网络论坛这种噪音环境下的话题检测面临着一定的困难与挑战。文中提出一种基于噪音过滤的话题发现模型,它从内容和用户参与度两个角度来检测论坛话题。在“水木社区”的“水木特快”上进行了相关的实验,实验结果表明该文提出的模型不仅可以检测突发话题,而且可以检测与这些话题相对应的用户社区。","计算机应用,中文信息处理,突发话题,网络论坛,时间序列"
2009-09-18,面向话题的新闻评论的情感特征选取,"陶富民,高 军,王腾蛟,周 凯","情感特征的提取是进行文本情感分析的一个非常重要的步骤,也是影响其结果好坏的主要因素。在该文中,作者提出一种新的特征提取方法来解决新闻评论的情感分析问题。在该方法中,首先根据评论和新闻的对比分析获得候选情感特征,然后经过相关的扩充和验证操作得到通用的情感特征,并将其用于新闻评论的情感分析。对新闻进行话题划分后进行更细粒度的情感分析:根据新闻话题信息,设计相应的话题相关的特征对比和验证过程,选取出面向话题的情感特征,最后用面向话题的情感特征对相应话题进行情感分析。实验证明,这种情感特征提取方法,对于新闻评论这种语句短、评论对象相对分散的评论,情感分析效果有较大的改进。","计算机应用,中文信息处理,情感分析,特征选取,特征扩展"
2009-09-11,面向用户互联网访问日志的异常点击分析,"王 倩,刘奕群,马少平,茹立云","随着互联网用户人数的日益增长,用户行为分析已经成为互联网技术领域重要的研究方法之一。在日志中去除异常点击,对于准确挖掘用户行为的意图和习惯十分重要。该文采用某公司提供的真实用户互联网访问日志,对日志中的连续点击,单IP多用户以及单用户多IP等可能的异常点击,从访问集中度,用户平均访问量等方面进行了分析。我们认为对于连续点击,用户行为分析研究人员可以分情况滤去多余点击或该用户所有点击,而对于单IP多用户和单用户多 IP的点击,我们建议不做处理。","计算机应用,中文信息处理,用户行为分析,互联网访问日志,异常点击"
2009-09-12,基于日志挖掘的搜索引擎用户行为分析,"岑荣伟,刘奕群,张 敏,茹立云,马少平","随着网络搜索用户的大规模增加,网络用户行为分析已成为网络信息检索系统进行架构分析、性能优化和系统维护的重要基石,是网络信息检索和知识挖掘的重要研究领域之一。为更好理解网络用户的搜索行为,该文基于7.56亿条真实网络用户行为日志,对用户行为进行分析和研究。我们主要考察了用户搜索行为中的查询长度、查询修改率、相关搜索点击率、首次/最后一次点击位置分布以及查询内点击数分布等信息。该文还基于不同类型的查询集合,考察用户在不同查询需求下的行为差异性。相关分析结果对搜索引擎算法优化和系统改进等都具有一定的参考意义。","计算机应用,中文信息处理,用户行为分析,搜索引擎,网络信息检索"
2009-09-11,基于检索历史上下文的个性化查询重构技术研究,"宋 巍,张 宇,刘 挺,李 生","基于检索历史隐式地学习用户偏好是个性化检索研究的热点,而根据用户检索历史重构新的查询输入是其中主要的研究内容。已有的研究在利用检索历史进行查询重构时,通常不区分检索历史中的内容是否与当前查询相关,而是将全部检索历史视为整体,因而使重构后的查询含有较多噪声。该文基于相关词语在上下文中大量共现的特征,将用户历史检索结果的网页摘要作为上下文语境,结合用户点击,选择检索历史中与当前查询共现程度最高的词语重构查询模型。对初始检索结果重排序的实验表明,该方法可以有效地选择相关词语,减少噪声。用p@5和NDCG两种指标评价,比最好的基准系统分别相对提高12.8%和7.2%,比初始排序结果相对提高 26.0% 和11.4%。","计算机应用,中文信息处理,个性化检索,隐式反馈,查询重构"
2009-09-03,基于版块的论坛增量搜集策略,"杜言琦,马 军","该文研究论坛的增量搜集问题。由于在论坛中同一主题通常分布在多个页面上,而传统增量搜集技术的抓取策略通常是基于单个页面,因此这些技术并不适于对论坛增量搜集。该文通过对许多论坛中版块变化规律的统计分析,提出了基于版块的论坛增量搜集策略。该策略将属于同一版块的所有页面看做一个整体,以它做为抓取的基本单位。同时该策略利用版块权重和局部时间规律确定抓取频率和抓取时间点。实验结果表明本策略对新增和新回复帖子的平均召回率为99.3%,并且与平均调度方法相比系统总延迟最高可减小42%。","计算机应用,中文信息处理,增量搜集,论坛爬虫,延迟"
2009-09-11,基于混合语言信息的词语搭配倾向判别方法,"王素格,杨安娜","具有较强褒贬倾向的词语搭配对于文本的情感分析具有重要的价值。该文提出了一种混合语言信息的词语搭配的倾向判别方法。该方法首先根据词语搭配六种模式的特点,确定出各模式的概率潜在语义模型,然后利用这些语义模型判别搭配的情感倾向。最后对部分包含情感词的搭配再利用规则修正其先前标注的情感倾向。基于汽车语料的实验结果表明,基于混合语言信息的词语搭配情感倾向判别方法优于单纯基于概率潜在语义模型或规则的方法。","计算机应用,中文信息处理,词语搭配,搭配模式,情感倾向判别,概率潜在语义模型"
2009-09-08,一种基于空间映射及尺度变换的聚类框架,"曾依灵,许洪波,吴高巍,程学旗,白 硕","传统聚类算法通常建立在显式的模型之上,很少考虑泛化模型以适应不同的数据,由此导致了模型不匹配问题。针对此问题,该文提出了一种基于空间映射(Mapping)及尺度变换(Rescaling)的聚类框架(简称M-R框架)。具体而言,M-R框架首先将语料映射到一组具有良好区分度的方向所构建的坐标系中,以统计各个簇的分布特性,然后根据这些分布特性对各个坐标轴进行尺度变换,以归一化语料中各个类簇的分布。如上两步操作伴随算法迭代执行,直至算法收敛。该文将M-R框架应用到K-means算法及谱聚类算法上以验证其性能,在国际标准评测语料上的实验表明,应用了M-R框架的K-means及谱聚类在所有语料集上获得了全面的性能提升。","计算机应用,中文信息处理,文本聚类,空间映射,尺度变换,模型不匹配"
2009-09-15,结合属性分布特征的模式匹配算法,"王 宇,方滨兴,吴 博,宋林海,郭 岩","该文提出了一种结合属性分布特征的Web模式匹配算法,属性分布特征包括属性对互斥特征和属性对共现特征。属性对互斥特征由属性对的互斥性和出现次数计算得出,这个特征隐含了属性对的语义相似程度。为了充分利用传统的属性名、属性值相似性特征,该文通过机器学习方法结合属性对互斥特征与相似性特征进行属性匹配。并以潜在的匹配属性对为基础,引入有约束的属性聚类方法进行Web模式匹配,聚类方法的约束条件来自属性对共现特征。实验结果表明,相对于仅使用相似性特征的方法,在不同的实验设置下,结合属性分布特征的Web模式匹配算法将F值提高了0.13到0.55。","计算机应用,中文信息处理,属性对互斥,属性对共现,Web模式匹配,约束聚类"
2009-09-25,文本分类中特征权重因子的作用研究,"张爱华,靖红芳,王 斌,徐 燕","在传统的基于向量空间的文本分类中,特征权重计算与特征选择过程完全割裂,特征选择函数的得分能反映特征的重要性,却未被纳入权重表示,造成特征表示不精确并影响分类性能。一些改进方法使用特征选择函数等修改TFIDF模型,提高了分类性能,但没有探究各权重因子如何影响分类的性能。该文以词频、逆文档频率及特征选择函数分别作为衡量特征的文档代表性、文档区分性及类别区分性的因子,通过实验测试了它们对分类性能的影响,得到文档代表性因子能使分类效果峰值最高但抵抗噪音特征能力差、文档区分性因子具有抗噪能力但性能不稳定、而类别区分性因子抗噪能力最强且性能最稳定的结论。最后给出权重表示的四点构造原则,并通过实验验证了其对分类性能的优化效果。","计算机应用,中文信息处理,文本分类,权重表示,权重因子作用,VSM"
2009-09-11,基于改进潜在语义分析的跨语言检索,"宁 健,林鸿飞","该文采用基于SVD和NMF矩阵分解相结合的改进潜在语义分析的方法为生物医学文献双语摘要进行建模,该模型将英汉双语摘要映射到同一语义空间,不需要外部词典和知识库,建立不同语言之间的对应关系,便于在双语空间中进行检索。该文充分利用医学文献双语摘要语料中的锚信息,通过不同的k值构建多个检索模型,计算每个模型的信任度,使得多个模型都对查询和文本的相似度做出贡献。在语义空间上进行项与项、文本与文本、项与文本之间的相似度计算,实现了双语摘要的跨语言检索,取得了较好的实验效果。","计算机应用,中文信息处理,改进潜在语义分析,语义空间,跨语言检索,SVD,NMF"
2009-07-06,面向专利文献的中文分词技术的研究,"张桂平,刘东生,尹宝生,徐立军,苗雪雷","针对专利文献的特点,该文提出了一种基于统计和规则相结合的多策略分词方法。该方法利用文献中潜在的切分标记,结合切分文本的上下文信息进行最大概率分词,并利用术语前后缀规律进行后处理。该方法充分利用了从大规模语料中获取的全局信息和切分文本的上下文信息,有效地解决了专利分词中未登录词难以识别问题。实验结果表明,该文方法在封闭和开放测试下分别取得了较好的结果,对未登录词的识别也有很好的效果。","计算机应用,中文信息处理,中文分词,专利文献,上下文信息"
2009-06-30,一种基于大知识库的亲属关系自动推理模型,"陈振宇,袁毓林,张秀松,周 强","我们采用“大知识库—小运算”的技术路线,提出一个汉语亲属关系的自动推理模型。首先,在充分研究汉语亲属关系的词汇—语法表达的基础上,给汉语常见的亲属关系及其情景语义建立认知模型。然后,据此构造大型的汉语亲属关系知识库,包括外围知识库和核心知识库两种。前者详尽列举亲属名词和称呼动词所涉及的各种句式,并给出相应的语义表达式;后者包括三个子库 性质库(刻画亲属关系中的性别、长幼等属性)、逆判断库(刻画“父—子”等反对称关系对子)和传递库(刻画通过中介人把称呼人与被称呼人联系起来的各种路径,共计3 600余条)。在此基础上,形成了一个汉语亲属关系自动推理模型,可以在已知ABC三边关系的任意两边时快速地推导出未知的另一边关系。","计算机应用,中文信息处理,亲属关系,自动推理,认知模型,知识库,逆判断,传递路径"
2009-05-10,信息处理用藏文分词单位研究,关 白,"分词单位作为分词系统的基本单位,是研究分词理论的基础,要确立分词单位就必须有相应的理论体系。该文结合藏文已有的语法著作和汉语语义分类体系建立与分词单位相应的词类划分体系;参照《资讯处理用中文分词规范》和《信息处理用现代汉语分词规范》等标准,从藏文文本语料出发,建立切分分词单位的九项基本原则和三项辅助原则,以此词类划分体系和切分原则为理论依据对藏文的分词单位进行详细说明。","计算机应用,中文信息处理,藏文分词,分词单位,信息处理,分词原则"
2009-12-02,自动构建时间基元规则库的中文时间表达式识别,"邬 桐,周雅倩,黄萱菁,吴立德","该文提出一种基于正则文法的时间表达式识别算法 它基于“时间基元”①进行规则构建,提高了时间表达式识别的召回率;同时使用基于错误驱动思想的规则剪枝算法,削减了从训练语料带来的噪声,提高了识别的正确率,两者搭配有效提高了系统整体性能。在ACE07中文语料上的实验结果显著超过了现有水平,F-score达到89.9%。该文提出的算法具有很好的通用性和扩展性,加以改进将可以有更广泛的应用。","计算机应用,中文信息处理,时间表达式识别,时间基元,Timex2,错误驱动,正则表达式"
2009-08-18,基于卷积树核的无指导中文实体关系抽取研究,"黄 晨,钱龙华,周国栋,朱巧明","该文提出了一种基于卷积树核的无指导中文实体关系抽取方法。该方法以最短路径包含树作为关系实例的结构化表示形式,以卷积树核函数作为树相似度计算方法,并采用分层聚类方法进行无指导中文实体关系抽取。在ACE RDC 2005中文基准语料库上的无指导关系抽取实验表明,采用该方法的F值最高可达到60.1,这说明基于卷积树核的无指导中文实体关系抽取是行之有效的。","计算机应用,中文信息处理,实体关系抽取,卷积树核,无指导学习,层次聚类"
2009-06-06,融合字特征的平滑最大熵模型消解交集型歧义,"任 惠,林鸿飞,杨志豪","交集型歧义的切分问题是分词阶段需要解决难点之一。该文将交集型歧义的消解问题转化为分类问题,并利用融合丰富字特征的最大熵模型解决该问题,为了克服最大熵建模时的数据稀疏问题,该文引入了不等式平滑技术和高斯平滑技术。我们在第二届国际分词竞赛的四个数据集上比较了高斯平滑技术、不等式平滑技术和频度折扣平滑技术,测试结果表明 不等式平滑技术和高斯平滑技术比频度折扣技术有显著提高,而它们之间不分伯仲,但是不等式平滑技术能使特征选择无缝嵌入到参数估计过程中,显著压缩模型规模。该方法在四个测试集上最终获得了96.27%、96.83%、96.56%、96.52%的消歧正确率,对比实验表明 丰富的特征使消歧性能分别提高了5.87%、5.64%、5.00%、5.00%,平滑技术使消歧性能分别提高了0.99%、0.93%、1.02%、1.37%,不等式平滑使分类模型分别压缩了38.7、19.9、44.6、9.7。","计算机应用,中文信息处理,分词,交集型歧义,融合丰富字特征,最大熵模型,平滑技术"
2009-06-10,网络新闻口语评论文本中人物对象识别方法,"林 琛,李弼程,周 杰","网络新闻口语评论文本中的人物对象是网络舆情的重要内容,是口语评论情感倾向性分析的基础。该文结合新闻口语评论中人物对象特点,提出了一种有效的人物对象自动识别方法。该方法首先在分词基础上,采用多频率综合判别对单字作为人物对象的可靠度进行评估,以获得稳定的识别线索;其次,根据线索划定处理窗口,利用改进频繁项挖掘算法,从窗口中提取候选人物对象;最后,对结果中存在的冗余进行优化处理。实验结果表明,新方法能够完整、有效地识别网络新闻口语评论文本中的人物对象。","计算机应用,中文信息处理,网络舆情,口语评论,人物对象,频繁项挖掘"
2009-07-29,Nave Bayes分类器制导的专业网页爬取算法,"韩国辉,陈 黎,梁时木,唐小棚,王亚强,于中华","从Web中快速、准确地检索出所需信息的迫切需求催生了专业搜索引擎技术。在专业搜索引擎中,网络爬虫(Crawler)负责在Web上搜集特定专业领域的信息,是专业搜索引擎的重要核心部件。该文对中文专业网页的爬取问题进行了研究,基于KL距离验证了网页内容与链接前后文在分布上的差异,在此基础上提出了以链接锚文本及其前后文为特征、Nave Bayes分类器制导的中文专业网页爬取算法,设计了自动获取带链接类标的训练数据的算法。以金融专业网页的爬取为例,分别对所提出的算法进行了离线和在线测试,结果表明,Nave Bayes分类器制导的网络爬虫可以达到近90%的专业网页收割率。",
2009-07-03,一种新的面向领域的鲁棒性文本分析算法,"陶县俊,邬晓钧,王晓东,郑 方","在自然语言处理的应用中,特别是在对口语文本、网络文本的处理中,待分析的文本经常会包含字词和句式上的错误。该文描述了一种基于线图分析方法改进的鲁棒性文本分析算法。该算法利用当前活动弧和规则库中的终结符,对基于领域词表的分词过程无法识别的语句串进行错误推测,将无法识别的语句串纠正为可能的正确文字。实验结果表明,在采用拼音的同音匹配进行推测纠错的情况下,该文所设计的鲁棒性文本分析算法相对于燕方法,分析度提高了14.78%,而语句平均分析循环次数增长为9.363%。","计算机应用,中文信息处理,线图分析方法,鲁棒性,错误推测"
2009-08-08,词义标注一致性检验系统的设计与实现,"乔剑敏,张仰森","词义消歧是自然语言处理领域的一个重要研究课题。词义标注的一致性将直接影响语料库的建设质量,进而直接或间接影响到其相关的应用领域。由于语言本身的复杂性与发展性以及算法设计的难点和缺陷,目前各种词义标注的算法与模型还不能百分之百正确地标注词义,即不能保证词义消歧的正确性与一致性。而人工校验在时间、人力方面的投入是个难题。该文在对《人民日报》语料、语句相似度算法和语义资源《知网》研究的基础上,提出了对《人民日报》语料词义标注进行一致性检验的方法。实验结果表明,此方法是有效的。","计算机应用,中文信息处理,词义标注,一致性检验,《知网》,语料,语句相似度"
2009-08-11,社会标注及其在信息检索中的应用研究综述,"靳延安,李瑞轩,文坤梅,辜希武,卢正鼎,段东圣","社会标注作为一种新型的网络资源管理和组织形式,在互联网和企业网中已经成为一种普遍的网络服务。社会标注具有标引、分类、资源发现和语义特性,这些特性可以帮助用户找到预期的信息。因此,可以利用社会标注来进行信息检索。该文首先对社会标注及标注对象和标注方法进行了概述。然后,从社会标注的分类特性、社区发现以及社会标注与语义搜索等方面进行综述评论。最后,讨论社会标注研究领域存在的挑战,并指出未来可能的研究方向。","计算机应用,中文信息处理,社会标注,信息检索,社区发现,分类"
2009-08-02,极性相似度计算在词汇倾向性识别中的应用,"宋 乐,何婷婷,王 倩,闻 彬","该文提出了一种新的基于HowNet相似度计算的词汇倾向性识别方法。该方法首先利用HowNet中的“良”、“莠”极性义原进行一种新的相似度——极性相似度的计算,再计算出词汇的极性值,进而识别出词汇的极性倾向。大量实验证明了该方法能够有效地区分词汇的极性,并且在第一届中文倾向性分析评测(COAE2008)比赛中取得了很好的效果。","计算机应用,中文信息处理,极性义原,极性相似度,极性值"
2009-02-09,基于高斯混合模型的生物医学领域双语句子对,"齐陈 相,林鸿飞,杨志豪","双语术语词典在生物医学跨语言检索系统中有着非常重要的地位,而双语句子对齐是构建双语词典的第一步工作。为了构想面向生物医学领域的双语词典,该文将分类思想和迁移学习方法引入汉英句子对齐任务中,将句子对齐任务看成一个多类分类任务,考虑生物医学领域双语摘要的锚信息,利用高斯混合模型完成分类目标。同时,在模型训练过程中,该文引入了迁移学习的思想,结合无噪音的《新概念英语》双语语料对模型的句子长度特征进行训练,使得模型在测试语料上句子对齐的正确率得到较大提高。","计算机应用,中文信息处理,句子对齐,高斯混合模型,迁移学习,锚信息"
2009-06-21,机器翻译系统融合技术综述,"李茂西,宗成庆","该文对机器翻译研究中的系统融合方法进行了全面综述和分析。根据在多系统输出结果的基础上进行融合的层次差异,我们将系统融合方法分为三类 句子级系统融合、短语级系统融合和词汇级系统融合。然后,针对这三种融合方法,该文分别介绍了它们各自具有代表性的研究工作,包括实现方法、置信度估计和解码算法等,并着重阐述了近年来使用广泛的词汇级系统融合方法中用于构造混淆网络的词对齐技术。最后,该文对这三类系统融合方法进行了比较、总结和展望。","人工智能,机器翻译,系统融合,最小贝叶斯风险解码,混淆网络解码,词对齐"
2009-07-20,加入调型信息的汉语孤立词识别研究,"王 鹏,胡 郁,戴礼荣,刘庆峰","汉语是一种有调语言,因此在汉语语音识别中,调型信息起着非常关键的作用。在现有的隐马尔可夫模型(Hidden Markov Model)框架下,如何有效地利用调型信息是有待研究的问题。现有的汉语语音识别系统中主要采用两种方式来使用调型信息 一种是基于Embedded Tone Model,即将调型特征向量与声学特征向量组成一个流去训练模型;一种是Explicit Tone Model,即将调型信息单独建模,再利用此模型优化原有的解码网络。该文将两种方法统一起来,首先利用Embedded Tone Model采用双流而非单流建模得到Nbest备选,再利用Explicit Tone Model对调进行左相关建模并对Nbest得分重新修正以得到识别结果,从而获得性能提升。与传统的无调模型相比,该文方法的识别率的平均绝对提升超过了3.0%,在第三测试集上的绝对提升达到了5.36%。","计算机应用,中文信息处理,计算机应用,汉语信息处理,汉语语音识别,调型信息,调型建模,双流建模"
2009-07-22,基于语音知识的音节切分,"汤 霖,黄建中,尹俊勋","在充分利用普通话水平测试试卷的文本信息、同一人的声母时长在常规语速下基本稳定、同一人的声母之间以及韵母之间的相对时长基本保持比例关系等先验知识的基础上,使用经小波变换后再重构的3个语音信号分量的累计能量特征为参数,提出了利用话者语音统计信息的两级音节切分算法,使音节切分精度达98.3%以上。","计算机应用,中文信息处理,音节切分,语音信号处理,普通话水平测试"
2009-09-04,普通话韵律结构对声韵母时长影响的分析,"梅 晓,熊子瑜","该研究基于大规模语音数据库,通过建立普通话连续语流中的声韵母时长预测模型,考察声韵母时长的影响因素,探讨普通话声韵母在连续语流中的时长变化类型与话语韵律结构之间的关系。初步研究结果表明 话语的韵律结构对声母时长的影响较小,而对韵母时长的影响较为显著,这种影响主要体现为 韵律单元末音节的韵母时长是否发生显著延长与话语的韵律结构密切相关,韵律大短语和语调短语末尾的音节通常会发生显著的韵母延长,韵律词内以及韵律词末尾的音节通常不会发生韵母延长;韵律小短语末尾的音节在韵母时长方面的表现比较混乱,规律性不明显,可能需要进一步做分化处理。","计算机应用,中文信息处理,声母时长,韵母时长,韵律结构"
2009-08-14,普通话测试信息分析,"王 璐,赵欣如,谢 簪,严志宇,谭军华,肖云鹏,李 峤,张学波,叶卫平","该文以普通话测试数据统计结果为依据,分析了方言背景和学科背景对普通话水平的影响,发现文学类学科普通话水平较高,其他学科不相上下;该文考察了普通话测试中的易错音节,分析了普通话测试的主要失分因素和普通话学习难点,如平舌擦音,翘舌擦音,舌面擦音,鼻韵母和上声声调;该文对测评员主观评分的相关性统计表明,不同测评员主观测评之间相关度较高,分数客观。","计算机应用,中文信息处理,普通话水平测试,方言背景,学科背景,易错音,分数相关性"
2009-03-27,哈萨克文信息处理的现状和发展方向,"木合亚提·尼亚孜别克,古力沙吾利","在信息日趋网络化的时代,作为中文信息处理中的子项哈萨克文信息处理技术也开始一步一步地进入研究当中,该文介绍了哈萨克文信息处理技术的现状、研究发展方向和一些关键概念、基本要素以及今后哈萨克文信息处理技术发展中存在的和需解决的问题。","计算机应用,中文信息处理,哈萨克文,信息处理,关键概念"
2009-04-26,维吾尔语框架语义知识库的概念设计,"阿里甫·库尔班,吾买尔江·库尔班,尼加提·阿不都肉苏力","该文对维吾尔语的框架语义描述体系及内容进行了初步探讨和尝试,建立了维吾尔语框架语义文档的树型结构。根据维吾尔语框架语义知识库的描述内容及框架语义网络自身的特点,该文在数据库中以维吾尔语框架语义为核心进行信息存储,设计了语义知识库的概念模型。为创建基于认知的维吾尔语框架语义知识库建设探索了一条可行的技术路线、方法和思路。","计算机应用,中文信息处理,维吾尔语, 框架语义, 概念设计, 实体—联系"
2010-02-03,面向新疆双语教学的远程教学系统的设计与实现,"依皮提哈尔·买买提,吾守尔·斯拉木","新疆是多民族的自治区,使用的主要民族语言文字有汉语和维吾尔语,考虑到这个特殊性,在深入研究国内优秀远程教学软件的基础上,本文提出了面向新疆双语教学的维汉双语远程教学系统的总体框架,分析了维汉双语远程教学系统的体系结构以及模块功能,论述了该系统主要模块中支持维汉双语、系统界面双语显示与切换等关键技术的实现。","计算机应用,中文信息处理,双语教学,远程教学系统,双语化,Unicode标准"
2009-11-11,基于语义角色标注的新闻领域复述句识别方法,"吴晓锋,宗成庆","复述(Paraphrase)句的识别可看作文本蕴含(Text Entailment)识别的一个子问题,传统的解决方法是通过词频或句法上的相似度来判断。即使用相同的文字书写的句子其含义也可能差别很大,而相同句法结构也不能保证意义一致。该文根据新闻语料的特点,提出了一种通过引入深层的语义角色标注来帮助识别新闻领域复述句的方法。该方法通过在语义角色这种结构化的含义表达形式中提取的特征来弥补传统方法的不足 先识别待判断的两个句子中所有谓词的语义角色,然后计算两个句子间对应语义角色的相似度,最后结合传统的句子相似度计算方法来进行相似性计算。实验证明,该文提出的方法能有效地提高复述语句的识别效果。","复述识别,语义角色标注,自然语言处理"
2009-08-31,基于合一句法和实体语义树的中文语义关系抽取,"虞欢欢,钱龙华,周国栋,朱巧明","该文提出了一种基于卷积树核函数的中文实体语义关系抽取方法,该方法通过在关系实例的结构化信息中加入实体语义信息,如实体类型、引用类型和GPE角色等,从而构造能有效捕获结构化信息和实体语义信息的合一句法和实体语义关系树,以提高中文语义关系抽取的性能。在ACE RDC 2005中文基准语料上进行的关系探测和关系抽取的实验表明,该方法能显著提高中文语义关系抽取性能,大类抽取的最佳F值达到67.0,这说明结构化句法信息和实体语义信息在中文语义关系抽取中具有互补性。","中文语义关系抽取,卷积树核函数, 实体语义信息"
2009-09-22,基于树核函数的“it”待消解项识别研究,"陈九昌,孔 芳,朱巧明,周国栋","该文在基于特征的英文代词指代消解平台上,使用复合核函数,研究指代消解中待消解项“it”的识别问题。围绕“it”是否是待消解项,该文采取有效策略获得“it”句法结构信息与平面特征信息,并将它们结合起来生成“it”待消解项分类器。在测试分类器性能的同时,将其运用到代词指代消解中以检验它对指代消解的作用。最后在ACE2003基准语料上实验表明采用复合核生成的分类器具有较高的准确率,并能显著提高代词指代消解性能。","待消解项识别,复合核,指代消解"
2009-10-27,基于条件随机场的蒙古语词切分研究,"赵 伟,侯宏旭,从 伟,宋美娜","词干和构形附加成分是蒙古语词的组成成分,在构形附加成分中包含着数、格、体、时等大量语法信息。利用这些语法信息有助于使用计算机对蒙古语进行有效处理。蒙古语词在结构上表现为一个整体,为了利用其中的语法信息需要识别出词干和各构形附加成分。通过分析蒙古语词的构形特点,提出一种有效的蒙古语词标注方法,并基于条件随机场模型构建了一个实用的蒙古语词切分系统。实验表明该系统的词切分准确率比现有蒙古语词切分系统的准确率有较大提高,达到了0.992。","蒙古语,词切分,词干,构形附加成分,条件随机场,统计语言模型"
2009-09-15,自然语言处理用藏语格助词的语法信息研究,"扎西加,顿珠次仁","该文主要探讨了藏语格助词的语法信息、语义信息、功能结构等内容,从而为建立藏语格助词的语法属性库提供详实的参数。这对句子进行分析与描述,观察句子歧义都有着重要的意义。","藏语格助词,语法信息"
2009-08-25,班智达藏文标注词典设计,"才智杰,才让卓玛","语料库加工是一项庞大的语言工程,其中分词标注是最基础性的工作,而分词标注词典是标注系统的重要组成,词典设计的优劣直接关系着分词标注的速度和效率。在设计国家语委项目《班智达藏文自动标注系统》的基础上,给出了分词标注词典库的结构及词典库索引查询算法。对85万字节藏语实验语料的分词和标注,分词准确率达99%,标注准确率达97%。","藏语语料库,分词,标注,词典,索引"
2009-11-15,古籍版本异文的自动发现,"肖 磊,陈小荷","该文提出了古籍版本异文自动发现方法 首先由bigram计算得到句珠相似度,根据相似度发现最有可能的句珠配对,然后在异文句珠中不断地去掉最长“同文”并输出异文。研究个案是三传春秋经,结果表明,句珠配对全部正确,异文配对算法也能够正确发现全部符合定义的异文。","古籍,版本异文,句珠,相似度"
2009-08-27,基于Stacking组合分类方法的中文情感分类研究,"李寿山,黄居仁","情感文本分类(简称情感分类)是一种面向主观信息分类的文本分类任务。目前,由于其广泛的应用前景,该任务在自然语言处理研究领域中得到了普遍关注,相继出现多种用于情感文本分类的有监督的分类方法。该文具体研究四种不同的分类方法在中文情感分类上的应用,并且采用一种基于Stacking的组合分类方法,用以组合不同的分类方法。实验结果表明,该组合方法在所有领域都能够获得比最好基分类方法更好的分类效果。从而克服了分类方法领域依赖的困境(不同领域需要选择不同基分类方法才能获得更好的分类结果)。","计算机应用,中文信息处理,情感分类,组合分类器"
2009-09-20,基于聚类分析的搜索引擎自动性能评价,"吴世勇,王明文","传统的搜索引擎性能评价方法需要人工标注标准答案集,需花费大量的人力物力,并且评价结果依赖于人工标注的准确性,效率较低。该文基于聚类分析的思路,提出了一种搜索引擎性能评价指标和自动进行搜索引擎性能评价的方法,此方法能自动计算信息类查询的覆盖范围,并根据其覆盖范围对检索结果进行聚类,通过类间距和类内距等指标实现检索性能的自动评价。实验结果表明,基于聚类指标的评价方法与人工标注的评价方法的评价结果是相一致的。","信息检索,性能评价,聚类分析"
2009-07-15,基于三维文档向量的自适应话题追踪器模型,"张 辉,周敬民,王 亮,赵莉萍","话题追踪(TT)是研究自动追踪事件动态发展过程的一种信息智能获取技术,是话题检测与追踪(TDT)技术的一个子任务,其目标在于自动发现新闻报道信息流中与某一已知话题有关的新报道。该文通过分析传统文档向量空间模型的不足,结合新闻报道的特征,提出了一种三维文档向量模型,在此基础上建立了一种符合新闻报道特征的话题模型。该话题模型在追踪过程中能够根据事件的动态发展进行自我学习和自我修正。结合话题模型,该文还设计了一种自适应的KNN新闻话题追踪器,从而形成了一种完整的中文话题追踪器模型。实验数据表明该方法在描述新闻话题、避免话题漂移方面具有一定优势,在中文话题追踪领域取得了较好效果。","话题追踪,话题模型,三维文档向量模型,自适应KNN追踪器"
2009-07-15,基于事件项语义图聚类的多文档摘要方法,"刘茂福,李文捷,姬东鸿","基于事件的抽取式摘要方法一般首先抽取那些描述重要事件的句子,然后把它们重组并生成摘要。该文将事件定义为事件项以及与其关联的命名实体,并聚焦从外部语义资源获取的事件项语义关系。首先基于事件项语义关系创建事件项语义关系图并使用改进的DBSCAN算法对事件项进行聚类,接着为每类选择一个代表事件项或者选择一类事件项来表示文档集的主题,最后从文档抽取那些包含代表项并且最重要的句子生成摘要。该文的实验结果证明在多文档自动摘要中考虑事件项语义关系是必要的和可行的。","基于事件的摘要,事件语义关系图, DBSCAN聚类算法"
2009-09-27,Web平行语料挖掘及其在机器翻译中的应用,"林 政,吕雅娟,刘 群,马希荣","双语平行语料库在自然语言处理领域有很多重要应用,但是大规模双语平行语料库的自动获取并不容易。该文提出了一种有效的从Web上获取高质量双语平行语料库的方案,研究了候选双语混合网页获取和平行句对抽取等关键技术。运用该文方法共获取了258万双语平行句对,平均正确率为93.75%,其中前150万句对的平均正确率达到96%。该文还提出句对质量排序和领域信息检索两种方法将Web数据应用于统计机器翻译的模型训练,在IWSLT评测数据上BLEU值可以提高2到5个百分点。","Web挖掘,平行语料库,句子对齐,统计机器翻译"
2009-09-15,汉蒙机器翻译系统中量词翻译研究,"王斯日古楞 ,斯琴图,那顺乌日图 ","在基于短语的汉蒙统计机器翻译系统的研究中,我们发现存在着大量的汉蒙量词翻译错误。该文对汉语和蒙古语中的量词翻译进行研究的基础上,提出了使用量词表进行翻译,总结出了一对一、多对一、一对零和一对多等汉语量词到蒙语量词翻译的对应关系,给出了各种对应中的翻译方法,通过实验证明这种方法明显提高了现有汉蒙机器翻译系统的性能。","汉蒙机器翻译,汉语量词,蒙古语量词"
2009-06-25,一种支持ANSI编码的中文文本压缩算法,"常为领,方滨兴,云晓春,王树鹏,余翔湛","该文提出了一种高效的中文文本压缩算法CRecode,算法根据中文文本中字词的概率分布特点,对中文字词根据其使用频率,采用8bit、16bit和24bit三种长度的编码重新编码,克服了Huffman编码在压缩中文数据时打乱数据中蕴含的语义信息,致使其压缩数据再压缩性差的缺点。测试中,CRecode在与现有主流压缩软件联合使用时,可提高压缩率4%到30%,最大平均压缩比可达2.86。CRecode作为独立压缩算法,压缩中文文本时可获得优于Huffman编码、接近于LZ系列算法的性能。","CRecode,数据压缩,Huffman,压缩算法"
2009-09-08,基于特征参数归一化的鲁棒语音识别方法综述,"肖云鹏,叶卫平","目前,自动语音识别系统往往会因为环境中复杂因素的影响,造成训练环境和测试环境存在不匹配现象,使得识别系统性能大幅度下降,极大地限制了语音识别技术的应用范围。近年来,很多鲁棒语音识别技术成功地被提出,这些技术的目标都是相同的,主要是提高系统的鲁棒性,进而提高识别率。其中,基于特征的归一化技术简单而有效,常常被作为鲁棒语音识别的首选方法,它主要是通过对特征向量的统计属性、累积密度函数或功率谱的归一化来补偿环境不匹配产生的影响。该文主要对目前主流的归一化方法进行介绍,其中包括倒谱矩归一化方法、直方图均衡化方法以及调频谱归一化方法等。","鲁棒语音识别,倒谱均值归一化,高阶倒谱矩归一化,直方图均衡化,倒谱形状归一化"
2009-10-21,维吾尔语中清化元音的实验语音学研究,"地里木拉提·吐尔逊,艾斯卡尔·艾木都拉","该文根据语音合成与识别等语音应用研究的需求,从文本分析模块入手,利用“维吾尔语语音声学参数库”,选择了带高元音/i/,/u/和/ü/的多音节词(双音节、三音节词),分别对其发生清化和保持原来浊特性时的三种高元音的时长,音高和音强进行了统计分析,归纳了其发生清化时的时长、共振峰和音强在开音节和闭音节中的分布模式,从实验语音学的角度出发,进一步探讨了维吾尔语中三个高元音的清化特性,并验证了语言学者凭听力和生理而总结出来的结论与声学上的结论的一致性。其目的是为了提高语音合成的自然度即更好的为自然语言处理服务。该项研究对维吾尔语语言乃至整个阿尔泰语系语言的韵律研究具有较高的参考价值。","自然语言处理,维吾尔语,话音,元音,清化"
2009-06-03,女书计算机键盘布局与输入法的研究,"田 微,王江晴,朱宗晓,刘 赛,程 立","目前对女书的研究都是以手写为基础,使得研究进展缓慢,女书信息化迫在眉睫。为此,该文依据键盘布局的基本理论、女书书写规则,分析了女书键盘布局的合理性,并以此为基础,设计出了较科学的女书部首输入法。最后,对此输入法做了样本测试,结果表明此输入法有较高的输入效率。此输入法对女书印刷和女书信息处理具有广泛的使用价值。","女书,键盘布局,部首输入法,字元"
2010-05-06,基于动词的汉语复合名词短语释义研究,"王 萌,黄居仁,俞士汶,李 斌","复合名词短语的语义解释的主要目的是恢复修饰语和中心词之间隐含的语义关系。该文针对汉语复合名词短语的语义解释,首次采用动态的策略,提出了“基于动词的短语释义”的方法,利用语料库及Web数据,自动获取复合名词短语的释义短语,实验结果表明,该方法不仅可以为复合名词短语提供多种可能的语义解释,而且能够反应相似的复合名词短语之间细微的语义差别。此外,该文的研究结果可以服务于问答系统、信息检索、词典编纂等多个应用领域。","汉语复合名词短语,语义解释,释义短语,释义动词"
2010-02-09,基于树剪枝的典籍文本快速切分方法研究——以《茶经》的翻译为例,"姜 欣 ,姜 怡  ,方 淼 ,汪榕培","以《茶经》的翻译为例,基于树剪枝理论提出了一种典籍文本快速切分方法。首先,采用似然比统计量计算两字、三字甚至多字候选单元;然后在此基础上基于树剪枝的思想构建了典籍文本快速切分的模型算法,并构建了基本流程图;最后,以《茶经》为例验证了本算法的有效性和合理性。理论分析和算例表明,该算法能有效地对典籍文本进行自动切分,并简化了计算时间的复杂度,在推广中国典籍的对外传译方面具有良好的应用前景。","切分,树剪枝,似然比,茶典籍,机辅翻译"
2009-08-10,基于语义树的中文词语相似度计算与分析,"张 亮1.2,尹存燕1,陈家骏1","词语相似度的分析与计算是自然语言处理关键技术之一,对句法分析、机器翻译、信息检索等能提供很好的帮助。基于语义资源Hownet的中文词语相似度计算是近年来的研究热点,但大多数的研究都是对中国科学院计算技术研究所刘群提出的计算方法的改进和完善。该文充分分析和利用新版Hownet(2007)的概念架构和语义多维表达形式,从概念的主类义原、主类义原框架以及概念特性描述三个方面综合分析词语相似度,并在计算中区分语义特征相似度和句法特征相似度。实验结果理想,与人的直观判断基本一致。","语义树,词语相似度,《知网》2007,语义距离"
2010-01-18,基于《知网》的词语相似度算法研究,"刘青磊,顾小丰","基于《知网》的词语(句子)相似度计算通常是把义原(词语)之间的最优匹配做为运算的基本单位的,最终的整体相似度数值可由每一部分的相似度值通过适当的加权计算合成而来,这样的做法往往会造成一些匹配对的信息重复和结构不合理。针对这个问题,该文通过统计出两个直接义原集合间的共有信息(共性)和差异信息(个性)来计算集合的相似度,并把此方法引入到词语(句子)的相似度计算中去。最终的实验比对结果表明该文所采用的方法更为稳定和有效。","《知网》,词语相似度,句子相似度,共有信息,差异信息"
2009-11-20,一种语义数据的核分类方法,"李志华,任秋英,顾 言,王士同","语义数据的内积计算是个难点问题,制约了有关语义数据的核分类方法的研究和发展。针对此问题,通过给出一种语义数据相异性度量测度的新定义、计算语义数据内积的简化方法、研究核方法和支撑向量机中的核函数的本质,提出了一种语义数据的核分类方法,并把方法向语义数据、连续属性构成的异构数据的分类问题进行了拓展。仿真实验表明方法具有一定的抗离群数据干扰能力,方法的总体性能优于文献中已有的其他方法。通过在异常检测领域中的应用研究,说明方法能高效地实现不平衡数据的分类,具有一定的实用价值。","核分类方法,语义数据,相异性度量测度,内积计算"
2009-12-02,基于LDA话题演化研究方法综述,"单 斌,李 芳","现实生活中不断有新话题的产生和旧话题的衰减,同时话题的内容也会随着时间发生变化。自动探测话题随时间的演化越来越受到人们的关注。Latent Dirichlet Allocation模型是近年提出的概率话题模型,已经在话题演化领域得到较为广泛的应用。该文提出了话题演化的两个方面 内容演化和强度演化,总结了基于LDA话题模型的话题演化方法,根据引入时间的不同方式将目前的研究方法分为三类 将时间信息结合到LDA模型、对文本集合后离散和先离散方法。在详细叙述这三种方法的基础上,针对时间粒度、是否在线等多个特征进行了对比,并且简要描述了目前广泛应用的话题演化评测方法。文章最后分析了目前存在的挑战,并且对该研究方向进行了展望。","话题模型,话题演化,Latent Dirichlet Allocation"
2009-12-21,利用上下文信息的统计机器翻译领域自适应,"曹 杰,吕雅娟,苏劲松,刘 群","统计机器翻译系统用于翻译领域文本时,常常会遇到跨领域的问题 当待翻译文本与训练语料来自同一领域时,通常会得到较好的翻译效果;当领域差别较大时,翻译质量会明显下降。某个特定领域的双语平行语料是有限的,相对来说,领域混杂的平行语料和特定领域的单语文本更容易获得。该文充分利用这一特点,提出了一种包含领域信息的翻译概率计算模型,该模型联合使用混合领域双语和特定领域源语言单语进行机器翻译领域自适应。实验显示,自适应模型在IWSLT机器翻译评测3个测试集上均比Baseline有提高,证明了该文方法的有效性。","统计机器翻译,领域自适应,上下文信息"
2010-02-09,对话行为信息在口语翻译中的应用,"周可艳,宗成庆","在口语翻译中,如何融入语义及语用信息一直是目前研究的难点之一。对话行为作为浅层话语结构描述的特征,近年来陆续应用于不同类型的翻译系统中。该文在介绍对话行为理论和口语标注语料的基础上,以基于短语的统计翻译系统为应用对象,提出了对话行为应用于翻译过程的三种方式。该方法通过对对话行为的自动分类,使训练语料—测试语料、开发集—测试集、源语言—目标语言的一致性得到提高,提高了翻译系统的性能,使最终的翻译结果可以更准确地反映源语言所要表达的对话意图。在汉英口语翻译评测数据上的实验证明,对话行为信息的加入使翻译系统的性能得到了有效的提高。","对话行为,口语翻译,对话行为分类"
2009-12-29,嵌入式文本相关说话人识别算法的研究与开发,"郭皓婷,郑 方,罗灿华,李银国","说话人识别技术以其方便、经济和易于被接受等特点日益成为人们生活和工作中重要且普及的用户身份验证方式,但是在嵌入式领域的应用中,现有算法难以很好地满足实时性的要求。该文研究了应用于语音识别的非线性分块算法,将其思想加以改进,以逐块对比的识别方式用于嵌入式的文本相关说话人识别,与传统的基于动态时间弯折的方法相比,在实时性方面取得了良好的实用效果。","说话人识别,文本相关,嵌入式,非线性分块"
2010-02-04,一种面向查询的多文档摘要方法,"叶 娜,蔡东风","面向查询的多文档摘要技术有两个难点 第一,为了保证摘要与查询密切相关,容易造成摘要内容重复,不够全面;第二,原始查询难以完整描述查询意图,需进行查询扩展,而现有查询扩展方法多依赖于外部语义资源。针对以上问题,该文提出一种面向查询的多文档摘要方法,利用主题分析技术识别出当前主题下的子主题,综合考虑句子所在的子主题与查询的相关度以及子主题的重要度两方面因素来选择摘要句,并根据词语在子主题之间的共现信息,在不使用任何外部知识的情况下,进行查询扩展。在DUC2006评测语料上的实验结果表明,与Baseline系统相比,该系统取得了更高的ROUGE评价值,基于子主题的查询扩展方法则进一步提高了摘要的质量。","面向查询,多文档摘要,子主题,相关度,查询扩展"
2010-01-14,搜索引擎查询推荐技术综述,"李亚楠,王 斌,李锦涛","查询推荐技术,其用于找出与初始查询或关键词相关的其他查询或关键词,被广泛用于搜索引擎和广告检索系统中。作为当今搜索引擎的必备技术之一,查询推荐技术研究正受到越来越多的关注,近几年出现了很多验证查询推荐可用性及改进其算法的研究工作。为此,该文对查询推荐的发展过程、技术方法、评价体系等方面进行了归纳和总结,分析了查询推荐面临的挑战并讨论了现有解决方法及未来研究思路,希望能对相关研究人员有所帮助。","计算机应用,中文信息处理,综述,查询推荐,信息检索"
2010-06-30,Kad网络节点资源探测分析,"刘祥涛,龚才春,刘 悦 ,白 硕","Kad网络中存在数以亿计的共享资源,而其中有相当一部分可被评定为敏感资源。为深入了解Kad网络上资源尤其是敏感资源的特征,运用Kad网络采集器 Rainbow对节点拥有的文件资源进行探测分析。该文发现 1)文件流行度和文件所对应的文件名数量都近似符合Zipf分布;2)利用同一个“文件内容哈希”(即file-content-hash)的多个文件名的共现词可以更准确地进行敏感判别;3)敏感资源占随机样本的6.34%,且敏感资源中74.8%为video文件。","对等网络,Kad网络,探测分析,敏感资源"
2010-01-05,一种主题爬虫文本分类器的构建,"姜 鹏,宋继华","该文利用DF与CHI统计量相结合的特征选取方法,针对互联网上对外汉语相关领域的网页进行特征提取,并在此基础上,构建了基于标题与正文相结合的两步式主题相关度判定分类器。基于该分类器做对外汉语相关主题的网页爬取工作,实验表明,效率和召回率比传统分类器都有较大程度的提高,目前该分类器已经用于为大型对外汉语语料库构建提供数据源。","DF,CHI统计量,分类器,主题爬取"
2009-11-30,启体书法字的矢量化,"曹 芳,武仲科,敖雪峰,周明全","矢量字以其可以方便地进行各种变换和高质量的显示及输出效果得到广泛应用,汉字的矢量化是中文信息处理的重要内容之一。该文通过非细化的矢量化算法中的轮廓线的方法,针对启功书法字中的3 755个一级汉字完成了矢量化,并建立了包括每个字的笔画的矢量表示、笔画顺序等信息的启功书法的矢量字库,为启体书法的逐笔临摹学习奠定了基础。文章详细论述了对书法字轮廓的提取过程,笔画分割过程和对提取结果的优化处理方法。","矢量化,书法,启体,笔画"
2010-04-20,甲骨文自由笔画输入法,"聂艳召,刘永革","甲骨文信息的数字化需要输入法的支持,已有甲骨文输入法的研究思路还存在可改进之处。从笔画的层面对甲骨文字形进行系统性的笔画分析,提出了一种基于笔画分析的编码方案,并进行了编程实现。运行结果显示该思路可行,可为甲骨文研究者提供方便的输入接口。","甲骨文,输入法,笔画"
2010-01-26,关于中文拼音输入法键盘字母布局的探究,"黄金文,金 华,王 凡,陈斌宏,何泳姝,陈晓伟,林庆文,黄晓明","为了改进现有键盘的字母布局,使之更能适用于汉字拼音输入法,该文根据键盘布局的设计原则、相关的科学原理和基于汉字、拼音字母使用频率的统计数据,设计出一种较为合理的、适用于汉字拼音输入法的键盘布局。该文从静态、动态工作量和左右手交替率三方面,与现有键盘的字母布局进行比较。在工作量方面,对于同一手指新的键盘布局依食指、中指、无名指、小指呈线性递减关系,较好地符合各手指实际可承受负荷量,而其左右手放宽条件下的交替率为0.748 33,分析数据可知新设计出的键盘字母布局在汉字拼音输入法的输入效率方面有着显著的提高。","计算机应用,中文信息处理,键盘布局,汉字拼音,拼音输入法"
2010-05-08,哈萨克语基本名词短语自动识别研究与实现,"孙瑞娜,古丽拉·阿东别克","以哈萨克语基本名词短语识别为目标,实现了哈萨克语基本名词短语自动识别系统。采用基于规则自动识别及人工标注的方法建立基本名词短语标注语料库,在此基础上,采用统计和规则相结合的识别方法,利用互信息进行基本名词短语边界预测,然后根据哈萨克语基本名词短语构成规则对预测边界进行调整,加入标注符,得到最终的识别结果。实验结果表明,两种方法封闭测试的识别精确率分别为80.2%和82.5%。","语料库,基本名词短语,哈萨克语,互信息,规则"
2009-10-17,基于Unicode编码的蒙古文输入法研究,"范道尔吉,白凤山,武慧娟","从Vista开始微软操作系统已经完全支持了传统蒙古文的输入、编辑和排版。该文在微软蒙古文输入法的基础上结合蒙古文的自身特点提出了一种新型蒙古文输入法算法。该算法支持自动变形计算、自动联想输入、自动学习和资源共享等功能。文中给出了自动变形计算的原理和详细算法过程,并详细探讨了蒙古文字典数据的存储和使用方法,最后提出了自动学习算法和资源共享技术的解决方案。","蒙古文输入法,Unicode,自动变形,Uniscribe"
2010-03-05,基于QTE的蒙古文显示机制研究与实现,"白凤山,范道尔吉,金宇新,吴 威,张利宏","蒙古语言是中国蒙古族使用的通用语言,由于蒙古文区别于其他文字的书写方式和其自身变形机制等特点,在很多通用的文字处理引擎中都不被支持。在嵌入式产品开发与应用领域中Linux加QTE已经成为流行方式。该文给出了一种在QTE环境上实现基于标准Unicode的蒙古文点阵显示和变形算法, 并自定义了支持蒙古文的QTE组件,扩展了QTE功能,为在Linux加QTE方式的嵌入式体系结构中处理蒙古文提供了一种解决方法。","QTE,Linux,蒙古文,Unicode"
2010-08-31,问答社区中回答质量的评价方法研究,"孔维泽,刘奕群,张 敏,马少平","问答社区已经成为网络信息获取的一种重要渠道,但其信息质量差异较大。该文研究了问答社区中回答质量的评价方法。具体考察了百度知道的问答社区环境,并对其构建了大规模的语料数据。针对百度知道的特点,文本提出的基于时序的特征、基于问题粒度的特征和基于百度知道社区用户的特征,从更多的角度对回答质量进行评价。利用分类学习的框架,该文综合了新设计的三方面特征和经典的文本特征、链接特征,对高质量和非高质量的回答进行分类。基于大规模问答语料的实验表明,在文本特征与链接特征的基础上,基于时序与基于问题粒度的特征能够有效地提高回答质量的评估效果。另外也发现,根据该文的回答质量评价框架做出的质量评分能够有效地预测最佳答案。","问答社区,质量评价"
2010-08-30,基于关联度模型的文本倾向性检索研究,"刘全升,姚天昉","该文在研究了信息检索理论与文本倾向性分析技术等的基础上,结合国内外关于观点检索的相关研究,提出了基于关联度的文本观点检索算法。它综合考虑了主题检索过程中的查询扩展、文本检索相关度、文本倾向性强度和检索主题与文本情感的关联度等对观点检索最后结果的影响。该算法从理论上考虑了观点检索不同因素之间的相互影响问题。通过对COAE2008观点检索子任务的实验数据进行实验,结果表明 该文提出的基于关联度的观点检索算法可以取得较好的效果。","观点检索,关联度,文本挖掘"
2010-08-25,Twitter中近似重复消息的判定方法研究,"曹 鹏,李静远,满 彤,刘 悦,程学旗","微博客是Web2.0出现以来的一个新生概念。著名的Twitter系统是微博客中具有代表性的一个,其全球用户已经超过1.6亿,在世界范围内具有重要影响力 目前知名政治家、社会名流和大企业几乎都是Twitter的用户。Twitter系统中的消息小于140个字符,而且语法不规范。同时,由于Twitter允许用户以多种格式自由转发消息,系统中存在大量内容重复或近似重复的消息。重复消息的存在加重了系统存储的负担,对用户阅读、理解以及分析消息的内容也造成了不利影响。该文分析了Twitter系统中转发消息的语法特点,并利用这些语法特点提取规则,把转发的消息变成普通消息。该文还提出统计字符种类和最短编辑距离两种字符串距离计算的方法以判定Twitter中近似重复的消息。该文还分析了Twitter消息发送的方式以及不同登录方式的消息特征。实验结果表明,两种方法具有扩展性强、实现简单、效率高等优点,能够有效地检测Twitter上的信息重复现象。","微博客,Twitter,重复消息"
2010-08-25,基于权重标准化SimRank方法的查询扩展技术研究,"马云龙,林 原,林鸿飞","查询扩展是信息检索中的一项重要技术。传统的局部分析查询扩展方法利用伪相关文档作为候选词集合,然而部分伪相关文档并不具有很高的相关性。该文利用真实的搜索引擎查询日志,建立了查询点击图,经过多次图结构的转化得到能够反映词之间关联程度的词项关系图,并在图结构的相似度算法SimRank的基础上,提出了一种基于权重标准化的改进SimRank方法,该方法利用词项关系图中词项的全局和间接关系,能够有效挖掘与原始查询相关联的扩展词。同时,为降低SimRank算法的计算复杂度,该文采用了剪枝等策略进行优化,使得计算效率有大幅提高。在TREC标准数据集上的实验表明,该文的方法可以有效地选择相关扩展词。MAP指标较局部分析查询扩展方法提高了1.81%,在P@10和P@20指标评价中效果分别提高了5.44%和3.73%。","搜索引擎,查询扩展,查询日志,SimRank"
2010-08-15,基于群体智慧的Web访问日志会话主题识别研究,"方 奇,刘奕群,张 敏,茹立云,马少平","Web访问日志中的会话(session)是指特定用户在一定时间范围内的访问行为的连续序列。会话主题(topic)是指会话中具有相同用户意图的部分。从会话中进一步识别出能体现用户意图的处理单元(topic)是进行用户访问行为分析的重要基础。目前相关工作主要集中在边界识别上,无法处理用户意图交叉情况。为了解决该问题,该文重新形式化定义了session和topic的相关概念,提出最大划分的求解任务,并设计出了基于用户群体智慧的会话主题识别算法。在使用大规模真实Web访问日志的实验中,我们的算法取得了不错的效果。","会话主题识别,Web访问日志"
2010-08-30,基于LDA模型的博客垃圾评论发现,"刁宇峰,杨 亮,林鸿飞","Blog(博客)作为一种新兴的网络媒体,在很大程度上增强了互联网的开放性,Blog已经成为互联网上的主要信息源之一,这也使得Blog空间中的垃圾评论成倍增长,因此如何识别垃圾评论成为面临的重要问题。该文首先借鉴处理垃圾邮件的方法,针对Blog本身的特点,使用规则初步过滤垃圾评论,然后对剩余评论,利用Latent Dirichlet Allocation(LDA) 这种能够提取文本隐含主题的产生式模型,对博客中的博文进行主题提取,并结合主题信息进行判断,从而识别Blog空间的垃圾评论。通过实验验证,该方法可以发现大多数垃圾评论,实验取得了较好的结果,使Blog信息更加准确、有效的为用户使用。","Blog,博文,LDA,主题,垃圾评论"
2010-08-21,面向互联网舆情的热词分析技术,"李渝勤,孙丽华","热词是一种网络词汇现象,反映了某一特定时空范围内人们普遍关注的问题。该文对热词分析的两项关键技术——热词发现和热词关联技术进行了深入的研究。在热词发现阶段,首先采用命名实体识别技术和高频串统计技术进行短语串的挖掘,继而采用基础权值和波动权值两项指标进行热度权值的计算。在热词关联阶段,按热词权值高低进行热词类的划分,通过同现率的原则确定热词类之间的关联计算。该文所采用的方法已经成功应用到TRS舆情监测系统的热点发现模块。","热词,命名实体识别,热度计算,波动权值,词群关系"
2010-08-22,大规模短文本的不完全聚类,"彭泽映,俞晓明,许洪波,刘春阳","聚类分析是数据挖掘的一个重要手段,人们可以通过聚类发现信息中潜在的热点或规律。至今,已经有大量聚类算法被研究和提出。随着互联网的日益普及,查询日志、Twitter等短文本信息逐渐在人们生活中起着越来越重要的作用。这类短文本信息数量巨大,通常可达到千万乃至亿级,现有的聚类算法在对这类大规模短文本信息进行聚类分析时往往显得异常无力。该文通过对实际应用中的短文本信息进行实验分析,发现了这类数据类别所具有的“长尾现象”,并由此提出了不完全聚类思想,可以有效地提高这类短文本信息的聚类性能。","短文本,聚类分析,不完全聚类"
2010-08-22,一种基于LDA的潜在语义区划分及Web文档聚类算法,"刘振鹿,王大玲,冯 时,张一飞,方东昊","该文应用LDA模型进行文档的潜在语义分析,将语义分布划分成低频、中频、高频语义区,以低频语义区的语义进行Web游离文档检测,以中、高频语义区的语义作为文档特征进行文档聚类,采用文档类别与语义互作用机制对聚类结果进行修正。与相关工作比较,该文不仅应用LDA模型表示文档,而且进行了深入的语义分布区域划分,并将分析结果应用于Web文档聚类。实验表明,该文提出的基于LDA的文档类别与语义互作用聚类算法获得了更好的聚类结果。","LDA,潜在语义,语义分布,文档聚类"
2010-08-19,面向信息检索的近邻语言模型,"韩中元,李 生,齐浩亮,杨沐昀","面向信息检索的语言模型对单篇文档构建语言模型,存在较严重的数据稀疏问题。该文认为利用文档的近邻信息能够更合理地反映词在文档中的分布,有助于数据稀疏问题的解决,因此将文档的近邻信息加入语言模型的平滑算法中,提出近邻语言模型。该文在TREC评测的典型文档集美国能源署文件(DOE)和《华尔街日报》(WSJ)数据集上测试了在不同近邻选择来源上近邻语言模型的性能。实验结果表明,近邻语言模型对检索性能有一定的提升。","信息检索,语言模型,近邻信息"
2010-08-13,基于搜索引擎的双语混合网页识别新方法,"冯艳卉,洪 宇,颜振祥,姚建民,朱巧明","该文提出了一种从搜索引擎返回的结果网页中获取双语网页的新方法,该方法分为两个任务。第一个任务是自动地检测并收集搜索引擎返回的结果网页中的数据记录。该步骤通过聚类的方法识别出有用的记录摘要并且为下一个任务即高质量双语混合网页的验证及其获取提供有效特征。该文中把双语混合网页的验证看作是有效的分类问题,该方法不依赖于特定领域和搜索引擎。基于从搜索引擎收集并经过人工标注的2 516条检索结果记录,该文提出的方法取得了81.3%的精确率和94.93%的召回率。","Web挖掘,双语混合网页,平行语料"
2010-08-21,语义词典归类不当现象自动发现,"邱立坤,邵艳秋","平行周遍原则是陈保亚(1999)提出的一种用于区分词与短语的理论,将词语分为既平行又周遍、平行不周遍和不平行不周遍三类,既平行又周遍的是短语,不宜收入词典。由于汉语语义词典在收词时并未严格遵循平行周遍原则,因此依据现有语义词典可以自动地归纳出许多平行周遍规则或者平行不周遍规则。假定有两部语义词典,则可以归纳出两套平行(不)周遍规则,每一套规则都有各自的正例和反例。一个词典中某一规则的反例如果同时是另一词典中的正例,就意味着前一部词典中的词语可能归类不当。基于这一思路,该文提出一个基于平行周遍原则的语义词典归类不当现象自动发现方法,实验结果证明了这一方法的有效性。","平行周遍原则,语义词典,归类,自动发现"
2010-08-18,基于上下文的真词错误检查及校对方法,"陆玉清,洪 宇,陆 军,姚建民,朱巧明","英文文本中的真词错误即输入的错词是和原词相似的另一个有效词。该文主要研究了对该类错误的检测。通过从所要检测的单词的上下文中提取句法和语义两个方面的特征,运用文档频率和信息增益进行特征筛选,实现了对上下文特征的有效提取。最终把判断该单词使用的正确与否看作分类问题,使用 Winnow分类算法进行训练和测试。通过5阶交叉验证,所收集的61组混淆集的平均正确率与召回率分别为96%,79.47%。","真词错误,特征筛选,混淆集,Winnow算法"
2010-08-18,面向文本拷贝检测的分布式索引,"张 玥,俞昊旻,张 奇,黄萱菁","如何对大规模文档集进行高效的拷贝检测是长期以来一直受到研究者们关注的问题。通常的拷贝检测算法都需要借助倒排索引。因此良好的索引结构对于算法性能至关重要。同时,随着文档集规模的增大,单机实现的索引已经不能满足拷贝检测的需求,需要引入分布式存储的索引。为了适应文档集规模的不断增大,良好的分布式索引应该同时具备较高的效率和可扩展性。为此该文比较了两种不同的分布式索引结构,Term-Split 索引和Doc-Split 索引,并且给出了Map-Reduce范式下建立这两种索引的实现,以及以这两种索引为基础的文本拷贝检测方法,Term-Split 方法和Doc-Split方法。在WT10G文档集上进行的实验表明Doc-Split 方法具有更好的效率和可扩展性。","拷贝检测,重复检测,Map-Reduce"
2010-08-17,基于结构挖掘的论坛检索模型,"杨小锐,林 磊,孙承杰,刘秉权","随着互联网的发展,网络论坛中蕴涵着数量巨大且质量较好的知识资源。因此对论坛信息进行有效地检索具有重要应用价值。该文研究适合于论坛数据的检索模型,以期能够充分利用论坛平台累积的海量数据来满足用户的信息需求。针对论坛页面和普通新闻页面的区别,该文提出关键帖抽取算法和论坛线索重构算法来选择论坛中信息含量丰富的帖子,并利用得到的帖子构建检索系统。实验表明该文提出的方法能够有效地提升论坛检索系统的效果。","论坛检索,排序支持向量机,关键帖抽取"
2010-08-20,一种基于HITS算法的Blog文摘方法,"苗 家,马 军,陈竹敏","Blog文章对应了大量评论信息,评论中又包含大量的噪声,因此如何结合Blog评论获取Blog文章的主要内容是许多基于Blog的应用所要面临的难题。以往提出的文摘方法大多是针对多文档文摘的通用方法,并未考虑Blog文章的特殊性,无法有效地结合评论来处理文章。该文通过分析Blog的特点提出了一种新的结合评论信息的Blog文摘方法。该方法首先基于特征计算出评论的权重,然后结合图模型使用HITS算法得到正文句子权重,进而得到文摘句。通过在凤凰博客数据集上的实验表明,该文方法在ROUGE测度上优于以往方法。","文档自动摘要,Blog,评论,HITS"
2010-08-15,基于浅层句法分析的中文语义角色标注研究,"王 鑫,孙薇薇,穗志方","语义角色标注是获取语义信息的一种重要手段。许多现有的语义角色标注都是在完全句法分析的基础上进行的,但由于现阶段中文完全句法分析器性能比较低,基于自动完全句法分析的中文语义角色标注效果并不理想。因此该文将中文语义角色标注建立在了浅层句法分析的基础上。在句法分析阶段,利用构词法获得词语的“伪中心语素”特征,有效缓解了词语级别的数据稀疏问题,从而提高了句法分析的性能,F值达到了0.93。在角色标注阶段,利用构词法获得了目标动词的语素特征,细粒度地描述了动词本身的结构,从而为角色标注提供了更多的信息。此外,该文还提出了句子的“粗框架”特征,有效模拟了基于完全句法分析的角色标注中的子类框架信息。该文所实现的角色标注系统的F值达到了0.74,比前人的工作(0.71)有较为显著的提升,从而证明了该文的方法是有效的。","语义角色标注,浅层句法分析,语素,构词法"
2010-08-31,基于Low-IDF-SIG的句子重复检测,"俞昊旻,张 玥,张 奇,黄萱菁","随着互联网上数据的爆炸式增长,互联网上产生了大量的重复数据。这些重复数据给搜索引擎、观点挖掘等许多Web应用带来了严峻的问题。目前绝大部分的重复检测的算法均着重考虑文档级别,不能有效地检测出两个文档中只有一部分互为拷贝的情况。而句子级别的重复检测正是解决这类问题的一个必要步骤。该文提出了一种快速有效的句子级别的特征抽取方法——Low-IDF-Sig算法,算法依据选定的先行词从句子中抽取出改进的Shingle特征以表示句子内容。真实语料库上的实验结果证明该文提出的算法能有效地提高句子级别重复检测任务的效率和精度。","近似重复检测,特征抽取,Low-IDF-SIG"
2010-08-15,基于北京大学中文网库的语义角色分类,"杨  敏,常宝宝","语义角色标注的研究方法中使用最频繁的一类是基于特征工程,将任务转化成分类问题使用机器学习的方法来解决,几乎所有的有指导语义角色标注采用的标注语料都是宾州大学命题库标注体系。近年来,北京大学开发出一套新的标注语料—北京大学中文网库,该文的目的在于测试这类研究方法在新语料的效果,验证之前所使用的特征是否对标注语料具有依赖性。通过实验发现前人方法中的一些不足,尤其个别特征在北大网库上作用更关键。","语义角色标注,北京大学中文网库,序列标注"
2010-08-11,面向查询的多模式自动摘要研究,"李 芳,何婷婷","为了满足用户的个性化需求,提供尽可能丰富、实用、方便的文摘结果,该文设计了面向查询的多文档自动文摘的多种摘要模式。在将查询返回的文档集合表示为以文本、段落为节点的双层复杂网络结构以发现子主题的基础上,除传统的摘要模式外,该文又设计了概括摘要、局部摘要、全局摘要和详细摘要这四种摘要模式,并给出了各种摘要的生成方法。支持用户以主题为线索自主漫游,按照一定的逻辑顺序浏览信息。","面向查询的多文档自动文摘,子主题发现,多模式摘要"
2010-08-20,音乐领域典型事件抽取方法研究,"丁 效,宋 凡,秦 兵,刘 挺","事件抽取是信息抽取领域一个重要的研究方向。该文从音乐领域的事件抽取出发,通过领域事件词聚类的方法自动发现音乐领域具有代表性的事件,然后采用基于关键词与触发词相结合的过滤方法简化了事件类型的识别过程。在事件元素识别中,该文采用了基于最大熵的事件元素识别方法。在该文构建的语料库下,最终事件类型识别的平均F值达到82.82%,事件元素识别的平均F值达到75.79%。","事件抽取,事件类型发现,事件类型识别,事件元素识别"
2010-08-05,依存信息在蛋白质关系抽取中的作用,"刘 兵,钱龙华,徐 华,周国栋","基于核函数的蛋白质关系(PPI)抽取可以捕获结构化信息,取得了较高的性能,但其计算复杂度过高。该文结合词汇、句法等信息,重点探讨了依存信息对基于特征向量的蛋白质关系(PPI)抽取的影响。在多个PPI语料库上的实验表明,依存信息和基本短语块信息可以有效提高基于特征向量的PPI抽取性能。特别要指出,在AIMed语料上的PPI抽取取得了54.7的F测度,是目前基于特征向量的PPI抽取系统的最好水平。","蛋白质关系抽取,支持向量机,依存信息"
2010-08-23,基于维基百科类别的文本特征表示,"王 锦,王会珍,张 俐","该文提出了基于维基百科类别体系的文本特征表示方法,方法是将文本中的词映射到维基百科的类别体系中,使用类别作为特征来对文本进行表示。基于维基类别的文本特征表示方法可以增强文本特征表示能力,降低文本特征空间维数。针对维基百科条目在语料中覆盖度不足的问题,该文提出了一种基于全局信息自学习维基百科类别的方法。该文构造基于维基百科类别为文本表示的分类系统,实验结果证明,基于维基百科类别作为文本表示特征,相对于词袋模型,具有明显的降维效果,在当特征数量较少时(如:&lt;700),分类的F1值提高了5.14%。","文本分类,维基百科类别,文本表示"
2010-08-27,基于相似度的网页标题抽取方法,"李国华,昝红英","目前网页标题的抽取方法大多结合HTML结构和标签特征进行抽取,但是这些方法并没有考虑标题与正文信息之间内容上的联系。该文提出一种基于相似度的网页标题抽取方法,该方法利用网页标题与正文信息之间的关系,通过计算语言“单位”之间的相似度和对应的权值,并引入HITS算法模型对权值进行调整,根据特定的选取方法抽取出真实标题。实验结果表明,该方法不仅对“非标准网页”的抽取达到满意的效果,而且对“标准网页”具有较高的泛化能力。","网页标题抽取,相似度,Web信息抽取"
2010-08-19,基于流形排序的查询推荐方法,"朱小飞,郭嘉丰,程学旗,杜 攀","针对传统查询推荐方法中存在的相关性度量问题和冗余性问题,该文中提出了一种新的基于流形排序的查询推荐方法。该方法利用查询数据内在的全局流形结构来获得查询之间的相关性,可以有效避免传统方法中相关性度量对高维稀疏查询数据处理的不足;同时,该方法通过提升结构上具有代表性的查询来达到减小查询推荐的冗余性。在一个大规模商业搜索引擎查询日志上的实验结果表明:使用流形排序的查询推荐方法要优于传统查询推荐方法和现有的Hitting-time Ranking方法。","查询推荐,流形排序,click-through data"
2010-08-30,基于日志分析的中文输入法用户行为研究,"许丹青,刘奕群,岑荣伟,马少平,茹立云,杨 磊","与拼音文字不同,用户在进行中文输入时需要借助输入法软件完成从拼音串到汉字串的转换过程,输入法因此成为中文用户进行人机交互的基础性工具,而输入法的相关技术研发也一直是学术界与产业界的关注热点。在中文输入法技术的研究中,用户的行为特点对输入法软件的词库建立、算法设计、交互方式设计与性能评价等多方面都有着至关重要的作用,但由于数据获取与分析的困难,这方面的相关研究尚不多见。该文利用某中文输入法在用户许可下收集的超过4.1亿条用户输入行为记录,进行了中文输入法用户行为的分析研究,针对不同类别应用程序的输入词频差异,不同用户在同类应用程序中的不同候选词条的选择等行为特点进行了挖掘分析,研究结果会对深入了解中文输入法用户行为,进而改进输入法软件性能具有一定的指导意义。","中文输入法,用户行为,日志分析"
2010-08-01,题录信息的机器翻译方法,"李贤华,于 淼,苏劲松,吕雅娟","该文针对题录信息中的人名、地址、机构名和公司名的不同特征,分别设计了不同的翻译方法,并依靠词典和翻译规则,实现了大部分内容的翻译。对于人名翻译,该文设计了拼音转换、假名转换和同音转换的翻译方法;对于地址、机构名和公司名的翻译,该文提出了先切分、再翻译、最后调序的翻译流程。实验表明,利用该文的方法翻译人名、地址、机构名及公司名,能够取得不错的翻译效果。","题录信息,机器翻译,人名翻译,地址翻译,机构名翻译"
2010-08-25,利用依存限制抽取长距离调序规则,"涂兆鹏,刘 群,林守勋","长距离调序是统计机器翻译领域的一个重要问题。层次短语模型提供了一个很好的解决方案,它使用层次短语规则可以很好地表示局部调序和长距离调序。但是,使用传统的算法抽取长距离层次规则将会导致规则表数量急剧增加,从而加大解码内存和时间消耗。为了解决这个问题,该文提出了一种利用依存限制抽取长距离调序规则的新方法。实验表明,该文的方法可以比基准系统高出0.74个BLEU点。","统计机器翻译,层次短语模型,长距离调序,依存限制"
2010-08-30,面向移动终端的统计机器翻译解码定点化方法,"李 响,徐金安,姜文斌,吕雅娟,刘 群","面向移动终端的统计机器翻译需求越来越多,但无浮点运算单元的处理器限制了翻译速度。该文提出了一种对统计机器翻译解码运算的定点化运算方法,缓解了无浮点运算单元的处理器对翻译速度的影响。基于PC和移动终端的实验表明,在保证翻译质量的情况下,利用定点处理浮点运算的解码器的运算速度较编译器模拟的浮点运算速度提高135.6%。因此,该方法可以有效地提高浮点运算能力薄弱的移动终端统计机器翻译速度。","统计机器翻译,定点化,移动终端"
2010-08-13,基于句对质量和覆盖度的统计机器翻译训练语料选取,"姚树杰,肖 桐,朱靖波","该文研究的目的是在待翻译文本未知的情况下,从已有的大规模平行语料中选取一个高质量的子集作为统计机器翻译系统的训练语料,以降低训练和解码代价。该文综合覆盖度和句对翻译质量两方面因素,提出一种从已有平行语料中获取高质量小规模训练子集的方法。在CWMT2008汉英翻译任务上的实验结果表明,利用本文的方法能够从现有大规模语料中选取高质量的子集,在减少80%训练语料的情况下达到与Baseline系统(使用全部训练语料)相当的翻译性能(BLEU值)。","句对质量评价,覆盖度,统计机器翻译,线性句对质量评价模型,训练语料选取"
2010-08-11,基于最大熵短语重排序模型的特征抽取算法改进,"孙 萌,姚建民,吕雅娟,姜文斌,刘 群","该文针对统计机器翻译中基于最大熵短语重排序模型特征抽取算法,提出一种改进算法。该算法能够抽取出更多准确的短语重排序信息,特别是逆序短语的特征信息,解决了原算法中最大熵训练时特征数据不平衡的问题,提高了翻译中短语重排序的准确率。以NIST MT 05 作为汉语到英语翻译的测试集,实验结果表明改进后的系统BLEU值比原系统提高0.65%。","最大熵,特征抽取,统计机器翻译,重排序模型"
2010-08-30,基于概率潜在语义分析的词汇情感倾向判别,"宋晓雷,王素格,李红霞,李德玉","该文利用概率潜在语义分析,给出了两种用于判别词汇情感倾向的方法。一是使用概率潜在语义分析获得目标词和基准词之间的相似度矩阵,再利用投票法决定其情感倾向;二是利用概率潜在语义分析获取目标词的语义聚类,然后借鉴基于同义词的词汇情感倾向判别方法对目标词的情感倾向做出判别。两种方法的优点是均可在没有外部资源的条件下,实现词汇情感倾向的判别。","概率潜在语义分析,数据稀疏,语义聚类,情感倾向"
2010-08-18,汉语情感问题类型分类研究,"李婷玉,葛正荣,姚天昉","随着网络搜索引擎技术的飞速发展,对于问答系统的需求愈发迫切。而问答系统处理问题的第一步就需要分辨情感问题和非情感问题并对情感问题进行分类。该文首先分析了当前问答系统和问题分类领域的研究现状,总结了一些存在的问题。然后针对情感问题从三个方面进行分类。在语义层面,提取了三个关键词;在语法层面,通过规则的制定,将其分成五种疑问句类型;在领域层面,通过搜索引擎的相关网页数量来进行判断。再对综合上述三个方面所开发出的测试系统进行分析。实验结果表明:对于情感问题的分类,从三个层面进行分析比较全面。","情感问题,问答系统,问题分类,自然语言处理"
2010-08-17,基于词典的名词性隐喻识别,"贾玉祥,俞士汶","隐喻是用一个事物来类比另外一个事物的语言表达,在自然语言中非常普遍,要实现自然语言理解隐喻处理不可避免。该文针对最基本的隐喻类型——名词性隐喻,提出基于词典的识别方法。结合同义词词林的语义距离与HowNet的语义关系来识别隐喻,考察隐喻与语义距离及语义关系之间的关联。","名词性隐喻,隐喻识别,词典,语义距离,语义关系"
2010-08-03,汉藏短语抽取,"诺明花,张立强,刘汇丹,吴 健,丁治明","该文将从汉藏法律法规和公文领域平行语料中提取双语短语对。考虑现阶段藏文资源匮乏,提出两步汉藏短语抽取方法。第一步是提取汉语有效语块,这部分工作不是该文工作重点。第二步是获取待翻译汉语短语的译文,该模块提出藏文词序列相交算法抽取藏文短语。该算法可以很好的抽取1-1和1-n连续和非连续藏文短语。","汉藏短语抽取,藏文信息处理,中文信息处理"
2010-03-29,论维吾尔语SUBS＋NP结构的形式化描述,阿孜古丽·夏力甫,"该文按照数理逻辑的方法从句法格式,论元及论元模式,动词和名词之间语义关系及逻辑表达式等四个方面对SUBS+NP结构进行初步的形式化描述为自然语言信息处理服务。结果表明维吾尔语SUBS+NP结构中动词与名词之间的多种语义关系决定动词可能选择的表达形式即“”型动名词形式和形动词形式,并且动词与名词之间的语义关系服从句法的编码,借助两种句法形式表现出不同的语义内容。","SUBS+NP结构 ,形式化 ,计算语言学"
2009-12-21,基于分词提取重复串的未登录词遗漏量化模型,"张海军,史树敏,丁溪源,黄河燕","基于重复串构造候选词集合是未登录词识别(UWI)的重要方法,目前有两种策略用于重复串提取:基于字符和基于分词。该文针对这两种策略实施了大量对比研究,并提出了基于分词提取重复串的未登录词遗漏量化模型,用以评估未登录词漏召问题。分析表明,该量化模型与实验数据之间具有良好的交互验证关系。根据对量化模型的讨论,该文得出了应用不同策略进行未登录词识别的可靠结论,该结论对后续研究具有一定的参考价值。","未登录词识别,重复串,条件随机域模型,中文分词"
2010-08-31,基于中心语匹配的共指消解,"张牧宇,黎耀炳,秦 兵,刘 挺","共指消解是自然语言处理的核心任务之一。在传统机器学习方法使用的平面特征基础上,该文提出一种利用中心语信息的新方法。该方法首先引进一种基于简单平面特征的实例匹配算法用于共指消解。在此基础上,又引入了先行语与照应语的中心语字符串作为新特征,并提出一种竞争模式对将中心语约束融合进实例匹配算法,提升了消解效果。该方法与其他只使用平面特征的传统机器学习方法相比,能充分地利用每一个训练实例的特征信息,进一步融合中心语字符串特征使消解效果更加准确。","中心语匹配,实例匹配,共指消解"
2010-08-27,一种基于加权投票的术语自动识别方法,"游宏梁,张 巍,沈钧毅,刘 挺","术语自动识别目的是获取领域术语表中未登录的规范化词汇,是信息抽取、文本挖掘等领域中的重要任务。近年来,利用统计方法抽取术语取得了一定进展,出现了C-Value、NC-Value、TermExtractor等有效方法。但是,对各种统计指标进行加权投票的方法研究较少。该文首先从大量已知术语中收集术语的词性模板,并借之抽取候选术语,接着利用了统计指标加权投票对这些候选术语进行排序。在IEEE 2006-2007电子工程领域文献上的实验结果表明,加权投票方法比任一单独指标的识别效果更好。","自动术语识别,投票算法,信息抽取,文本挖掘"
2010-08-30,面向人名消歧任务的人名识别系统,"时迎超,王会珍,肖 桐,胡明涵","CLP2010 (CIPS-SIGHAN Joint Conference on Chinese Language Processing)的人名消歧评测的任务是个聚类问题 对给定的一组文档,按照文档中出现的指定查询词所指向的人进行聚类。由于是用“字”串匹配的方法从新华社的语料库中抽出所有含有该查询词的文档。所以对于这个任务,首要问题是判定查询词是否是人名,是完整人名还是人名的一部分。为此该文实现了一个基于多实体识别系统整合和启发式规则的后处理方法的人名识别系统,从而实现对文档中的人名,特别是查询词所涉及的人名的识别。在CLP2010的评测方给的训练集上的实验表明,查询词涉及的人名的识别正确率达到98.89%。","人名识别,人名消歧,系统整合,启发式规则"
2010-05-17,基于核心句及句法关系的评价对象抽取,"张 莉,钱玲飞,许 鑫","意见挖掘已成为近年来的热点问题,该文针对COAE2009评测中的意见挖掘任务的一项子任务——评价对象抽取进行了研究。首先提出利用核心句进行学习的思想,继而确定了10种句法关系作为语言特征,将原始句和核心句分别基于词、词性和句法关系利用条件随机场模型进行学习和比较,在后期又利用二次学习的方式进一步提高了抽取性能。实验取得了相对不错的抽取效果,证明我们提出的方法是可行的,且具有一定的应用价值。","意见挖掘,评价对象抽取,核心句,句法关系,条件随机场"
2010-04-14,采用改进重采样和BRF方法的定义抽取研究,"潘 湑,顾宏斌","为了从专业领域语料中发现并获取所有的专业术语定义,该文提出了使用分类方法进行专业术语定义抽取的方法。该文采用一种基于实例距离分布信息的过采样方法,将其与随机欠采样方法结合用以建立平衡训练语料,并使用BRF(Balanced Random Forest)方法来获得C4.5决策树的聚合分类结果。该方法获得了最好65%的F1-measure成绩和78%的F2-measure成绩,超过了仅使用BRF方法取得的成绩。","自然语言处理,术语定义,定义抽取,文本分类,重采样"
2010-09-09,汉语框架自动识别中的歧义消解,"李济洪,高亚慧,王瑞波,李国臣","该文研究了汉语框架自动识别中的歧义消解问题,即对给定句子中的目标词,基于其上下文环境,从现有的框架库中,为该目标词自动标注一个合适的框架。该文将此任务看作分类问题,使用最大熵建模,选用词、词性、基本块、依存句法树上的若干特征,并使用开窗口技术和BOW策略,以目前汉语框架语义知识库中的88个词元的2 077条例句为训练、测试语料,进行了3-fold交叉验证实验,最好结果取得69.28%的精确率(Accuracy)。","汉语框架语义知识库,框架语义,框架消歧,最大熵模型"
2010-07-08,基于可信度模型的中文人名识别研究,"倪 吉,孔 芳,朱巧明,李培峰","该文根据中国人名的形成方式,总结和统计了人名的用字特征和边界模板特征,通过计算人名内聚度、人名区分度和边界模板可信度的综合概率作为人名可信度,对文本中人名进行识别或对已识别的人名进行纠正。该文将可信度检测模块嵌入到一个简易的命名实体平台中,在MSRA的语料上进行测试,实验结果说明可信度模型使得平台的人名识别F值提高了2.27%,整个系统的人名识别F值达到了91.72%。","人名识别,可信度,内聚度,边界模板"
2010-09-01,基于树核函数的中文语义角色分类研究,"吴方磊,李军辉,朱巧明,李培峰","该文探索了基于树核函数的中文语义角色分类,重点研究如何获取有效的结构化信息特征。在最小句法树结构的基础上,根据语义角色分类的特点,进一步定义了三种不同的句法结构,并使用复合核将基于树核和基于特征的方法结合。在中文PropBank语料上的结果表明,基于树核函数的方法在中文语义角色分类任务中能够取得较好的结果,精确率达到91.79%。同时,与基于特征方法的结合,基于树核函数的方法能够进一步提高前者性能,精确率达到94.28%,优于同类系统。","语义角色标注,语义角色分类,树核"
2010-01-26,基于依存树库的文本聚类研究,"高 松,冯志伟","文本聚类是信息检索的重要内容。为了避免使用计算过程复杂的聚类算法,并能从语言学角度对聚类特征和聚类结果进行分析和解释,该文提出了采用句法分布信息进行文本聚类的方法。在汉语依存树库中,得出10种具有显著差异的词类依存关系,以其中5种依存关系作为聚类特征,访谈会话类和新闻播报类文本的相似度分别为71.98%和83.13%。实验结果验证了该方法利用依存关系对文本聚类的可行性和有效性。","文本聚类,聚类特征,依存树库,依存关系,词类"
2010-08-10,网络舆情信息源影响力的评估研究,"郭 岩,刘春阳,余智华,张 瑾,戴 媛","文章通过对网络舆情、信息源、影响力等概念的深入研究,构建网络舆情信息源影响力评估体系。评估方法试图从根本上抓住网络舆情信息源影响力的本质特点 除了考虑信息源的表现力,还考虑网民对影响力的反馈,以及信息源转载信息这一行为中隐含的对同行信息源影响力的反馈。在量化影响力时,文章借鉴网络链接分析算法PageRank,提出算法SrcRank对信息源重要度进行排名。实例分析结果表明,评估方法能够客观而合理地评价网络舆情信息源的影响力。","网络舆情,信息源影响力,评价指标体系,PageRank"
2010-06-18,意见时空元素的研究,"刘 军,姚天昉,仇 伟","为了满足新应用需求,该文将时空元素引入到意见模型中。在此基础上,提出了意见重要因子的概念,分析了时间重要因子的计算公式,对手机和汽车论坛的来源重要因子进行了实验。在时空元素的应用上,对汽车评论进行了趋势挖掘,探讨了意见趋势的挖掘方法和实验评估方法。","时空元素,意见模型,重要因子,意见趋势,挖掘方法"
2010-07-25,基于产品属性的条件句倾向性分析,"杨 源,林鸿飞","该文主要识别产品评论中的条件句并分析其倾向性,判断评论者对产品属性持积极或是消极的态度。条件句中一般都含有条件连接词,但是有些条件句中没有条件连接词,称为隐式条件句。经过观察,发现隐式条件句中含有一些体现条件关系的词,称之为隐式条件词。识别条件句时,主要依据条件连接词和隐式条件词及其词性以及类序列规则进行分类;分析属性倾向性时,依据条件连接词和隐式条件词把条件句分为假设条件句、让步条件句、特定条件句和无条件句四类,并把条件句的类别用于SVM分类。通过实验证明了该方法有助于条件句倾向性分类。","条件句,条件句识别,倾向性"
2010-06-01,唐代以来汉语文学作品中的字频演变,"刘宇凡,郭金忠,陈清华","研究历史上各个时期汉语文学作品中的字频分布具有重要意义,可以帮助我们更加深入研究汉语言的历史演变,但这在以前的语言统计工作中是缺乏的。该文对唐代以来的文学作品按不同时期进行分类建立语料库,字频分析的结果表明自唐代以来人们使用汉字的习惯处于不断变化之中,时期越相近,汉字的使用习惯就更具一致性。从分布上看,不同时期的字频都可以用一个指数截断的幂律函数进行很好的拟合,随着历史的发展,幂律性质不断衰减而指数性质不断增强。","汉语文学作品,字频分布,指数截断的幂律"
2010-02-26,基于用户行为模型的计算机辅助翻译方法,"叶 娜,张桂平,韩亚冬,蔡东风","与全自动机器翻译相比,计算机辅助翻译技术更具实用性,已成为机器翻译领域的一个研究热点。传统的辅助翻译过程中,用户只能被动接受系统提供的辅助译文,并进行翻译后编辑操作。该文提出一种基于用户行为模型的辅助翻译方法,通过实时记录用户的后编辑过程,分析出用户的翻译决策,建立用户行为模型,使得翻译系统能够动态获取和共享用户的翻译知识,从而提高辅助译文的质量。实验结果表明,在同一篇文档前30%文本的后编辑过程中建立的用户行为模型,使余下70%文本的辅助译文的BLEU值平均提高了4.9%,用户模型中翻译知识的准确率达到94.1%。","辅助翻译,后编辑,用户行为模型,翻译知识,BLEU"
2010-07-27,口语对话中冗余词汇识别方法研究,"翟飞飞,宗成庆","冗余现象是口语对话中普遍存在的特殊语言现象之一,它的存在常常会影响口语句子的理解和翻译。该文基于真实口语对话语料对冗余现象进行了分析,并在词汇层面对冗余现象进行了分类,然后对口语中的冗余词汇进行了统计识别方法研究。通过对冗余词汇处理前后的口语句子翻译实验,结果表明,预先对冗余现象进行处理,能够改善口语翻译的译文质量。","冗余现象, 最大熵分类器, 支持向量机, 条件随机场"
2010-07-14,汉藏短语对抽取中短语译文获取方法研究,"诺明花,吴 健,刘汇丹,丁治明","该文从法律法规和公文领域汉藏语料中对待翻译汉语短语提取藏语译文。目前普遍采用的短语对抽取方法需要依赖于词性或句法分析等资源或词对齐技术。考虑现阶段藏文资源不足,词法句法相关技术不成熟,该文提出藏文词串频率统计方法(TSM)和藏文词序列相交算法(TIA)两种方法来获取藏语译文。其中TSM抽取1-1连续和非连续短语准确率达到90%左右,但遗漏1-n情况。TIA能够抽取1-n连续和非连续藏文语块,准确率达到81%。","藏文语块,短语译文获取,藏文信息处理,中文信息处理"
2010-07-17,彝文自动分词技术研究,陈顺强,"该文介绍了彝文自动分词的技术。首先阐述了研究彝文自动分词的必要性和重要意义,然后介绍了彝文分词规范的原则及词表,讨论了彝文分词的算法,最后根据彝文的特性,设计了基于Java语言的彝文自动分词软件并得出了良好的分词结果。","自动分词,彝文,分词单位"
2011-04-28,>维吾尔语广播新闻敏感词检索系统的研究,"木合塔尔·沙地克,李 晓,布合力齐姑丽·瓦斯力","维吾尔语广播新闻敏感词检索系统是以HMM为基础。在MATLAB平台上设计实现的。该系统的特点包括 1.由于维吾尔语敏感词数量不多,该系统语音语料库很小。2.由于广播新闻中的发音较为标准规范,在识别中避免了说话人发音上的不规范,这有利于语音识别系统性能的提高。3.由于选择词素为识别基元,易于识别基元端点检测。","维吾尔语,广播新闻,敏感词识别,HMM,MATLAB"
2011-04-15,西双版纳傣文新闻网站与数字报刊技术研究,"殷建民,刀福祥,唐金宝,玉康龙","该文介绍了西双版纳傣文新闻网站与数字报刊系统的研究内容与关键技术,涉及西双版纳新老傣文编码/显现字符集、输入法和嵌入式字库的研究以及版面数字化技术、网站发布技术、新闻信息多渠道采集技术、多媒体共享稿库技术和中文新闻信息标准的应用。","西双版纳傣文,新闻网站,数字报刊,编码/显现字符集,输入法"
2011-04-03,基于朴素贝叶斯分类器的朝鲜语文本分类的研究,"周国强,崔荣一","该文基于朴素贝叶斯分类器对朝鲜语文本分类进行了研究。首先,利用基于类别选择的特征选择方法对朝鲜语文本进行特征选择,并使用类TF-IDF估算方法计算权重;其次,构造朴素贝叶斯分类器;最后,利用分类器实现对朝鲜语文本的分类。实验表明,该方法在朝鲜语文本分类中具有较好的效果,为朝汉结合文本分类提供了一定的依据。","朝鲜语,朴素贝叶斯,文本分类,TF-IDF"
2011-04-10,基于栏目的藏文网页文本自动分类方法,"胥桂仙,向春丞,翁 彧,赵小兵,杨国胜","该文提出了一种简单、快速的藏文网页文本分类方法。该方法利用网页栏目中词条的类别特征,结合网页文本提取技术,实现了快速、精确地将藏文网页文本归于预定义类别中。实验表明,该方法具有很高的网页文本分类正确率,对构建高质量多类别藏文语料库有重要作用。","藏文信息处理,文本分类,藏文网页分类"
2011-04-23,基于双语约束的蒙古语无监督依存分析,"刘 凯,乌日力嘎 ,斯钦图,姜文斌,刘 群","句法分析在自然语言处理的实际应用中扮演着重要的角色。当前各少数民族语言包括蒙古语的句法分析研究还处在相对滞后的阶段。同时给其他相关研究带来了相应的困难。该文提出了一种基于双语约束的蒙语的无监督依存分析方法。能够在无需蒙语依存树库及蒙语句法的情况下,对蒙语进行无监督的依存句法分析。并且获得了较好的效果,在人工标注的测试集上有向及无向的正确率分别达到了67.2%及73.3%,可以实际应用到自然语言处理中了。","蒙古语,无监督句法分析,依存分析,双语约束"
2011-04-18,蒙古语有向图形态分析器的判别式词干词缀切分,"姜文斌 ,吴金星,乌日力嘎,那顺乌日图,刘 群","蒙古语形态分析中,我们之前的有向图模型取得了较高的性能。这种建模方式以图状结构刻画句中词干和词缀之间的概率关系,从而借助上下文信息为每个词确定最佳的切分标注候选。为每个词尽可能地枚举出所有合法的切分标注候选,是有向图模型有效工作的前提。该文提出了一种基于判别式分类的词干词缀切分策略,与之前基于词干表和词缀表的枚举方案相比,该方法对于词中含有未登录词干的情形具有更好的泛化能力。以20万词规模的三级标注人工语料库为训练数据,采用判别式词干词缀切分的有向图形态分析器,对于含有未登录词干的情形,词级切分标注正确率提高了7个百分点。","蒙古语,词法分析,词性标注,词干提取,有向图,判别式"
2011-04-19,蒙古文停用词和英文停用词比较研究,"巩 政,关高娃","该文采用联合熵算法(Union Entropy,UE)初步确定了蒙古文停用词,接着从初步确定的蒙古文停用词中去掉蒙古文实体名词及同形异义词,再通过对英文停用词和蒙古文停用词的词性比较,确定了蒙古文停用词表。最后用蒙古文停用词表和英文停用词表进行了文档信息检索的对比实验。实验结果表明,用该文所述方法确定的蒙古文停用词表进行蒙古文文档检索,比用英文停用词翻译成蒙古文进行蒙古文文档检索的准确率更高。","蒙古文停用词,蒙古文信息检索,英文停用词"
2011-04-20,最大熵和规则相结合的藏文句子边界识别方法,"李 响,才藏太,姜文斌,吕雅娟,刘 群","句子边界识别是藏文信息处理领域中一项重要的基础性工作,该文提出了一种基于最大熵和规则相结合的方法识别藏语句子边界。首先,利用藏语边界词表识别歧义的句子边界,最后采用最大熵模型识别规则无法识别的歧义句子边界。该方法有效利用藏语句子边界规则减少了最大熵模型因训练语料稀疏或低劣而导致对句子边界的误判。实验表明,该文提出的方法具有较好的性能,F1值可达97.78%。","最大熵,句子边界识别,藏文信息处理"
2011-04-11,一种改进的维吾尔语句子相似度计算方法,"卡哈尔江·阿比的热西提,吐尔根·依布拉音,姚天昉,艾山·吾买尔,艾山·毛力尼亚孜","在基于实例的维吾尔语汉语机器翻译系统中维吾尔语相似度计算起重要作用。维吾尔语的黏着性特性要求对单词进行词干提取。本文提出的方法结合简单的句子结构相似度计算方法,通过对单词词干提取进行句子相似度计算。小规模实验结果比较接近人工评价的句子相似度。","维吾尔语句子相似度计算,EBMT,句子结构相似度"
2011-04-17,央金藏文分词系统,"史晓东,卢亚军","藏文分词是藏文信息处理的一个基本步骤,该文描述了我们将一个基于HMM的汉语分词系统Segtag移植到藏文的过程,取得了91%的准确率。又在错误分析的基础上,进行了训练词性的取舍、人名识别等处理,进一步提高了准确率。","藏文分词,自然语言处理,HMM"
2011-04-13,基于词典的汉藏句子对齐研究与实现,"于 新,吴 健,洪锦玲","双语语料库加工的关键技术之一是对齐,构建句子级别的对齐语料是构建语料库最基本的任务。该文参考其他语言句子对齐的成熟的方法,针对藏文语言的特殊性,提出基于词典的汉藏句子对齐。整理了对齐所用双语词典,并对其词语覆盖率进行了评价。在汉藏句子对齐过程中发现汉语与藏文的分词粒度不同的问题,采用在藏汉词典中进一步查词并在汉语句子中比对的方法,使正确句对的得分增加,从而提高对齐正确率。采用该方法准确率为 81.11%。","汉藏句子对齐,词典,分词粒度,平行语料库,藏文信息处理"
2011-04-15,基于WAMP的藏汉英互译在线词典的设计与实现,"周毛先,头旦才让,才让加","根据目前在线藏汉英词典使用的实际需求, 青海师范大学藏文信息处理省部共建教育部重点实验室设计实现了一种基于WAMP平台的藏汉英互译在线词典,并给出了词典数据库和查询页面的具体设计方法和关键代码。经测试,该在线词典根据用户的需要,输入单字和词就可以在藏汉英三语间交互查询并快速检索到对应的译词。词典采用B/S结构,它的实现有助于藏汉英三语间的交流和学习。","藏文, 在线词典, WAMP, B/S结构, 数据库"
2011-04-23,藏语语料库TEI标记规范探讨,"扎西加,高定国","在语言信息处理过程中,大规模真实文本处理已成为一个研究热点。藏语语料库的标记在汉藏英机器翻译、信息检索、文本数据挖掘、词典编纂的研究工作中占很重要的地位。为了便于数据交换和共享,该文基于TEI编码的藏语语料,对藏语语料库中文本的属性信息和结构信息标记做了系统而全面的探讨。","藏语,语料库,TEI标记"
2011-04-30,多民族语言本体知识库构建技术,"赵小兵,邱莉榕,赵铁军","语义本体是共享概念模型的显示的形式化规范说明,其目标是将杂乱无章的信息源转变为有序易用的知识源。语义本体知识库的构建是文本自动处理的一个重要环节,跨语言信息检索、信息抽取、自动翻译等领域中都有广泛的应用。该文旨在描述统一标准、统一接口的多民族语言本体知识库的创建思路,以及包含的若干问题,例如 多民族语言中共有概念的一般表示与各民族语言特有的事物表达方式的规律,基于词汇语义的、包括汉语、英语及少数民族语言在内的多民族语言语义本体的表示理论与方法等。","知识库,语义本体,词典扩充,本体学习"
2011-03-22,面向形态丰富语言的多粒度翻译融合,"王志洋,吕雅娟,刘 群","形态丰富语言由于其复杂的形态变化,会导致大词汇量和数据稀疏问题,这给统计机器翻译带来了巨大挑战。该文通过将这类语言表示为不同的粒度,然后分别进行翻译;由于不同的粒度能表征语言不同层面的特点,通过对不同粒度的翻译结果进行词级系统融合,便可生成更好的译文。维吾尔语、蒙古语到汉语的两组翻译实验表明,这种多粒度系统融合方法改善了翻译效果,BLEU值比最好的单系统分别提高了+1.41%和+2.03%。","形态丰富语言,多粒度,系统融合"
2011-04-29,维吾尔语中汉族人名的识别及翻译,"李佳正1,刘 凯1,麦热哈巴·艾力1,2,吕雅娟1,刘 群1,吐尔根·依布拉音2","该文研究了一种维吾尔语中汉族人名的识别和翻译方法。该方法在词典等传统方法的基础上,运用语言模型实现维语中的汉族人名的识别和翻译。针对维语人名的构词和拼写特点,增加了名词词缀识别预处理模块,补充了维语字母到汉语拼音的映射规则,有效提高了人名识别的正确率及召回率。在1 000句含有汉族人名的维语语料上进行测试,汉族人名识别的正确率和召回率分别达到75.2%和91.5%。","语言模型,名词词缀,拼写规则,人名识别及翻译"
2011-04-27,汉蒙统计机器翻译中的调序方法研究,"王斯日古楞,斯琴图,那顺乌日图","在基于短语的汉蒙统计机器翻译系统的研究中,我们发现存在着严重的语序错误。该文在对汉语和蒙古语句子语序进行研究的基础上,提出了基于蒙古语语序的汉语句子调序方法; 同时介绍了调序规则和调序算法的设计;最后给出了具体实验。实验证明这种方法明显提高了现有汉蒙机器翻译系统的性能。","汉蒙统计机器翻译,调序,规则"
2011-04-19,蒙古语标准音辅音组合的协同发音研究包,"桂兰,呼 和","该文在以往实验研究的基础上,利用美国KAY公司6300型电子腭位仪(EPG)、3700Multi-Speech和南开大学“桌上语音工作室”(MiniSpeechLab)等生理和声学分析仪器和软件,通过观察辅音组合的声学语图、动态腭位图和LCV曲线图,比较系统地描述和归纳了蒙古语辅音组合中相邻音段之间的协同发音规律。","蒙古语标准音,辅音组合,协同发音"
2011-04-28,LPC及F0参数组合基于GMM电话语音说话人识别,"伊·达瓦,吾守尔·斯拉木,匂坂 芳典","该文报告了组合LPC参数以及基频F0的高斯混合模型(GMM)电话语音说话人自动识别技术的实验研究结果。该研究在基线试验中GMM使用16混合共分散对角矩阵,特征量为LPC倒谱系数。而在开发系统测试中分别利用语音的全发话区间和有声区间两部分参数增加基频参数进行试验,并给出实验比较结果。在50人电话通话开放集自动切分语音流实验中正确识别率为76.97%,而提案方法为80.29%,改善率为3.32%。接近人工切分语音流时的识别率82.34%。","电话语音,说话人识别,LPC,F0,GMM"
2010-09-13,藏语句子相似度算法的研究,安见才让,"该文提出了一种藏语句子相似度的计算方法,即采用散列单词倒排索引和基于句长相似度粗选的算法,快速从语料库中筛选出候选句子的集合,散列单词倒排索引能够有效提高算法的查找速度;再采用基于词形和连续单词序列相似度的多策略精选算法,可以有效衡量两个藏语句子的相似程度。实验结果证明算法是有效的。","自然语言处理,语料库,连续单词序列,藏语,句子相似度"
2010-12-14,框架元素语义核心词自动识别研究,"康旭珍,李 茹,李双红","该文基于汉语框架网,利用框架核心依存图形式化地表示一个汉语句子,使得对句子能够进行深层语义理解。为了得到框架核心依存图,需要提取其中框架元素的语义核心词。该文较为系统地描述了框架元素的语义核心词的识别问题。我们利用条件随机场模型、最大熵模型和支持向量机模型来识别框架元素语义核心词,并分别对这三种不同的模型所选的特征集进行了分析,且通过构造不同的特征模板进行对比实验,选取其中较优的特征模板和模型。结果表明,条件随机场模型具有较好的识别性能,在对其特征模板做进一步改进的基础上,识别效率也得到一定的提高。其中对简单型和复合型短语类型框架元素语义核心词识别的平均正确率分别达到了97.34%和94.03%。","框架元素,框架核心依存图,条件随机场,最大熵模型,SVM模型"
2010-03-08,基于短语统计机器翻译模型蒙古文形态切分,"李 文,李 淼,梁 青,朱 海,应玉龙,乌达巴拉","该文结合最小上下文构成代价模型,借鉴并利用统计机器翻译的方法,尝试解决蒙古文形态切分问题。基于短语的统计机器翻译形态蒙文切分模型和最小上下文构成代价模型分别对词表词和未登录词进行形态切分。前者选取了短语机器翻译系统中三个常用的模型,包括短语翻译模型、词汇化翻译模型和语言模型,最小上下文构成代价模型考虑了一元词素上下文环境和词缀N-gram上下文环境。实验结果显示 基于短语统计机器翻译形态切分模型对词表词切分,最小上下文构成代价模型对未登录词处理后,总体的切分准确率达到96.94%。此外,词素融入机器翻译系统中后,译文质量有了显著的提高,更进一步的证实了本方法的有效性和实用性。","形态学,形态切分,机器翻译,统计模型"
2011-05-01,网页质量评价体系的研究,"魏 超,陈 飞,许丹青,张 敏,刘奕群,马少平","网络数据的飞速增长为搜索引擎带来了巨大的存储和网络服务压力,大量冗余、低质量乃至垃圾数据造成了搜索引擎存储与运算能力的巨大浪费,在这种情况下,如何建立适合万维网实际应用环境的网页数据质量评估体系与评估算法成为了信息检索领域的重要研究课题。在前人工作的基础上,通过网络用户及网页设计人员的参与,文章提出了包括权威知名度、内容、时效性和网页外观呈现四个维度十三个因素的网页质量评价体系;标注数据显示我们的网页质量评价体系具有较强的可操作性,标注结果比较一致;文章最后使用Ordinal Logistic Regression 模型对评价体系的各个维度的重要性进行了分析并得出了一些启发性的结论 互联网网页内容和实效性能否满足用户需求是决定其质量的重要因素。","信息检索,网页质量评价,Ordinal Logistic Regression"
2011-05-11,权衡熵和相关度的自动摘要技术研究,"罗文娟,马慧芳,何 清,史忠植","生成高质量的文档摘要需要用简约而不丢失信息的描述文档,是自动摘要技术的一大难题。该文认为高质量的文档摘要必须尽量多的覆盖原始文档中的信息,同时尽可能的保持紧凑。从这一角度出发,从文档中抽取出熵和相关度这两组特征用以权衡摘要的信息覆盖率和紧凑性。该文采用基于回归的有监督摘要技术对提取的特征进行权衡,并且采用单文档摘要和多文档摘要进行了系统的实验。实验结果证明对于单文档摘要和多文档摘要,权衡熵和相关度均能有效地提高文档摘要的质量。","自动摘要,句子特征抽取,熵,相关度"
2011-03-15,从Web获取部分整体关系语料的方法,"曹馨宇,曹存根","部分整体关系获取是知识获取中的重要组成部分。Web逐步成为知识获取的重要资源之一。搜索引擎是从Web中获取部分整体关系知识的有效手段之一,我们将Web中包含部分整体关系的检索结果集合称为部分整体关系语料。由于目前主流搜索引擎尚不支持语义搜索,如何构造有效的查询以得到富含部分整体关系的语料,从而进一步获取部分整体关系,就成为一个重要的问题。该文提出了一种新的查询构造方法,目的在于从Web中获取部分整体关系语料。该方法能够构造基于语境词的查询,进而利用现有的搜索引擎从Web中获取部分整体关系语料。该方法在两个方面与人工构造查询方法和基于语料库查询构造查询方法所获取的语料进行对比,其一是语料中含有部分整体关系的语句数量;二是从语料中进一步获取部分整体关系的难易程度。实验结果表明,该方法远远优于后两者。","部分整体关系获取,语料获取,查询构造"
2011-03-18,搜索引擎日志中“N+V”型主谓短语研究,"赵红改,肖诗斌,王洪俊,吕学强","“N+V”型结构能够构成定中偏正、状中偏正和主谓三种不同结构的短语。基于搜狗日志语料,对“N+V”型主谓短语从其各组成要素特点、音节特点和句法功能三方面进行研究,着重从语义方面对“V”进行阐述。文中,还对实验数据进行深入的分析和实证,针对“N+V”型短语的句法结构歧义问题,提供了解决方案,这为提高中文搜索引擎的检索质量和搜索引擎用短语词典构建提供了重要的理论依据。","搜索引擎,“N+V”型结构,主谓短语,句法功能"
2011-04-01,一种基于查询特性的查询结果缓存与预取方法,"马宏远,王 斌","针对搜索引擎查询结果缓存与预取问题,该文提出了一种基于查询特性的搜索引擎查询结果缓存与预取方法,该方法包括用来指导预取的查询结果页码预测模型和缓存与预取算法框架,用于提高搜索引擎系统性能。通过对国内某著名中文商业搜索引擎的某段时间的用户查询日志分析得出,用户对不同查询返回的查询结果所浏览的页数具有显著的非均衡性,结合该特性设计查询结果页码预测模型来进行预取和分区缓存。在该搜索引擎两个月的大规模真实用户查询日志上的实验结果表明,与传统的方法相比,该方法可以获得3.5%~8.45%的缓存命中率提升。","搜索引擎, 性能优化, 查询结果, 缓存, 预取"
2010-10-10,时间关系识别研究进展,"谭红叶,郑家恒,梁吉业","时间关系识别是自然语言中语义处理的一个重要任务,近年来得到了广泛的关注和快速的发展。该文参考大量的文献资料,总结了当前时间关系识别在标准、资源和评测中的进展,分析了研究中采用的多种方法和技术,探讨了研究中存在的问题和未来研究的重点。","时间关系,自然语言处理,综述"
2010-09-30,语义信息与CRF结合的汉语功能块自动识别,"刘海霞,黄德根","为了构建汉语功能块自动识别系统,该文利用条件随机域模型对经过正确词语切分和词性标注处理的汉语句子进行功能块边界识别和功能信息标注处理,通过在特征提取阶段优化组合丰富的上下文特征,得到功能块识别的精确率、召回率和F1-measure值分别为85.84%、85.07%和85.45%。在此基础上,该文引入由词义聚合关系将汉语单词组织起来的《同义词词林》作为语义资源,把其中的语义信息作为特征加入到功能块的识别过程,缓解了数据稀疏以及歧义问题对识别结果造成的影响,使得上述三个性能指标分别提高到86.21%、85.31%和85.76%。","汉语功能块,条件随机域(CRFs),语义信息,歧义结构"
2010-09-28,汉语缩略语自动处理研究现状,王厚峰,"缩略语是自然语言中广泛使用的一类典型语言单元,也是最主要的新词来源。而新词会造成自然语言处理多个层面的困难。该文分析了汉语缩略语的特点,对汉语缩略语的构成形式作了总结,对汉语缩略语处理的问题作了划分,并针对不同的问题,重点介绍了在汉语中的研究现状;之后简要比较了汉语缩略语与英语缩略语的差异,并对英语缩略语处理的一些典型方法作了分析。","缩略语识别,缩略语扩展,缩略语预测,缩略语挖掘"
2010-06-18,面向俄文NLP的形态自动分析研究与实现,"李 峰,易绵竹","在俄文自然语言处理中形态分析往往是必不可少的模块,在国内虽有个别理论研究,却还没有可以应用于生产的案例。该文系统归纳了国内外俄文形态自动分析方法,深入剖析了俄罗斯以及欧美等其他国家具有代表意义的俄文形态分析器,并在此基础上提出了多策略融合的俄文形态自动分析方法,测试表明即使将该方法应用于专业领域,也能取得令人较为满意的效果。","自然语言处理,俄文,形态自动分析,算法"
2010-10-26,基于遗传算法的文本过滤模型及收敛性分析,"朱振方,刘培玉,李少辉,赵 静,王乾龙","过滤模板的生成是网络信息过滤中一个至关重要的问题。针对模板生成中的非线性特征,借鉴遗传算法可以在全局范围内寻找最优解的特性,引入遗传算法解决文本信息过滤问题,并应用基于集合论的方法证明其理论可行性。在实际应用中,应用遗传算法生成模板进行了文本分类和文本过滤试验,并根据应用过程中存在的问题提出了遗传算子的自适应策略。理论证明以及实验结果都表明,该方法具有可行性,能够在信息过滤中取得较好的结果。","文本过滤,模糊理论,遗传算法,收敛性"
2010-11-16,基于N-gram超核的中文倾向性句子识别,"廖祥文,李艺红","倾向性句子识别是文本倾向性分析的重要组成部分,其目的是识别文档中具有情感倾向的主观性句子。中文句子的倾向性不仅与倾向词有关,而且还跟句法、语义等因素有关,这使得倾向性句子识别不能简单地从词语的倾向性来统计得到。该文提出了一种基于N-gram超核的中文倾向性句子识别分类算法。该算法基于句子的句法、语义等特征构造N-gram超核函数,并采用基于该超核函数的支持向量机分类器识别中文倾向性句子。实验结果表明,与多项式核、N-gram核等单核函数相比,基于N-gram超核的中文倾向性句子识别算法在一定程度上能有效识别倾向性句子。","倾向性句子识别,N-gram超核函数,倾向性分析"
2010-08-29,蒙古语词法分析的有向图模型,"姜文斌,吴金星,长 青,那顺乌日图,刘 群,赵理莉","我们为蒙古语词法分析建立了一种生成式的概率统计模型。该模型将蒙古语语句的词法分析结果描述为有向图结构,图中节点表示分析结果中的词干、词缀及其相应标注,而边则表示节点之间的转移或生成关系。特别地,在本工作中我们刻画了词干到词干转移概率、词缀到词缀转移概率、词干到词缀生成概率、相应的标注之间的三种转移或生成概率,以及词干或词缀到相应标注相互生成概率。以内蒙古大学开发的20万词规模的三级标注人工语料库为训练数据,该模型取得了词级切分正确率95.1%,词级联合切分与标注正确率93%的成绩。","蒙古语,词法分析,词语切分,词性标注,词干提取,有向图"
2010-09-20,基于音素评分模型的发音标准度评测研究,"严 可,戴礼荣","在计算机辅助语言学习系统中,后验概率是普通话水平测试(PSC)电子化系统衡量考生发音标准程度的重要指标,但后验概率与人工的主观评分存在着显著差别。该文提出了“音素评分模型”的思想,对后验概率进行变换。该文研究了线性和非线性的sigmoid音素评分模型,并发现线性音素评分模型有闭式全局最优解,非线性音素评分模型可用梯度下降法求解。在全国采集的498人的普通话考试现场数据集上的实验表明该策略能使系统评分性能有明显的提升 当后验概率在全音素概率空间中计算时,可使系统性能提升约42%;当后验概率在优化的概率空间中计算时,能使系统性能提升约23%～27%。","语音评测,音素评分模型,后验概率,普通话水平测试"
2010-05-26,基于弹性网格的西夏文字识别,"门光福,潘 晨,柳长青","随着国内外对西夏学研究的不断深入,收藏于世界各地的大批西夏古籍文献通过影印方式陆续出版。如何将这些西夏古籍文献数字化、文本化则有着极其重要的意义。该文采用弹性网格方法及线性判别分析(Linear Discriminant Analysis,LDA)方法对西夏文字识别进行了研究。首先对西夏影印文献进行预处理、细化,然后根据西夏文字笔画分布构造非均匀的弹性网格,将弹性网格分别作用于西夏文字的四个方向分量上,统计像素点在网格内的概率分布作为特征,最后使用LDA方法对提取的特征降维处理。对240类共9 600个西夏文字做4重交叉验证,平均识别率可达87.99%,实验表明该方法是有效的。","西夏字,弹性网格,方向特征,线性判别分析(LDA)"
2010-12-05,朝鲜文字信息结构的研究,"崔荣一,金世珍","该文研究了朝鲜文字空间结构中不同位置上的基本字母对文字结构的分类所提供的信息贡献。首先,提出了文字的结构距离的概念与计算方法,描述了不同结构之间的差异;其次,研究了文字结构的等价类划分方法以及文字结构的概率分布;最后,通过计算结构分类时不同位置上的基本字母的信息增益,刻画了文字中信息的分布结构。对实际朝鲜语文档的实验表明,c1-v2、c1-v1-c3、c1-v2-c3型结构的文字具有显著的高概率特性,v1、v2类型和c3类型字母对结构分类的影响最大。","朝鲜文字,文字结构等价类,结构距离,信息增益"
2010-09-21,少数民族汉语考试的作文辅助评分系统研究,"蔡 黎,彭星源,赵 军","随着计算机的普遍应用以及计算机技术的快速发展,计算机辅助性测试和计算机自适应性测试都已先后成为现实。计算机辅助评分,也称作计算机自动评分,就成为人们所希望的下一代计算机辅助工具。中文辅助评分系统的研究尚处于起步阶段,据我们了解还没有一个能大规模使用的系统。我们研究了许多英文的辅助评分系统,并按照文章中的算法提取特征,但是特征的相关度并不高。在该文中,我们利用统计自然语言处理和信息检索的技术提取作文写作水平和作文主题特征。在建模时,融入样本分数分布和一位评分员的评分的信息,创造性的提出三重分段回归模型。实验表明,利用我们的辅助评分系统协助评分,在节省一半阅卷量的情况下,精度可以达到97%以上。","作文辅助评分,汉语,主题特征,写作水平特征"
2011-09-15,下一站在哪里？,"董振东,董 强,郝长伶","该文简要回顾了中文信息处理30年的主要成果,以及近20年来中文信息处理中的计算语言学研究的状况。该文分析了汉语与英语的主要差异,讨论了语言的共性与个性。该文表示了对于中文大规模语料的词性标注、树库建设的质疑。该文提出未来的中文语言资源建设的一些设想,期望一些新的尝试,提出以语义取代现有的句法,以深度标注取代现有的浅层标注,具体将包括标注的目标的定点化,内容的多样化,步骤的阶段化,标注人员的大众化、群体化。文章还提出了未来发展的关键点 技术的融合,人本计算。","中文信息处理,语言数据资源,语料标注,句法,语义"
2011-09-19,综合型语言知识库及其前景,"俞士汶,穗志方,朱学锋","北京大学计算语言学研究所自1986年起,历时25年建成综合型语言知识库(CLKB)。CLKB包括6个语言知识库、10项规范与标准、基础软件工具集和4个应用系统,它们相互支撑,形成一个有机整体。CLKB的系列化的语言知识涵盖词、词组、句子、篇章各单位和词法、句法、语义各层面,从汉语向多语言辐射,从通用领域深入到专业领域。尽管CLKB已形成阶段性成果,但它仍在发展中。该文重点介绍CLKB的语言知识库,也探讨其发展方向。","自然语言处理,计算语言学,语言工程,综合型语言知识库,现代汉语语法信息词典"
2011-10-17,从IBM深度问答系统战胜顶尖人类选手所想到的,黄昌宁,"2011年2月14-16日,IBM的深度问答系统在美国Jeopardy电视竞答节目中一举打败该节目的两位前冠军,凸显了计算机在自然语言处理(NLP)技术上超越人类的智能行为,这是人工智能研究历史上意义非凡的里程碑。该文通过这一事件来回顾国内外自然语言处理和自动问答技术研究的某些得失,借此纪念中国中文信息学会成立30周年华诞。","人工智能,自然语言处理,自动问答系统,deepQA, IBM watson"
2011-09-25,新一代搜索引擎的研发战略,周 明,"理想的搜索引擎不仅应该快捷地帮助找到所需要的信息,还应该像银行的窗口一样提供个性化的服务。更进一步,它还应该象专家一样辅助用户做出决策并快速完成各种任务。当前,互联网搜索领域酝酿着前所未有的新机遇。庞大的网民数量为搜索的发展提供了广阔的空间。 社会关系网络和移动互联网推动搜索引擎的技术将会发生重大的改变。自然语言处理对用户意图的理解和对文本的理解能力的提高,将使得搜索的准确性也将持续改善。然而搜索引擎的成功是多方面因素共同作用的结果,需要技术和战略的巧妙配合。该文将深入分析目前搜索引擎技术发展的趋势,指出应该关注的若干重要领域,并且探讨了搜索引擎的研发战略。","搜索引擎,研发战略,实时搜索,社会关系网络"
2011-10-11,汉语共时语料库与追踪语料库语料库语言学的新方向,"邹嘉彦,邝蔼儿,路 斌,蔡永富","随着信息技术的不断提升、互联网的普及,汉语自然语言处理的难题不断得到解决,汉语语料库的发展和语料库语言学的应用也面临着新的契机。如何持续充分应用庞大的多种语料库,并协同与配合语言学和人文、社会科学多个领域,来追踪了解各种语言现象及其背后的社会文化深层含义,是语料库语言学可以承担的新任务。LIVAC汉语共时语料库持续处理和分析泛华语七个地区十七年四亿字的语料,可真正起到“时间锦囊”的作用,为紧密追踪、科学观察泛华地区语言现象及有关社会文化演变,提供了坚实的基础和科学依据。该文介绍LIVAC如何由汉语“共时语料库”演变为“追踪语料库”。","语料库语言学,LIVAC汉语语料库,共时语料库,追踪语料库"
2011-09-15,汉语框架语义网构建及其应用技术研究,刘开瑛,"汉语框架语义网(Chinese FrameNet,CFN)是一个以Fillmore的框架语义学为理论基础、以英文FrameNet为参照、以汉语语料事实为依据的供计算机使用的汉语词汇语义数据库。该文首先介绍了汉语框架语义网的构建基础——框架语义学以及英语的框架语义网工程,然后具体分析了汉语框架语义网的构建技术,并对基于汉语框架网的语义角色自动标注研究进行了介绍,25个框架的交叉验证的实验结果的准确率、召回率、F1-值分别达到74.16%,52.70%,61.62%;最后,介绍了几个基于汉语框架语义网的研究课题的进展情况。","汉语框架网,语义角色标注,框架语义依存图"
2011-09-24,语言技术平台,"刘 挺,车万翔,李正华","中文信息处理不仅需要基础数据平台的支撑,而且需要基础技术平台的支撑。该文介绍了我们历经八年研制并不断改进的语言技术平台LTP(Language Technology Platform)。该平台包括中文词法分析、句法分析以及语义分析等多项中文处理技术,其中的句法语义分析技术在CoNLL 2009国际评测中获得第一名的成绩。该平台自2006年起对学术界免费共享,2010年获得中国中文信息学会“钱伟长中文信息处理科学技术奖一等奖”,目前已有400多家国内外研究机构签约共享该平台。2011年6月,该平台开放源代码,同行们不仅可以利用该平台的结果进行上层技术研究,而且可以和我们一起改进该平台本身。","中文信息处理,语言技术平台"
2011-09-19,基于句法的统计机器翻译模型与方法,刘 群,"该文总结了我们近几年来在基于句法的统计机器翻译方面所做的研究工作,特别是基于源语言句法的一系列统计机器翻译模型与方法,具体包括 基于最大熵括号转录语法的翻译模型,基于源语言短语结构树的树到串翻译模型及其相应的基于树的翻译方法,基于森林的翻译方法和句法分析与解码一体化翻译方法,基于源语言依存树的翻译模型。","统计机器翻译,基于句法的翻译模型,基于句法的翻译方法"
2011-09-16,互联网机器翻译,"王海峰,吴 华,刘占一","该文在回顾机器翻译发展的基础上,总结了主要的机器翻译方法,并主要阐述互联网机器翻译的特点及面临的挑战。面向互联网机器翻译的应用需求,并针对互联网资源具有海量、高噪声、时效性、稀疏的特点,提出了多策略混合翻译方法、资源挖掘和过滤以及分布式处理技术、领域自适应技术,针对数据稀疏论述枢轴语言技术和新语种快速部署技术;然后结合翻译与搜索技术,阐述翻译个性化特点和方案。最后,论述机器翻译技术和产品的应用。","互联网机器翻译,混合机器翻译,搜索与翻译结合"
2011-09-14,以机器翻译技术为核心的多语信息处理研究,"赵铁军,曹海龙","该文介绍了哈尔滨工业大学教育部—微软语言语音重点实验室在多语信息处理方面的研究进展和成果。首先综述了国内外的研究现状,然后重点介绍在统计机器翻译、机器翻译应用、机器翻译评价、跨语言信息检索等方面的研究工作。","机器翻译,多语信息处理,自然语言处理"
2011-09-19,中文搜索引擎用户行为的演化分析,"马少平,刘奕群,刘 健,张 敏,祝建华,茹立云","搜索引擎已经成为人们生活和工作中不可或缺的信息获取工具,对于互联网信息的合理、充分利用发挥着至关重要的作用。用户行为分析一直是搜索引擎提升性能的重要途径,但当前的搜索用户行为分析技术多局限在较短时间段,缺乏对长期时间内用户行为的演化分析研究。基于商业搜索引擎提供的海量规模日志数据,对2006年到2011年间中文搜索引擎用户行为的演化规律进行了分析挖掘,从中得到的结论对于进行搜索技术未来发展方向的讨论具有一定的参考价值。","搜索引擎,用户行为分析,演化分析"
2011-09-22,开放式文本信息抽取,"赵 军,刘 康,周光有,蔡 黎","信息抽取研究已经从传统的限定类别、限定领域信息抽取任务发展到开放类别、开放领域信息抽取。技术手段也从基于人工标注语料库的统计方法发展为有效地挖掘和集成多源异构网络知识并与统计方法结合进行开放式信息抽取。该文在回顾文本信息抽取研究历史的基础上,重点介绍开放式实体抽取、实体消歧和关系抽取的任务、难点、方法、评测、技术水平和存在问题,并结合课题组的研究积累,对文本信息抽取的发展方向以及在网络知识工程、问答系统中的应用进行分析讨论。","开放式信息抽取,知识工程,文本理解"
2011-10-10,网络信息的检索与挖掘回顾,"程学旗,郭嘉丰,靳小龙","随着互联网的蓬勃发展,海量的网络信息成为了迄今为止最大规模的数据资源。如何利用海量网络信息,为人们提供智能应用,更好的解决人们的信息需求,成为了互联网领域的挑战性问题,也催生了对海量网络信息检索与挖掘的广泛研究。该文从信息表达、信息检索与信息挖掘三个方向入手,结合近年来对网络信息相关领域的研究与实践,对网络信息检索与挖掘的发展变化历程、目前存在的问题以及未来的发展趋势进行总结和分析。","信息表达,信息检索,信息挖掘"
2011-09-20,文本情感倾向分析,"黄萱菁,张 奇,吴苑斌","近年来,文本情感倾向研究受到研究界和企业界越来越多的关注,成为了自然语言处理、信息检索、数据挖掘等领域的研究热点之一。随着研究的不断深入,大量情感倾向分析的新方法、新问题也不断涌现。该文重点对文本情感倾向研究的前沿进展进行概括和分析。首先,结合近年来的研究成果,对文本情感倾向分析的两类主要问题进行了定义,并归纳了不同的倾向性表示方法。接下来,对倾向性分类、倾向性信息抽取、语料库与评测以及倾向性分析应用等方面的研究现状进行介绍。最后,总结了情感倾向性分析技术并对未来的发展进行了展望。由于国内对于文本情感倾向分析的研究起步较早,在一些问题的研究上处于国际前沿水平,已经发表了许多高水平论文,该文也将对此加以介绍。","倾向性分析,评价挖掘,倾向性分类,综述"
2011-10-19,基于声学统计建模的语音合成技术研究,"胡 郁,凌震华,王仁华,戴礼荣","该文介绍基于声学统计建模的语音合成技术,重点回顾中国科学技术大学讯飞语音实验室在语音合成领域这一前沿发展方向的创新性工作成果。具体包括 融合发音动作参数与声学参数,提高声学参数生成的灵活性;以最小生成误差准则取代最大似然准则,提高合成语音的音质;使用单元挑选与波形拼接方法取代参数合成器重构,改善参数语音合成器在合成语音音质上的不足。以上技术创新使得语音合成系统在自然度、表现力、灵活性及多语种应用等方面的性能都有进一步的提升,并推动语音合成技术在呼叫中心信息服务、移动嵌入式设备人机语音交互、智能语音教学等领域的广泛引用。","语音合成,隐马尔可夫模型,参数合成,单元挑选"
2011-10-24,言语信息处理的进展,"蔡莲红,贾 珈,郑 方","该文介绍了言语信息处理的进展,特别提到汉语言语处理的现状。言语信息处理涉及到言语识别、说话人识别、言语合成、言语知觉计算等。带口音和随意发音的言语识别有力的支持了语言学习与口语水平测评等应用;跨信道、环境噪音、多说话人、短语音、时变语音等因素存在的情况下提高识别正确率,是说话人识别的研究热点;言语合成主要关注多语言合成、情感言语合成、可视言语合成等;言语知觉计算开展了言语测听、噪声抑制算法、助听器频响补偿方法、语音信号增强算法等研究。将言语处理技术与语言、网络有效结合,促进了更加和谐的人机言语交互。","言语识别,说话人识别,言语合成,言语知觉计算"
2011-10-08,面向第二语言学习的口语大规模智能评估技术研究,"王士进,李宏言,柯登峰,李 鹏,高 鹏,徐 波","探索英语学习以及少数民族汉语学习的有效方法是中国语言教育面临的重大问题。研究客观公正的口语智能评估与诊断技术,对于促进计算机辅助语言教学(CALL)具有重要意义。根据近年来大规模中学英语以及少数民族汉语水平考试(MHK)中口语评估的应用需求,该文总结了中国科学院自动化研究所在口语内容识别与确认、口语发音评估、口语流利度评估、口语韵律评估等几个方面的研究进展。","中文信息处理,口语评估,发音评估,流利度评估,韵律评估"
2011-09-19,新疆少数民族语言文字信息处理研究与应用,"吐尔根·依布拉音,袁保社","该文主要对国内开展维吾尔、哈萨克、柯尔克孜等少数民族语言信息处理以来的相关研究工作进行了介绍和评价。在此基础上对维吾尔、哈萨克、柯尔克孜文信息处理的进一步发展进行了展望。目的是为了探讨如何加速推进维吾尔、哈萨克、柯尔克孜文信息处理技术的发展。通过对维、哈、柯文操作系统、信息技术标准、语言信息处理及综合应用等四个方面历史和现状的介绍及简单评价,对维、哈、柯语信息处理的发展方向做了相关描述。","维吾尔文,哈萨克文,柯尔克孜文,信息处理,操作系统,自然语言,标准"
2011-09-13,面向自然语言处理的大规模汉藏(藏汉)双语语料库构建技术研究,才让加,"双语语料库建设及其自动对齐研究对计算语言学的发展具有重要的意义。目前国内外已建立了各类汉英双语语料库以及服务于汉英机器翻译的双语对齐语料库和短语库。为了少数民族语言的机器翻译的研究从一开始就从较高起点起步,需要对汉藏双语文本的篇章级、段落级、句子级自动对齐技术进行研究,为开发和研究汉藏机器翻译奠定基础。主要研究汉藏双语语料库对齐、汉藏双语词典抽取、双语语料的收集、整理、存储以及检索等关键技术。最终研究结果是藏文编码的自动识别与转换技术,藏语语料库构建技术、汉藏双语词典抽取技术、汉藏平行语料库句子和词语对齐技术,并建立面向汉藏机器翻译的大规模汉藏双语对齐语料库。","汉藏机器翻译,汉藏双语语料库,编码,对齐技术"
2011-09-15,蒙古语语言知识库的建立与应用,那顺乌日图,"建立一个较为完整的、能够为自然语言处理系统提供知识支撑的语言知识库是蒙古文信息处理当务之急。目前蒙古语语言知识库建设已取得阶段性成果,知识库已初具规模,但也仍然存在一些亟待解决的理论和技术问题。该文对蒙古语语言知识库的主要结构和内容,蒙古语语言知识库的应用,尚待解决的问题进行介绍和探讨。","蒙古语,知识库,语言资源,语义信息,语义词典"
2011-09-23,朝鲜语自然语言处理研究管窥,毕玉德,"朝鲜语作为一种跨境语言,中、朝、韩三国在自然语言处理都进行了一定的研究。该文从自然语言处理研究的基础研究、资源建设以及应用性研究和系统开发等几个方面,分别简要综述了三国的研究情况,并针对中国朝鲜语自然语言处理研究和发展提出了一些建议。","自然语言处理,朝鲜语,中国,朝鲜,韩国"
2011-09-03,彝文信息处理技术三十年发展历程与展望,沙马拉毅,"通过30年的研究与实践,彝文信息处理的研究成果已经广泛应用于新闻出版、教学科研、国家机关等各个领域,应用于全国党代会、全国人民代表大会、全国政协会议等全国性的大会 ,使古老彝文告别了铅与火的时代,进入了光与电的时代,有力推进了彝族社会现代化信息时代的进程。在彝族社会发展历程中,具有划时代意义。","彝族文字,信息处理,30年成就"
2011-09-17,壮文与壮文信息处理,"刘连芳,顾 林,黄家裕,温家凯","壮族的文字分为古壮文和现代壮文。古壮文主要出现在古籍中,民间也仍在使用,是活体字。1990年起广西研制了古壮文造字/编辑/排版工具、释义字典和借音壮字数据库。现代壮文创制于1955年,是壮族扫盲/教学、国家党政重要文件、人民币、广西公共场所使用的文字。目前已有编辑工具、英汉壮释义词典及辅助翻译软件。今后应研究壮文标准,研究开发全文检索和古壮—现代壮—汉—英互译系统,以推动壮文教育、出版、交流与遗产保护。","古壮文,现代壮文,信息处理"
2011-01-31,动态自适应加权的多分类器融合词义消歧模型,"张仰森,郭 江","词义消歧一直是自然语言处理中的热点和难题。集成方法被认为是机器学习研究的四大趋势之一,在系统研究已有集成学习方法在汉语词义消歧中的应用后,借鉴模式识别领域集成分类器思想,提出了一种动态自适应加权投票的多分类器集成方法来构建融合分类器。实验结果表明,所提融合分类器模型对汉语文本自动消歧结果的准确率提高较大。","词义消歧,分类器,多分类器融合,上下文特征"
2011-01-10,结合结构下文及词汇信息的汉语句法分析方法,"陈 功,罗森林,陈开江,冯 扬,潘丽敏","针对句法分析中上下文无关语法模型对句子信息利用的不足,通过融入结构下文和部分词汇信息,提出两种基于概率上下文无关语法模型的短语结构消歧方法,以达到消解结构歧义的目的;引入分层分析的算法,通过损失一定的时间效率使得在提高分析准确率的同时保证分析结果的全面性。实验结果表明,融入结构下文及词汇信息的汉语句法分析方法,利用了更多的句子信息,与上下文无关语法相比有着更强的消歧能力。","汉语句法分析,概率上下文无关语法,结构下文相关,词汇信息,分层分析"
2011-11-01,第七届全国机器翻译研讨会机器翻译评测总结,"赵红梅,吕雅娟,贲国生,黄 云,刘 群","该文介绍了第七届全国机器翻译研讨会(CWMT2011)机器翻译评测的具体情况。本次评测重点关注各种语言到汉语的翻译,除了汉英、英汉、日汉三个语言对以外,评测还新增了五种民族语言(藏语、蒙古语、维吾尔语、哈萨克语、柯尔克孜语)到汉语的翻译评测。共有19家国内外单位的165个系统参加此次评测。除了介绍评测项目的设置、评测数据的准备、评测流程、参评单位等,本文还重点介绍了CWMT2011的评测结果,并对评测结果进行了分析,用实例说明了与评测结果相关的几个因素 源语言与目标语言是否相似、评测领域是否集中、测试集与训练及开发集语料是否相似、训练语料的规模、参评系统的技术和成熟度等。","机器翻译,机器翻译评测,BLEU-SBP,WoodPecker评测"
2011-01-18,层次短语翻译模型的介词短语调序,"冯 洋,张冬冬,刘 群","在不同的语言中,句法成分的相对位置往往不同,介词短语表现尤为明显,因此正确的对介词短语进行调序对提高翻译质量至关重要。层次短语模型借助于形式语法规则,具有较强的处理长距离调序的能力,但是其并不对短语的句法成分进行区分,这会导致规则的使用不当,从而引起翻译错误。该文在层次短语模型的基础上,针对介词短语进行处理。首先利用条件随机场模型识别出介词短语,然后抽取出带有介词短语的规则,构建一个新的同步上下文无关文法。解码的时候,在这个同步上下文无关文法定义的空间里搜索找到最优的译文。相对于层次短语模型,该方法在我们内部的英汉数据集上调高了0.8个BLEU百分点,在NIST 2008 英汉翻译数据集上提高了0.5个BLEU百分点。","统计机器翻译,层次短语模型,介词短语调序,条件随机场"
2011-01-14,面向层次短语翻译的词汇化调序方法研究,"肖欣延,刘 洋,刘 群,林守勋","词汇化信息在短语调序中有重要的作用。然而层次短语翻译模型调序时并不考虑变量所泛化的短语的词汇化信息,因此该模型调序的歧义性较大。为此该文提出面向层次短语模型的词汇化调序方法。我们定义变量与邻接词语的调序关系,并使用变量所泛化短语片段的边界词信息来指导调序。在大规模语料的汉语到英语翻译评测任务中,我们的方法在NIST 2003-2005测试数据上获得了0.6～1.2 BLEU值的提高。","统计机器翻译,层次短语,词汇化调序"
2011-01-15,基于同义实体扩展的冗余信息去重,"姜孟晋,周雅倩,黄萱菁","冗余信息去重是信息抽取中的重要任务,对于多元素表示的信息,该文针对以往对各个元素统一处理所存在的问题,将信息元素进行分类,由各类元素的冗余判断难易出发,归纳相似度计算方法,并将各相似度作为特征,通过分类器判断信息间的冗余性。同时对最难判断的命名实体信息元素,该文从其他易判断相似性的信息元素出发,通过同义命名实体的自动扩展,提高信息去重的效果。","信息抽取,信息去重,命名实体"
2010-12-28,基于词共现的文档表示模型,"常 鹏,冯 楠","文档表示模型是文本自动处理的基础,是将非结构化的文本数据转化为结构化数据的有效手段。然而,目前通用的空间向量模型(Vector Space Model,VSM)是以单个的词汇为基础的文档表示模型,因其忽略了词间的关联关系,导致文本挖掘的准确率难以得到很大的提升。该文以词共现分析为基础,讨论了文档主题与词的二阶关系之间的潜在联系,进而定义了词共现度及与文档主题相关度的量化计算方法,利用关联规则算法抽取出文档集上的词共现组合,提出了基于词共现组合的文档向量主题表示模型(Co-occurrence Term based Vector Space Model, CTVSM),定义了基于CTVSM的文档相似度。实验表明,CTVSM能够准确反映文档之间的相关关系,比经典的文档向量空间模型(Vector Space Model,VSM)具有更强的主题区分能力。","文档建模, 词共现, 文档相似度, 文本挖掘"
2010-12-31,基于事件抽取的网络新闻多文档自动摘要,"韩永峰,许旭阳,李弼程,朱武斌,陈 刚","目前,有代表性的自动摘要方法是根据文本片段进行聚类,较传统方法避免了信息冗余,但网络新闻文本中有些文本片段和主题无关,影响了聚类的效果,导致最终生成的摘要不够简洁。为此,该文引入事件抽取技术,提出了一种基于事件抽取的网络新闻多文档自动摘要方法。该方法首先通过二元分类器辨析出文本中的事件和非事件;然后通过聚类将文档原来以段落或句子为单位的物理划分转化为以事件为单位的内容逻辑划分,最后通过主旨事件抽取、排序及润色,生成摘要。实验结果表明,该方法是有效的,显著提高了生成摘要的质量。","事件抽取,中文信息处理,分类,新闻文档,聚类,自动摘要"
2010-09-11,基于粗糙集与贝叶斯决策的不良网页过滤研究,"孙 艳,周学广","不良网页过滤是一种两类网页分类问题。提出了一种基于粗糙集与贝叶斯决策相结合的不良网页分类过滤方法,首先利用粗糙集理论的区分矩阵和区分函数得到网页分类决策的属性约简;然后通过贝叶斯决策理论对网页进行分类与过滤决策。仿真实验表明,该方法在不良网页分类过滤系统中开销小,过滤准确度高,因而在快速过滤不良网页的应用中具有工程应用价值。","信息安全,网页过滤,粗糙集,区分矩阵,贝叶斯决策"
2011-06-10,基于层次结构的多策略中文微博情感分析和特征抽取,"谢丽星1,周 明2,孙茂松1","随着Web2.0时代的兴起,与微博相关的研究得到了学术界和工业界的广泛关注。该文使用新浪API获取数据,针对中文微博消息展开了情感分析方面的研究。我们对于三种情感分析的方法进行了深入研究,包括表情符号的规则方法、情感词典的规则方法、基于SVM的层次结构的多策略方法,实验表明基于SVM的层次结构多策略方法效果最好。其次,针对层次结构的多策略方法的特征选择进行了详细分析,包括主题无关、主题相关的特征。实验表明使用主题无关的特征时获得的准确率为66.467%。引入主题相关的特征后,准确率提升至67.283%。","新浪微博,情感分析,SVM"
2010-11-03,基于情感分布的微博热点事件发现,"杨 亮,林 原,林鸿飞","微博(Micro-Blog,Twitter)是互联网上的一种重要媒体,以简短、便捷的方式表达用户的观点,并实现多发布工具即时分享,已经成为热点事件产生和传播的重要场所,因此微博平台中热点事件发现等方面研究的重要性便突显出来了。该文依据热点事件的出现会使用户所发微博中情感词数量增多,情感发生变化,提出了情感分布语言模型,通过分析相邻时段间情感分布语言模型间的差异,实现对热点事件的发现。实验结果表明该文方法可以有效地从微博平台中发现热点事件,并且有助于对微博平台中热点事件的管理和监控。","微博,热点事件,情感分布语言模型"
2011-01-26,维吾尔语词法中音变现象的自动还原模型,"麦热哈巴·艾力,姜文斌,吐尔根·依布拉音","该文针对维吾尔语的音变现象,提出了一种自动还原模型。与以往方法不同的是,此模型中我们把音变现象泛化,先假设维吾尔语中所有语音都有音变现象,从而将还原问题转化为类似于词性标注问题,再利用标注的方法解决了还原操作。在新疆多语种信息技术重点实验室手工标注的《维吾尔语百万词词法分析语料库》上做了实验,还原模块作为维吾尔语词法分析器的一部分,把词法分析器功能的F值从84.1%提高到了91.4%,同时维吾尔语中词缀数目最多、变形情况最复杂的动词词干的还原正确率也达到了88.6%,实际应用中完全可以被接受。","维吾尔语,词法分析,维吾尔语变音现象"
2011-01-04,SegT一个实用的藏文分词系统,"刘汇丹,诺明花,赵维纳,吴 健,贺也平","在分析现有藏文分词方法的基础上,该文重点研究了藏文分词中的格助词分块、临界词识别、词频统计、交集型歧义检测和消歧等问题并提出了相应的方法。应用这些方法,设计实现了一个藏文分词系统SegT。该系统采用格助词分块并识别临界词,然后采用最大匹配方法分词,并进行紧缩词识别。系统采用双向切分检测交集型歧义字段并使用预先统计的词频信息进行消歧。实验结果表明,该文设计的格助词分块和临界词识别方法可以将分词速度提高15%左右,但格助词分块对分词效果没有明显提高或降低。系统最终分词正确率为96.98%,基本达到了实用的水平。","藏文分词,格助词,临界词识别,词频统计,藏文信息处理,中文信息处理"
2010-11-07,基于能量分布和共振峰结构的汉语鼻音检测,"陈 斌,张连海,牛 铜,王 波","该文提出了一种基于能量分布和共振峰结构的汉语鼻音检测方法,该方法首先基于Seneff听觉谱提取了一组描述音段能量分布和共振峰结构的特征参数,然后采用支持向量机模型进行检测和分类,得到候选的鼻音,最后根据音段持续时间、前端韵母能量、高低频能量差、中低频能量比等特征对候选的鼻音进行后处理,去除插入错误,提高鼻音检测的准确率。实验结果表明,干净语音鼻音检测准确率可以达到90.4%,信噪比10dB的语音鼻音检测准确率可达到84.4%以上。","鼻音检测,能量分布,共振峰结构,Seneff听觉模型"
2010-10-02,从实验语音学角度探析维吾尔语鼻音的声学特征,艾斯卡尔·艾木都拉,"该文根据语音合成与识别等语音应用研究的需求,从文本分析模块入手,利用“维吾尔语语音声学参数库”,选择了包含鼻音m、n和的单音节以及多音节词,提取它们的声学参数并进行统计分析,归纳了其共振峰、音强和时长分布模式,研究了鼻音的两个变体,从实验语音学的角度出发进一步探讨了鼻音的声学特性,并总结出了一系列结论。其目的是为了提高语音合成的自然度即更好的为自然语言处理服务。该项研究结果对维吾尔语语言乃至整个阿尔泰语系语言的韵律研究具有较高的参考价值。","维吾尔语,鼻音,共振峰,时长,音强,语音,变体"
2010-09-20,第二语言学习者汉语声调范畴浮现的模拟研究,陈 默,"第二语言学习者汉语声调范畴的习得一直是汉语学习的难点之一。为了深入研究声调范畴的认知机制,该研究采用动态的生长型树形自组织映射模型,模拟了英语母语者汉语声调范畴的认知发展过程。由于新发展的自组织模型既具有良好的拓扑映射性,又具有动态的容量扩展性,所以能很好地模拟英语母语者汉语声调范畴认知的动态发展过程。模拟结果跟行为实验结果呈现出非常好的一致性,这样既证明了行为实验中汉语声调范畴的动态发展过程,也为汉语声调认知范畴的机制研究提供了机理上的解释。通过对声调范畴习得过程中的一些模式和机制的研究,为声调教学提出了一些有益的建议。","声调范畴,计算机模拟,生长型树形结构自组织映射模型"
2011-09-21,统计与词典相结合的领域自适应中文分词,"张梅山,邓知龙,车万翔,刘 挺","基于统计的中文分词方法由于训练语料领域的限制,导致其领域自适应性能力较差。相比分词训练语料,领域词典的获取要容易许多,而且能为分词提供丰富的领域信息。该文通过将词典信息以特征的方式融入到统计分词模型(该文使用CRF统计模型)中来实现领域自适应性。实验表明,这种方法显著提高了统计中文分词的领域自适应能力。当测试领域和训练领域相同时,分词的F-measure值提升了2%;当测试领域和训练领域不同时,分词的F-measure值提升了6%。","中文分词,CRF,领域自适应"
2011-08-05,一种利用注疏的《左传》分词新方法,"徐润华,陈小荷","先秦文献的注疏文献中包含有大量词汇语义知识,是先秦文献自动分词的重要依据。该文以篇幅最大的先秦文献《左传》为研究对象,在对《左传》及其注疏文献进行自动对齐的基础上,提出了一种利用注疏的《左传》分词新方法。分词实验的F值达到89.0% ,较之baseline有明显提升。该方法无需训练语料,利用注疏文献辅助分词的思想也适合推广到其他先秦文献的自动分词任务中去。","先秦文献,注疏文献,自动对齐,自动分词"
2011-09-20,基于主动学习的中文依存句法分析,"车万翔,张梅山,刘 挺","目前依存句法分析仍主要采用有指导的机器学习方法,即需要大规模高质量的树库作为训练语料,而现阶段中文依存树库资源相对较少,树库标注又是一件费时费力的工作。面对大量未标注语料,该文将主动学习应用到中文依存句法分析,优先选择句法模型预测不准的实例交由人工标注。该文提出并比较了多种衡量依存句法模型预测可信度的准则。实验表明,一方面,与随机选择标注实例相比,当使用相同数目训练实例时,主动学习使中文依存分析性能最高提升0.8%;另一方面,主动学习使依存分析达到相同准确率时只需标注更少量实例,人工标注量最多可减少30%。","主动学习,依存句法,不确定性度量,委员会投票"
2011-09-15,树库中的歧义组合考察,"李艳娇,杨尔弘","汉语树库是汉语信息处理的宝贵资源,其中包含了丰富的句子结构及成分组合信息,对树库中的词性串组合进行考察,是有效利用树库信息的基础工作。该文对汉语树库中的歧义组合进行考察,发现汉语中的结构歧义很大程度上要靠词语的语义特征来消解,仅仅依靠词语的语法特征(如词类信息)是无法解决的。","歧义组合,语义关系,树库"
2011-09-17,基于序列标注的全词消歧方法,"周 云,王 挺,易绵竹,张禄彭,王之元","全词消歧(All-Words Word Sense Disambiguation)可以看作一个序列标注问题,该文提出了两种基于序列标注的全词消歧方法,它们分别基于隐马尔可夫模型(Hidden Markov Model, HMM)和最大熵马尔可夫模型(Maximum Entropy Markov Model, MEMM)。首先,我们用HMM对全词消歧进行建模。然后,针对HMM只能利用词形观察值的缺点,我们将上述HMM模型推广为MEMM模型,将大量上下文特征集成到模型中。对于全词消歧这类超大状态问题,在HMM和MEMM模型中均存在数据稀疏和时间复杂度过高的问题,我们通过柱状搜索Viterbi算法和平滑策略来解决。最后,我们在Senseval-2和Senseval-3的数据集上进行了评测,该文提出的MEMM方法的F1值为0.654,超过了该评测上所有的基于序列标注的方法。","全词消歧,隐马尔可夫模型,最大熵马尔可夫模型,超大状态问题"
2011-09-13,基于HNC的汉语词语知识库改进,"王青海,马海慧,池毓焕,李 颖,董凌冲","汉语词语知识库是HNC知识库系统的重要组成部分,目前其结构设计简单,加大了对HNC符号解析的难度。该文在分析了HNC的编码特点的基础上,改进了汉语词语知识库模型,阐述了改进后汉语词语知识库实体属性的设计方法和知识库的填写原则,并用实例说明了改进后的词语知识库可以提高自然语言处理的效率。","汉语词语知识库,HNC理论,关系数据库"
2011-09-05,基于依存树距离识别论元的语义角色标注系统,"王 鑫,穗志方","在基于依存的语义角色标注研究中,大多数系统采用机器学习方法进行论元识别和分类。该文分析了依存树的特点,发现论元集中分布于依存树上的特定局部范围内,因此提出一种基于依存树距离的论元识别方法。该方法将候选论元限制在与目标动词的依存树距离不超过3 的范围内,通过制订规则,提取目标动词的最佳候选论元集合。在CoNLL2009中文语料上采用正确的依存树,识别出了98.5%的论元。在此基础上,结合基于机器学习的角色分类,系统F值达到89.46%,比前人的方法 (81.68%)有了较为显著的提升。","论元识别,基于依存树距离的方法,语义角色标注"
2011-09-20,基于FrameNet框架关系的文本蕴含识别,"张 鹏,李国臣,李 茹 ,刘海静,石向荣,Collin Baker","文本蕴含识别是处理自然语言中广泛存在的同义异形现象的一种有效途径。该文基于FrameNet中框架及框架之间的八种关系,结合WordNet中词汇间的语义关系,提出了一种文本蕴含识别方法。在给定文本T和假设H中词元激起的框架基础上,该方法利用深度优先搜索,在FrameNet框架关系图中,查询T和H中框架之间的上下位关系;再使用WordNet中语义关系比较二者的框架元素是否一致或相似。实验对RTE2007中50个文本对进行了测试,达到了76.6%的准确率,略高于RTE2007评测的最优结果。","文本蕴含识别,FrameNet,框架关系"
2011-09-10,服务于内容侧面发现的框架识别,"王 荀,李素建,宋 涛,姜伯平","文本内容通常包含多个侧面,全面地识别这些内容侧面对自然语言处理有重要地意义。传统的统计方法使用简单特征难以识别出所有的内容侧面。以自动摘要为例,传统的抽取式方法多以词频为主要特征,一些重要的句子常因重复度不高被舍弃。要想全面地覆盖原始文本的重要信息,就要识别出文本描述的内容侧面。该文以框架语义学为指导,使用FrameNet语料库作为知识库,综合多种特征来标注文本描述的框架,在此基础上识别文本所包含的内容侧面。该方法在新闻语料上取得了较好地结果,达到了61%的正确率。","FrameNet语料库,内容侧面发现,框架识别"
2011-09-17,基于CRFs的评价对象抽取特征研究,"王荣洋,鞠久朋,李寿山,周国栋","评价对象是情感分析中情感信息的一个重要组成部分。该文基于条件随机场模型,研究多种特征在评价对象抽取任务中的表现,并将特征归纳为词法、依存关系、相对位置、语义四大类别。其中,重点引入语义角色标注新特征。在实验中,我们在三个不同的数据集上考查了各个特征及其组合对系统性能的影响,作了详细地比较研究。另外,实验结果表明新提出的语义角色标注特征对评价对象抽取有很好地指示作用。","情感分析,评价对象抽取,特征组合,语义角色标注"
2011-09-30,基于条件随机场与Web数据的缩略语预测,"焦 妍,王厚峰,张龙凯","缩略语在自然语言中被广泛使用。因其是新词的重要来源之一,成为了自然语言处理领域的一大问题。该文以汉语为对象,研究了从完整形式预测缩略语形式的方法。首先,使用条件随机场模型对完整形式进行序列标注,生成缩略语候选集合。再利用搜索引擎获取网络数据,并通过不同策略利用网络数据对各候选依次评估,结合各项评估分数进行重排序,选择最终的缩略语结果。实验结果表明,增加Web信息之后,缩略语预测的准确率可以提高约五个百分点。","缩略语,CRF模型,网页数据"
2011-09-12,基于双语平行语料的中文缩略语提取方法,"刘友强,李 斌,奚 宁,陈家骏","汉语缩略语在现代汉语中被广泛使用,其研究对于中文信息处理有着重要地意义。该文提出了一种从英汉平行语料库中自动提取汉语缩略语的方法。首先对双语语料进行词对齐,再抽取出与词对齐信息一致的双语短语对,然后用SVM分类器提取出质量高的双语短语对,最后再从质量高的短语对集合中利用相同英文及少量汉语缩略—全称对应规则提取出汉语缩略语及全称语对。实验结果表明,利用平行语料的双语对译信息,自动提取出的缩略语具有较高地准确率,可以作为一种自动获取缩略语词典的有效方法。","缩略语,平行语料库,短语抽取,分类"
2011-09-26,基于维基百科和模式聚类的实体关系抽取方法,"张苇如,孙 乐,韩先培","该文提出了一种基于维基百科和模式聚类的方法,旨在从开放文本中抽取高准确率的中文关系实体对。首次使用从人工标注知识体系知网到维基百科实体映射的方式获取关系实例,并且充分利用了维基百科的结构化特性,该方法很好地解决了实体识别的问题,生成了准确而显著的句子实例;进一步,提出了显著性假设和关键词假设,在此基础上构建基于关键词的分类及层次聚类算法,显著提升了模式的可信度。实验结果表明该方法有效提升了句子实例及模式的质量,获得了良好的抽取性能。","关系抽取,维基百科,模式聚类"
2011-09-30,基于并列结构的概念实例和属性的同步提取方法,"李文杰,穗志方","在概念实例和属性的提取研究中,针对基于模式的方法召回率比较低的特点,该文提出了一种基于并列结构的概念实例和属性的同步提取方法。首先利用并列结构模式去网页集合中提取同类词语集合,然后再用基于种子的弱指导方法去学习实例和属性共现的上下文模式,最后再通过模式去提取候选实例或候选属性。在此过程中,每提取出一个候选,就将该候选所在的同类词语集合合并到候选集合中。实验结果表明,该文的方法在不降低准确率的基础上,能大大提高提取结果的召回率。","并列结构,搜索引擎,实例提取,属性提取,上下文模式"
2011-09-08,商品品牌名称挖掘,"何正焱,王厚峰","百度百科包含了大量的实体和丰富的链接与分类关系,在中文领域含有大量人类知识,能够弥补普通词典词汇覆盖面小的缺点。 在商品品牌名称挖掘中,该文提出了发现新的品牌名称的基于图模型的半指导方法。利用百度百科中词条间的相关关系和开放分类,该文使用不同的准则计算词条间的相似度,结合词条和分类的关联性,分类与分类之间的关联性,使用标记传播算法,在130万个词条上进行了品牌名称的挖掘,取得了较好地效果。","商品名挖掘,半监督学习,图算法"
2011-09-19,面向冗余度控制的中文多文档自动文摘,"王红玲,周国栋,朱巧明","多文档自动文摘能够帮助人们自动、快速地获取信息,是目前的一个研究热点。相比于单文档自动文摘,多文档自动文摘需要更多考虑文档之间的相关性,以及文档信息之间的冗余性。因此如何控制信息冗余是多文档自动文摘的一个关键所在。该文在考虑文摘特性的基础上提出了一个冗余度控制模型,该模型通过计算文本单元在主题概率分布之间的相似度来决定句子的选择,从而达到控制冗余的目的。实验结果表明,该方法能够有效降低冗余度,且总体性能优于现有的自动文摘系统。","冗余度控制,多文档自动文摘,中文自动文摘"
2011-09-28,文本摘要问题中的句子抽取方法研究,"张龙凯,王厚峰","抽取式摘要是从正文中按照一定策略抽取重要句子组成摘要。该文提出了一种句子抽取方法。基本思想是将句子的抽取看作序列标注问题,采用条件随机场模型对句子进行二类标注,根据标注结果抽出句子以生成摘要。由于不在摘要中的句子的数量远大于摘要中的句子数量,标注过程倾向于拒绝将句子标注为摘要句,针对此问题该文引入了修正因子进行修正。实验表明该方法具有较好地效果。","文本摘要,句子抽取,条件随机场"
2011-09-21,基于词汇评分的汉语作文自动评分,"彭星源,柯登峰,赵 知,陈振标,徐 波","该文研究了通过作文词汇评分来实现汉语作文自动评分的新算法。在作文评分应与词汇评分高度相关的假设基础上,实现了这种关系的量化计算。该文从通用词表方法、常规方法以及提出的三种改进算法上进行方法性能的比较,并对比了E-rater作文评分系统中同样采用基于词汇方法的性能。实验结果表明,基于新的词汇评分的作文评分方法相关度①接近0.7的水平,高于E-rater中采用的基于词汇的方法的相关度。同时,这一方法的结果已经接近于人工作文评分的相关度。","词汇评分,作文自动评分"
2011-09-25,基于跨语言广义向量空间模型的跨语言文档聚类方法,"唐国瑜,夏云庆,张 民,郑 方","跨语言文档聚类主要是将跨语言文档按照内容或者话题组织为不同的类簇。该文通过采用跨语言词相似度计算将单语广义向量空间模型(Generalized Vector Space Model, GVSM)拓展到跨语言文档表示中,即跨语言广义空间向量模型(Cross-Lingual Generalized Vector Space Model,CLGVSM),并且比较了不同相似度在文档聚类下的性能。同时提出了适用于GVSM的特征选择算法。实验证明,采用SOCPMI词汇相似度度量算法构造GVSM时,跨语言文档聚类的性能优于LSA。","跨语言文档聚类,跨语言广义向量空间模型,文档聚类,跨语言信息检索"
2011-01-19,多信息融合的新闻节目主题划分方法,"余骁捷,吴 及,孔繁庭,李树森","对新闻播报节目进行自动主题划分,可以有效地组织和利用新闻播报类数据。目前自动故事单元划分的研究以视频数据为主,音频的语音识别文本中包含丰富的语义信息,同时声音事件的转换也可以提供很多重要信息,能够有效的进行基于语义的主题划分。根据这些信息,该文提出了一种基于规则的多信息融合的方法,利用切分点邻域的音频类型信息来修正使用语义信息的切分结果,完成主题划分。实验表明根据规则进行特征融合后,新闻节目主题划分的F-估值为64.8%,错误概率Pk和WindowDiff分别达到18.3%和24.5%。","新闻节目主题划分,改进的SeLeCT算法,信息融合"
2011-09-15,面向移进—归约句法分析器的单模型系统整合算法,"马 骥,朱慕华,肖 桐,朱靖波","该文提出了一种面向移进—归约句法分析器的单模型系统整合算法。在训练阶段,该方法通过调整训练数据的分布,来构建用于整合的多个移进—归约句法分析器。在解码阶段,该方法首先使用各个移进—归约句法分析器对待分析的句子进行句法分析,然后利用一个线性模型对各句法分析器输出的句法树进行评分,从中选出得分最高的句法树作为最终结果。该文中的实验是在宾州英文树库上进行的。实验结果表明,该文中的方法能够显著改善基准系统的性能。","句法分析,系统整合, 移进—归约句法分析器"
2011-09-28,最大生成树算法和决策式算法相结合的中文依存关系解析,"周惠巍,黄德根,高 洁,杨元生","基于最大生成树解析算法和决策式解析算法的互补关系,提出了最大生成树解析算法和决策式解析算法相结合的中文依存关系解析方法。结合方法利用Nivre模型的依存关系解析结果和依存度修正最大生成树模型有向边的权重,再搜索最大生成树作为依存树。使用宾州中文树库中的4 500句语料作十折交叉测试,结合模型的依存关系正确率达到了86.49%。结果表明该文提出的结合方法有效地提高了的中文依存关系解析性能。","中文依存关系解析,最大生成树算法,决策式算法"
2011-09-01,基于特征结构的汉语主谓谓语句语义标注研究,"陈 波,姬东鸿,吕 晨","建构大规模的汉语语义资源,是当前中文信息处理的重要任务之一。但是其中语义分析的传统方法存在一些问题,不能很好的反映汉语中各个词语或成分之间的语义关联。该文提出了基于特征结构的语义标注方法,并在此基础上建构了一个大规模的汉语语义资源。以汉语主谓谓语句为例,探讨了特征结构的标注方法。结果表明,特征结构分析解决了以往传统标注方法对汉语特殊句型无法表示的难题,包含更多的语义信息,其标注效率更高, 标注精度也更高。","特征结构,主谓谓语句,语义标注,语义资源"
2011-09-14,基于统计方法的蒙古语依存句法分析模型,"斯·劳格劳,华沙宝,萨如拉","蒙古语文信息处理已初步完成字、词处理阶段的基本任务,正在步入句处理阶段,并且在国家自然科学基金的资助下构建了蒙古语依存树库MDTB。该文以MDTB为训练和评测数据,设计实现了一种基于词汇依存概率的蒙古语依存句法分析模型。目前,该模型的无标记准确率、有标记准确率和核心词准确率分别达到了71.24%、61.42%和93.05%。","蒙古文,依存语法,句法分析,概率模型"
2011-09-04,基于不平衡数据的中文情感分类,"王中卿,李寿山,朱巧明,李培峰,周国栋","近些年来,情感分类在自然语言处理研究领域获得了显著的发展。然而,大部分已有的研究都假设参与分类的正类样本和负类样本一样多,而实际情况中正负类数据的分布往往是不平衡的。该文收集四个产品领域的中文评论文本,发现正类样本的数目远远多于负类样本。针对不平衡数据的中文情感分类,提出了一种基于欠采样和多分类算法的集成学习框架。在四个不同领域的实验结果表明,我们的方法能够显著提高分类性能,并明显优于目前主流的多种不平衡分类方法。","情感分类,不平衡分类,集成学习"
2011-09-25,基于语境歧义词的句子情感倾向性分析,"宋艳雪,张绍武,林鸿飞","该文从情感的角度研究语境歧义词的搭配,这种搭配对文本情感倾向性分析方面具有实际重要的意义。首先使用关联规则挖掘的方法确定语境歧义词候选搭配集,然后通过PMI过滤后判断每对搭配词是否具有情感倾向性,最终构建语境歧义词搭配词典。采用语义分析的方法,在构建的语境歧义词搭配词典基础上对句子进行情感倾向性分析。通过在COAE2008语料集和情感语料库上进行实验,证明了在判断句子情感倾向性时考虑到语境歧义词的重要性,其对句子进行情感倾向性判断的正确率有很大的影响。","语境歧义词,关联规则,倾向性,情感计算"
2011-09-10,基于语义块的事件倾向性分析研究,"韦向峰,张 全,缪建明,池毓焕","事件的倾向性分析对网络舆情分析和事件趋势分析都具有重要意义。该文把影响倾向性分析的词语分为四类 对象词、褒贬词、逻辑词和程度词,建立了语句倾向性分析的二元模型和三元模型,在语句语义块分析的基础上实现对语句和篇章的倾向性获取。实验中首先确定三个事件实例的关键对象和立场,然后根据语句倾向性分析获得文章对于对象的褒贬态度和立场。实验表明语义块的范围限制有助于提高事件倾向性分析的准确性,立场分析则是事件倾向性分析的关键所在。","倾向性,语义块,立场分析"
2011-09-04,规则和统计相结合的中文地址翻译方法,"于 淼,吕雅娟,苏劲松,李贤华","该文研究了一种规则和统计相结合的中文地址翻译方法。首先利用区划词典、关键字词典和模式表进行分词及词语类型标注,并根据词语类型划分地址单元;然后,以统计翻译模型为基础结合少量的翻译词典和人工模板对地址单元进行翻译;最后,将地址单元的翻译结果以逆序粘合在一起,形成最终译文。实验表明,利用该方法翻译中文地址能够取得较好地翻译效果。","中文地址,机器翻译,地址单元"
2011-09-13,一种适用于机器翻译的汉语分词方法,"奚 宁,李博渊,黄书剑,陈家骏","汉语分词是搭建汉语到其他语言的统计机器翻译系统的一项重要工作。从单语语料中训练得到的传统分词模型并不一定完全适合机器翻译[1]。该文提出了一种基于单语和双语知识的适应于统计机器翻译系统的分词方法。首先利用对齐可信度的概念从双语字对齐语料中抽取可信对齐集合,然后根据可信对齐集合对双语语料中的中文部分重新分词;接着将重新分词的结果和单语分词工具的分词结果相融合,得到新的分词结果,并将其作为训练语料,利用条件随机场模型训练出一个融合了单双语知识的分词工具。该文用该工具对机器翻译所需的训练集、开发集和测试集进行分词,并在基于短语的统计机器翻译系统上进行实验。实验结果表明,该文所提的方法提高了系统性能。","中文分词, 统计机器翻译, 对齐可信度"
2011-09-18,基于“固结词串”实例的中文分词研究,"修 驰,宋 柔","近几年的中文分词研究中,基于条件随机场(CRF)模型的中文分词方法得到了广泛的关注。但是这种分词方法在处理歧义切分方面存在一定的问题。CRF虽然可以消除大部分原有的分词歧义,却会带来更多新的错误切分。该文尝试找到一种简单的、基于“固结词串”实例的机器学习方法解决分词歧义问题。实验结果表明,该方法可以简单有效的解决原有的分词歧义问题,并且不会产生更多新的歧义切分。","中文分词,CRF,固结词串,分词歧义,机器学习"
2011-09-20,基于词典信息的先秦汉语全文词义标注方法研究,"张颖杰,李 斌,陈家骏,陈小荷","词义消歧是自然语言处理中的一项基础任务,古汉语信息处理也急需深层次的语义标注工作。该文针对先秦古汉语这一特殊的语言材料,在训练语料和语义资源匮乏的条件下,采用《汉语大词典2.0》作为知识来源,将其词条释义作为义类,每个义项的例句作为训练语料,使用基于支持向量机(SVM)的半指导方法对《左传》进行全文的词义标注。按照频度不同、义项数量不同的原则,我们随机选取了22个词进行了人工检查,平均正确率达到67%。该方法可以广泛用于缺乏训练语料的古汉语义项标注工作,能够在古汉语全文词义标注的起步阶段提供初始结果,为人工标注词语义项提供良好的数据底本,补正传统词典释义不全的问题,进一步丰富汉语史发展研究资料。","词义消歧,义项标注,古汉语,自然语言处理"
2011-08-30,基于电子商务用户行为的同义词识别,"张书娟,董喜双,关 毅","该文研究了电子商务领域同义词的自动识别问题。电子商务领域的同义词是指对同一事物或概念的不同表达,即在商品描述和检索中可以相互替换的词,针对该领域新词多、错别字多、近义词多的特点,提出基于用户行为的同义词识别方法。首先通过并列关系符号切分商品标题和基于SimRank思想聚集查询两种方法获取候选集合,进而获取两词的字面特征以及标题、查询、点击等用户行为特征,然后借助Gradient Boost Decision Tree模型判断是否同义。实验表明同义词识别准确率达到56.52%。","同义词识别,用户行为,SimRank,Gradient Boost Decision Tree"
2011-09-03,构建大规模的汉语事件知识库,"周 强,王俊俊,陈丽欧","该文提出了一种静态知识库和动态标注库相结合的汉语事件知识库构建方法。在统一的设计框架下,将相关事件知识拆分成五个相对独立的知识子库,并通过各子库之间的内在联系使之互相参照互为补充。经过有效拆分和信息联动,增强信息的丰富性和可靠性,同时细化工作的粒度,具有较好的可操作性。以此为基础,开发完成一个汉语“存在拥有类”事件知识库,其中静态知识库覆盖72个情境和1 548个词语义项,动态标注库包含598个事件目标动词的10万句标注结果,取得了较好的实验效果。","事件内容分析,事件语义标注资源,汉语事件知识库"
2011-09-11,事件信息结构分析,"杨尔弘,曾青青,李婷婷","该文通过考察事件词在文本篇章结构中的分布方式,指出突发事件新闻报道文本中包含主线信息链和副线信息链。主线信息链中包含了文本的事件信息,是事件信息提取重点考虑的文本内容部分;副线信息链则由文本结构中的“评价”、“背景”以及“情节”部分的细节信息等组成,是事件信息提取时可以忽略的文本内容部分。事件信息的结构可以进一步分解为前核心事件链、核心事件链、次生事件链和后次生事件链。该文通过定义事件词,以其为触发,探索了事件信息结构的识别与获取,并借助《知网》(HowNet)提高了事件词对信息刻画的有效性和区分度。","事件词,事件信息结构,主线信息链,副线信息链"
2011-09-11,基于关联度的汉藏多词单元等价对抽取方法,"诺明花,刘汇丹,吴 健,丁治明","针对为汉藏辅助翻译系统建立汉藏多词单元翻译词典这一任务,该文提出了CMWEPM模型。该模型首先依据关联度和结合度来确定汉语语料中多词单元的边界,然后根据词对齐信息分别抽取严格和约束多词单元等价对,从而形成汉藏多词单元等价对。CMWEPM模型根据不同长度和频次对多词单元进行分类,并为不同类型设定不同阈值,最终提高了汉藏多词单元等价对的召回率,从而能够间接地提高汉藏辅助翻译系统的翻译质量。","藏文信息处理,多词单元,关联度"
2011-09-08,评论挖掘中产品属性归类问题研究,"杨 源,马云龙,林鸿飞","该文主要把产品评论中属性的不同描述进行归类。在产品评论中,同类的属性会有不同的描述,例如,手机的“外形”和“设计”指的是同类属性。同类属性虽然有不同的描述,但是在句中却和相同的情感词搭配使用。该文首先抽取评论句中属性和情感词的搭配关系,形成一个二部图,然后用权重标准化SimRank计算不同属性之间的相似度,并把所得的结果与半监督学习中的贝叶斯分类器进行融合,得到了更好的分类结果。通过实验证明了此方法的有效性。","属性,归类,SimRank,半监督学习"
2011-09-13,中文维基百科的结构化信息抽取及词语相关度计算方法,"涂新辉1,2,张红春1,2,周琨峰1,2,何婷婷1,2","维基百科作为一个以开放和用户协作编辑为特点的Web 2.0知识库系统,具有知识面覆盖度广,结构化程度高,信息更新速度快等优点。然而,维基百科的官方仅提供一些半结构化的数据文件,很多有用的结构化信息和数据,并不能直接地获取和利用。因此,该文首先从这些数据文件中抽取整理出多种结构化信息;然后,对维基百科中的各种信息建立了对象模型,并提供了一套开放的应用程序接口,大大降低了利用维基百科信息的难度;最后,利用维基百科中获取的信息,该文提出了一种基于链接所对应主题页面所属类别的词语语义相关度计算方法。","语义相关度,中文维基百科,结构化信息"
2011-09-15,基于依存关系的旅游景点评论的特征—观点对抽取,"王素格,吴苏红","特征—观点对的抽取是观点挖掘中非常重要的研究课题之一。该文首先利用依存语法对句子进行了依存分析,在此基础上研究了旅游评论文本中特征-观点对的抽取。利用词对间的依存关系,构建了获取含有特征和观点词语的组块规则,并设计了候选特征的识别算法和特征—观点对的抽取算法。该文对山西旅游景点评论语料进行了实验,结果表明,特征—观点对的抽取整体的F1值达到了87.10%,验证了方法的有效性。","特征-观点对,依存关系,组块"
2011-09-21,基于多特征表示的本体概念挂载,"徐立恒,刘 洋,来斯惟,刘 康,田 野,王渝丽,赵 军","该文研究了一种基于多特征表示的本体概念挂载方法。以中国大百科知识体系作为本体体系结构,抽取网络知识库条目作为本体概念,通过分析条目中文本内容、语义标签和半结构化信息获得本体概念间层级关系。该文将中国大百科知识体系扩展为百万级概念的多领域中文本体,为进一步抽取本体概念的属性、概念之间的非层级关系以及支持问答服务等应用建立了良好的基础。实验证明该方法相对于单一特征方法能够提高11.8%的挂载精度。","本体,多特征,概念挂载"
2011-01-25,文本处理中的MapReduce技术,"李 锐,王 斌","用于文本处理的很多数据集已经达到TB、PB甚至更大规模,传统的单机方法难以对这些数据进行有效处理。近年来出现的MapReduce计算框架能够以简洁的形式和分布式的方案来解决大规模数据的并行处理问题,得到了学术界和工业界的广泛认可和使用。目前,MapReduce已经被用于自然语言处理、机器学习及大规模图处理等领域。该文首先对MapReduce做了简单的介绍,并分析了其特点、优势还有不足;然后对MapReduce近年来在文本处理各个方面的应用进行分类总结和整理;最后对MapReduce的系统和性能方面的研究也做了一些介绍与展望。","文本处理,MapReduce,分布式计算,综述,Hadoop"
2011-03-16,微博文本处理研究综述,"张剑峰,夏云庆,姚建民","微博是一个基于关系的信息分享、传播以及获取平台。用户可以通过WEB、WAP以及各种客户端组件,以140字左右的文字更新信息,并实现即时分享。由于微博发展迅猛,微博文本已经形成了大规模积累,针对微博文本的研究已经成为了一个十分重要的课题。该文对微博文本进行了定义,阐述了微博文本研究的重要性,并从微博文本的不同应用领域出发,对微博文本的研究现状进行了综述,介绍了目前已经存在的微博文本数据集和应用系统。","微博文本,语言分析,文本处理"
2011-04-26,基于甲骨文字形动态描述库的甲骨文输入方法,"栗青生,吴琴霞,王 蕾","该文分析了目前常用的甲骨文字在编码和输入方面的问题和不足,给出了一种甲骨文字形动态描述的方法。该方法在现代汉字的编码和书写规范基础上,使用有向笔段和笔元对甲骨文进行描述,用扩展的编码区域和外部描述字形库相结合的方式,解决了甲骨文字特别是异体字和没有识别的甲骨文字的输入和输出问题。","甲骨文字形, 字形描述,输入,编码"
2011-01-11,古汉语双字词自动获取方法的比较与分析,"段 磊,韩 芳,宋继华","词汇的自动获取在自然语言生成、计算词典编纂、句法分析以及语料库语言学等领域均有着重要的研究价值。该文针对古汉语双字词的自动获取问题,以《史记》全文语料为例,分别应用基于频率、互信息、假设检验的统计方法获取古汉语双字词,并结合人工标注结果进行了详细的比较和分析,评价了各方法的优缺点及可靠性,为不同应用背景下的古汉语双字词自动获取提供了相应的解决方案。","中文信息处理,古汉语,史记,双字词,统计模型"
2011-07-22,基于《知网》的中文信息结构消歧研究,"张瑞霞,庄晋林,杨国增","《中文信息结构库》是《知网》的重要组成部分之一,可以作为中文语义分析的规则库,对其进行消歧是实际应用的基础之一。因此,该文首先对中文信息结构进行了形式化描述;接着对其进行优先级划分;然后根据其构成形式提出了四种不同的消歧方法 即词性序列消歧法、图相容匹配消歧法、图相容度计算消歧法、基于实例的语义相似度计算消歧法;最后针对不同优先级的中文信息结构集设计了不同消歧流程。实验结果证明消歧正确率达到了90% 以上。","知网,中文信息结构,消歧,图相容度,语义相似度"
2011-11-23,汉语传统语法及其在中文信息处理中的应用展望,"彭炜明,宋继华,王 宁,康明吉","汉语传统语法首推黎锦熙《新著国语文法》为代表。黎氏语法是以讲句子成分和句子格局为主要特征的语法体系,被称为“句本位”的语法。该文首先简要回顾了汉语语法体系自《马氏文通》以来的变化发展历史,梳理了传统语法与结构语法两大流派的主要思想和理论特色。然后从汉语树库角度剖析了当前中文信息处理领域主流语法体系的优缺点,并将它们与传统语法体系做了深入的比较分析,得出将传统语法应用于中文信息处理的必要性。最后讨论传统语法在中文信息处理领域应用需要面对的几个关键问题。","中文信息处理,传统语法,黎氏语法,句本位,句子格局,句子成分"
2011-10-14,维吾尔语动词体范畴的有限状态自动机的构建,"阿孜古丽·夏力甫,早克热·卡德尔,吐尔根·依布拉音","维吾尔语动词的体范畴是维吾尔语动词语法范畴中极为复杂的范畴,也是维吾尔语信息处理中的难点问题之一,计算机对维吾尔语动词体范畴的处理是在对人称、时、否定等语法范畴处理之后才进行处理。但是难点就是体范畴重叠问题的解决。维吾尔语动词的体范畴词尾按照一定的规则连接在词干,这使得维吾尔语动词体范畴的重叠形式可用有限状态自动机形式化描述。因此它根据重叠规则构造从右向左的非确定自动机,之后把从右向左方向的自动机转换成从左向右的非确定自动机,最后把非确定自动机转换成确定自动机来实现维吾尔语动词体范畴的形式化描述。","维吾尔语,动词,体范畴, 有限状态自动机, 形式化"
2011-09-15,藏语机读音标SAMPA_ST的设计,"于洪志,高 璐,李永宏,郑文思","该文选取具有代表意义的藏语卫藏方言的拉萨话、安多方言的夏河话以及康方言的德格话进行语言调查;整理归纳藏语三大方言音系,包括单辅音、复辅音、单元音、复合元音和辅音韵尾,以及三大方言声调;依照SAMPA的规则建立适合于藏语三大方言的机读音标,并设计了SAMPA_ST的自动标注系统,实现文音转换功能,为语音的韵律特征分析和语音工程的研究提供依据。","SAMPA_ST,国际音标,声母,韵母,声调"
2011-08-08,中文歧义研究25年——以《中文信息学报》论文为例,"张禄彭,易绵竹,周  云","过去的25年间中文信息处理领域的歧义研究取得了长足进步,涌现出大量科研成果。该文试图以中国中文信息学会会刊《中文信息学报》刊载的论文为例,着重从研究对象和研究方法两个方面观察探讨歧义研究的进展、特点和大体趋势。文章分时间段从多个角度对中文歧义研究进行定量统计分析,述评结合,针对歧义研究的现状提出了建议。","歧义,消歧,《中文信息学报》,统计分析,研究对象,研究方法"
2011-09-22,基于随机特征子空间的半监督情感分类方法研究,"苏 艳,居胜峰,王中卿,李寿山,周国栋","情感分类是目前自然语言处理领域的一个热点研究问题。该文关注情感分类中的半监督学习方法(即基于少量标注样本和大量未标注样本进行学习的方式),提出了一种新的基于动态随机特征子空间的半监督学习方法。首先,动态生成多个随机特征子空间;然后,基于协同训练(Co-training)在每个特征子空间中挑选置信度高的未标注样本;最后使用这些挑选出的样本更新训练模型。实验结果表明我们的方法明显优于传统的静态产生方式及其他现有的半监督方法。此外该文还探索了特征子空间的划分数目问题。","情感分类,半监督学习方法,特征子空间"
2011-09-18,评价对象及其倾向性的抽取和判别,"顾正甲,姚天昉","基于主观性文本的意见挖掘技术是一种在多种领域都有广泛应用的语言技术。该文把评价性语素作为研究对象,在哈尔滨工业大学的语言技术平台(LTP)对语料处理结果的基础上,利用SBV极性传递法为核心,引入指代消解、ATT链算法和互信息法对语料中的评价对象进行抽取,并在对极性词进行倾向性判别时,充分考虑了不同类型的句子,以及副词、连词对极性的影响,尤其是对一般副词、贬义副词和副词“太”作了详细地探讨,最后提出了一个综合的解决方案。该方案结构层次清晰,易于理解,并且其算法复杂度较低。但由于利用的是较为浅层的句法分析结果和基于经验的语言模式方法,该文提出的方案对句法分析结果的依赖度较大。","评价对象,倾向性,SBV极性传递法,指代消解"
2011-09-15,基于非完备信息系统的评价对象情感聚类,"王素格,尹学倩,李 茹,张 杰,吕云云","该文利用领域本体对产品评论文本中的评价对象进行抽取和整合,在此基础上,建立产品性能的非完备信息系统,将特征的情感倾向寓于特征的权重计算之中。对非完备信息系统,给出了基于差别矩阵的启发式特征约简方法,通过特征降维处理,达到了减少特征的冗余度和数据稀疏性的目的。对降维后的非完备信息系统采用K-Means聚类算法,实现了评价对象情感聚类。为了验证该文提出方法的有效性,在真实汽车评论文本数据上进行实验, 实验结果表明,在对特征进行一定程度的降维后,仍表现出较好的聚类效果。","非完备信息系统,评价对象,本体,特征降维,聚类"
2011-09-21,基于情绪词的非监督中文情感分类方法研究,"代大明,王中卿,李寿山,李培峰,朱巧明","情感分类任务旨在识别文本所表达的情感色彩信息(例如,褒或者贬,支持或者反对)。该文提出一种基于情绪词的中文情感分类方法,使用大规模未标记数据和少量情绪词实现情感分类。具体来讲,首先使用情绪词从未标注数据中抽取高正确率的自动标注数据作为训练样本,然后采用半监督学习方法训练分类器进行情感分类。实验表明,该文提出的方法在产品评论与酒店评论两个领域的情感分类任务中取得了较好地分类效果。","情感分类,情绪词,非监督学习,协同训练"
2011-09-15,微博客中转发行为的预测研究,"张 旸,路 荣,杨 青","在微博客中,转发对信息的传播有着至关重要的影响,各种各样的信息正是通过转发得以在微博客上广泛且迅速的传播。另外在很多领域中,例如,市场营销、政治选举和热点提取等,也都需要深入探讨转发的各种特性。该文中,我们以Twitter为例,通过预测一条tweet是否会被转发,研究微博客中的转发行为。为解决这个问题,我们使用机器学习中的分类算法,并通过对微博上不同特征的重要性进行分析,提出了基于特征加权的预测模型。实验表明,我们的特征加权模型很好的解决了微博客中的转发预测问题,大约86%的微博能被成功预测。","微博客,转发,特征加权模型"
2011-09-11,采用数据挖掘的自动化推荐技术的研究,"陈庆章,汤仲喆,王 凯,姚 敏,裴玉洁","随着网络的迅速发展,各种数据量变得庞大且分散,利用关键词检索数据的传统方式变得相当费时。为了减少用户在网络上的搜寻时间,提供用户更确切的内容信息,自动化推荐系统(Automatic Recommender System)应运而生。该研究将人工神经网络中的自适应共振理论(Adaptive Resonance Theory,ART)和数据挖掘技术结合起来,建构了一个可自动聚类族群特征且能挖掘出关联规则的自动化在线推荐机制。同时将用于用户聚类的ART算法进行了改进,提出了MART聚类算法,使由推荐系统得出的结果变得更加合理和灵活。","自动化推荐系统,自适应共振理论,数据挖掘,关联规则"
2011-09-13,基于幂律分布的网络用户快速排序算法,"张 玥,张宏莉,张伟哲","随着网络论坛、博客、微博的发展,引出社会网络中的用户排序问题。将在线网络论坛中用户映射为节点,用户评论过程中形成的回复关系映射为有向关联图,其节点度符合幂律分布。且论坛中用户的主题发布行为和回复关系符合Pagerank算法的互增强和随机游走特性,因此选用Pagerank算法排序用户影响力。该文提出的研究问题 如何提高用户排序应用中数据的存储和运行效率。天涯网络论坛中80%以上用户入度为0,据此,根据入度是否为0划分为两个集合,对入度为0集合按出度构造链接表,设计了基于集合划分的高效排序算法SD-Rank。SD-Rank时空复杂性为O(V′),V′为入度非0节点集。对天涯网络论坛真实用户数据的实验结果表明 SD-Rank算法时空复杂性优于Pagerank算法。","幂律,入度,集合划分,快速排序"
2011-09-25,日本地震的微博热点事件分析,"王 昊,杨 亮,林鸿飞","北京时间2011年3月11日日本发生强烈地震,随后在新浪微博上引发了热烈的讨论。该文利用基于情感的HITS算法对日本地震发生后一周内爬取的新浪微博进行事件分析。首先将候选主题词与情感类别构成二部图,再根据HITS算法的得分和候选主题词的频率,计算候选主题词的得分,得到每日的主题词。然后采用互信息的特征选取的方法分析了特定主题词在七天中的变化,以此分析日本地震中的主题变化,同时采用基于规则的情感分类的方法分析人们在特定主题词下表现的情感。该文通过实验证明了基于情感的HITS算法的可行性,同时发现实验语料中网民讨论的话题以两天为单位,以及在微博上对于日本地震,网民并不是表现出高兴或悲哀的情感,而是更倾向于表现出赞扬和贬责这类体现争论的情感。","热点事件发现,倾向性分析,基于情感先验的HITS算法,日本地震"
2011-07-28,搜索引擎日志中“N+V”和“N1+N2+V”型短语自动识别,"赵红改,吕学强,施水才,郑 丽","正确识别搜索引擎日志中的短语,对搜索引擎用短语词典构建和提高搜索引擎性能具有重要的作用。该文提出一种应用条件随机场实现对搜狗日志语料中“N+V”和“N1+N2+V”型短语自动识别的方法。模型的特征集包含词、词性和词语长度。由人工设计候选特征集,从中选择有效的特征构成特征模板,训练生成用于短语自动识别的条件随机场模型。封闭测试和开放测试的实验结果表明,模型能够实现对这两种短语的有效识别。","条件随机场模型,搜索引擎日志,“N+V”型短语,“N1+N2+V”型短语,特征模板"
2011-09-05,基于半监督话题模型的用户查询日志命名实体挖掘,"曹 雷,郭嘉丰,白 露,程学旗","基于用户查询日志的命名实体挖掘,目标是从用户查询日志中挖掘具有指定类别的命名实体。已有研究工作提出一种基于种子实体的挖掘方法,利用实体类别与候选实体之间的模板分布相似性来对候选实体进行排序。然而该挖掘方法忽略了命名实体具有歧义性、查询模板具有多义性和未标注实体信息,因而不能够有效的对候选实体进行排序。该文采用半监督话题模型,利用查询模板之间的关系来学习实体类别的模板分布,进而改善候选实体的排序效果。实验结果表明了该文提出方法的有效性。","用户查询日志,命名实体挖掘,半监督话题模型"
2011-09-21,基于逐点互信息的查询结构分析,"朱亚东,张 成,俞晓明,程学旗","Web搜索引擎中,对用户查询结构的有效分析,能更好地理解用户的查询意图,促进检索效果的提升。该文提出了一种简单高效的基于逐点互信息的查询结构分析方法,该方法包含了基于MapReduce的离线训练算法,以及一种自下向上的在线查询树构建算法。实验显示,该方法具有很高的切分速度,并能取得不错的可比较的切分效果。进一步的,该方法对检索性能的提升,也有明显的促进作用,在MAP,p@5,p@10评价指标上,都取得了不错的性能提升。","查询结构分析,MapReduce,在线查询树"
2011-09-15,融合基本特征和词袋绑定特征的问句特征模型,"杨思春,高 超,秦 锋,戴新宇,陈家骏","针对当前问句分类研究中特征提取的处理开销较大,提出一种融合基本特征和词袋绑定特征的问句特征模型。在分别提取问句中的词袋、词性、词义等基本特征及其对应的词袋绑定特征的基础上,通过将基本特征与词袋绑定特征进行融合,以获取更加高效的问句特征集合。在哈尔滨工业大学中文问句集上的实验结果表明,这种新的问句特征模型不仅具有实现简单、处理开销小的优点,而且有效弥补了单纯基本特征或词袋绑定特征在句法语义表达方面的不足,进一步提高了问句分类的准确率。","问答系统,问句分类,特征模型,词袋绑定"
2011-09-13,排序学习中数据噪音敏感度分析,"牛树梓,程学旗,郭嘉丰","排序学习是当前信息检索领域研究热点之一。为了避免训练集中噪音的影响,当前排序学习算法较多关注鲁棒性。已有的工作发现相同的排序学习方法的性能在不同的数据集上会有截然不同的噪音敏感度。模型改变是导致性能下降的直接原因,而模型又是从训练集学习到的,因此根源在于训练数据的某些特性。该文根据具体排序学习场景分析得出影响噪音敏感度的根本原因在于训练集中文档对分布的结论,并在LETOR3.0上的实验验证了这一结论。","排序学习,数据质量,噪音敏感"
2011-09-20,基于语义和结构的XML文档相似度的计算方法,"宋 玲,吕 强,邓 薇,吕晓琳","个性化信息服务通过了解用户的兴趣爱好,为不同的用户提供不同的信息服务。XML是一种标示语言,是Web文档表示和交换的常用相关标准,因此XML文档之间相似度计算问题对于个性化推荐与信息检索非常重要,为此提出了一个计算XML文档之间的语义和结构相似度的方法XMLSim。首先,基于节点标记对之间的语义相似度和编辑距离计算节点标记对之间的相似度;在分析了路径上节点具有的偏序关系之后,将路径之间相似度问题抽象为最大相似子序列(MSS,Maximal Similar Subsequence)问题,并利用动态规划对MSS问题求解得到路径相似度NpathSim。最后,XML文档之间的相似度XMLSim通过路径集合之间的最大NPathSim的平均值得到。","XML相似度,动态规划,语义和结构"
2011-09-13,基于全局用户意图的评论自动估价方法研究,"陆 军,洪 宇,陆剑江,姚建民,朱巧明","评论是一种反映事物价值的重要主观信息。该文从用户角度出发,提出一种基于全局用户意图的商品评论自动估价方法。该研究首先定义了一种简易的评论价值划分标准(“实用”和“垃圾”评论),借以实现前瞻性的方法尝试。在此基础上,该文采用SVM分类器作为划分评论价值类别(二元分类问题)的基本平台,并基于这一平台重点考察三种影响评论价值的特征 1)属性热度;2)内容可信度;3)用户情感和观点。该文在文本结构特征的基础上,加入上述三类反映用户意图的特征进行评论价值判定,并在大规模商品评论语料集中进行测试。实验表明通过引入用户意图特征,评论自动估价的性能有较大幅度提高。","评论价值,属性抽取,观点挖掘,评论可信度"
2011-09-15,基于网页查询结果的广告查询扩展研究,"刘文飞,林鸿飞","在计算广告学中,为用户查询返回相关的广告一直是研究的热点。然而用户的查询一般比较简短,广告的表示也局限在简短的创意和一些竞价词上,返回符合用户查询意图的广告十分困难。为了解决这个问题,该文提出利用多特征融合的方法进行广告查询扩展,先将查询输入到搜索引擎中,获得Top-k网页查询结果,将它们作为获取扩展词的外部资源,由于采用一般的特征选取方法获取扩展词采用的特征比较单一,缺乏语义信息,容易产生主题漂移现象,该文通过计算扩展词和查询词在网页查询结果中的共现度,并融合传统的TF特征和词性信息,获得与原始查询语义相关的扩展词。在真实的广告语料上的实验结果显示,基于多特征融合的选择广告扩展词的方法能有效地提高返回广告的相关性。","搜索广告,查询扩展,词共现"
2011-09-14,基于遗传规划集成学习的网络作弊检测,"牛小飞,马 军,马少平,张冬梅","网络作弊检测是搜索引擎的重要挑战之一,该文提出基于遗传规划的集成学习方法 (简记为GPENL)来检测网络作弊。该方法首先通过欠抽样技术从原训练集中抽样得到t个不同的训练集;然后使用c个不同的分类算法对t个训练集进行训练得到t*c个基分类器;最后利用遗传规划得到t*c个基分类器的集成方式。新方法不仅将欠抽样技术和集成学习融合起来提高非平衡数据集的分类性能,还能方便地集成不同类型的基分类器。在WEBSPAM-UK2006数据集上所做的实验表明无论是同态集成还是异态集成,GPENL均能提高分类的性能,且异态集成比同态集成更加有效;GPENL比AdaBoost、Bagging、RandomForest、多数投票集成、EDKC算法和基于Prediction Spamicity的方法取得更高的F-度量值。","网络作弊,集成学习,遗传规划,非平衡数据集分类"
2011-09-15,一种抵抗链接作弊的PageRank改进算法,"贺志明,王丽宏,张 刚,程学旗","大量的基于链接的搜索引擎作弊方法对传统PageRank算法造成了巨大的影响,例如,链接农场、交换链接、黄金链、财富链等使得网页的PageRank值失去了公正性和权威性。该文在分析多种作弊方法对传统PageRank算法所造成的不利影响的基础上,提出了一种可以抵抗链接作弊的三阶段PageRank算法－TSPageRank算法,该文对TSPageRank算法的原理进行了详细分析,并通过实验证明TSPageRank算法比传统的PageRank算法在效果上提高了59.4%,能够有效地提升重要网页的PageRank值,并降低作弊网页的PageRank值。","搜索引擎作弊,PageRank算法,链接农场"
2011-05-13,基于广义话题理论的话题句识别,"蒋玉茹,宋 柔","汉语标点句句首话题缺失是机器翻译、信息抽取准确率不高的原因之一。该文从广义话题理论出发,根据汉语话题结构的特点,提出标点句的话题句识别研究方案,包括两个阶段性任务 单个标点句的话题句识别和序列标点句的话题句序列构建。识别出标点句的话题句也就找到了标点句句首缺失的话题。该文解决单个标点句的话题句识别任务,主要采用语义泛化和编辑距离两种手段。实验中开放测试的准确率比基线高出12.51个百分点。该结果说明,运用广义话题理论进行单个标点句的话题句识别可产生明显的效果。","标点句,广义话题,话题结构,话题句,话题句识别"
2011-06-21,基于分治策略的组块分析,"周俏丽,刘 新,郎文静,蔡东风","组块分析的主要任务是语块的识别和划分,它使句法分析的任务在某种程度上得到简化。针对长句子组块分析所遇到的困难,该文提出了一种基于分治策略的组块分析方法。该方法的基本思想是首先对句子进行最长名词短语识别,根据识别的结果,将句子分解为最长名词短语部分和句子框架部分;然后,针对不同的分析单元选用不同的模型加以分析,再将分析结果进行组合,完成整个组块分析过程。该方法将整句分解为更小的组块分析单元,降低了句子的复杂度。通过在宾州中文树库CTB4数据集上的实验结果显示,各种组块识别结果平均F1值结果为91.79%,优于目前其他的组块分析方法。","汉语组块分析,分治策略,句法分析,最长名词短语,条件随机场,支持向量机"
2011-04-19,从计算机辅助翻译到协同翻译,"叶 娜,张桂平,韩亚冬,蔡东风","由于机器翻译系统的译文质量仍难以达到实用化要求,计算机辅助翻译技术逐渐成为研究热点,并且取得了很好的实际效果,大大提高了翻译产业的生产率。随着辅助翻译规模的不断扩大,多名在空间上分散的用户被组织起来共同完成一项翻译任务已成为普遍现象,这种新的翻译模式称为协同翻译。该文对计算机辅助翻译和协同翻译技术进行综述,首先从辅助译文生成、译后编辑和系统反馈学习等方面介绍了计算机辅助翻译技术的常用方法和研究进展,随后讨论了计算机辅助翻译与协同翻译之间的联系和区别,分析了协同翻译技术所面临的主要问题,并介绍了现有研究的解决方法。最后对协同翻译的未来发展方向进行了展望。","计算机辅助翻译, 协同翻译, 用户, 辅助译文, 译后编辑"
2011-05-26,基于语言学知识的查询个性化潜力预测,"陈 晨,赵铁军,李 生,杨沐昀,齐浩亮 ","大多数关于个性化信息检索的研究都是针对所有查询的,很少有研究试图回答哪些查询将受益于个性化信息检索。从大规模知识库中挖掘大量的语言学知识,用于预测查询的个性化潜力,这些知识包括概念词、歧义词、同义词等。使用语言学知识作为特征,预测查询的个性化潜力,可以减少查询日志的数据稀疏问题的影响。实验结果表明该方法的有效性和可行性。","查询个性化潜力,语言学知识,查询日志"
2011-03-25,基于用户特性的搜索引擎查询结果缓存与预取,"马宏远,王 斌","针对搜索引擎查询结果缓存与预取问题,与传统的基于查询特性相关的方法不同,提出了一种基于用户特性的缓存与预取方法,用于提高搜索引擎系统性能,尤其针对部分用户效果更显著。通过对国内某著名商业搜索引擎用户的查询贡献分析得出,用户对搜索引擎的贡献具有长尾分布特性,结合该特性设计查询结果预测模型来进行预取和分区缓存。在该搜索引擎两个月的大规模真实用户查询日志上的实验结果表明,与传统的基于查询特性的典型方法相比,该方法可以获得3.03%~4.17%的命中率提升,对于查询贡献最大的0.25%的用户群体,可以获得20.52%~28.2%的命中率提升。","查询结果缓存,用户特性,性能优化"
2011-10-27,微博及中文微博信息处理研究综述,"文坤梅,徐 帅,李瑞轩,辜希武,李玉华","微博即微博客,是Web2.0时代下衍生出的一种新型社会网络,其简单快捷的操作方式和随时随地发布信息的互动形式成为互联网的一大亮点。自2006年美国Obvious公司推出全球首个微博服务Twitter后,微博以惊人的发展速度受到国内外研究人员的广泛关注。该文首先对以Twitter为代表的微博其研究现状进行综述,主要包括(1)微博社会网络的特性分析,如微博用户网络的结构特征、微博用户的影响力分析及消息网络的信息传播机制等;(2)微博内容的语义分析,对微博中的情感语义分析进行了重点阐述;(3)微博的相关应用,包括微博在事件监测与预警、安全隐私及实时检索中的应用。然后概述了中文微博的研究现状,包括中文微博的特性及知识发现,分析了中文微博与英文微博的主要区别。最后讨论目前微博研究中存在的问题及未来中文微博的研究方向。","Twitter,中文微博,信息处理"
2011-04-25,基于SimRank的跨领域情感倾向性分析算法研究,"吕韶华,杨 亮,林鸿飞","情感倾向性判断是指根据文本表述分析文本的倾向性,即发表文本的作者所持有的支持或反对的态度,对于特定领域的情感倾向性研究尤以运用监督分类方法所得出的实验结果较为理想。但若将此类方法直接运用于不同领域的文本,其效果却难以尽如人意。在这种情况下,如何利用已标注情感倾向性的源领域文本去判断未知情感倾向性的目标领域文本的倾向性,即跨领域的情感倾向性分析问题——成为当前研究的热点。为此,该文提出一种基于SimRank的跨领域情感倾向性分析算法,把在源领域和目标领域中共现的词汇作为连接两个领域的桥梁,利用情感词典和SimRank算法找出潜在情感空间,然后使用SVM对已标注的源领域进行训练进而得到训练模型,以便利用此模型预测目标领域的情感倾向性。该文亦通过相关实验所得到的实验结果表明了此方法的有效性。","跨领域,倾向性判断,SimRank,支撑向量机"
2011-04-25,基于情感向量空间模型的歌曲情感标签预测模型,"李 静,林鸿飞,李瑞敏","音乐的情感标签预测对音乐的情感分析有着重要的意义。该文提出了一种基于情感向量空间模型的歌曲情感标签预测算法,首先,提取歌词中的情感特征词构建情感空间向量模型,然后利用SVM分类器对已知情感标签的音乐进行训练,通过分类技术找到与待预测歌曲情感主类一致的歌曲集合,最后,通过歌词的情感相似度计算找到最邻近的k首歌曲,将其标签推荐给待预测歌曲。实验发现本文提出的情感向量空间模型和“情感词—情感标签”共现的特征降维方法比传统的文本特征向量模型能够更好地提高歌曲情感分类准确率。同时,在分类基础上进行的情感标签预测方法可以有效地防止音乐“主类情感漂移”,比最近邻居方法达到更好的标签预测准确率。","标签预测,特征降维,情感分类,情感向量空间模型"
2011-04-26,基于机器学习方法的英文事件代词消解研究,"张 宁,孔 芳,李培峰,周国栋,朱巧明","与实体指代不同,事件指代因为其先行词候选是一个事件,与名词性的指代词具有完全不同的语义分类体系,因此适用于实体指代消歧的大多数特征都不能用于事件指代消歧。该文给出了一个基于机器学习方法的事件代词指代消歧平台,详细介绍了平台的实例生成和特征选择过程,并给出了平台在OntoNotes3.0语料上的事件代词指代消歧的结果,对结果进行了分析。从实验结果可以看到,给出的平台获得了较好的系统性能。","事件代词指代消歧,机器学习方法,实例生成,特征选择"
2011-12-23,基于区分加权干扰属性投影的语种识别方法,"刘伟伟,吉立新,李邵梅,何赞园","传统NAP,投影矩阵的训练需要繁杂的参数调整和大量的标注语料,投影后不能彻底去除干扰信息且会造成一定的信息损失。为此,该文提出一种DWNAP算法,首先通过统计各语种训练语音协方差矩阵的特征值离散度,对干扰源进行量化估计,利用规整的估计值作为各语种的区分性权重参与投影矩阵的训练。汉日英三种语言的测试结果表明,相对于传统NAP提出的DWNAP有效地提高了系统识别性能,EER相对降低了7.51%。","语种识别,失配补偿,区分加权干扰属性投影,干扰属性投影"
2011-06-21,常用现代汉语副词用法自动识别研究,"张坤丽,赵 丹,昝红英,柴玉梅","副词以其功能和用法的繁杂多样,引起了众多学者的研究。该文以构建三位一体的副词用法词典、副词用法规则库和副词用法语料库为基础,首先基于规则的方法对副词用法自动识别进行研究,对《人民日报》语料中的副词识别准确率达到了84.86%;然后,基于统计的方法,用不同特征模板、不同上下文窗口以及不同模型等对语料中常用副词进行识别。实验结果表明,基于统计的方法对副词用法自动识别研究有较好的效果。","副词用法自动识别,副词用法规则,条件随机场,最大熵,支持向量机"
2011-08-15,基于用法的现代汉语连词结构短语识别研究,"昝红英,周丽娟,张坤丽","连词能够连接词语、短语、小句、句子乃至句群,连词结构短语是连词所连接对象的一种,不同的连词形成不同长度、不同关系的连词结构短语。该文根据虚词用法知识库中的连词用法,构建了连词结构短语识别规则,实现了基于规则的连词结构短语识别,并将连词用法作为特征采用条件随机场模型实现了基于统计的连词结构短语识别。实验结果表明,统计的识别效果高于规则的识别效果,连词用法能够较好地用于连词结构短语的识别中。","连词结构短语,连词用法,条件随机场"
2011-09-21,一种针对新闻话题的多文档文摘技术,"岳大鹏,饶 岚,王 挺","多文档文摘技术能帮助用户减少不必要的阅读时间,有广阔的应用前景。该文以新闻报道为处理对象,以MMR(Maximal Marginal Relevance)文摘提取算法为基础,针对目前新闻报道往往以专题形式组织展现的特点,提出了一种基于话题的多文档文摘方法。这种方法以话题关键字为打分依据,同时考虑句子位置特征等信息对句子的重要性进行评分。 该文利用TDT4的新闻报道语料对上述文摘方法进行了试验评价,将基于话题的文摘系统和两个Baseline文摘系统进行比较,取得了较好的实验结果,尤其在5%的压缩比例下有明显优势。","自动文摘,话题,自然语言处理,新闻"
2011-09-15,社交网络中新闻趋势的预测分析,"路 荣,张 旸,杨 青","新闻在社交网络平台中的发展趋势,可以用其在该网络中出现的频率的移动平均值来跟踪。该文利用两条不同时间周期的移动平均值来分别跟踪新闻的相对短期趋势和相对长期趋势,并定义这两种趋势的差值为该新闻趋势发展的趋势动量。当趋势动量值为正,该新闻将有更加热门的可能,反之,则表明该新闻的关注度正在降低。而该趋势动量值本身的大小变化,也同样能为新闻趋势的变化提供预测。实验证明,该文的方法简单有效,能很好地对社交网络中新闻未来可能发展趋势做出即时、准确的预测。","社交网络,新闻趋势预测,移动平均"
2011-09-14,网民重要度建模方法研究,"袁继鹏,张 瑾,郭 岩,戴 媛,李 静","网民重要度分析可以有效区别出不同网民之间的重要性,从中进行重要网民的提取识别,为重点人物社区分析提供判别依据。在网民重要性影响因素的分析基础上,该文提出了一种基于指标体系的网民重要度模型NI模型(Netizen Importance)。该模型综合考虑了网民在信息发布和关联关系等多方面的特征,采用层次分析法确定指标权重,在此基础上确定网民重要度的具体计算方法。在大规模数据集Twitter上的对比实验表明,通过NI模型得出的重要网民在实际应用中更有价值,更能全面反映网民的重要性。","网民重要度,网民建模,网民评价指标"
2011-09-15,基于衰退理论的Flickr热点事件检测方法,"薛  冉,马  军,韩晓晖,陈竹敏","该文提出了一种基于衰退理论对Flickr数据进行热点事件检测的方法。该方法首先将从Flickr图像中提取的视觉词汇(Visual Words)与图像的文本信息加权合并成文档。然后训练LDA模型获得文档的主题分布作为其最终向量表示。在此基础上提出了一种改进的Single-Pass算法进行事件检测,该算法不仅考虑了图片的地理位置信息,而且基于衰退理论(Aging Theory)对检测到的事件进行生命周期建模,以便计算事件在每个时间段的能量值。最后,根据能量值进行事件排序,获得给定时间段内的热点事件。在真实Flickr数据集上的实验结果表明所提出的方法在精确率、召回率和F1测度上优于传统事件检测方法。","事件检测,视觉词汇,地理信息,LDA,衰退理论"
2011-09-15,网络维吾尔文判别及其文本长度下界的探讨,"倪耀群,曹 鹏,许洪波,唐慧丰,程学旗","将维吾尔文从阿拉伯文、哈萨克文、柯尔克孜文等以阿拉伯字母为基础书写的类似文字中识别出来,是维文信息处理的基础。作者对维吾尔字符的编码优化后使用N元语法模型实现了维吾尔文的快速语种判别,准确率超过98%。经过错误分析,发现错误判别的文本主要集中在论坛和微博客中,这些文本有效字符数太少,语言特征不充分。最后作者计算了四种语言真实网络文本中的所有公共子串,并对文种判别所需要的最短字符串长度进行了分析。","老维文,语种识别,最大公共子串"
2011-09-19,构建查询需求形式分类体系,"王 超,朱 彤,刘奕群,马少平","查询歧义作为查询分类的子问题在信息检索领域已经得到了很多的关注,现有的研究主要是对查询内容上的歧义进行分类,而忽略了用户查询需求形式上的歧义。该文针对查询需求歧义问题进行了研究,提出了相应的查询需求分类模型。该文利用网页目录构建用户需求形式分类体系及站点列表,在大规模商业搜索引擎日志上进行用户点击覆盖检测,从而得到对查询需求形式的描述。该文的贡献在于提供了一种实际可行的查询需求分类方法,搜索引擎可以根据用户需求的区别调整排序方式,从而改善搜索性能。","查询歧义,查询分类,需求形式"
2011-09-20,基于线索树双层聚类的微博话题检测,"马 彬,洪 宇,陆剑江,姚建民,朱巧明","微博作为一种全新的信息发布模式,在极大程度上增强了网络信息的开放性和互动性,但同时也造成微博空间内信息量的裂变式增长。利用话题检测技术将微博文本信息按照话题进行归类和组织,可以帮助用户在动态变化的信息环境下高效获取个性信息或热点话题。该文针对微博文本短、半结构、上下文信息丰富等特点,提出了基于线索树的双层聚类的话题检测方法,通过利用融合了时序特征和作者信息的话题模型(Temporal-Author-Topic, TAT)进行线索树内的局部聚类,借以实现垃圾微博的过滤,最后利用整合后的线索树进行全局话题检测。实验结果显示该方法在解决数据稀疏方面取得了较好的效果,话题检测的F值达到31.2%。","微博文本,话题检测,TAT模型,线索树,LDA特征选择"
2011-05-16,基于大规模语料库的汉语词义相似度计算方法,"石 静,吴云芳,邱立坤,吕学强","词义相似度的计算是自然语言处理领域的关键问题之一,它在信息检索中的查询扩展、机器翻译中的模块识别,以及句法分析、词义消歧等任务中都发挥着重要的作用。该文研究了基于大规模语料库的汉语词义相似度计算方法,系统地比较分析了上下文特征权值的选择、向量相似度计算方法、基于窗口和基于依存关系的表征形式、新闻语体和网络语体的差异。实验结果表明,在网络语言语料上,基于窗口选取上下文特征,用互信息PMI来计算权值,采用cosine来计算相似度,取得了最好的词义相似度结果。","词义相似度,上下文特征,权值选择,依存关系"
2011-05-03,一种基于搭配的中文词汇语义相似度计算方法,"王 石,曹存根,裴亚军,夏 飞","词汇间的语义相似度计算在自然语言处理相关的许多应用中有基础作用。该文提出了一种新的计算方法,具有高效实用、准确率较高的特点。该方法从传统的分布相似度假设“相似的词汇出现在相似的上下文中”出发,提出不再采用词汇在句子中的邻接词,而是采用词汇在二词名词短语中的搭配词作为其上下文,将更能体现词汇的语义特征,可取得更好的计算结果。在自动构建大规模二词名词短语的基础上,首先基于tf-idf构造直接和间接搭配词向量,然后通过计算搭配词向量间的余弦距离得到词汇间的语义相似度。为了便于与相关方法比较,构建了基于人工评分的中文词汇语义相似度基准测试集,在该测试集中的名、动、形容词中,方法分别得到了0.703、0.509、0.700的相关系数,及100%的覆盖率。","语义相似度,词汇搭配,相似度基准测试集"
2011-06-21,基于双语依存关系映射的中英文词表构建研究,"徐 华,刘丹丹,钱龙华,周国栋","基于上下文的双语词表构建方法是比较流行的基于可比较双语语料库的双语词表构建方法。特别地,依存上下文模型从句子的依存树上抽取词语的上下文特征,由于依存关系更能体现词语之间的共现关系,因而这种方法提高了构建双语词表的性能。该文在此基础上,进一步提出了依存关系映射模型, 即通过同时匹配依存树中的上下文词语、依存关系类型和方向来实现双语词表的构建。在FBIS语料库上的实验表明,该方法在中文—英文和英文—中文两个方向上的双语词表构建上均取得了较好的性能,这说明了依存关系映射模型在双语词表构建中的有效性。","双语词表构建,依存上下文模型,依存关系映射"
2011-09-30,网页中商品“属性—值”关系的自动抽取方法研究,"唐 伟,洪 宇,冯艳卉,姚建民,朱巧明","商品属性及其对应值的自动挖掘,对于基于Web的商品市场需求分析、商品推荐、售后服务等诸多领域有重要的应用价值。该文提出一种基于网页标题的模板构建方法,从结构化网页中抽取完整的商品“属性—值”关系。该方法包含四个关键技术 1)利用商品网页标题构建领域相关的属性词包;2)基于预设分隔符细化文本节点;3)结合领域商品属性词包获取种子“属性—值”关系;4)结合网页布局信息和字符信息来筛选与构建模板。该文的实验基于相机和手机两个领域展开,获得94.68%的准确率和90.57%的召回率。","商品“属性—值”关系抽取,Web数据挖掘,模板构建"
2011-11-06,事件超图模型及类型识别,"肖 升,何炎祥","为避免向量空间模型的独立性假设影响事件类型识别,该文提出了一种基于超图的事件类型识别方法。该方法首先用事件超图描写事件元素间的多元有序关系;然后用事件超图模型(由事件超图添加类型组件和层面组件后构成)描述某个(某类)事件在不同观测层面的属性及其结构;最后根据事件的属性及其结构计算其相似度,并借此完成事件类型识别。实验结果显示,此方法识别效率的平均F值达到83.0%,与基于向量空间模型的支持向量机方法和最大熵方法相比,此方法也具有一定优势。","事件抽取,事件类型识别,超图,有向超图,事件超图模型,事件相似度"
2011-09-06,一种基于社会化标签的信息检索方法,"李 鹏,王 斌,晋 薇","社会化标签提供了网页信息的额外描述,直观上对搜索具有重要价值。该文提出一种新颖的利用社会化标签的分类属性进行检索的方法。该方法通过将群体的标注信息建模为高层类别来估计话题模型,然后基于该话题模型来对语言模型进行平滑。建模方法可以降低标注稀疏性的影响,有效地表达标签含义,从而提升检索效果。基于TREC评测构建的数据集上的实验结果表明,该方法优于基于LDA的检索方法以及现有其他基于标签数据的检索方法。","社会化标注,标签,语言模型,话题模型"
2011-08-30,中文博客多方面话题情感分析研究,"傅向华,刘  国,郭岩岩,郭武彪","博客是Web环境中个人表达观点和情感的一种重要载体,一般涉及较宽泛的话题,蕴含丰富的舆情信息。现有针对有关社会事件的用户产生内容进行情感分析的研究多数以篇章级为处理粒度,尚不能满足博客文本深度情感分析的需求。该文提出一种基于LDA话题模型与Hownet词典的中文博客多方面话题情感分析方法。该方法首先利用数据语料训练LDA话题模型,然后以滑动窗口为基本处理单位,利用训练好的LDA模型对博客文本进行话题识别与划分;在此基础上,基于Hownet词典对划分后的话题段落进行情感倾向计算。该方法有助于同时识别博客文本所涉及的多方面子话题及每个子话题上的情感倾向。实验结果表明,该方法不仅能获得较好的话题划分结果,也有助于改善情感分析的准确率。","多方面情感分析,博客情感分析,LDA模型,HowNet词典"
2012-02-29,第三届中文倾向性分析评测(COAE2011)语料的构建与分析,"廖祥文,许洪波,孙 乐,姚天昉","文本倾向性分析已成为自然语言处理领域研究的热点问题之一。为进一步推动中文倾向性分析的研究,中国中文信息学会信息检索专业委员会举办了第三届中文倾向性分析评测(COAE2011)。该次评测主要关注领域和上下文语境(Context)对中文倾向性分析的影响。该文主要介绍COAE2011评测语料的构建及其对评测的支撑 首先介绍了COAE2011语料的领域选取、媒介分布等获取过程,然后详细阐述语料的标注原则与方法,最后依据评测结果分析领域和上下文语境因素对倾向性的影响。COAE2011语料的建立将为中文倾向性分析提供强大的资源支持。","中文信息处理, 倾向性分析, 倾向性语料库, 文本编码规范"
2011-07-05,统计机器翻译中一致性解码方法比较分析,"段 楠,李 沐,周 明","该文对近年来统计机器翻译研究中出现的多种一致性解码方法进行比较与分析。根据现有一致性解码方法对(单个或多个)统计机器翻译系统输出结果使用方式的不同,首先将其归纳为两大类 基于翻译假设重排序的一致性解码方法和基于翻译假设重组合的一致性解码方法;然后,针对每类方法,分别回顾其最具代表性的研究工作;最后,通过在大规模中—英机器翻译评测数据上的对比实验,对该文中介绍的多种方法进行比较,并对该课题未来研究方向进行展望。","自然语言处理,统计机器翻译,一致性解码,最小贝叶斯风险解码,系统融合"
2011-03-29,BFS-CTC汉语句义结构标注语料库,"刘盈盈,罗森林,冯 扬,韩 磊,陈 功,王 倩","句义结构分析是汉语语义分析中不可逾越的重要环节,为了满足汉语句义结构分析的需要,基于现代汉语语义学理论构建了一种层次化的汉语句义结构模型,定义了标注规范和标记形式,建设了一个汉语句义结构标注语料库BFS-CTC(Beijing Forest Studio-Chinese Tagged Corpus)。标注内容方面,基于句义结构模型的定义标注了句义结构句型层、描述层、对象层和细节层中所包含的各个要素及其组合关系,包括句义类型、谓词及其时态、语义格类型等信息,并且提供了词法和短语结构句法信息,便于词法、句法、句义的对照分析研究;语料库组织结构方面,该语料库包括四个部分,即原始句子库、词法标注库、句法标注库和句义结构标注库,可根据研究的需要,在词法、句法、句义结构标注的基础上进行深加工,在核心标注库的基础上添加更多具有针对性的扩展标注库,利用句子的唯一ID号进行识别和使用;语料来源和规模方面,语料全部来自新闻语料,经过人工收集、整理,合理覆盖了主谓句、非主谓句、把字句等六种主要句式类型,规模已达到10 000句。同其他语义标注库相比,BFS-CTC基于现代汉语语义学,提供了多层次的句义结构标注信息,兼容进行了词法和语法标注,各类标注既可以单独使用也可综合使用进行横向分析,可用于自然语言处理多方面的研究,进一步推动汉语语义分析的研究和发展。","自然语言处理,语义标注,句义结构,语料库"
2011-07-11,基于统计的记叙文语句焦点的分布特点研究,"赵建军,杨玉芳,吕士楠","该文通过20人对30篇汉语记叙文中语句焦点的标定结果,结合文本标注和统计分析,对焦点在词类和语义角色中的分布规律进行了探讨。结果主要发现,记叙文语篇中焦点词大约占实词总数的五分之一。形容词成为焦点的概率远高于其他词类。焦点在语义角色中分布的总体趋势是 客体论元的焦点化倾向最高,其次是外围论元,最低的是主体论元和谓词部分。","记叙文,焦点分布,语义角色"
2011-07-19,基于组合核的蛋白质交互关系抽取,"李丽双,刘 洋,黄德根","蛋白质交互关系(PPI)抽取是生物医学信息抽取领域的一个重要部分,具有很高的应用价值和实际意义。该文使用一种基于SVM的组合核方法进行蛋白质关系抽取,将基于特征的平面核和基于结构的卷积树核组合。一棵完整的句法解析树中包含了较多噪声,需对其修剪以提高PPI抽取效果。首先讨论不同的树的剪裁策略对实验结果的影响,分别使用完全树、最小完全树、最小树和最短路径闭包树进行实验,最短路径闭包树效果最好;然后在最短路径闭包树的基础上提出一种动态拓展树,该树取得了明显优于其他解析树的效果。最后基于组合核在AIMED上进行10倍交叉实验,精确率、召回率和F值分别达到了82.40%、51.30%和63.23%。","蛋白质交互关系抽取,SVM,树核,组合核,修剪策略"
2011-07-24,“方言同音字汇”自动生成软件①的设计及实现,"程南昌,侯 敏","“方言同音字汇”整理是方言调查的基础性工作,靠手工制作十分繁难。该文论述了“方言同音字汇”自动生成软件的设计原理及实现过程。软件的主要功能是,根据用户事先给定的韵、声、调排序依据和排序顺序,对已经录入的方言字表进行排序,排序技术采用对应韵、声、调与字表所有字目的一个四重循环,最终生成“同音字汇竖排表”。此外,该文对软件的实用性能进行了分析,并对软件的应用进行了一定的说明。实践证明,该软件完全能够满足方言调查实用化的需求。","方言同音字汇,竖排表, 自动生成, 设计原理"
2011-07-21,针对发音质量评测的声学模型优化算法,"严 可,魏 思,戴礼荣","在发音质量评测研究中,传统仅用发音标准的数据进行声学建模,难以描述实际测试面临的非标准发音,使得训练与测试的失配在所难免。针对上述问题,该文提出一种利用覆盖各种发音的数据,根据最小化机器分与人工分均方误差准则进行声学模型优化的算法。实验在普通话水平考试现场3 685份数据(其中498份测试,3 187份训练)上进行。实验表明采用优化算法得到的针对发音质量的评测声学模型相比传统建模方式得到的声学模型有显著的优势。","计算机辅助学习,区分性训练,普通话水平测试,发音质量评测"
2011-05-12,新标准体系下蒙古文变形显现模型的设计与实现,"王 震,刘汇丹,吴 健","国家标准GB 25914-2010的提出,为蒙古文变形规则提供了统一的可实施的标准。目前还缺乏完全符合该标准的蒙古文变形引擎和OpenType蒙古文字库。针对这一问题,该文提出了一种符合新标准的蒙古文变形模型,该模型具有高效率和通用性。我们利用蒙古文变形模型分别在KDE平台下的复杂文本布局引擎Qt4和GNOME平台下的Pango中实现了对蒙古文的变形支持。实验结果证明了该模型的有效性。其中,通过对Pango增加蒙古文变形支持,GNOME平台下的Firefox等应用程序也能正确显示蒙古文。该模型的实现,为研制符合新标准的以GNOME或者KDE为桌面环境的蒙古文操作系统奠定了基础。","国家标准,蒙古文变形模型,OpenType字库,Qt4,Pango"
2011-06-10,现代藏语助动词结尾句子边界识别方法,"赵维纳,于 新,刘汇丹,李 琳,王 磊5,吴 健","藏语句子边界的正确识别是藏文文本处理首先要解决的问题。而藏语书面语中标点符号的特殊性是造成藏语句子边界识别困难的主要原因。该文主要对现代书面藏语中常见的以藏语助动词结尾的藏语句子边界识别进行研究,结合藏文标点符号的特点提出藏语助动词结尾句子边界识别方法。","藏语分句,藏语句子边界识别,藏语信息处理,中文信息处理"
2011-04-29,水书键盘输入系统研究与实现,"陈笑蓉,杨撼岳,郑高山,黄 千","水族文字被称为水书。为了满足水书研究者和出版业界的需要,设计了水字字符集的Unicode编码,利用字体制作软件建立了水字TrueType字库。该文提出了一种基于笔形特征的编码方法,依据编码规则取水字3个角的笔形组成有序序列,为水字编码。利用Windows系统的IMM-IME机制,实现了水字笔形输入法。","水书,Unicode,字库,输入法"
2011-11-22,一种融合实体语义知识的实体集合扩展方法,"齐振宇,刘 康,赵 军","实体集合扩展是开放式信息抽取的一个重要问题,该问题研究如何从一个语义类的若干实体(称为种子)出发,得到该类别的更多实体。现有实体集合扩展方法主要使用上下文模板或种子在语料中的分布信息进行抽取,其缺点是无法解决种子的歧义问题,而该问题会影响方法的有效性。在该文中,作者提出了一种融合实体语义知识的实体集合扩展方法,通过引入语义知识来解决种子歧义性问题。新方法通过使用Wikipedia实现了语义知识的引入,并把基于语义知识的扩展方法和基于模板的扩展方法相融合。实验表明,与单纯基于上下文方法相比,该文方法在准确率上提升了18.5%,召回率上提升了6.8%,MAP值上提升了22.8%。","实体集合扩展,知识库,语义知识"
2011-12-08,基于结构化学习的语句压缩研究,"张永磊,王红玲,周国栋","近年来随着各类信息的日益增多,语句压缩作为自动摘要的重要部分也越来越引起研究者的关注。然而当前针对语句压缩的研究才刚刚展开,存在压缩效果不佳、没有统一的自动评测指标等问题。该文在简单的删除单词的方法框架下,采用基于特征权重的最大边缘训练的结构化学习方法实现语句压缩。同时该文还提出了两种新的自动评价指标(N-Gram和BLEU)来评价语句压缩的性能。实验结果表明,采用结构化学习方法能够在保持较好压缩率的情况下保留源语句的主要信息,并且新提出的两个评价指标能够有效反映语句压缩性能。","语句压缩,结构化学习,自动评测"
2011-11-28,一种无指导的隐式篇章关系推理方法研究,"周小佩,洪 宇,车婷婷,姚建民,朱巧明","该文提出一种基于信息检索的无指导方法,用于推理隐式篇章片段之间的语义连接关系,如因果关系、转折关系等。该文基于Google搜索引擎,抽取在句子结构以及语义层面上均与原隐式片段相似的显式片段,通过分析和识别相关显式关系来间接推理隐式关系。主要包括以下三个模块 构建高质量查询关键词并抽取候选显式关系;结合三种隐式关系推理模型(相似度、置信度、关联度),综合考察查询关键词以及候选关系的质量;基于排序学习的方法,统计高质量候选关系中的类别分布以实现最终隐式关系的推理。该文采用Penn Discourse TreeBank 2.0篇章语料库,最终方法精确率达到54.3%,与有指导的方法相比,提高了约14.3%。","隐式篇章关系,无指导,信息检索,PDTB 2.0"
2011-09-06,从Web中获取部分整体关系,"曹馨宇,曹存根 ,吴昱明","随着互联网的迅速发展,Web逐步成为知识获取的重要资源。部分整体关系获取是知识获取中的重要组成部分。该文提出了一种利用搜索引擎从Web中获取部分整体关系的方法。首先构造一种基于部分整体关系分类的意图查询,利用意图查询可以有针对性地从Web中获取尽可能多的包含部分整体关系语料。然后根据网页中的HTML标记和意图查询的格式过滤语料,并从中抽取候选部分整体关系,最后基于部分整体关系在自然语言表述中的特点和汉语的构词规律,提出用于验证候选部分整体关系的度量标准。实验结果表明,该方法取得了较高的准确率和F值。在前20个结果中准确率为86%,最优F值为64%。","部分整体关系,知识获取,关系获取"
2011-10-15,基于依存特征的汉语框架语义角色自动标注,"王智强,李 茹,阴志洲,刘海静  ,李双红","语义角色标注是浅层语义分析的一种实现方式。目前汉语框架语义角色自动标注一般被看作以词为基本标注单元的序列标注问题,而已有研究中仅在词、词性层面来选取特征,标注结果并不理想。该文利用树条件随机场模型,通过在词、词性层面特征的基础上依次加入不同类型的依存特征,研究依存特征对汉语框架语义角色标注的影响。实验设置了8类,共24种特征模板,结果显示,加入依序特征的最优模版使标注结果的F值提高近3%,特别是对较长框架语义角色的标注结果有较好的改善。","框架语义角色,依存特征,T-CRF模型"
2011-12-16,维吾尔语框架语义角色标注标记集研究,"阿里甫·库尔班,吾买尔江·库尔班,房鼎益","该文阐述了对词一级的维吾尔语框架语义网络构建过程中,制订和规范化维吾尔语框架语义角色的语义类型和标注标记符集、短语类型和标注符号集、句法功能的标注符号集的研究。研究内容对基于阿拉伯字符的维吾尔语框架语义成分的依存关系、语义角色分解与自动识别技术,语义角色知识库的构建和自动标注等相关技术提供基础研究服务。","维吾尔语,框架语义,语义角色, 短语类型,句法功能, 标记集"
2012-01-01,搜索引擎日志短语标注规范,"舒 燕,吕学强","语料标注是语料库构建的一项重要的基础性工作。基于搜狗日志,该文借助XML文档的结构化特点,将语料标注转换成节点属性的改写,根据语料的特点,制定了一套服务于搜索引擎用短语词典构建的短语语料标注加工规范及执行原则,并对标注集及加工规范进行了详细描述。利用此规范,已完成145 645条查询词串的标注,而且标注质量很高。","语料标注,搜狗日志,短语词典,加工规范"
2011-07-26,基于分段的藏字校对算法研究,安见才让,"该文提出了一种规则和藏字语法分析相结合的藏字自动校对算法, 不使用藏字字典和大规模语料库。通过研究藏字构字语法,得到藏字的结构特征,进而对藏字的字母组合进行分段处理,简化藏字构字复杂度,研究出各分段部分的构字规则,然后按照规则进行字的校对。实验表明,系统对现代藏文字的查错率达100%。","藏字,分段,规则,自动校对"
2011-11-11,词性对中英文文本聚类的影响研究,"韩 普,王东波,刘艳云,苏新宁","不同词性特征在文本聚类中有不同的贡献度。该文对四组有代表性的中英文数据集,利用三种聚类算法验证了四种主要词性及其组合对中英文文本聚类的影响。实验结果表明,在中文和英文两种语言中,名词均是表征文本内容的最重要词性,动词、形容词和副词均对文本聚类结果有帮助,仅选择名词作为特征聚类的结果与保留所有词性聚类的结果相近,但可大大降低文本的维度;选用名词为文本特征不能实现最好的聚类效果;相对其他词性组合和单一词性,采用名词、动词、形容词和副词的组合特征往往可以实现更好的聚类效果。在词性所占的比例以及单一词性聚类的结果上,同一词性在中英文文本聚类中呈现出较大差异。相对于英文,不同词性特征及其组合在中文文本聚类中呈现的差异更为稳定。","词性标注,文本聚类,文本特征"
2011-11-08,关于两个汉字部件规范的一点思考,"王道平,黄文丽","《信息处理用GB 13000.1字符集汉字部件规范》和《现代常用字部件及部件名称规范》这两个文件对规范汉字部件具有重要意义。但在实践中,它们存在着部件过多以及没有可行的拆分规则这两个最突出的问题,其原因是规范的制定缺乏系统和深层次的思考。要从根本上解决这些问题,就必须重新审视其中的拆分规则,将拆分规则和部件制定融为一体,并立足于有利于汉字输入、汉字教学和汉字检索这三者统一的层面来完善汉字部件规范。实验证明,这样做行之有效。","汉字部件,拆分规则,汉字输入,汉字教学,汉字检索,规范"
2011-11-06,基于数学形态学的甲骨拓片字形特征提取方法,"酆格斐,顾绍通,杨亦鸣","甲骨拓片字形特征提取是对计算机辅助进行甲骨拓片复原、识别和断代等工作都非常关键的第一步。为了尽可能准确地把甲骨拓片从背景噪声中分离出来,首先对原始甲骨拓片图形进行预处理,然后再应用数学形态学方法对甲骨拓片进行图像处理和分析,提取出12项指标用于表现甲骨拓片字形特征,并构造了一个基于数学形态学方法的甲骨拓片字形特征提取系统。通过对《甲骨文合集》实验数据进行基于字形特征的甲骨拓片图像匹配验证,实验结果表明 数学形态学处理方法能有效地提取出较好地反映甲骨文字的笔画形态和结构的字形特征。","数学形态学,甲骨拓片,字形特征,特征提取"
2011-09-13,基于篇章上下文的统计机器翻译方法,"于 惠,谢 军,熊 皓,吕雅娟,刘 群,林守勋","上下文信息对于统计机器翻译(Statistical Machine Translation,SMT)中的规则选择是很重要的,但是之前的SMT模型只利用了句子内部的上下文信息,没有利用到整个篇章的上下文信息。该文提出了一种利用篇章上下文信息的方法来提高规则选择的准确性,从而提高翻译的质量。首先利用向量空间模型获得训练语料的文档和测试集中文档的相似度,然后把相似度作为一个新的特征加入到短语模型中。实验结果表明,在英语到汉语的翻译工作中,该方法可以显著提高翻译质量。在NIST-08和CWMT-08两个测试集上BLEU值都有显著的提高。","统计机器翻译,上下文信息,向量空间模型"
2011-09-09,触摸屏手写汉字笔画的笔力模糊评价,"樊 亮,戴 永,覃冰梅","该文以触摸屏手写汉字为例,提出了一种分析手写汉字笔力的模糊方法,并通过笔力分析实现对触摸屏上手写汉字的书写质量评价。该方法首先通过提取手写模板字中笔画的关键点信息进行隶属度计算,建立模糊模板矩阵,然后将实写汉字的隶属度与模板隶属度数据进行贴近度计算得出综合的笔力评价。实验表明,该方法的评价效果具有较强客观性、鲁棒性,也可推广到其他文种。","触摸屏,手写汉字,质量评价,笔力分析,模糊方法"
2011-12-19,维吾尔文智能输入法研究,"米日姑·肉孜,吐尔根·依布拉音,麦热哈巴·艾力","开发智能化的输入法是维吾尔文输入技术的一个发展方向,也是维吾尔文自然语言信息处理的基础性研究课题。该文根据维吾尔文自身特点,对用户输入过程中所犯各种错误进行分析,设计并实现了词语搭配知识库,提出了基于二元语法模型的一种新型维吾尔文输入算法,并给出了实现其关键模块功能的程序流程图。它具有自动预测、自动联想输入和自动校对等功能。","中文信息处理,维吾尔文,智能输入法,语言模型,自动预测,自动联想"
2011-10-10,藏文音节规则库的建立与应用分析,"珠 杰,欧 珠,格桑多吉,扎西加,高红梅","藏文音节具有独特的构造方法,不同的构造位上有不同的藏文字符,根据不同的组合,构成了千变万化的藏文音节,由于字符的语音特性,藏文组合形式上有很多的限制。该文借助藏文文法规则和藏汉大词典,建立了现代藏文音节规则库,并分析了可能的应用领域。","藏文,藏文规则,词频"
2012-06-12,发音相似的朝鲜语和汉语单元音辨识方法,"芦世丹,崔荣一","该文主要基于共振峰对六对发音相似的朝汉语单元音的分类方法进行了研究。首先,提取音频文件的前三个共振峰F1、F2、F3;其次,分析六对发音相似的朝汉语单元音的共振峰分布差异,针对不同的分类对象选择不同的共振峰频率特征参数或其组合形式作为分类特征;最后,采用信息增益方法确定分类阈值并对朝汉语单元音进行分类。实验结果表明,朝鲜语单元音和具有相似发音的汉语单元音之间存在可区分性,所采用的方法计算过程简单,获得了良好的分类效果。","朝鲜语单元音,汉语单元音(单韵母),语种识别,共振峰频率,信息增益"
2012-03-05,改进的维吾尔语Web文本后缀树聚类,"邹志华,田生伟,禹  龙,冯冠军","该文提出了改进的维吾尔语Web文本后缀树聚类算法STCU,其中后缀树的构建以维吾尔语句子为基本单位。针对维吾尔语语言和Web文本特点,文中对词语进行词干提取,构建了维吾尔语绝对停用词表和相对停用词表,采用文档频率和词性结合的方法提取关键短语,改进了合并基类的二进制方法,根据语料类别数自动调整聚类类别阈值,利用最一般短语对聚类类别进行描述,有效地改善了文本聚类的质量。与传统的后缀树聚类算法相比,聚类全面率提高了44.51%,聚类准确率提高了11.74%,错误率降低了0.94%。实验结果表明 改进的后缀树算法在Web文本聚类的精度和效率方面具有较强的优越性。","维吾尔语,后缀树,短语聚类,停用词表,文档频率"
2011-12-24,汉语组块分析研究综述,"李业刚,黄河燕","组块分析作为浅层句法分析的代表,既可以满足很多语言信息处理系统对于句法功能的需求,又可以作为子任务,在词法分析和完全句法分析以及语义分析中间架起一座桥梁,为句子进行进一步深入分析提供有力的支持,因此众多的研究将注意力集中于组块分析上。该文主要对组块的定义和分类、组块识别方法、组块的标注和评测以及组块内部关系分析等几方面的研究进展进行详细的综述。最后,探讨了组块分析存在的问题并对未来的发展方向进行了展望。","中文信息处理,浅层句法分析,组块分析,组块识别"
2012-01-28,基于统计学习模型的句法分析方法综述,"吴伟成,周俊生,曲维光","句法分析是自然语言处理领域中重要的基础研究问题之一。近年来,基于统计学习模型的句法分析方法研究受到了广泛关注,多种模型与算法先后被提出。从采用的学习模型和算法类型着手,该文系统地对各种主流和前沿方法进行了归纳与分类,着重对各类模型和算法的思想进行了分析和对比,并对中文句法分析的研究现状进行了综述;最后,对句法分析下一步的研究方向与趋势进行了展望。","句法分析, 统计学习模型, 生成式模型, 判别式模型, 移进—归约决策,面向数据的句法分析"
2012-01-13,篇章分析技术综述,"徐 凡,朱巧明,周国栋","篇章作为词和句子之后的一种文本分析粒度在自然语言理解和自然语言生成中起到至关重要的作用。该文从计算语言学角度出发,对中英文篇章分析技术的研究现状进行了综述。介绍了中英文篇章分析技术在自然语言处理中的应用,并分别从篇章理论、篇章语料库及评测、篇章分析器的自动构建等方面详细阐述了中英文篇章分析技术。最后归纳出篇章分析技术后续研究的几个方向。","篇章,篇章分析,语料库,评测"
2012-03-12,叙事生成方法研究综述,"诸 峰,曹存根","随着人工智能和自然语言处理技术的飞速发展,近年来,关于叙事自动生成的研究逐渐被人们所关注和重视。该文介绍了叙事生成的相关概念、历史背景以及当前的研究现状,总结和归纳了目前主要的叙事生成研究方法,包括基于智能规划的方法、基于常识和知识本体的方法、基于故事文法的方法等。在此基础上,对各类方法的基本思想、相关工作及主要优缺点进行了深入的分析,并探讨了当前叙事生成研究中存在的不足及未来的发展趋势。","叙事生成,故事生成,叙事智能,自然语言生成"
2011-12-27,现代汉字形声字声符在普通话中的表音度测查,"胡韧奋,曹 冰,杜健一","“形声”作为一种重要的造字方式,构筑了汉字家族中最为庞大的一支。造字之初,形声字以形符表义,以声符表音。随着时代的发展,声符的表音度渐渐发生变化,为人们准确地标音读字造成了一定困难。该文试采用聚类分析的方法,以普通话中3 500常用汉字为对象,结合语言学理论和计算机知识,依据声符表音程度相同、相似和不同制定详细分级标准,并得到每一层级的形声字表和百分数据,从而对现代汉字中形声字声符的表音度情况进行系统、直观而全面地呈现,以期为现代汉字规范的制定和汉语教学提供一定的参考和佐证。","现代汉字,形声字,声符,表音度,聚类分析"
2012-02-27,普通话发音评估性能改进,"齐 欣,肖云鹏,叶卫平","为减少噪声环境对评估性能的影响,该文将PNCC参数引入普通话发音评估。结果表明,其评分相关性在普通话测试实录音数据库上较传统MFCC参数提高了6.6%。在此基础上,对汉语声学模型拆分方法进行了研究,提出将声母介音+韵母模型拆分方法应用到发音评估中。使用这种拆分方式的评估系统总错误率降低5.6%,专家打分相关性则提高了0.056。该文还对模型最佳状态数的选取进行讨论,并提出模型状态数混合和不同配置综合评分两种混合评分方案,在相关性上较同等条件下3状态模型分别提高了0.021和0.017。","发音评估,PNCC,模型拆分,HMM状态数"
2011-12-14,基于输出概率分布的集外词拒绝,"黄石磊,刘 轶,程 刚","该文提出了一种基于音子HMM输出概率分布(OPD)计算集外词(OOV)拒绝的方法,该方法主要用于语音识别中的验证阶段。与动态垃圾模型中使用经过排序的概率数值的方法相比,OPD向量包含了更多的信息。每个音素的置信值都是以OPD向量为输入的支持向量机(SVM)分别计算出,并得到词的置信度,确定候选词被接受或拒绝。实验结果表明,所提出的方法在语音识别的验证任务中,与传统的动态垃圾模型相比,等错误率EER值相对降低了11.0%。","语音识别,关键词确认,置信度"
2011-12-06,语料对中文名词短语指代消解影响研究,"高俊伟,孔 芳,朱巧明,李培峰","指代是自然语言中一种常见的语言现象,对简化语言,减少冗余有很大的作用。指代消解是用计算机找出这些指代现象的一个过程。近几年英文指代消解研究取得了很大的成就,然而,中文指代消解研究目前还较少,一方面是由于中文自然语言处理的研究起步较晚,相关的知识较少,另外一方面就是中文相关的语料库较少,目前已知的仅有ACE2005, OntoNotes等。为了探讨语料库对中文名词短语指代消解的影响,该文实现了一个基于有监督学习方法的中文名词短语指代消解平台和一个基于无监督聚类方法的中文名词短语指代消解平台,在此平台的基础上从语料库的数量和质量两个方面来探讨语料对中文名词短语指代消解的影响。","指代消解,名词短语,无监督,聚类,语料"
2012-01-16,基于层叠CRFs的中文句子评价对象抽取,"郑敏洁,雷志城,廖祥文,陈国龙","中文句子评价对象抽取是指在中文句子中抽取评论所针对的对象或对象的属性。目前国内相关研究工作尚未能有效识别复合词评价对象和未登陆评价对象。针对以上两种情况,该文提出了一种基于层叠条件随机场的中文句子评价对象抽取方法。该方法首先通过低层条件随机场获得候选评价对象集,然后通过降噪模型对噪声进行过滤、补充模型对缺失的候选评价对象进行补充、合并模型对复合短语候选评价对象进行合并,最后由高层模型抽取出评价对象。实验结果显示,与基于线性链条件随机场的识别方法相比,该方法准确率、召回率和F1值分别提升1.62%、5.75％和4.17%,能有效地识别复合词评价对象和未登录评价对象,从而提高中文句子评价对象的识别精度。","评价对象,层叠条件随机场,降噪模型,补充模型"
2011-10-31,领域问答系统中的文本错误自动发现方法,"刘亮亮,王 石,王东升,汪平仄,曹存根","文本自动校对是自然语言处理的一个挑战性的研究课题,也是一个难题。该文对中文的错误类型和原因进行分析,提出了一种基于领域问答系统用户问题日志的错别字自动发现方法。该方法首先对语料进行分词,然后对分词的结果中出现的散串进行合并,对分词中的多字词和合并的串进行相似词串聚类,对相似词串的上下文语境进行统计分析,从中自动获取错别字对。实验表明,该系统获得71.32%的召回率,82.6%的准确率。","文本自动校对,问答系统,非词错误,真词错误,错别字对"
2011-11-03,基于短语串实例的汉藏辅助翻译,"熊 维,吴 健,刘汇丹,张立强","目前汉藏机器翻译的研究主要集中在基于规则的方法上,主要原因在于汉藏的平行语料等基础资源相对匮乏,不方便做大规模的基于统计的汉藏机器翻译实验。该文依据汉藏辅助翻译项目的实际需求,在平行语料资源较少的情况下,提出了一种基于短语串实例的机器翻译方法,为辅助翻译提供候选译文。该方法主要利用词语对齐信息来充分挖掘现有平行语料资源信息。实验结果表明,该文提出的基于短语串实例方法优于传统基于句子实例的翻译,能够检索出任意长度的短语串翻译实例。在实验测试集上,该方法与默认参数下的Moses相比,翻译的BULE值接近Moses,短语翻译实例串的召回率提高了约9.71%。在平均句长为20个词的测试语料上,翻译速度达到平均每句0.175s,满足辅助翻译实时性的要求。","机器翻译,辅助翻译,基于短语的机器翻译,基于实例的机器翻译"
2012-12-10,URL模式与HTML结构相结合的平行网页获取方法,"刘 奇,刘 洋,孙茂松","平行语料库是对机器翻译、跨语言信息检索等应用技术具有重要支撑作用的基础数据资源。虽然互联网上的平行网页数量巨大且持续增长,但由于平行网站的异构性和复杂性,如何快速自动获取高质量的平行网页进而构造平行语料库仍然是巨大的挑战。该文提出了一种URL模式与HTML结构相结合的平行网页获取方法,首先利用HTML结构实现平行网页的递归访问,其次使用URL模式优化遍历平行网站的拓扑顺序,从而实现高效准确的平行网页获取。在联合国与香港政府①两个平行网站上的实验表明,该方法相对传统获取方法在获取时间上减少50%以上,准确率提高15%,并显著提高了机器翻译的质量(BLEU 值分别提高1.6 和0.7 个百分点)。","平行网页获取,平行语料库,URL模式,HTML结构"
2012-12-13,基于众包的词汇联想网络的获取和分析,"丁 宇,车万翔,刘 挺,张梅山","词典是汉语自然语言处理中非常重要的一类资源,它能为汉语词法句法以及语义分析等提供资源支撑。该文采用众包方法构建汉语语义相关性词典,该词典是通过触发词联想的方式间接获取的,因此又称为词汇联想网络。词汇联想网络相比传统词典具有以下特点 (1)获取代价低;(2)面向互联网,易扩展;(3)词语关系从人的认知角度来建立,符合人的直觉。该文详细介绍词汇联想网络的获取方法并对已获取的数据进行分析,另外,将词汇联想网络与《知网》、《同义词词林》以及微博文本ngram进行比较说明其上述特点。","众包,语义相关性词典,词汇联想网络"
2012-12-14,基于词义类簇的文本聚类,"唐国瑜,夏云庆,张 民,郑 方","文档表示是文本聚类的重要组成部分,该文旨在通过改进文档表示改进文本聚类。同义词和多义词现象是文档表示所面临的重要挑战。为此该文提出了词义类簇模型(Sense Cluster Model,SCM),在词义类簇空间上表示文档。SCM首先构造词义类簇空间,然后将文档表示在词义类簇空间上,获得每篇文档在每个词义类簇的概率。在词义类簇空间构造这一步骤中,首先利用词义归纳技术从文本中自动发现词义,接着采用词义聚类技术识别相同或者相似的词义从而获得词义类簇。词义类簇空间构造后,该文首先进行词义消歧,然后利用词义消歧的结果将文档表示在词义空间上。实验表明,SCM在标准测试集上的性能优于基线系统以及经典话题模型LDA。","文档聚类,文档表示,话题模型"
2012-12-15,基于集成学习的半监督情感分类方法研究,"高 伟,王中卿,李寿山","情感分类旨在对文本所表达的情感色彩类别进行分类的任务。该文研究基于半监督学习的情感分类方法,即在很少规模的标注样本的基础上,借助非标注样本提高情感分类性能。为了提高半监督学习能力,该文提出了一种基于一致性标签的集成方法,用于融合两种主流的半监督情感分类方法:基于随机特征子空间的协同训练方法和标签传播方法。首先,使用这两种半监督学习方法训练出的分类器对未标注样本进行标注;其次,选取出标注一致的未标注样本;最后,使用这些挑选出的样本更新训练模型。实验结果表明,该方法能够有效降低对未标注样本的误标注率,从而获得比任一种半监督学习方法更好的分类效果。","情感分类,半监督,集成学习"
2013-03-07,中国机器翻译研究的机遇与挑战——第八届全国机器翻译研讨会总结与展望,"杜金华,张 萌,宗成庆,孙 乐","随着统计方法逐渐成为机器翻译研究的主流,机器翻译系统评测的分值越来越高,人们对机器翻译的信心和期望逐渐增加,社会对机器翻译应用的需求也越来越大。然而,现有的机器翻译理论和方法在系统性能上提升的空间逐渐减小,而且距离用户实际需求仍有很长的路要走。那么,面对期望、面对需求,机器翻译之路应该如何走？为此,第八届全国机器翻译研讨会对当前机器翻译研究所面临的挑战和机遇进行了深入研讨。该文详细介绍了该次研讨会六个专题的讨论情况,对机器翻译研究面临的机遇和挑战进行了认真的分析和总结。","机器翻译理论,机器翻译应用,语音翻译,少数民族语言,机器翻译评测"
2012-12-20,基于对偶分解的词语对齐搜索算法,"沈世奇,刘 洋,孙茂松","词语对齐旨在计算平行文本中词语之间的对应关系,对机器翻译、双语词典构造等多项自然语言处理任务都具有重要的影响。虽然近年来词语对齐在建模和训练算法方面取得了显著的进展,但搜索算法往往都采用简单的贪心策略,面临着搜索错误较大的问题。该文提出了一种基于对偶分解的词语对齐搜索算法,将复杂问题分解为两个相对简单的子问题,迭代求解直至收敛于最优解。由于对偶分解能够保证求解的收敛性和最优性,该文提出的搜索算法在2005年度863计划词语对齐评测数据集上显著超过GIZA++和判别式词语对齐系统,对齐错误率分别降低4.2%和1.1%。","词语对齐,判别式模型,搜索算法,对偶分解"
2012-09-03,基于ListMLE排序学习方法的机器译文自动评价研究,"李茂西,江爱文,王明文","机器翻译译文质量的自动评价是推动机器翻译技术快速发展的一条重要途径。该文提出了基于List-MLE 排序学习方法的译文自动评价方法。在此基础上,探讨引入刻画译文流利度和忠实度的特征,来进一步提高译文自动评价结果和人工评价结果的一致性。实验结果表明,在评价WMT11德英任务和IWSLT08 BTEC CE ASR任务上的多个翻译系统的输出译文质量时,该文提出的方法预测准确率高于BLEU尺度和基于RankSVM的译文评价方法。","机器译文评价, 排序学习, ListMLE方法, 人工评价, 自动评价"
2012-12-15,基于序列标注的中文分词、词性标注模型比较分析,"刘一佳,车万翔,刘 挺,张梅山","该文对三种不同的分词词性标注模型进行了比较。这三种模型分别为一个序列标注串行模型,一个基于字分类的联合模型和一个将这两种模型使用Stacked Learning框架进行集成的融合模型。通过在《人民日报》、CoNLL09、CTB5.0和CTB7.0四个数据集上进行比较分析,最终实验结果表明分类联合模型能取得比较好的速度,融合模型能取得比较好的准确率,而普通串行模型处于速度和准确率的平衡位置。最后该文将准确率最好的融合模型和相关前沿工作在CTB5.0和CTB7.0上进行了对比, 该融合模型均取得了最好的结果。","中文分词,词性标注,Stacked Learning"
2012-10-13,基于主动学习的本体概念关系判断,"张桂平,李文博,王裴岩","该文依据关系判断任务特点将主动学习应用到本体概念关系的辅助判断中,对边缘采样、熵采样、最不确信采样等主动学习查询生成策略进行了比较研究。在此基础上,从实际应用角度出发,讨论了在三种不同样本初始情况下主动学习技术的应用。对于初始样本正反例充足的情况,采用基于熵采样和边缘采样产生查询;对于初始样本仅有正例的情况,依据样本相似度主动的学习策略生成候选反例;对于缺乏初始样本的情况,使用概念在样本间距离等统计信息,同时生成候选正例和候选反例。从而,实现了在概念关系判定过程中对用户反馈信息的有效利用。","本体,概念关系,辅助判断,主动学习"
2013-01-05,基于词元语义特征的汉语框架排歧研究,"李国臣,张立凡,李 茹,刘海静,石 佼","框架排歧指的是在一个给定的句子中,判断句中目标词激起的语义场景与该目标词可能激起的哪个框架一致,则将该框架分配给当前的目标词。框架排歧最重要的一个步骤就是特征选择,目前常用的方法是人工特征选择方法,但是这种方法不能有效地利用每个目标词的语义特征,而且大量实验表明,不同的目标词取得最好的结果时所用的特征模板是不同的。因此,该文为每个目标词设置一个特征模板,并提出了特征模板的自动选择算法,首先从语料中抽取特征构成特征集,然后利用打分机制,把特征集中得分最高的特征逐个加入到特征模板中,直到相邻两次的得分不再增加。该文借助汉语框架网语义资源,利用最大熵模型建模,使用自动特征选择算法选出特征模板,并进行5-fold交叉验证,平均精确率可达到84.46%。","框架排歧,汉语框架网语义资源,自动特征选择,词元语义特征"
2012-05-16,基于条件随机场的藏语自动分词方法研究与实现,"李亚超,加羊吉,宗成庆,于洪志","藏语自动分词是藏语信息处理的基础性关键问题,而紧缩词识别是藏语分词中的重点和难点。目前公开的紧缩词识别方法都是基于规则的方法,需要词库支持。该文提出了一种基于条件随机场的紧缩词识别方法,并在此基础上实现了基于条件随机场的藏语自动分词系统。实验结果表明,基于条件随机场的紧缩词识别方法快速、有效,而且可以方便地与分词模块相结合,显著提高了藏语分词的效果。","藏语自动分词,条件随机场,紧缩词识别,格助词"
2012-12-20,藏语判断、存在动词识别策略,"李 琳,龙从军","判断动词与存在动词在藏语中使用频度高,兼类现象频繁,在不同语境下具有不同的含义。既可以表示判断、存在和领有意义,也可作为语法标记表达复杂的体貌、示证意义。判断、存在动词的多功能性给藏文文本分词标注、句型识别等工作带来较大的困难。借助藏语语法的研究成果和真实藏文文本,我们对这两类词的上下文语境进行了分析和归纳,进而提出了辨别这两类词的方法。首先,考察判断动词和存在动词在不同语境下的左右特征词;然后,建立了识别规则库,从肯定与否定两个方面判别其词性并标注。","藏语, 判断动词,存在动词,自动识别"
2012-12-11,基于中心语块扩展的汉藏基本名词短语对的识别,"诺明花,刘汇丹,马龙龙,吴 健,丁治明","该文提出汉藏基本名词短语对齐框架。从汉语基本名词短语出发,找藏文正确译文过程中,参考英汉短语对齐的方法,针对藏语的特殊性,提出基于中心语块扩展的藏语基本名词短语识别方法。提出词典与自动词对齐结果相结合的方法和基于序列相交的方法抽取藏语中心语块,再以扩展可信度为依据扩展中心语块。实验结果表明,基于序列相交的方法所抽取的汉藏基本名词短语对能够节省人工校正的工作量,有效辅助于汉藏基本名词短语库的建设。","藏文信息处理,基本名词短语,中心语块扩展"
2012-12-10,基于依存语法的蒙古语语义角色分类及其标记研究,"包晓荣,华沙宝,达胡白乙拉","该文从蒙古文信息处理角度出发,着重参考了其他语言语义角色标注的理论方法和蒙古语语义角色相关研究成果,结合蒙古语依存句法树库的特征,通过手工标注分析研究,制定了基于依存语法的蒙古语语义角色分类及其标记。","蒙古语语料库,依存语法,语义角色"
2012-07-19,简繁对应关系与简繁转换,"王立军,王晓明,吴  健","简繁转换系统的最根本问题是语言文字的应用问题。其核心是针对中国大陆、中国港澳台等不同应用环境的简繁汉字对应关系和术语对照表。这项工作是十分复杂的,不可能一蹴而就,需要分阶段逐步完成。首要工作是要做好大陆地区简繁对应关系的分解,研制出适用于大陆内部的简繁转换系统。这一系统应该包括六大步骤,其中“字境”概念的引入,为提高简繁转换的准确率提供了有力的支撑。","简化字,繁体字,简繁对应关系,简繁转换"
2012-12-19,基于混合相关的Markov网络信息检索扩展模型,"甘丽新,涂 伟,王明文,石 松","查询扩展是提高检索性能的有效方法。为了弥补在数据集中由于词对没有直接出现而导致无法统计出词间关系进行查询扩展的缺陷,该文通过提取Markov网络中的词团信息来量化词间的混合相关性,将强化后的词间混合相关性应用于信息检索扩展模型中。实验表明 基于混合相关的Markov网络信息检索扩展模型的检索效果优于基于直接相关的查询扩展模型;此外,该文提出的模型在总体检索性能上略优于基于团的Markov网络信息检索模型,但在词团提取上大大减少了计算开销。","混合相关,Markov网络,查询扩展"
2012-12-17,基于用户生成内容的产品搜索模型,"王海雷,章彦星,赵海玉,张 铭","以消费者行为分析和离散选择的相关理论为基础,通过对用户生成内容进行特征粒度的情感分析,同时从产品的客观数据和用户生成的主观内容中提取模型特征,使用有监督的学习训练MNL模型预测产品的消费者剩余作为搜索排序的依据,并实现了手机、笔记本电脑和数码相机类的产品搜索系统。双盲实验表明,该文提出的产品搜索模型搜索效果比基准算法有显著的提高。","产品搜索,MNL模型,情感分析,特征选取,用户生成内容"
2012-12-20,基于排序学习的微博用户推荐,"彭泽环,孙 乐,韩先培,石 贝","该文在分析总结影响微博用户推荐的四大类信息,包括用户的内容信息、个人信息、交互信息和社交拓扑信息的基础上,提出一个基于排序学习的微博用户推荐框架,排序学习的本质是用机器学习中的分类或回归方法解决排序问题,该框架可以综合各类信息特征进行用户推荐。实验结果表明 (1)融合多个特征综合推荐通常可以取得更好的推荐效果;(2)基于用户个人信息、交互信息、社交拓扑信息的推荐效果均好于基于用户内容的推荐效果。","排序学习,用户推荐,微博"
2012-10-09,维吾尔语评论文本主题抽取研究,"禹 龙,田生伟,黄 俊","主题抽取是意见挖掘的核心任务之一。该文面向维吾尔语评论文本, 针对显式主题和隐式主题, 提出了一种陈述级的主题抽取方法。该方法采用GLR-Cascaded LDA模型抽取段落级的局部主题、篇章级的全局主题, 建立全局—局部主题关系, 并将这些关系对应到每个意见陈述中; 然后运用Bootstrapping和模式匹配的方法进行显式陈述的主题抽取; 最后使用隐式主题推断算法推断隐式陈述的主题。主题抽取的最终目标是为每个意见陈述建立意见陈述—主题四元组&lt;OC, GT, LT, CT&gt;。实验结果证明了该方法在主题抽取任务中的有效性。","主题抽取, 陈述级, 显式主题, 隐式主题, 维吾尔语"
2012-12-08,不平衡情感分类中的特征选择方法研究,"王志昊,王中卿,李寿山,李培峰","随着网络的发展,情感分类任务受到广大研究人员的密切关注。针对情感分类中的不平衡数据分布和高维特征问题,该文比较研究了四种经典的特征选择方法在不平衡情感分类中的应用。同时,该文提出了三种不同的特征选择模式并实验比较了这三种模式在分类和降维性能方面的表现。实验结果表明在不平衡数据的情感分类任务中,特征选择方法能够在不损失分类效果的前提下显著降低特征向量的维度。此外,特征选择方法中信息增益(IG)结合“先随机欠采样后特征选择”模式能够取得最佳的分类效果。","情感分类,不平衡数据,特征选择"
2012-12-18,一种启发式多标记分类器选择与排序策略,"李 哲,王志海,何颖婧,付 彬","在多标记分类问题当中,多标记分类器的目的是为实例预测一个与其关联的标记集合。典型方法之一是将多标记分类问题转化为多个二类分类问题,这些二类分类器之间可以存在一定的关系。简单地考虑标记间依赖关系可以在一定程度上改善分类性能,但同时计算复杂度也是必须考虑的问题。该文提出了一种利用多标记间依赖关系的有序分类器集合算法,该算法通过启发式的搜索策略寻找分类器之间的某种次序,这种次序可以更好地反映标记间的依赖关系。在实验中,该文选取了来自不同领域的数据集和多个评价指标,实验结果表明该文所提出的算法比一般多标记分类算法具有更好的分类性能。","多标记分类,文本分类,数据挖掘"
2013-06-12,基于表示学习的中文分词算法探索,"来斯惟,徐立恒,陈玉博,刘 康,赵 军","分词是中文自然语言处理中的一个关键基础技术。通过基于字的统计机器学习方法学习判断词边界是当前中文分词的主流做法。然而,传统机器学习方法严重依赖人工设计的特征,而验证特征的有效性需要不断的尝试和修改,是一项费时费力的工作。随着基于神经网络的表示学习方法的兴起,使得自动学习特征成为可能。该文探索了一种基于表示学习的中文分词方法。首先从大规模语料中无监督地学习中文字的语义向量,然后将字的语义向量应用于基于神经网络的有监督中文分词。实验表明,表示学习算法是一种有效的中文分词方法,但是我们仍然发现,由于语料规模等的限制,表示学习方法尚不能完全取代传统基于人工设计特征的有监督机器学习方法。","表示学习,中文分词"
2013-06-15,多语料库中汉语四字格的切分和识别研究,"徐润华,曲维光,陈小荷,王东波","汉语四字格的能产性和派生性极强,利用四字格模式创造出的新词数量在现代汉语词汇中一直呈上升趋势。该文将研究的目光投向分词语料库中的四字格,对语料库中的四字格进行了系统的分类和归纳,并对语料库内部和语料库之间的四字格切分不一致现象进行了详细的调查统计。最后,针对四字格的切分不一致数据引入条件随机场(CRF)模型,对多语料库中的汉语四字格进行识别实验,封闭测试和开放测试的识别精度均达到93%以上。","四字格,分词语料库,切分不一致,CRF模型"
2013-06-01,基于中文拼音输入法数据的汉语方言词汇自动识别,"张 燕,张 扬,孙茂松","方言研究领域中的语音研究、词汇研究及语法研究是方言研究的三个重要组成部分,如何识别方言词汇,是方言词汇研究首要的环节。目前,汉语方言词汇研究的语料收集与整理主要通过专家人工整理的形式进行,耗时耗力。 随着信息技术的发展,人们的交流广泛通过网络进行,而输入法数据包含海量的语料资源以及地域信息,可以帮助进行方言词汇语料的自动发现。然而,目前尚没有文献研究如何利用拼音输入法数据对方言词汇进行系统化分析,因此在本文中,我们探讨借助中文输入法的用户行为来自动发现各地域方言词汇的方法。特别的,我们归纳得到输入法数据中表征方言词汇的两类特征,并基于对特征的不同组合识别方言词汇。最后我们通过实验评价了两类特征的不同组合方法对方言词汇识别效果的影响。","方言词汇识别,中文拼音输入法,特征融合"
2013-06-03,基于多步聚类的汉语命名实体识别和歧义消解,"李广一,王厚峰","命名实体识别和歧义消解是自然语言理解的重要研究内容。针对提供实体知识库情况下的命名实体识别和歧义消解任务,该文提出了一种基于多步聚类的方法。首先通过两轮聚类将命名实体与知识库实体定义链接,然后通过层次聚合式聚类对知识库中未出现的实体进行聚类,最后进行普通词的识别和基于K-Means聚类的结果调整。在CLP-2012的汉语命名实体识别和歧义消解评测数据上的实验表明,该文的方法表现出良好的性能,在测试集上的F值高出评测参赛队伍最好水平6.46%,达到86.68%。","命名实体识别,命名实体消歧,聚类"
2013-06-12,汉语虚词用法在依存句法分析中的应用研究昝,"红英,张静杰,娄鑫坡","虚词在现代汉语中占有重要地位,虚词与词序一起构成现代汉语的句法手段,对句法分析有重要的影响。依存句法分析是自然语言处理领域研究的热点,为了提高依存关系的识别效果,该文考虑将虚词用法应用到依存关系的识别过程中。通过对虚词用法的研究,以及对依存句法分析各种依存关系识别情况的分析,发现并列关系与虚词中的连词关系密切。作者在并列关系识别过程中加入连词的用法信息,从而提高了并列关系的识别效果。实验结果表明,包含连词的并列关系的LAS及UAS分别提高了3.43%和2.29%。","虚词用法,依存句法分析,并列关系"
2013-06-11,面向中文专利文献的有标记并列结构的统计分析,"石 翠,周俏丽,张桂平","该文在中文专利语料的基础上,统计分析了中文专利文献中有标记并列结构的内部特征和外部特征。内部特征主要考察了中文专利文献中有标记并列结构的并列标记、并列结构内部分析和词性分布等。外部特征主要统计了可能的边界特征词,并分析了有标记并列结构在中文专利文献中出现的外部环境。","有标记并列结构,中文专利文献,内部特征,外部特征"
2013-06-22,基于图模型的语义角色标注重排序,"熊 皓,刘 群,吕雅娟","传统的语义角色标注模型使用的都是本地特征,不利于捕捉一些全局性的标注错误。该文提出使用图模型对语义角色标注结果进行重排序,利用标记传播迭代算法对标注的结果进行重排序,保证全局标注结果的一致性。该文在PropBank上的实验表明,采用重排序后标注性能有了2.4个F值的显著提升。在不使用系统融合技术的情况下,标注的结果是当前世界最好的性能。","语义角色标注,图模型,重排序"
2013-06-30,基于特征结构的汉语连动句语义标注研究,"陈 波,姬东鸿,吕 晨","对汉语特殊句型的语义分析是当前中文信息处理的难点之一。现有的传统语义分析方法存在一些问题,不能很好的反映汉语中各个词语或成分之间的语义关联。该文以汉语连动句为例,提出了基于特征结构模型的语义标注方法,探讨了连动句的语义标注模型,并在此基础上建构了一个大规模的汉语语义资源。结果表明,特征结构模型能够对连动句中的主语与多个谓语动词、多个宾语之间的复杂语义关系进行全面准确的描述,为面向汉语的自然语言处理提供了一种不同的语义分析方法。","特征结构,连动句,语义标注,语义资源"
2013-06-11,面向细粒度意见挖掘的情感本体树及自动构建,"郭 冲,王振宇","该文定义了一种用于细粒度意见挖掘的情感本体树结构,并基于细粒度意见要素抽取技术提出本体树的自动构建方法。重点研究了评价搭配抽取算法、搭配倾向预测算法以及特征聚合算法,并在解决搭配倾向预测及特征聚合问题时引入了互联网资源的自然标注性。在COAE2011实验3评测数据集上的实验结果证明了该算法在评价搭配抽取、搭配倾向预测上都取得了很好的效果。","情感本体树,评价搭配,倾向性预测,特征聚合"
2013-06-15,基于BootStrapping的集成分类器的中文观点句识别方法,"吕云云,李 旸,王素格","领域相关的大规模和高质量的标注训练数据是分类器性能的重要保证,而标注训练语料是一件费时费力的工作。该文提出了一种采用小规模标注语料识别中文观点句的方法。首先采用Bootstrapping方法扩展训练语料,分别训练贝叶斯、支持向量机和最大熵分类器。最后,通过给三个训练好的分类器赋权获得一个集成分类器。实验结果表明,集成后的分类器性能优于单分类器,并且该方法在使用部分标注训练数据的情况下也能取得与采用全部标注训练数据相近的实验结果。","观点句识别,BootStrapping,集成分类器"
2013-06-15,基于序列标注模型的情绪原因识别方法,"李逸薇,李寿山,黄居仁,高 伟","情绪原因识别是情绪分析中的一个重要研究任务。该任务旨在自动分析出导致某一情绪发生的原因描述。该文将情绪原因识别任务建模为序列标注模型,即将情绪词相关的子句当成序列,进而整体标注出哪些属于原因子句。具体实现中,我们使用条件随机场(CRF)模型进行求解,并结合了基本词特征、词性特征、距离特征、上下文特征及语言学特征等多种特征进行原因识别。实验结果表明,所采用的这些特征对于原因识别都有一定帮助,特别是上下文特征。此外,我们发现在使用类似特征集合的情况下,序列标注模型能够获得比分类模型更好的识别效果。","序列标注,情绪原因识别,上下文特征,语言学规则特征"
2013-06-15,适用于中国外语学习者的英文作文全自动集成评分算法,"李 霞,刘建达","中国英语学习者人数众多,迫切需要针对中国学生特点的、有效适用于大规模英文作文数据的全自动评分算法,以解决中国现有英语教学和大规模英语考试中英文作文批改量大和难度大的瓶颈问题。该文提出了一种能够有效识别中国英语学习者写作特点并能自动识别特征维数的特征选择方法,并在此基础上提出了适用于不平衡分布数据的集成分类评分算法。对来自中国英语学习者语料库中大学英语四、六级不同主题下的1 115篇作文的分类结果显示,该文提出的算法比传统的分类评分算法在类内及类间平均分类准确度、召回率及F度量值上均有较大幅度的提升。","作文自动评分,不平衡数据分类,多项式朴素贝叶斯"
2013-06-30,先秦词汇的时代特征自动获取及文献时代的自动判定,"刘 浏,李 斌,曲维光,陈小荷","词汇的时代特征能反应词汇在一个时代发展变化的规律。该文将先秦分为前春秋、春秋和战国三个时代,获取并研究这三个时代的时代独有词、时代特征词及时代发源词。该文提出两种自动判断先秦文献时代的方法,分别基于向量相似度和朴素贝叶斯分类器,在25种先秦文献上后者的分类性能更稳定。最后该文使用朴素贝叶斯分类器验证了《列子》并非成书于先秦。","先秦词汇,时代,向量空间模型,朴素贝叶斯分类器"
2013-06-11,基于种子词汇的话题标签抽取研究,"寇宛秋,李 芳","传统话题模型用词项概率分布表示话题,在可解释性上存在很大的不足。该文在Latent Dirichlet Allocation(LDA)的结果上提出了一种基于种子词汇的话题标签抽取方法。首先根据提出的权重计算公式抽取每个话题的种子词,然后,采用bootstrapping思想,迭代产生包含种子词汇的关键短语集合,最后根据短语的完整性和泛化度选择话题标签。该文对两会报告话题和新闻事件话题进行实验,通过结果展示和人工评测,该方法抽取的话题标签能够较准确地表达话题的语义信息。","话题标签,种子词抽取,bootstrapping算法"
2013-06-10,基于迭代方法的多层Markov网络信息检索模型,"洪 欢,王明文,万剑怡,廖亚男","查询扩展是提高检索效果的有效方法,传统的查询扩展方法大都以单个查询词的相关性来扩展查询词,没有充分考虑词项之间、文档之间以及查询之间的相关性,使得扩展效果不佳。针对此问题,该文首先通过分别构造词项子空间和文档子空间的Markov网络,用于提取出最大词团和最大文档团,然后根据词团与文档团的映射关系将词团分为文档依赖和非文档依赖词团,并构建基于文档团依赖的Markov网络检索模型做初次检索,从返回的检索结果集合中构造出查询子空间的Markov网络,用于提取出最大查询团,最后,采用迭代的方法计算文档与查询的相关概率,并构建出最终的基于迭代方法的多层Markov网络信息检索模型。实验结果表明 该文的模型能较好地提高检索效果。","Markov网络,查询扩展,文档依赖,团,信息检索"
2013-06-11,基于事件语义特征的中文文本蕴含识别,"刘茂福,李 妍,姬东鸿","为了强化文本蕴含系统深层语义分析与推理能力,该文提出了基于事件语义特征的中文文本蕴含识别方法。该方法基于事件标注语料生成事件图,将文本间的蕴含关系转化为事件图间的蕴含关系;利用最大公共子图的事件图相似度算法计算事件语义特征,与统计特征、词汇语义特征和句法特征一起使用支持向量机进行分类,得到初步实验结果,再经过基于事件语义规则集合的修正处理得到最后的识别结果。实验结果表明基于事件语义特征的中文文本蕴含识别方法可以更有效地对中文文本蕴含关系进行识别。","文本蕴含,事件语义特征,最大公共子图,支持向量机"
2013-06-22,基于句法结构约束的模糊限制信息范围检测,"周惠巍,杨 欢,黄德根,李 瑶,李丽双","模糊限制信息检测用于区分模糊限制信息与事实信息,提高抽取信息的真实性和可靠性。模糊限制信息范围的界定具有依赖于语义和句法结构的特点,是模糊限制信息检测的一个难点。该文提出一种基于句法结构约束的模糊限制信息范围检测方法,基于依存结构树和短语结构树构建决策树,获取句法结构约束集,用于产生句法结构约束特征,并加入到条件随机域模型中进行模糊限制信息范围检测。实验采用CoNLL-2010共享任务数据集,在标准的模糊限制语标注语料上,获得了70.28%的F值,比采用普通的句法结构特征提高了4.22%。","模糊限制信息范围检测,句法结构约束,决策树,条件随机域"
2013-06-11,基于图的查询日志实体别名抽取方法,"石 贝,孙 乐,韩先培","实体的别名是指同一个实体的不同名称。传统的别名抽取方法存在训练语料构建困难和时效性差这两个问题。针对这两个问题,该文提出了一种基于图的查询日志实体别名抽取方法。该方法利用查询日志的上下文信息和查询链接信息,构建了二层图(包括别名候选图层和查询链接图层),并通过随机游走算法对图中的候选别名进行排序。实验结果表明 1)该方法准确率达到了71.8%,证明该方法可行有效。2)使用查询链接信息进行别名抽取优于使用上下文信息进行别名抽取。这两种信息的结合能获得更好的别名抽取效果。","查询日志,别名抽取"
2013-06-01,基于历史模型的蒙古文自动词性标注研究,"赵建东,高光来,飞 龙","蒙古文自动词性标注方面的研究工作较少,制约了对蒙古文的机器翻译、语法分析及语义分析等领域的深入研究。针对于此,提出了加入lookahead学习机制的基于历史模型的蒙古文自动词性标注方法。实验表明,加入lookahead学习机制的基于历史模型的蒙古文自动词性标注方法对蒙古文的未登录词、集内词、总体词自动词性标注的准确率分别达到了71.276 6%、99.148 2%、95.301 0%,说明此方法可以较好地进行蒙古文的自动词性标注。","历史模型,lookahead,蒙古文,自动词性标注"
2013-06-11,融合音节特征的最大熵藏文词性标注研究,"于洪志,李亚超,汪 昆,冷本扎西","藏文词性标注是藏文信息处理中非常重要的基础性问题,该文以最大熵模型为基本框架,根据藏文的构词特征及统计分析结果,定义并选取特征模板,研究了融合语言特征的最大熵藏文词性标注模型。实验结果表明,最大熵模型能够较好的处理藏文词性标注问题,音节特征可以显著提高藏文词性标注的效果,与基准系统相比使错误率降低了6.4%。","藏文,词性标注,最大熵,形态特征"
2013-06-02,基于词对依存分类的藏语树库半自动构建研究华,"却才让,姜文斌,赵海兴,刘 群 ","依据依存句法理论,该文制订了藏语句法标注体系及层次结构。通过分析构建藏语依存树库中存在的问题,提出了半自动依存树库构建模式,针对藏语特性提出了融合丰富特征的词对依存分类模型和依存边标注模型,实现了依存树库构建可视化工具,校对构建了1.1万句藏语依存句法树后,在基线系统下经实验验证,依存识别正确率提高了3%,使构建藏语依存树库工作取得了有效进展。","藏语依存句法,词对依存分类,藏语树库,藏语依存标注工具"
2013-06-15,基于字符串相似度的维吾尔语中汉语借词识别,"米成刚,杨雅婷,周 喜,李 晓,杨明忠","维汉机器翻译过程中会出现较多的未登录词,这些未登录词一部分属于借词(人名、地名等)。该文提出一种新颖的根据借词与原语言词发音相似这一特性进行维吾尔语中汉语借词识别的方法。该方法对已有语料进行训练,得到面向维吾尔语中汉语借词识别的维吾尔语拉丁化规则;根据以上规则对维吾尔语拉丁化,并对汉语词进行拼音化,将借词发音相似转换为字符串相似这一易量化标准;提出了位置相关的最小编辑距离模型、加权公共子序列模型以及二者的带参数融合模型。实验结果表明,综合考虑字符串全局相似性和局部相似性的带参数融合模型取得了最佳的识别效果。","借词,未登录词,发音相似度,字符串相似度"
2013-06-12,基于功能词缀串的维吾尔语词性标注方法,"王海波,祖漪清,力提甫·托乎提","维吾尔语作为一种典型的黏着语,通过丰富的功能词缀来表达各种语法和语气。该文探讨了“词干词性标注方法”与“词缀词性标注方法”在维吾尔语自然语言处理中的优缺点。在大规模语料库中,统计了常用词缀串的数量、频次和覆盖度,以此来判断词缀词性标注方法在自然语言处理中的可行性。以力提甫·托乎提教授的维吾尔语生成语法理论为指导,对词缀串的词性标注进行了相应的语法定义,并且在实际语料中进行了小规模词性标注实验。该文提出的基于词缀串的词性标注方法不仅适用于维吾尔语,也适用于有着大量相似词缀的突厥语族其他语言。","维吾尔语,词缀串,词性标注"
2013-06-11,适用于特定领域机器翻译的汉语分词方法,"苏 晨,张玉洁,郭 振,徐金安","在特定领域的汉英机器翻译系统开发过程中,大量新词的出现导致汉语分词精度下降,而特定领域缺少标注语料使得有监督学习技术的性能难以提高。这直接导致抽取的翻译知识中出现很多错误,严重影响翻译质量。为解决这个问题,该文实现了基于生语料的领域自适应分词模型和双语引导的汉语分词,并提出融合多种分词结果的方法,通过构建格状结构(Lattice)并使用动态规划算法得到最佳汉语分词结果。为了验证所提方法,我们在NTCIR-10的汉英数据集上进行了评价实验。实验结果表明,该文提出的融合多种分词结果的汉语分词方法在分词精度F值和统计机器翻译的BLEU值上均得到了提高。","汉语分词,领域适应,双语引导,Lattice,机器翻译"
2013-06-01,有限语料汉蒙统计机器翻译调序方法研究,"陈 雷,李 淼,张 健,曾伟辉","自统计机器翻译技术出现以来,调序一直是语序差异显著的语言对互译系统中的关键问题,基于大规模语料训练的调序方法得到了广泛研究。目前汉蒙双语语料资源十分有限,使得现有的依赖于大规模语料和语言学知识的调序方法难以取得良好效果。该文对已有的相关研究进行了分析,提出了在有限语料条件下的汉蒙统计机器翻译调序方法。该方法依据语言学知识获取对译文语序影响显著的短语类型,研究这些短语类型的调序方案,并融入已有的调序模型实现调序的优化。实验表明该方法在有限语料条件下的效果提升显著。","统计机器翻译,调序,动词短语,有限语料"
2013-07-13,HDP与互信息相结合的中文无指导分词,"曹自强,李素建","该文探讨了无指导条件下的中文分词,这对构建语言无关的健壮分词系统大有裨益。互信息与HDP(Hierarchical Dirichlet Process)是无指导情况下常用的分词模型,该文将两者结合,并改进了采样算法。不考虑标点符号,在两份大小不同的测试语料上获得的F值为0.693与0.741,相比baseline的HDP分别提升了5.8%和3.9%。该文还用该模型进行了半指导分词,实验结果比常用的CRF有指导分词提升了2.6%。","HDP,互信息,无指导分词"
2013-08-11,上古汉语分词及词性标注语料库的构建——以《淮南子》为范例,"留金腾,宋 彦,夏 飞","该文介绍了以《淮南子》为文本的上古汉语分词及词性标注语料库及其构建过程。该文采取了自动分词与词性标注并结合人工校正的方法构建该语料库,其中自动过程使用领域适应方法优化标注模型,在分词和词性标注上均显著提升了标注性能。分析了上古汉语的词汇特点,并以此为基础描述了一些显式的词汇形态特征,将其运用于我们的自动分词及词性标注中,特别对词性标注系统带来了有效帮助。总结并分析了自动分词和词性标注中出现的错误,最后描述了整个语料库的词汇和词性分布特点。提出的方法在《淮南子》的标注过程中得到了验证,为日后扩展到其他古汉语资源提供了参考。同时,基于该文工作得到的《淮南子》语料库也为日后的古汉语研究提供了有益的资源。","上古汉语语料库,分词,词性标注,领域适应"
2013-07-31,基于混合策略的汉语最长名词短语识别,"钱小飞,侯 敏","该文提出一种基于语言知识评价的分类器集成方法,利用自动获得的搭配资源和人工评价规则,融合了基于支持向量机的最长名词短语识别结果和基于条件随机场的归约识别结果,进一步基于确定性规则有针对性地识别了分类器易出错的特殊结构,提高了对连续动词介词和连续名词造成的边界歧义的识别能力。实验取得了89.30%的正确率和89.62%的召回率,多词结构F1值较归约方法提高了0.75%。","最长名词短语识别,语言知识评价,分类器集成,规则"
2013-07-25,基于生成词库论和论元结构理论的语义知识体系研究,袁毓林,"该文讨论如何构造合适的汉语语义描写体系并建设相应的语义知识库,从而为文本语义的计算机自动分析提供可靠的资源。文章提出的技术路线是 在生成词库论和论元结构理论的指导下,分别描写名词的物性结构和动词、形容词的论元结构(包括物性角色或论元角色集合及其句法配置格式集合),标定名词、动词和形容词的情感评价色彩,揭示相关名词、动词和形容词的物性角色和论元角色之间的关联和推导关系,从而形成比较完整的关于名词、动词和形容词的实体指称、概念关系和情感评价等多层面的语义知识。最后,还展示了这种多层面的语义知识在语义自动计算中的运用案例。","语义描写体系,语义知识库,物性结构,论元结构,情感评价,语义关联"
2013-08-03,基于中文维基百科的词语语义相关度计算,"万富强,吴云芳","语义相关度计算在信息检索、词义消歧、自动文摘、拼写校正等自然语言处理中均扮演着重要的角色。该文采用基于维基百科的显性语义分析方法计算汉语词语之间的语义相关度。基于中文维基百科,将词表示为带权重的概念向量,进而将词之间相关度的计算转化为相应的概念向量的比较。进一步,引入页面的先验概率,利用维基百科页面之间的链接信息对概念向量各分量的值进行修正。实验结果表明,使用该方法计算汉语语义相关度,与人工标注标准的斯皮尔曼等级相关系数可以达到0.52,显著改善了相关度计算的结果。","语义相关度,显性语义分析,中文维基百科,先验概率,概念向量"
2013-08-02,中文事件事实性信息语料库的构建方法,"曹 媛,朱巧明,李培峰","事件事实性表达事件是否是事实的确定性程度,在文档中表现这一属性的是特定的句子结构和词汇。该文在充分研究影响中文事件事实性的句子成分的基础上,提出了五类事件事实性相关信息并给出了具体的标注规则。最后,在ACE 2005中文语料库的基础上完成了Movement事件的事实性标注,并对标注完成的语料库进行了相关的统计和分析,为后续研究提供基础。","事实性,语料库,标注"
2013-07-27,基于HNC概念关联性的领域判定研究,"池哲洁,张 全","在概念层次网络理论中,领域是语境单元的一个要素,而领域判定是语境单元萃取的重要课题之一。该文提出一种利用领域概念以及概念关联式进行领域判定的方法,通过在概念基元层面进行频数统计、概念合并及概念汇总实现领域的判定。对政治、经济、军事三个领域的语料进行测试,结果表明,使用概念关联式能够改进领域判定的效果,其F1值分别达到90.61%、90.83%、90.99%,比不使用概念关联式的情况分别提高了7.7%、12.76%、5.01%。最后,与基于关键词方法的对比结果也显示使用概念基元的方法效果较好。","概念基元,概念关联式,领域判定"
2013-08-27,中文篇章级句间语义关系识别,"张牧宇,宋 原,秦 兵,刘 挺","篇章句间关系识别(Discourse Relation Recognition)是篇章分析的重要内容,该文对中文篇章句间关系识别任务进行初步探索,包括显式篇章句间关系识别与隐式篇章句间关系识别两类任务。针对显式篇章句间关系,我们提出基于关联词规则的方法进行识别,取得了很好的效果;针对隐式篇章句间关系,我们抽取词汇、句法、语义等特征,采用有指导模型进行识别。该文的分析和实验结果为后续研究提供了参考和基本对照系统。","中文篇章级语义分析,显式篇章句间关系,隐式篇章句间关系"
2013-07-15,联合语义角色标注和指代消解,"熊 皓,刘 群,吕雅娟","语义角色标注和指代消解是自然语言处理中两个重要的基础研究任务,该文提出使用马尔科夫逻辑网络对两个任务进行联合学习推导。通过提出8条联合学习规则,在OntoNote5.0的实验中证明采用联合学习推导后,两者的标注性能都比单独标注任务有了1.6个F值的性能提升。","语义角色标注,指代消解,马尔科夫逻辑网络"
2013-07-18,基于概念特征的汉语交互类言说动词语义分析及同义词群的建构,"肖 珊,郭婷婷","人工智能的最重要内容之一是自然语言处理,建设词汇语义知识库是语言信息处理技术取得进步的重要保障。目前国内外基于同义词群(Synset)的词网研究存在结构不甚严谨,语义颗粒度不够精细,应用范围比较有限的问题。以“词群—词位”变体理论为基础建立多维特征概念语义网(Multi-D WordNet)是对这些问题进行解决的有效尝试,通过分析与描写基本词位(概念本体)与变体之间、变体与变体之间的关系,找出概念属性的差异,从而构建颗粒度更为精细的同义词群。言说动词的次范畴类“交互类”动词可作为研究个案,初步用来探讨这类典型词的语义结构及词群系统的建构规律。","同义词群,“词群—词位变体”理论,“交互类”言说动词"
2013-06-15,基于双语信息和标签传播算法的中文情感词典构建方法,"李寿山,李逸薇,黄居仁,苏 艳","文本情感分析是目前自然语言处理领域的一个热点研究问题,具有广泛的实用价值和理论研究意义。情感词典构建则是文本情感分析的一项基础任务,即将词语按照情感倾向分为褒义、中性或者贬义。然而,中文情感词典构建存在两个主要问题 1)许多情感词存在多义、歧义的现象,即一个词语在不同语境中它的语义倾向也不尽相同,这给词语的情感计算带来困难;2)由国内外相关研究现状可知,中文情感字典建设的可用资源相对较少。考虑到英文情感分析研究中存在大量语料和词典,该文借助机器翻译系统,结合双语言资源的约束信息,利用标签传播算法(LP)计算词语的情感信息。在四个领域的实验结果显示我们的方法能获得一个分类精度高、覆盖领域语境的中文情感词典。","情感分析,双语信息,情感字典,标签传播"
2013-06-20,一种基于情绪表达与情绪认知分离的新型情绪词典,"徐睿峰,邹承天,郑燕珍,徐 军,桂 林,刘 滨,王晓龙","目前的情绪词典通常对情绪词语进行情绪类别和强度的标注,但缺乏对词语的情绪表达和情绪认知结果进行区分的能力。同时,直接在词语条目上进行标注经常由于词语的语义歧义导致情绪标注结果存在歧义。该文在对个体情绪产生和迁移机制进行分析的基础上,建立了基于“刺激认知—反射表达”的文本情绪计算框架。并在此框架下对情绪相关词语的功能和特性进行分析,探索了一种新型情绪词典建设方法。首先,引入HowNet提供的词语语义信息,将同一词语转变为不同语义的多个词条进行标注减少情绪标注歧义。其次,将词语的情绪表达方式和情绪认知结果加以区分,分别标注从不同角度观测到的词条情绪类别和强度,同时对词语的情绪表达和情绪认知类型进行了细化分类。最终初步构建出一个具有清晰框架、丰富情绪信息和较低歧义的新型情绪词典。","情绪词典,情绪认知,情绪表达,词语语义"
2013-07-28,面向半监督情感分类的特征选择方法研究,"王志昊,王中卿,李寿山,李培峰,施寒潇","特征选择旨在降低高维度特征空间,进而简化问题和优化学习方法。已有的研究显示特征提取方法能够有效降低监督学习的情感分类中的特征维度空间。同以往研究不一样的是,该文首次探讨半监督情感分类中的特征提取方法,提出一种基于二部图的特征选择方法。该方法首先借助二部图模型来表述文档与单词间的关系;然后,结合小规模标注样本的标签信息和二部图模型,利用标签传播(LP)算法计算每个特征的情感概率;最后,按照特征的情感概率进行排序进而实现特征选择。多个领域的实验结果表明,在半监督情感分类任务中,基于二部图的特征选择方法明显优于随机特征选择,在保证分类效果不下降(甚至提高)的前提下有效降低了特征空间维度。","情感分类,半监督学习,二部图,标签传播,特征选择"
2013-06-20,评价短语的倾向性分析研究,"侯 敏,滕永林,陈毓麒","评价短语是评价因子之一,是汉语倾向性研究的重要组成部分。评价短语可以分为“评价词+评价词”、“修饰词+评价词”、“普通词+评价词”、“修饰词+普通词”、“普通词+普通词”5类。评价短语类型不同,采用的倾向性分析策略也不同。短语计算规则和短语评价词典的互动是该文采用的基本方法。在制定短语计算规则时应遵守共性与个性相结合的原则;建立短语评价词典时应遵循最小评价因子原则。实验证明,短语计算规则与短语词典的建立提高了倾向性分析系统的准确率,是一种行之有效的方法。","评价短语,倾向性分析,短语评价词典,评价短语计算规则,最小评价因子原则"
2013-07-14,基于多特征融合的中文比较句识别算法,"张 辰,冯 冲,刘全超,师 超,黄河燕,周海云","观点承载着文本的重要信息,而比较句是观点评论中一种常见的句式现象。针对中文比较句识别问题,该文提出了一种基于规则与统计相结合的方法并进行实验。该方法先对语料及其分词结果进行规范化处理,再通过基于比较特征词词典与句法结构模板、依存关系相结合的方法进行泛提取。然后设计一种CSR规则提取算法,并利用CRF挖掘实体对象信息及语义角色信息。最后利用SVM分类器,选取不同特征维数,找到使性能达到最优的特征形式完成精提取。","比较句,规则,CRF,SVM"
2013-06-01,应用hLDA进行多文档主题建模关键因素研究,"衡 伟,于 佳,李 蕾,刘咏彬","hLDA(层次潜在狄利克雷分配)在层次主题建模中的良好效果已经得到广泛验证。为了实现半监督或无监督,通常采用交叉验证或抽样超参来确定参数。但由于语料特征、建模需求等不确定因素,参数调节方法、建模效果和效率都是实际应用中的难点。该文首先结合贝叶斯线索和范围线索构成的统一分析框架,研究hLDA主题建模中的关键影响因素,然后给出一个切实有效的建模策略及流程,最终结合ACL MultiLing 2013多文档摘要语料进行实际建模效果评估。","层次潜在狄利克雷分配,层次主题建模,统一分析框架"
2013-06-10,汉语自然话语的音高下倾,"王茂林,訾广玲,熊 玮,林茂灿","该研究使用电话对话语料,在统计的基础上对语句的音高下倾进行了考察。发现绝大多数语句的音高都是逐渐下降的,音高曲线前高后低的走势有其生理上的原因,并且具有标界功能。少数语句音高不下降,这与词语的载义重度、焦点及音节本调有关。该文又对陈述句和疑问句的音高进行了考察,发现与陈述句相比,疑问句的整体音域较大,句末无疑问语气词的是非问句末尾两音节间的降幅较小。","自然话语,音高,下倾"
2013-06-15,基于强制对齐的层次短语模型过滤和优化,"付晓寅,魏 玮,卢世祥,徐 波","该文提出一种层次短语模型过滤和优化方法。该方法在采用传统方法训练得到层次短语规则的基础上,通过强制对齐同时构建源语言和目标语言的解析树,从中过滤并抽取对齐的层次短语规则,最后利用这些规则重新估计翻译模型的翻译概率。该方法不需要引入任何语言学知识,适合大规模语料训练模型。在大规模中英翻译评测任务中,采用该方法训练的模型与传统层次短语模型相比,不仅能够过滤50%左右规则,同时获得0.8～1.2 BLEU值的提高。","统计机器翻译,层次短语,强制对齐,模型训练"
2013-07-15,基于虚拟上下文的统计机器翻译短语表的过滤,"殷 乐,张玉洁,徐金安","在基于短语的统计机器翻译系统中,自动抽取的短语表中不可避免的包含大量的冗余和错误的短语对,这浪费了解码资源又影响翻译质量。为了缓解这个问题,该文提出一种基于虚拟上下文的过滤短语表的方法。该方法引入虚拟上下文计算短语对的得分增量;并通过计算最大和最小的短语对的得分增量,设计了一种对短语对重排序的过滤策略。我们在NTCIR-9的中英数据上进行了验证实验,结果显示,当短语表的规模下降到原来的47%时,翻译质量的BLEU值提高了0.000 5;当短语表的规模下降到原来的30%时,BLEU值仅下降0.000 6。实验结果表明,在大规模短语表的过滤中,该文的方法是有效可行的。","基于短语的统计机器翻译,短语表过滤,虚拟上下文"
2013-07-10,一种基于分类的平行语料选择方法,"王 星,涂兆鹏,谢 军,吕雅娟,姚建民","大规模高质量双语平行语料库是构造高质量统计机器翻译系统的重要基础,但语料库中的噪声影响着统计机器翻译系统的性能,因此有必要对大规模语料库中语料进行筛选。区别于传统的语料选择排序模型,本文提出一种基于分类的平行语料选择方法。通过少数句对特征构造差异较大的分类器训练句对,在该训练句对上使用更多的句对特征对分类器进行训练,然后对其他未分类句对进行分类。相比于基准系统,我们的方法不仅缩减40%训练语料规模,同时在NIST测试数据集合上将BLEU值提高了0.87个百分点。","统计机器翻译,平行语料选择"
2013-06-12,基于中英平行专利语料的短语复述自动抽取研究,"李 莉,刘知远,孙茂松","短语复述自动抽取是自然语言处理领域的重要研究课题之一,已广泛应用于信息检索、问答系统、文档分类等任务中。而专利语料作为人类知识和技术的载体,内容丰富,实现基于中英平行专利语料的短语复述自动抽取对于技术主题相关的自然语言处理任务的效果提升具有积极意义。该文利用基于统计机器翻译的短语复述抽取技术从中英平行专利语料中抽取短语复述,并利用基于组块分析的技术过滤短语复述抽取结果。而且,为了处理对齐错误和翻译歧义引起的短语复述抽取错误,我们利用分布相似度对短语复述抽取结果进行重排序。实验表明,基于统计机器翻译的短语复述抽取在中英文上准确率分别为43.20%和43.60%,而经过基于组块分析的过滤技术后准确率分别提升至75.50%和52.40%。同时,利用分布相似度的重排序算法也能够有效改进抽取效果。",
2013-07-15,汉英篇章结构平行语料库的对齐标注研究,冯文贺,"篇章结构平行语料库是对具有对译关系的双语文本标注了平行篇章结构信息的语料库。对齐标注是汉英篇章结构平行语料库的核心理论基础。该文提出“结构对齐,关系对齐”的对齐标注策略,应用于切分对齐、层次结构对齐、关系对齐、中心对齐等环节,实现了对齐和标注并行、单位对齐和结构对齐共进的平行语料库工作模式。本策略辅之以相应标注平台和工作程序以及相应难点解决方案,被证明是一种高效的篇章结构平行语料库工作方式。","平行语料库,对齐标注,篇章结构"
2013-07-15,蒙古文输入法输入码方案研究,"白双成,张劲松,呼斯勒","科学合理的输入码方案对一个输入法至关重要。通过输入码重码量分布和平均码长等量化指标,综合分析比较了蒙古文读音输入法可使用的三类七种输入码方案,提出了以音节为编码单位的支持模糊输入的输入码方案,应用于项目组新版输入法中获得推广普及。试验结果和推广应用经验表明,新输入码方案顺应人的思维和记忆的同时可保证较高的录入速度。","蒙古文,输入法,输入码,模糊输入"
2013-07-22,基于统计翻译框架的蒙古文自动拼写校对方法,"苏传捷,侯宏旭,杨 萍,员华瑞","在以国际标准编码存储的传统蒙古文电子文本中,拼写错误十分普遍。人工校对这些错误不仅速度慢而且成本高。该文提出了一种基于统计翻译框架的传统蒙古文自动拼写校对方法,将拼写校对看作是从错误词到正确词的翻译。该文使用改进的基于短语的统计机器翻译模型来构建拼写校对模型,然后对测试文本进行校对。实验结果表明,该方法可以快速、有效地校对拼写错误,而且不依赖于特定语言的语法知识。使用该方法对包含1 026个正确词、1 102个错误词的测试集进行拼写校对,校对后文本中的正确词所占比例最高可达97.55%。","蒙古文,拼写检查,拼写校对,机器翻译"
2013-07-20,维哈柯及蒙语多文种语言相似性考查研究,"王 玲,达瓦·伊德木草,吾守尔·斯拉木","该文以阿勒泰语系下的维哈柯及蒙古语多语言平行文本和语音语料为研究对象,分别对比多语言文本量化序列向量及语音声学音律特征的相似度,研究语言信息间存在的相通性。试验发现,同语系同语族黏着语言相似度较高 文本相似性达85%;声频特征相似性达95%。从而确认在同语系多种黏着语言间创建语言信息共享云模的可行性,这将有利于实现语言文本及语音信息的跨语言转换处理,极大降低少数民族语言信息处理成本。","同语系同语族语言,平行语料,声学音律特征,基频F0,相似性考查"
2013-07-25,傣文自动分词系统的设计与实现,"高廷丽,陶建华,戴红亮,李 雅","傣文自动分词是傣文信息处理中的基础工作,是后续进行傣文输入法开发、傣文自动机器翻译系统开发、傣文文本信息抽取等傣文信息处理的基础,受限于傣语语料库技术,傣文自然语言处理技术较为薄弱。本文首先对傣文特点进行了分析, 并在此基础上构建了傣文语料库,同时将中文分词方法应用到傣文中,结合傣文自身的特点,设计了一个基于音节序列标注的傣文分词系统,经过实验,该分词系统达到了95.58%的综合评价值。","傣文,分词,CRF,绝对切分词"
2013-08-01,规则与统计相结合的日语时间表达式识别,"赵紫玉,徐金安,张玉洁,刘江鸣","该文提出了一种基于自定义知识库强化获取规则集,以及规则与统计模型相结合的日语时间表达式识别方法。在按照Timex2标准对时间表达进行细化分类的基础上,我们结合日语时间词的特点,渐进地扩展重构日语时间表达式知识库,实现基于知识库获取的规则集的优化更新,旨在不断提高时间表达式的识别精准度。同时,融合CRF统计模型提高日语时间表达式识别的泛化能力。实验结果显示开放测试F1值达0.898 7。","知识库,规则集,统计模型"
2013-07-21,基于新闻语料库的越南语框架语义标注研究,林 丽,"越南是中国的重要邻国,相应的越南语海量信息处理正日益凸显其重要性。参考国内外有关框架语义标注的研究和实践,我们在构建越南语新闻语料库,对越南语文本进行分词和词性标注、命名实体标注等研究的基础上,尝试构建越南语框架语义知识库并初步探索了框架语义标注在越南语新闻事件抽取中的应用。","框架语义,标注,越南语,新闻"
2003-02-10,统计机器翻译综述,刘群,"本文综述了基于信源信道思想和基于最大熵思想的统计机器翻译方法并介绍了统计机器翻译的评测方法。基于信源信道的方法将翻译概率表示为一个语言模型和一个翻译模型。而基于最大熵的方法则是利用一系列实数值特征函数的线性组合来求解最优的译文。基于最大熵的统计机器翻译方法比基于信源信道的方法更具有一般性,后者可以看做前者的一个特例。","人工智能,机器翻译,综述,统计机器翻译,信源信道模型,最大熵方法"
2002-06-17,一种中文分词词典新机制——双字哈希机制,"李庆虎,陈玉健,孙家广","汉语自动分词是汉语信息处理的前提,词典是汉语自动分词的基础,分词词典机制的优劣直接影响到中文分词的速度和效率。本文首先分析了分词词典机制在中文分词中的重要性及已有的三种典型词典机制,并在此基础上根据汉语中双字词语较多的特点提出了一种新的分词词典机制——双字哈希机制,在不提升已有典型词典机制空间复杂度与维护复杂度的情况下,提高了中文分词的速度和效率。","计算机应用,中文信息处理,中文分词,双字哈希"
2002-10-22,一种提高中文搜索引擎检索质量的HTML解析方法,"宋睿华,马少平,陈刚,李景阳","中文搜索引擎经常会返回大量的无关项或者不含具体信息的间接项,产生这类问题的一个原因是网页中存在着大量与主题无关的文字。对使用关键字检索方法的搜索引擎来说,想在检索或者后处理阶段解决这类问题不仅要付出一定代价,而且在大多数情况下是不可能的。在这篇论文中,我们提出了网页噪声的概念,并针对中文网页的特点,实现了一种对网页自动分块并去噪的HTML解析方法,从而达到在预处理阶段消除潜在无关项和间接项的目的。实验结果表明,该方法能够在不占用查询时间的前提下100%地消除中文搜索引擎隐藏的间接项,以及大约11%的无法过滤或隐藏的无关项或间接项,从而大幅度提高检索结果的查准率。","计算机应用,中文信息处理,HTML解析,降噪,分块模型,搜索引擎"
2003-02-26,基于综合因素的汉语连续语音库语料自动选取,"康恒,刘文举","大词汇量连续语音识别系统的性能很大程度上取决于语音库的质量,而语音库设计的中心环节就是语料选取。但是传统语料选取方法往往考虑因素单一,不利于语音识别系统有效利用语言信息。本语音库的语料选取方法综合考虑了多种因素:三音子覆盖率、三音子覆盖效率、三音子稀疏度、常用词分布等,并完全实现程序自动选取,充分利用了原始语料,使选取结果的信息量更加丰富。程序自动选取结果可以覆盖94.1%的三音子,75.4%的最常用词,覆盖效率和稀疏度也比传统方法有了较大改善。","计算机应用,中文信息处理,语音库,三音子,高频词,覆盖率"
2003-02-27,连续语音识别中声学建模的组合聚类算法研究,"韩兆兵,贾磊,张树武,徐波","基于三音子连续语音识别的一个关键问题是在有限训练数据的条件下对大量声学模型参数的鲁棒性估计。为了解决这个问题,有两个主要的上下文相关的聚类算法被提出,它们是合并(Agglomerative Clustering)聚类(AGG)和决策树(Tree-based)聚类(TB) 。本文分析了这两种算法的优缺点,并分别对其进行了改进,然后提出了最大似然框架下组合聚类算法。大词汇量连续语音识别(LVCSR)的实验结果表明,和单一的决策树聚类算法比较,提出的组合聚类算法对识别率有显著的提高。","计算机应用,中文信息处理,语音识别,合并聚类,决策树聚类,声学建模"
2003-03-19,汉语语句中短语间停顿的自动预测方法,"聂鑫,王作英","在文语转换(TTS)系统中,正确标记短语间的停顿对提高合成语音的自然度起着重要作用。本文介绍了一种在汉语语句中自动预测短语间停顿的方法。首先,文本进行分词,并转换为一列由词性标记所组成的序列;然后使用马尔可夫模型,利用人工标注数据库训练词语连接处词性标注序列的概率分布和连接类型序列的距离信息,得到输入的词性标记序列对应的具有最大似然概率的连接类型序列,最后利用后处理规则进行适当的纠错。本文针对不同的模型参数进行了测试,短语间停顿自动预测的召回率和连接类型正确率分别达到了68.2%和85.1% ,取得了比较满意的结果。","计算机应用,中文信息处理,短语间停顿,词性标注,马尔可夫模型"
2002-12-03,中文语音合成系统中的文本标准化方法,"陈志刚,胡国平,王熙法","文本标准化是对输入文本进行分析,生成其中非汉字符号的拼音、节奏等信息的过程。本文提出了一种层次化的、基于外部规则的标准化方法,通过规则匹配识别这些符号,并给出各种正确信息。本文首先介绍了分析树的概念,其次给出构造规则的步骤,利用权值控制规则的匹配顺序,最后给出实验结果。实验结果表明:这种方法具有很好的易维护性和可扩展性,开放测试的正确率达到99.76%。","计算机应用,中文信息处理,文本标准化,特殊符号,外部规则"
2002-12-11,一种基于ICA的汉字信息隐秘传输方法,"陆红琳,程义民,王以孝,田源","本文描述了一种基于独立成份分析(ICA)的汉字信息隐密传输方法。该方法以彩色图像为寄主图像,对其进行ICA分解,求出其中的独立成分,再将汉字信息以编码形式,隐藏在对彩色图像质量影响最小的独立成分低位端,从而实现汉字信息的隐秘传输。该方法已经在PC机上进行了模拟,实验结果表明,该方法在保证图像质量条件下,有较高的嵌入率和较好的可靠性。","计算机应用,中文信息处理,隐秘传输,独立成份分析(ICA),彩色图像,汉字编码"
2003-03-11,词性标注中生词处理算法研究,"张孝飞,陈肇雄,黄河燕,蔡智","词性兼类是自然语言理解必须解决的一类非常重要的歧义现象,尤其是对生词的词性歧义处理有很大的难度。文章基于隐马尔科夫模型(HMM),通过将生词的词性标注问题转化为求词汇发射概率,在词性标注中提出了一种生词处理的新方法。该方法除了用到一个标注好的单语语料库外,没使用任何其他资源(比如语法词典、语法规则等),封闭测试正确率达97%左右,开放测试正确率也达95%左右,基本上达到了实用的程度。同时还给出了与其他同样基于HMM的词性标注方法的测试比较结果,结果表明本文方法的标注正确率有较大的提高。","计算机应用,中文信息处理,自然语言理解,词性兼类,隐马尔科夫模型,语料库"
2003-03-25,汉英双语语料库中名词短语的自动对应,"刘冬明,赵军,杨尔弘","本文提出了一种在汉英双语语料库句子对齐的基础上,自动进行汉英名词短语划分和对应的方法。该方法的主要特点在于在无需严格识别汉语名词短语的情况下,对高频短语和低频短语分别进行处理,对于高频短语,利用英语短语和汉语词在双语语料库中的关联信息,采用一种迭代重估算法进行双语短语的对应;对于低频短语,根据双语词典中源词和译词之间的对应信息,结合一套人工编写的句法规则进行双语低频短语的对应。该方法能够从整体上把握对应信息,并具有很高的覆盖率。","人工智能,机器翻译,名词短语识别,短语对齐,迭代重估,相似度"
2003-03-13,基于记忆的自适应汉语语言模型的研究,"曲卫民,张俊林,孙乐,孙玉芳","基于记忆的自适应语言模型虽然在一定程度上增强了语言模型对不同领域的适应性,但其假设过于简单,即认为一个在文章的前面部分出现过的词往往会在后面重复出现,它没有考虑到常用词的影响,以及不同单词间的相互影响。本文针对这一问题从两个方面对原有模型进行了改进,一是采用TFIDF公式代替了原有的简单频率统计法;二是建立了一种基于记忆的扩展二元模型,并采用权重过滤法以节省模型计算量。实验表明这两种改进在很大程度上提高了原有模型的性能,增强了模型的自适应性。","计算机应用,中文信息处理,语言模型,自适应,TFIDF公式,扩展二元模型"
2003-01-17,HNC作用效应句的汉英句类转换,"张克亮,黄曾旸","作用效应句是作用句的一个特殊子类,是HNC57组基本句类中一个极富个性的重要句类。从HNC概念网络的角度看,作用效应句主要由使役类动词和逼迫类动词直接形成,或者由一般作用类动词(含泛动类动词) 通过“得”字结构间接形成。由这三类动词形成的作用效应句遵循不同的句类转换和格式转换规则,因此在汉英机器翻译中,需要采取不同的句类转换框架,以确保译文语句句法语义结构的正确性。初步的试验表明,有关作用效应句的这些句类-格式转换规则具有很好的适用性和覆盖率。","人工智能,机器翻译,HNC理论,作用效应句,句类转换,语句格式转换,转换框架"
2003-02-21,名人网页的相关度评价,"昝红英,苏玉梅,孙斌,俞士汶","本文介绍了北京大学天网知名度系统的设计与开发工作,重点论述了中文名人网页相关度评价的因素、算法和相应的检索结果。针对目前搜索引擎服务的不足之处,该工作旨在改进网上信息服务的质量,提高个性化网上信息服务的能力。本系统在北京大学天网搜索引擎的基础上,利用自然语言处理、特别是中文信息提取的新技术,结合网页信息的特点,针对名人网页的检索提出了一种新的网页相关度评价算法,改善了检索结果排序的合理性,提高了名人网页检索服务的质量。","计算机应用,中文信息处理,相关度,检索服务,信息提取,特征信息"
2003-01-20,基于对话回合衰减的cache语言模型在线自适应研究,"何伟,李红莲,袁保宗,林碧琴","目前由于特定任务域语料的稀疏并且难以收集,这严重阻碍了对话系统的可移植性。如何利用在线收集的少量训练语料,实现语言模型的快速自适应,从而有效提高对话系统在新任务域的识别率是本文的目的所在。本文对传统cache模型修正后,提出了基于历史单元衰减的cache语言模型,以在线递增方式收集语料进行自适应,并与通用语言模型进行线性插值。在对话系统中,以对话回合为历史单元,也可称为基于对话回合衰减的cache语言模型。在两个完全不同任务域——颐和园导游与火车票订票任务域进行的实验表明,在自适应语料不到1千句时,与无自适应模型相比,有监督模式下的识别错误率分别降低了47.8%和74.0% ,无监督模式下的识别错误率分别降低了30.1%和51.1%。","计算机应用,中文信息处理,口语对话系统,语言模型,cache自适应"
2003-03-28,基于韵律特征和语法信息的韵律边界检测模型,"吴晓如,王仁华,刘庆峰","韵律短语边界的自动检测,对语音合成中语料库的韵律标注以及语音识别中韵律短语的自动划分都有重要意义。本文通过对影响韵律短语边界的声学、韵律等参量的分析,得到和韵律短语边界关联性较大的一组声学特征参数、韵律环境参数和语法信息;同时引入语音合成中的韵律预测思想,在假定所有音节边界均为非韵律短语边界时,预测每个音节的基频。最后使用决策树模型,将音节边界处的韵律环境信息、语法信息以及预测结果作为决策树的输入,利用决策树综合判定当前音节边界是否为韵律短语的边界。实验表明,这种方法对于基于确定性文本(text-dependent)的语音韵律短语边界的检测,具有较好效果,同时可以显著提高语音合成中语料库的标注效率和标注结果的一致性。","计算机应用,中文信息处理,韵律边界的自动检测,韵律预测,决策树,分类与回归树"
2003-03-18,复杂彩色文本图像中字符的提取,"陈又新,刘长松,丁晓青","从复杂彩色文本图像中提取和识别字符已经成为一个既困难又有趣的问题。本文给出了一个具有创新性和实用性的区域生长算法用于彩色图像的分割:彩色图像游程邻接算法CRAG(color run-length adjacency graph algorithm)。我们将该算法用于彩色文本图像,首先得到图像的彩色连通域,再对这些连通域的平均颜色进行颜色聚类,可得到若干个聚类中心,然后根据不同的颜色中心将图像分为相应的彩色层面,最后通过连通域分析判断所需的文字层。该生长算法修改并扩展了传统的BAG算法,并将其运用于彩色印刷体文本图像中,充分利用了彩色图像的颜色和位置信息。实验结果表明新的方法能很好的从彩色印刷图像中提取多种常见的艺术字,并具有较高的提取速度,同时保留了文字和背景图像的原始色彩,便于将来的图像恢复。","人工智能,模式识别,字符提取,图像分割,CRAG算法,区域生长,彩色文本图像"
2002-09-20,甲骨文象形码编码方法研究,"肖明,赵慧,甘仲惟","甲骨文因字形独特、年代久远,所以一直没能进行有效编码。本文吸取现代编码思想,采用模糊数学模型分析甲骨文的部件(字根)特点,对其进行模糊聚类,并使用32个字符(25个英文字母和7个阿拉伯数字)作为码元,与甲骨文中的500多个字根相对应,实现了一字一码的编码方案。在此基础上,运用信息论中的熵理论,分析了这种编码的效率和科学性,得出甲骨文编码的最佳码长大致接近于3,从而为5000多个甲骨文字进行科学编码提供理论基础。","计算机应用,中文信息处理,甲骨文,字根,象形码,模糊聚类,熵,码长"
2015-11-27,《中文信息学报》第三届编辑委员会,,,
2003-05-28,机器翻译评测的新进展,"张剑,吴际,周明","机器翻译评测对机器翻译的研究和开发具有至关重要的作用,对其的研究一直是国内外机器翻译界的重点课题。本文首先全面地介绍了最近出现的而且受到极大关注的机器翻译评测技术,即IBM公司的BLEU机器翻译评测标准和NIST采用的机器翻译评测技术。实验表明,自动翻译评测技术能够接近人工评价,评测结果也是可接受的。因此,采用自动翻译评测技术能够给自然语言处理的研究人员和开发人员带来很大的便利性。本文还展示了一个开放式的可扩展的自动翻译评测的平台,完全实现了BLEU和NIST评测标准,并做出了一定的改进使得该系统具有良好的使用性和可扩展性。","人工智能,机器翻译,自动评测"
2003-04-25,从汉语格关系表示生成日语,"戴新宇,陈家骏,王启祥","本文介绍了一个基于转换翻译的汉日机器翻译系统中日语生成子系统的设计和实现。文章首先描述了一种基于格关系的汉语依存分析树,分析树结点记录语法语义以及格关系信息;然后,针对日语的特征,分析了日语生成中的主要问题,包括译词选择、用言活用形确定、助词添加等;给出基于规则的日语生成系统的组织结构,重点介绍生成规则系统的设计和实现。最后,给出规则描述的实例以及翻译实例,提出进一步改进本系统的初步想法。","人工智能,机器翻译,格语法,汉语分析,日语生成"
2003-05-12,基于事件框架的事件相关文档的智能检索研究,"吴平博,陈群秀,马亮","在事件相关文档的检索中,事件主题的迁移和分化与相似事件的干扰是影响系统性能的两个主要因素。本文提出了一种基于事件框架知识和事件主体信息的检索方法。该方法对事件相关评价函数进行了的改进:首先,从事件语料中提炼出事件的框架知识、从事件文档中挖掘出表达事件主体的信息,然后将这些知识和信息进行向量化,最后利用向量化的结果对相关度评价函数进行优化。实验结果表明该方法是有效的,明显提高了事件相关文档的检索性能。","计算机应用,中文信息处理,智能检索,事件相关文档,事件框架,事件主体"
2003-03-24,汉语语句的自动改写,"张玉洁,山本和英","在基于转换方式的口语机器翻译中,口语的多样性和不规则性加重了转换模块的处理负担。另外,由于缺少双语语料库和懂双语的语言学家,使得翻译知识的开发很困难或成本很高。为了解决这些问题,我们提出了在翻译前对源语言的语句进行自动改写的方法,试图通过加强源语言的处理来分散转换模块的负担。本文介绍了汉日口语机器翻译系统中汉语语句改写模块的开发。作者在分析了口语句子的改写目标后,提出了基于模板匹配的改写方法和从改写语料库中获取改写模板的半自动化方法。作者还介绍了改写模块的设计与实现,以及评价试验和结果。","人工智能,机器翻译,语句改写,汉语口语,模板匹配,语句改写语料库"
2003-05-19,基于目标驱动的多层MLLR自适应算法,"穆向禹,贾磊,张树武,徐波","本文在对语音识别中基于自适应回归树的极大似然线性变换(MLLR)模型自适应算法深刻分析的基础上,提出了一种基于目标驱动的多层MLLR自适应(TMLLR)算法。这种算法基于目标驱动的原则,引入反馈机制,根据目标函数似然概率的增加来动态决定MLLR变换的变换类,大大提高了系统的识别率。并且由于这种算法的特殊多层结构,减少了许多中间的冗余计算,算法在具有较高的自适应精度的同时还具有较快的自适应速度。在有监督自适应实验中,经过此算法自适应后的系统识别率比基于自适应回归树的MLLR算法自适应后系统的误识率降低了10% ,自适应速度也比基于自适应回归树的MLLR算法快近一倍。","计算机应用,中文信息处理,语音识别,模型自适应,自适应回归树,极大似然线性变换"
2003-05-28,多字体印刷藏文字符识别,"王华,丁晓青","藏文字符识别系统是中文多文种信息处理系统的重要组成部分,但至今国内外的研究基本处于空白。本文提出了一种基于统计模式识别的多字体印刷藏文字符识别方法:从字符轮廓中抽取方向线素特征,利用线性鉴别分析(LDA)压缩降维后得到紧凑的字符特征向量。采用基于置信度分析的两级分类策略,设计了带偏差欧氏距离分类器(EDD)完成高效的粗分类,细分类采用修正二次鉴别函数(MQDF)。通过实验选取恰当的分类器参数后,在容量为177,600字符(300样本/字符类)的测试集上的识别率达到99.79%,证明了该方法的有效性。","人工智能,模式识别,藏文字符识别,方向线素特征,线性鉴别分析,带偏差欧氏距离,修正二次鉴别函数"
2002-12-24,基于智能技术的远程教育答疑系统研究,"高光来,王玉峰","网上答疑系统是现代远程教育系统中不可缺少的一部分,然而当前的答疑系统只是根据用户的输入对题库中的问题进行简单的关键词匹配,查询精度和用户界面满足不了用户的需求。针对以上缺点,本文给出一个应用语义网络原理构筑起来的智能答疑系统。文章分析了建立智能答疑系统的必要性,由此提出了一个基于限定领域的智能答疑系统模型及其技术路线,并以两门大学计算机课程作为知识库来源,实现了系统的功能。试验结果表明,本文所提出的方法有效地提高了查询精度,用户界面友好方便。","计算机应用,中文信息处理,智能答疑,智能体,语义网络,数据挖掘,远程教育,导航器"
2003-02-08,古文字字库建设的几个问题,张再兴,"随着古文字信息化处理研究的发展,古文字的标准字库建设已经显得十分迫切。本文探讨了古文字标准字库建设中需要注意的四个方面的问题:通过建立古文字资料库,穷尽性地收集整理古文字字形保证字形收集的全面性,通过拓片扫描造字保证所造字形的准确性;在字形与字之间建立对应关系时须考虑两者之间的异用、歧释、异体等复杂关系;字形归纳过程中应遵循形体的归并原则和区别原则;字符进入标准字符集时的分级应根据字频原则和形频原则。","计算机应用,中文信息处理,古文字,字库,字形"
2015-11-28,《中文信息学报》第三届编辑委员会,,,
2003-05-10,面向特定领域的汉语句法主干分析,"齐浩亮,杨沐昀,孟遥,韩习武,赵铁军","本文提出了一种面向特定领域的汉语句法主干分析方法。该方法中包括浅层句法分析、模板匹配两个关键环节,形成用模板表示的句法主干。在浅层句法分析中,本文使用了级联的隐马尔可夫模型进行了短语的归并;而后以已有的汉语句子模板为基础,进行模板匹配以达到句法主干分析的目标。在针对体育新闻领域语料的开放测试中,模板匹配的精确率和召回率分别达到了98.04%和81.43% ,句子级的精确率和召回率分别达到了96.97%、84.85% ,实验表明该方法在特定领域是有效的。","人工智能,自然语言处理,浅层句法分析,句法主干分析,模板"
2003-05-20,Web页面信息块的自动分割,"瞿有利,于浩,徐国伟,西野文人","随着Internet的发展,Web页面数量的急剧增加,如何快速有效地获取信息变得越来越重要。一类Web页面往往包含着多个信息单元,它们在展现上排列紧凑、风格相似,在HTML语法上具有类似的模式,例如一个BBS页面上多个发言,每个信息被称为一个信息块。对于信息抽取、信息过滤等应用,需要首先将原始页面中分割为若干合适的信息块以便于后续的处理。本文提出了一种自动将Web页面分割为信息块的方法:首先通过创建Web页面结构化的HMTL分析树,然后根据包含有效文本量等确定包含信息块的子树,最后根据子树深度信息利用2-rank PAT算法进行分割。通过对BBS页面的信息块抽取实验,证明了该方法的有效性。","计算机应用,中文信息处理,Web页面,信息提取,信息块"
2003-05-08,基于决策树的汉语未登录词识别,"秦文,苑春法","未登录词识别是汉语分词处理中的一个难点。在大规模中文文本的自动分词处理中,未登录词是造成分词错识误的一个重要原因。本文首先把未登录词识别问题看成一种分类问题。即分词程序处理后产生的分词碎片分为‘合’(合成未登录词)和‘分’(分为两单字词)两类。然后用决策树的方法来解决这个分类的问题。从语料库及现代汉语语素数据库中共统计出六类知识:前字前位成词概率、后字后位成词概率、前字自由度、后字自由度、互信息、单字词共现概率。用这些知识作为属性构建了训练集。最后用C4.5算法生成了决策树。在分词程序已经识别出一定数量的未登录词而仍有分词碎片情况下使用该方法,开放测试的召回率:69.42%,正确率:40.41%。实验结果表明,基于决策树的未登录词识别是一种值得继续探讨的方法。","人工智能,自然语言处理,未登录词识别,数据挖掘,决策树,C4.5算法"
2003-06-08,语料库中熟语的标记问题,"安娜,刘海涛,侯敏","熟语是自然语言中普遍存在的语言现象。本文分析了国内现有语料库对熟语的标注方式,发现这种方式对语料库的进一步加工是有问题的。为了在语料库标注阶段把熟语问题处理好,本文从信息处理的角度将熟语中的成语、惯用语、歇后语、习用语、专门语以及缩略语归为固定语的范畴,进而提出根据固定语的语法功能给定词性标记,再根据它们的词汇特征给定词汇范畴标记的双层标记法,这样在一定程度上解决了熟语的语料库标注问题。","人工智能,自然语言处理,熟语,固定语,标注,语料库"
2003-09-22,中文文本分类中特征抽取方法的比较研究,"代六玲,黄河燕,陈肇雄","本文比较研究了在中文文本分类中特征选取方法对分类效果的影响。考察了文档频率DF、信息增益IG、互信息MI、χ2分布CHI四种不同的特征选取方法。采用支持向量机(SVM)和KNN两种不同的分类器以考察不同抽取方法的有效性。实验结果表明,在英文文本分类中表现良好的特征抽取方法(IG、MI和CHI)在不加修正的情况下并不适合中文文本分类。文中从理论上分析了产生差异的原因,并分析了可能的矫正方法包括采用超大规模训练语料和采用组合的特征抽取方法。最后通过实验验证组合特征抽取方法的有效性。","计算机应用,中文信息处理,文本自动分类,特征抽取,支持向量机,KNN"
2003-02-17,基于邻接矩阵全文索引模型的文本压缩技术,"陶晓鹏,胡运发","基于不定长单词的压缩模型的压缩效率高于基于字符的压缩模型,但是它的最优符号集的寻找算法是NP完全问题,本文提出了一种基于贪心算法的计算最小汉字平均熵的方法,发现一个局部最优的单词表。这种方法的关键是将文本的邻接矩阵索引作为统计基础,邻接矩阵全文索引是论文提出的一种新的全文索引模型,它忠实地反映了原始文本,很利于进行原始文本的初步统计,因此算法效率得以提高,其时间复杂度与文本的汉字种数成线性关系,能够适应在线需要。并且,算法生成的压缩模型的压缩比是0.47,比基于字的压缩模型的压缩效率提高25%。","计算机应用,中文信息处理,邻接矩阵,文本压缩,压缩模型,基于不定长单词的Huffman编码"
2003-08-06,信息抽取模式自动生成方法的研究,"郑家恒,王兴义,李飞","模式匹配是信息抽取系统通常使用的方法,如何生成信息抽取模式就成为信息抽取的关键问题。由于手工编写模式的代价太大,本文尝试采用聚类方法自动生成针对中文文本的信息抽取模式。通过计算模式实例间的相似度,采用单链法聚类,将模式实例划分为不同的类别,每个类别对应一个模式,将同一类别中的模式实例进行合并就可以得到最终的信息抽取模式。以农作物信息文本为实验语料,进行了聚类测试,错分率与漏分率分别为0.21%和1.07%,合并后的模式覆盖了人工分析提出的25类中的24类。","人工智能,自然语言处理,信息抽取,模式匹配,信息抽取模式"
2004,第2届学生计算语言学研讨会(SWCL 2004),,"“学生计算语言学研讨会”是由中国中文信息学会发起的系列学术会议,其目的旨在培养青年计算语言学科技工作者,其特色是全部活动完全由学生自己组织。继“第1届学生计算语言学研讨会(SWCL2002)”于2002年8月在北京大学计算语言学研究所成功召开之后,“第2届学生计算语言学研讨会(SWCL2004)”将由北京语言大学信息科学学院承办,拟于2004年暑假期间在北京语言大学举行。",
2003-06-10,EBMT系统中的多词单元翻译词典获取研究,"程洁,杜利民","EBMT系统是一种基于语料库的机器翻译方法,其主要思想是通过类比原理进行翻译。如何从语料库中提取出一个实用的翻译词典进行系统的辅助翻译已经越来越多的引起关注。本文探讨了如何结合阈值和关联度提取的方法获取多词单元翻译词典,在这两种方法中,阈值提取受主观影响太大,关联值提取效率太低,都不能很好的满足翻译词典提取的要求。本文提出的算法利用阈值提取出备选多词单元,其中提出了四点规则弱化主观影响且保证全面覆盖所有多词单元,降低了阈值本身所带来的不精确度的影响,然后对计算结果进行三层过滤,进一步提高了准确率;该算法还合并了单词译成多词单元和多词单元互译两部分词典的提取,提高了工作效率。","人工智能,机器翻译,EBMT,翻译词典,多词单元"
2003-07-16,灰度名片图像快速倾斜检测和校正方法,"卜飞宇,刘长松,丁晓青","本文针对名片OCR系统的要求,提出了一种新的根据图像扫描时产生的黑色边缘来检测灰度名片图像倾斜角度的方法。该方法先检测出名片的四条边缘拟合直线,由四条边缘拟合直线的倾斜角度来确定名片图像倾斜角度,然后采用逐段整块搬移的方法来对图像进行倾斜校正,再根据边缘拟合直线位置去除黑边。实验表明,该方法具有很快的速度和很高的正确率,是一种实用价值较高的方法。而且,该方法能推广应用于其它灰度和彩色扫描图像的倾斜检测和校正。","人工智能,模式识别,灰度图像,倾斜检测,倾斜校正,边缘拟合直线"
2015-11-28,首届全国少数民族青年计算语言学家学术研讨会,,"随着国家西部大开发政策的实施和政府上网工程的启动,以及电子商务的发展,西部地区少数民族语言信息处理工作越来越显得迫切和重要。少数民族语言信息处理起步晚、进度慢、人力财力投入少,远远不能满足社会发展的需求,数字鸿沟不断加大,引起了国家相关部门和领导层的高度重视。要缩短社会需求和研究进展的差距,就必须加大投入,指定和完善政策措施。据我们了解国家这方面新的政策措施即将出台。在这种环境下召开一次全国性的、以民族语言信息处理为内容的专题会议,将对我国的民族语文信息处理工作起到很大的推动作用。",
2003-06-17,分段模型在解码假设检验中的应用,"张翼燕,刘文举,徐波","本文主要研究了分段模型(以参数轨迹模型为例)在解码假设检验中的应用。分段模型与传统的HMM相比,具有更加精确的建模能力。多年来人们一直致力于研究它对语音识别性能的提高,而忽视了其它方面的应用。本文提出了分段模型校验的方法,对HMM的识别结果进行二次处理,克服了传统方法在不同句子间不具有可比性的缺点,简单而有效;在此基础上,为了满足系统的特殊要求,训练Fisher分类器,选择分段模型而非HMM的N-Best信息作为特征输入,验证了分段模型得分作为可信度指标时的优秀区分能力。实验结果表明,在第一类错误率为5%的情况下,最好的第二类错误率可以降到25.265%。这体现了系统良好的拒识性能。","人工智能,自然语言处理,解码假设检验,分段模型,参数轨迹模型"
2003-08-11,基于遗传径向基神经网络的声音转换,"左国玉,刘文举,阮晓钢","声音转换技术可以将一个人的语音模式转换为与其特性不同的另一个人语音模式,使转换语音保持源说话人原有语音信息内容不变,而具有目标说话人的声音特点。本文研究了由遗传算法训练的RBF神经网络捕获说话人的语音频谱包络映射关系,以实现不同说话人之间声音特性的转换。实验对六个普通话单元音音素的转换语音质量分别作了客观和主观评估,结果表明用神经网络方法可以获得所期望的转换语音性能。实验结果还说明,与K-均值法相比,用遗传算法训练神经网络可以增强网络的全局寻优能力,使转换语音与目标语音的平均频谱失真距离减小约10%。","人工智能,自然语言处理,声音转换,RBF神经网络,遗传算法,线谱频"
2015-11-28,《中文信息学报》征稿简则,,一、《中文信息学报》主要刊登中文信息的基础理论、应用技术、中文信息处理系统及设备、中文信息的自动输入和人工编码输入、汉字字形信息、自然语言处理、计算语言学及民族语言文字信息处理及网上信息处理等方面的研究论文、技术报告、综述、通讯、简报、国内外学术活动等。,
2003-07-25,基于SVM的中文组块分析,"李珩,朱靖波,姚天顺","基于SVM(support vector machine)理论的分类算法,由于其完善的理论基础和良好的实验结果,目前已逐渐引起国内外研究者的关注。和其他分类算法相比,基于结构风险最小化原则的SVM在小样本模式识别中表现较好的泛化能力。文本组块分析作为句法分析的预处理阶段,通过将文本划分成一组互不重叠的片断,来达到降低句法分析的难度。本文将中文组块识别问题看成分类问题,并利用SVM加以解决。实验结果证明,SVM算法在汉语组块识别方面是有效的,在哈尔滨工业大学树库语料测试的结果是F=88.67%,并且特别适用于有限的汉语带标信息的情况。","计算机应用,中文信息处理,支持向量机,结构风险最小化,文本组块"
2003-05-11,Web信息检索结果融合中的按位加权插入合并算法,"张敏,金奕江,马少平","在Internet中,由于海量数据的多样性,在分布式数据集合上进行有效的检索就成为Web信息检索的一种必要方式。由此,引出多个检索结果的融合问题。对不同检索结果的相似度评分可能完全不可比的情况,本文给出一种新的解决方案:按位加权插入合并算法。在18GB的大规模web标准测试集上的实验证明,该算法始终能够提高综合检索性能,且分布数据集检索结果越好,则合并后性能改善越多。其中系统平均精度提高接近10%,突破了传统方法对分布数据集结果合并的综合效果总是低于使用集中数据集检索的性能局限。","计算机应用,中文信息处理,Web信息检索,数据集选择,结果融合,基于排序的融合"
2003-09-05,OpenE:一种基于n-gram共现的自动机器翻译评测方法,"孙连恒,杨莹,姚天顺","在机器翻译研究领域中,评测工作发挥着重要的作用,它不仅仅是简单地对各个系统输出结果进行比较,它还对关键技术的发展起到了促进作用。译文质量的评测工作长期以来一直以人工的方式进行。随着机器翻译研究发展的需要,自动的译文评测研究已经成为机器翻译研究中的一个重要课题。本文讨论了基于n-gram共现的自动机器翻译评测框架,介绍了BLEU、NIST、OpenE三种自动评价方法,并通过实验详细分析了三种方法的优缺点。其中的OpenE采用了本文提出了一种新的片断信息量计算方法。它有效地利用了一个局部语料库(参考译文库)和全局语料库(目标语句子库)。实验结果表明这种方法对于机器翻译评价来说是比较有效的。","人工智能,机器翻译,机器翻译评测,信息量计算,n-gram共现"
2003-12-01,一种改善的基于语言模型的中文检索系统研究,"张俊林,曲为民,孙乐,孙玉芳","最近几年提出的语言模型检索系统将语音识别领域的语言模型技术引入信息检索领域并改善了检索系统的性能,但是其隐含的词汇间相互独立的假设并不符合实际情况。尽管统计翻译模型考虑了词汇间的同义词因素,但是由于它没有考虑词汇上下文信息,所以对于解决多义词词义的区分并无帮助。我们提出了触发语言模型检索方法来改善这一状况,通过训练语料得到词汇在一定上下文中的相关比率,同时利用查询条件所含词汇计算触发词汇集合来区别查询条件词汇的具体含义并将相关参数引入文档语言模型形成触发语言模型。实验结果表明我们提出的这个方法显著改善了检索系统的性能,与经典语言模型方法相比,触发语言模型方法的平均查准率提高了约12%,召回率提高了10.8%。","计算机应用,中文信息处理,语言模型,信息检索,触发"
2003-08-06,汉语语料词性标注自动校对方法的研究,"钱揖丽,郑家恒","兼类词的词类排歧是汉语语料词性标注中的难点问题,它严重影响语料的词性标注质量。针对这一难点问题,本文提出了一种兼类词词性标注的自动校对方法。它利用数据挖掘的方法从正确标注的训练语料中挖掘获取有效信息,自动生成兼类词词性校对规则,并应用获取的规则实现对机器初始标注语料的自动校对,从而提高语料中兼类词的词性标注质量。分别对50万汉语语料做封闭测试和开放测试,结果显示,校对后语料的兼类词词性标注正确率分别可提高11.32%和5.97%。","计算机应用,中文信息处理,兼类词,汉语词性标注,自动校对,粗糙集"
2003-08-11,基于对话语音的与文本无关的说话人确认系统的研究,"陈雁翔,戴蓓倩,周曦,李辉","本文建立了一个基于对话语音的与文本无关的说话人确认系统,它和传统的与文本无关的说话人确认系统的关键不同在于,训练及测试语音不再只包含一个人而都是对话语音,因此需要分割出属于不同说话人的语音段,以建立说话人模型和实现最终判决。文中详细介绍了高斯混合模型-背景模型(GMM-UBM)这种说话人确认系统的框架,重点讨论了基于GLR(Generalized Likelihood Ratio)距离测度的无监督语音分割算法。最终阐述的输出评分的规整方法即ZNORM(Zero Normalization)和持续时间修正,可以使确认系统的性能提高近10%。","计算机应用,中文信息处理,对话语音,GLR距离测度,无监督语音分割"
2003-11-21,语音合成中的韵律关联模型,"吴志勇,蔡莲红","基于大规模语音数据库的文语转换系统(Text-to-Speech , TTS)中,如何选取合适的语音基元是提高合成语音自然度的重要因素。本文研究了连续语流中的韵律关联现象,提出了包含韵律关联参数的汉语韵律特征参数集,基于数据挖掘中的关联规则模型(Association Rules Model)建立韵律关联模型,并将该模型应用于基元选取。实验表明,该方法有效地利用了语音基元的韵律及关联信息,符合人耳的知觉感受,使得合成语音自然度的主观评测MOS(Mean Opinion Score)得分与不考虑韵律关联时的结果相比提高了12.22%(3.49/3.11)。","计算机应用,中文信息处理,文语转换,基元选取,韵律关联"
2003-12-07,基于不对称性的相似汉字识别方法,"孙羽菲,陈艳,张玉志","相似字识别的正确与否对整个识别系统的准确性和可用性都有着极大的影响。在实际应用中,我们发现相似汉字之间的误识存在不对称性,并对这种不对称现象的成因进行了细致的探讨和分析。基于这种不对称性,本文提出了一种分类的部分空间方法来解决相似字的识别问题。相似字按其结构特点被分成若干基本类别,不同类别在相应的部分空间提取不同的特征进行比较,以达到正确识别相似字的目的。实验结果表明了本方法的有效性,相似字识别的准确性得到了很大的提高,其中易错相似字的识别正确率平均提高了4.55个百分点,不易错相似字的识别正确率平均提高了0.38个百分点。","人工智能,模式识别,不对称性,相似汉字识别,部分空间法,分类"
2003-03-02,快速中文字符串模糊匹配算法,"陈开渠,赵洁,彭志威","本文解决了中文字符串模糊匹配的两个主要问题:空间问题和时间问题。目前字符串模糊匹配的两个主要方法是位向量方法和过滤方法。由于汉字众多,应用位向量方法时,需要大量空间。对于某些内存很少的小型计算机,比如嵌入式系统,这将会是一个问题。本文改进了位向量方法,使其在应用于中文字符串时,空间需求降低到约5%。本文还利用汉字非常多的特点,提出一种新的基于过滤方法的中文字符串模糊匹配算法,BPM-BM,其速度比世界上最快的算法至少提高14%;在大部分情况下,是其速度的1.5～2倍。","计算机应用,中文信息处理,字符串匹配,模糊匹配,中文字符串匹配"
2003-10-13,多文种环境下汉字内码识别算法的研究,"李培峰,朱巧明,钱培德","汉字内码向ISO/IEC 10646过渡是实现计算机用文字编码统一的必然趋势,但目前在一段时间内仍将存在多种汉字内码并存的情况,所以实现汉字内码的自动识别是保证汉字多内码并存的关键。本文主要探讨了如何在多内码并存的多文种环境中实现汉字内码自动识别的问题,并提供了多种汉字内码识别算法,包括基于内码分布、标点符号特征、字频特征和语义特征的识别算法等。在此基础上,本文对不同的识别算法进行分析和评估。在对目标样本的测试中,以上算法的识别率最高可以达到99.9%以上。","计算机应用,中文信息处理,多文种环境,汉字内码,识别算法"
2003-09-03,信息技术名词定名的系统分析方法与评价指标体系,"王有志,赵敏,陈俊峰","在多年学习、使用、翻译与参与评审几种规范IT名词集的基础上,本文将系统分析方法用于对此类名词集的定名与评价。其基本方法,一是从名词集中拆分出基础要素——名词元;二是提出八项量化指标:印误率,英语拼写不一致与不规范率,非必要的一多与多一对应率,英汉名词不对等率,与交叉学科名词有异率,与国标定名不符率,收词欠完备与冗余率及总体值得修榷率。并以目前收录最全的IT规范名词集为例,通过名词元对这些量化指标进行了模拟计算,计算结果证明该方法是合理可行的。这种方法原则上也适用于自然科学技术其他学科的名词或术语定名。","计算机应用,中文信息处理,科技名词,英汉名词定名,信息技术,IT,系统分析"
2015-11-29,《中文信息学报》第三届编辑委员会,,,
2004-01-13,基于本体的跨语言信息检索模型,"王进,陈恩红,张振亚,王煦法","随着网络信息的日益丰富和用户需求的提高,人们已经不能满足于仅仅在同一语种中进行检索,跨语言的信息检索(CLIR)因而受到人们越来越多的关注。为此,本文提出了一种新的基于语义的跨语言信息检索模型Onto-CLIR,该模型在传统信息检索技术的基础上,利用本体来刻画不同语言中对应的领域知识,以解决从查询语言到检索语言之间转换过程中出现的语义损失和曲解等问题,从而保证在检索过程中能够有效地遵循用户的查询意图,获得预期的检索信息。本文以体育新闻检索为背景,以英文查询作为查询请求,检索来自新浪网的体育类新闻,结果表明采用基于本体的跨语言信息检索方法之后检索的查全率和查准率平均提高10个百分点左右,有效地改善了检索性能。","计算机应用,中文信息处理,本体,跨语言信息检索,语义"
2003-05-20,面向多语言的机器翻译支撑环境设计与实现,"魏勇鹏,陈群秀","在日汉机器翻译系统由DOS移植到Windows环境后,针对进一步扩充资源和调试开发过程中遇到的词典管理不便、工具使用麻烦、翻译及编辑界面不友好、日志维护不完善等问题,我们开发了面向多语言的机器翻译支撑环境子系统。该子系统实现了词典资源管理、翻译界面、开发工具集成、系统日志维护等功能,在编码上兼容Unicode,在调用原系统翻译功能和开发工具时使用动态链接库技术,以期成为一个面向多语言的、可为不同机器翻译系统所用的开发调试支撑环境。","人工智能,机器翻译,支撑环境,多语言"
2003-11-03,中文文本分类中的特征选择研究,"周茜,赵明生,扈旻","本文介绍和比较了八种用于文本分类的特征选择方法,其中把应用于二元分类器中的优势率改造成适用于多类问题的形式,并提出了一种新的类别区分词的特征选择方法,结合两种不同的分类方法:文本相似度方法和Na?ve Bayes方法,在两个不同的数据集上分别作了训练和测试,结果表明,在这八种文本特征选择方法中,多类优势率和类别区分词方法取得了最好的选择效果。其中,当用Na?ve Bayes分类方法对各类分布严重不均的13890样本集作训练和测试时,当特征维数大于8000以后,用类别区分词作特征选择得到的宏F1值比用IG作特征选择得到的宏F1值高出3%～5%左右。","计算机应用,中文信息处理,文本分类,特征选择,类别区分词"
2004-01-31,基于网络的中文问答系统及信息抽取算法研究,"崔桓,蔡东风,苗雪雷","问答系统(Question Answering System)能用准确、简洁的答案回答用户用自然语言提出的问题。目前多数问答系统利用大规模文本作为抽取答案的知识库,而网络上丰富的资源为问答系统提供了另外一种良好的知识来源,对于回答简短、基于事实的问题非常有效。本文对基于网络的问答系统研究现状作了简要的介绍,分析了网络信息的特点。我们提出了一种基于语句相似度计算的答案抽取方法,在此基础上实现了一个基于网络的中文问答系统。该系统只利用网络搜索引擎返回结果中的摘要部分作为答案抽取的资源,从而节省了下载、分析网络源文本的时间。实验结果表明该系统对人名、数量及时间类型的问题效果显著,对测试问题集的MRR值达到0.51。","计算机应用,中文信息处理,问答系统,句子相似度,信息抽取"
2003-12-02,面向信息处理的语境形式化研究,"李德华,刘根辉","在自然语言处理研究领域中,句法研究已经取得了可喜的进展,语义研究也日益受到重视。但要真正实现计算机理解自然语言的目标,还必须进一步深入开展语用分析研究,目前还很少有这方面的研究成果。语境是语用学研究中的重要内容,本文首先讨论了现代语言学关于语境的定义,对计算语言学中的语境作了科学的界定,然后给出了语境及其相关概念的形式定义,并结合汉语实例进行了分析。最后指出“计算语用学”这一计算语言学领域中的新兴学科将大有可为。","人工智能,自然语言处理,语境,语境形式化,计算语用学"
2003-10-09,对偶性概念的HNC阐释,"李颖,池毓焕","本文首先从计算语言学的角度对传统语义学和古典哲学进行了反思,提出了对偶性概念思想,并指出,区分两类对偶(黑氏对偶与非黑氏对偶)对自然语言处理中揭示概念之间关联性有重要意义;然后对两类对偶的内涵分别进行了范定,特别是非黑氏对偶的12种子类给出了详细的定义;接着从语言概念空间和对偶空间的相互映射中,说明了对偶性概念在HNC概念基元表示中的地位。这些多侧面多角度的对偶性概念阐释,有利于对偶性概念在自然语言处理中的应用。","人工智能,自然语言处理,HNC理论,黑氏对偶,非黑氏对偶"
2003-12-22,基于知识图的汉语基本名词短语分析模型,"张瑞霞,张蕾","本文提出了一种基于知识图的汉语baseNP分析模型。它以知识图为知识表示方法,利用《知网》为语义知识资源,采用以语义为主、语法为辅的策略,先为短语中的每一个实词构造“词图”,然后合并“词图”而组成“短语图”,最后得到一个关于汉语baseNP结构信息和语义信息的知识图。因此它不仅分析了汉语baseNP结构的内部句法关系,而且分析了汉语baseNP结构成分间的语义关系并以知识图的形式表示出了这种语义关系。实验结果表明这个模型对于汉语baseNP的分析是有效的。","人工智能,自然语言处理,知识图,知网,基本名词短语"
2003-05-26,嵌入式汉语TTS系统的设计与实现,"刘涛,叶振兴,蔡莲红","针对手持设备和PDA存储量较小的特点,本文提出了基于音节基频包络特征、采用k中心点算法聚类裁减音库容量的方法。聚类结果的听辩实验和统计分析表明此算法可以保证聚类内部音节样本的相似性及类间样本的相异性。经过对汉语可选合成基元的分析,系统中首次引入声韵母半音节与音节作为混合基元,构造了基于混合基元的音库。经过对样本集分别聚类裁减,进一步压缩了音库容量,并在PDA平台上实现了嵌入式TTS系统。","计算机应用,中文信息处理,嵌入式,汉语语音合成系统,音节聚类,混合基元"
2003-06-30,说话人自适应训练方法在连续语音识别中的应用,"罗骏,欧智坚,王作英","自适应技术在近年来得到越来越多的重视,其中应用广泛的包括MAP、MLLR,该技术利用少量特定人数据就可以调整码本,快速地提升识别性能,它要求原始的码本有很好的说话人无关性。本文介绍了结合MLLR自适应的说话人自适应训练(Speaker Adaptive Training,以下简称SAT)算法,这种方法将每个说话人码本视为说话人无关码本经过线性变换的结果,在此基础上训练的说话人无关码本更有效剔除了说话人相关信息,因此在说话人自适应中时能根据特定数据调整更好地逼近说话人特性,从而有更好的性能表现。","计算机应用,中文信息处理,自适应,MLLR,SAT"
2003-10-30,一种手写体大写金额串的分割新方法,"陈强,吕俊洋,夏德深","手写体大写金额串的分割将直接影响识别的准确率。为了提高分割的准确率,同时保证较快的分割速度,本文采用了由粗分割和细分割组成的两步分割方法。重点介绍交叉字符和相连字符的分割方法。对于交叉的字符提出了加窗处理的中点连线分割方法,它较其它方法具有简单准确的优点;对于单笔相连的字符,先在细化字符图象上找到候选笔划的候选分割点,然后用本文提出的简明的评价准则来确定最优分割点,提高了粗分割的精度。上述方法应用于银行支票手写体大写金额的分割,取得了很好的分割效果。","人工智能,模式识别,手写体大写金额串,字符分割,最优分割路径"
2003-08-13,基于多元激励的高质量语音合成声学模型,"陶建华,康永国","传统的参数语音合成系统,多采用单纯的源滤波模型,缺少变化,通常导致在韵律变化较大或生成特定语气时,音质损伤较大。本文则在语音逆滤波过程的基础上,对声源在不同韵律特征和音色条件下的变化进行了仔细的比较分析,通过声源的重构、分类,进而形成了适用于多种韵律特征和音色特征的多元激励(Multi - Source , MS)模型。在此基础构建了基于多元激励的语音合成的声学模型,在一定意义上较大的提高了语音合成在大范围语气变化中的合成质量,对个性化语音合成,以及超小型语音合成系统的建立起到了较好的推动作用。","计算机应用,中文信息处理,语音合成,声学模型,声源,多元激励"
2003-12-25,利用主语和谓语的句法关系识别谓语中心词,"李国臣,孟静","谓语中心词识别对于整个句子的句法分析起着重要的作用。目前已有的谓语中心词识别方法,利用谓语中心词候选项的静态语法特征和动态语法特征来确定谓语中心词。在此基础上,本文提出一种利用句子的主语和谓语之间的句法关系来识别谓语中心词的方法。该方法除了利用谓语中心词候选项的静态语法特征和动态语法特征外,还利用主谓语之间的句法关系识别谓语中心词。实验表明,与传统方法相比,这种方法对谓语中心词的识别正确率可以提高3%左右。","人工智能,自然语言处理,谓语中心词识别,主谓语之间的句法关系"
2004-04-02,一种改进的基于记忆的自适应汉语语言模型,"张俊林,孙乐,孙玉芳","基于记忆的自适应语言模型虽然在一定程度上增强了语言模型对不同领域的适应性,但其假设过于简单,即认为一个在文章的前面部分出现过的词往往会在后面重复出现。通过对一些文本的观察分析,我们认为作者在书写文章的时候,除了常常使用前文中出现过的词汇外,为了避免用词单调,还会在行文过程中使用前文出现过词汇的近义词或者同义词。另外,一篇文章总是围绕某个主题展开,所以在文章中出现的许多词汇往往在语义上有很大的相关性。我们对基于记忆的语言模型进行了扩展,利用汉语义类词典,将与缓存中所保留词汇语义上相近或者相关的词汇也引入缓存。实验表明这种改进在很大程度上提高了原有模型的性能,与n元语言模型相比困惑度下降了4011% ,有效地增强了语言模型的自适应性。","人工智能,自然语言处理,语言模型,自适应,同义词词林,困惑度"
2004-05-11,汉语中的零形回指及其在汉英机器翻译中的处理对策,"侯敏,孙建军","回指是语篇衔接的重要手段,零形回指是汉语中常见的一种回指形式。由于汉语、英语是不同类型的语言,因此零形回指对汉英机器翻译会产生一定的影响。本文详细分析了汉语零形回指的确认、类型、产生的原因及使用的条件,指出其对汉英机器翻译造成的主要障碍是生成的英语句子在结构上不合语法,并提出在句组层面上解决问题的算法。","人工智能,机器翻译,汉英机器翻译,零形回指,句组"
2004-05-26,基于词类串的汉语句子结构相似度计算方法,"王荣波,池哲儒","句子相似度的衡量是基于实例机器翻译研究中最重要的一个内容。对于基于实例的汉英机器翻译研究,汉语句子相似度衡量的准确性,直接影响到最后翻译结果的输出。本文提出了一种汉语句子结构相似性的计算方法。该方法比较两个句子的词类信息串,进行最优匹配,得到一个结构相似性的值。在小句子集上的初步实验结果表明,该方法可行,有效,符合人的直观判断。","人工智能,机器翻译,基于实例机器翻译,汉英机器翻译,句子相似度衡量,自然语言处理"
2004-05-10,从搭配知识获取最优种子的词义消歧方法,"全昌勤,何婷婷,姬东鸿,刘辉","基于统计的词义消歧模型的一个关键问题是如何自动从语料库中获取指示词,虽然通过学习初始搭配实例能够在语料库中获取更多的搭配知识,但人工获取质量较好的初始搭配是比较困难的,并且无法保证有效的扩大搭配知识。针对该问题,提出了通过机器学习初始搭配实例获取最优种子,再由最优种子扩增更多指示词,最后利用这些指示词实现具有多个义项的多义词消歧。采用该方法对8 个多义词进行消歧的测试实验中取得了8717 %的平均正确率。","人工智能,自然语言处理,自然语言处理,词义消歧,搭配,种子优选"
2004-05-21,基于向量空间模型的文本分类系统的研究与实现,"陈治纲,何丕廉,孙越恒,郑小慎","文本分类是信息处理的一个重要的研究课题,它可以有效的解决信息杂乱的现象并有助于定位所需的信息。本文综合考虑了频度、分散度和集中度等几项测试指标,提出了一种新的特征抽取算法,克服了传统的从单一或片面的测试指标进行特征抽取所造成的特征“过度拟合”问题,并基于此实现了二级分类模式的文本分类系统。和类中心分类法相比,实验结果表明二级分类模式具有较高的精度和召回率。","计算机应用,中文信息处理,文本分类,测试指标,特征抽取,二级分类模式"
2004-05-07,信息检索策略性能的云模型评价方法,"康海燕,李彦芳,林培光,樊孝忠","在信息检索中,目前常见的评价方法仅能反映检索策略的平均性能,不能反映策略的稳定性、随机性等问题,因此对检索策略的评价不够全面。本研究提出了基于云模型的检索策略评价方法,该方法建立了定性评价和定量数据之间的自然转换,这种转换是通过严格的数学方法来实现的,用该方法评价检索策略,不仅能反映策略的平均性能,而且能反映策略的稳定性。实验数据表明,该方法是切实可行的,评价结果更加逼近实际情况。该方法也可以用于文本分类策略的评价。","计算机应用,中文信息处理,信息检索,云模型,策略性能评价"
2004-02-19,自然语言文本水印,"张宇,刘挺,陈毅恒,赵世奇,李生","本文主要介绍了基于自然语言处理的文本水印技术,也即自然语言文本水印技术。该技术是在不改变文本原意的前提下,将需要隐藏的文本信息(水印信息) 插入到原始文本中的一种信息隐藏技术。这种技术对于确认信息来源和信息的秘密传送,以及版权维护等方面都有着很大的应用价值。本文首先给出了基于自然语言处理技术的文本水印的概念、特点及攻击模型,并对文本水印的研究现状进行了分析。通过分析可以看出,自然语言文本水印技术有着更好的灵活性,并且在适度的攻击下,不会破坏水印信息。本文详细介绍了文本水印系统的设计过程,包括该技术的基础数学理论- 二次余数理论。最后详细介绍了两种自然语言文本水印嵌入方法,分别是基于句法分析和基于语义的水印嵌入方法。","人工智能,自然语言处理,文本水印,二次余数,本体语义"
2004-04-16,现代藏字全集的属性统计研究,"高定国,龚育昌","藏文基本属性的研究是藏文信息处理技术的基础,现代藏字的研究是藏文信息处理的重点。藏字全集是有限集,为了更好地研究现代藏字,本文以现代藏字为研究对象,按照现代藏文文法的规律,对全部现代藏字用计算机辅助统计了藏字全集的个数、藏字的字长、藏字的结构方式、位置特征、字符频度以及所有现代藏字中的整基字丁,并且简要地分析了这些数据。这些数据可以较全面地反映现代藏字的本质特征,可为藏文研究和藏字信息处理提供基础数据。","计算机应用,中文信息处理,藏字全集,藏字结构,藏字频度"
2004-03-24,汉语口语对话系统中语义分析的消歧策略,"刘蓓,杜利民","框架语义分析是目前汉语口语对话系统中常用的语义解析方法,本文分析了语义分析过程中容易产生的两种典型歧义现象- 结构歧义和语义关系歧义。并针对这两种歧义结构,分别提出基于语义PCFG模型的结构歧义消歧策略以及基于语义期待模型EM的语义关系歧义消歧策略,并给出了有效的消歧算法。实验结果表明综合运用本文提出的消歧策略后,基线系统理解模块的句子语义分析正确率大大提高,从原来的7517 %上升到9115 % ,而且标志语义单元理解率的三项指标,准确率,召回率和精度也平均提高了10 %。","计算机应用,中文信息处理,口语对话系统,语义分析,消歧,算法"
2004-04-20,多项式回归的汉语时长预测模型,"孙璐,胡郁,王仁华","时长信息是韵律的重要组成部分,对于语音合成的自然度和可懂度都有不可忽视的作用。时长预测是建立对时长有影响的韵律环境与自然语流中音段时长的对应关系。本文引入了统计学中etasquared 的概念研究汉语中韵律环境因素对时长的影响,设计了残差算法定量分析属性之间的交互作用,由此建立了多项式回归的汉语时长预测模型。实验结果表明,使用5～6 个韵律属性基本上就能够建立比较相关的对应关系,和使用同样韵律属性的Wagon 回归树的效果相比有明显的优势。","计算机应用,中文信息处理,时长建模,多项式,交互作用"
2004-07-21,基于凸包像素比特征的粘连汉字切分,"魏湘辉,马少平","汉字切分正确与否直接影响了汉字识别系统的识别率,粘连汉字则是切分中的难点。本文将基于背景细化的切分方法应用于《四库全书》的两字符粘连汉字数据集,并针对其中切分路径选择问题,提出了一种新特征- 凸包像素比,反映了在不同切分路径下汉字结构变化的特性。实验结果表明该特征对多种分类器均能有效地提高切分路径选择的正确率。其中在使用基于高斯混合模型分类器时取得了8816 %正确率。","人工智能,模式识别,粘连汉字,汉字切分,背景细化,凸包"
2004-03-20,进一步的“正易全”——三级汉字编码输入法,张小衡,"本文报告“正易全”汉字输入法的新进展。从整体上来讲,正易全已发展成为全字笔顺、全字笔组和2 21 笔组三级输入法系列。前两级简单灵活,键选率极低,方便大字集查检;第三级在常用字和通用字中表现极佳,适合日常快速打字。在编码技术上,多笔笔组码元的选用、单结构的定义和多结构字的二部划分等方面都作了进一步的简化、系统化和规律化。此外,码表在GB1300011 字符集的基础上增加了1164 个港澳台地区用字或字形。","计算机应用,中文信息处理,汉字输入,字形码,笔组"
2004-05-09,综合型语言知识库的建设与利用,"俞士汶,段慧明,朱学锋,张化瑞","语言知识库的规模和质量决定了自然语言处理系统的成败。经过18年的努力,北京大学计算语言学研究所已经积累了一系列颇具规模、质量上乘的语言数据资源:现代汉语语法信息词典,大规模基本标注语料库,现代汉语语义词典,中文概念词典,不同单位对齐的双语语料库,多个专业领域的术语库,现代汉语短语结构规则库,中国古代诗词语料库等等。本项研究将把这些语言数据资源集成为一个综合型的语言知识库。集成不同的语言数据资源时,必须克服它们之间的“缝隙”。规划中的综合型语言知识库除了有统一的友好的使用界面和方便的应用程序接口外,还将提供支持知识挖掘的工具软件,促使现有的语言数据资源从初级产品形式向深加工产品形式不断发展;提供多种形式的知识传播和信息服务机制,让综合型语言知识库为语言信息处理研究、语言学本体研究和语言教学提供全方位的、多层次的支持。","计算机应用,中文信息处理,语言处理,语言知识库,语言数据资源,电子词典,语料库"
2004-03-04,语料库词性标注一致性检查方法研究,"张虎,郑家恒,刘江","在对大规模语料库进行深加工时,保证词性标注的一致性已成为建设高质量语料库的首要问题。本文提出了基于聚类和分类的语料库词性标注一致性检查的新方法,该方法避开了以前一贯采用的规则或统计的方法,利用聚类和分类的思想,对范例进行聚类并求出阈值,对测试数据分类来确定其标注的正误,进而得出每篇文章的词性标注一致性情况,进一步保证大规模语料库标注的正确性。","计算机应用,中文信息处理,词性标注一致性,兼类词,聚类"
2004-04-22,基于统计的网页正文信息抽取方法的研究,"孙承杰,关毅","为了把自然语言处理技术有效的运用到网页文档中,本文提出了一种依靠统计信息,从中文新闻类网页中抽取正文内容的方法。该方法先根据网页中的HTML 标记把网页表示成一棵树,然后利用树中每个结点包含的中文字符数从中选择包含正文信息的结点。该方法克服了传统的网页内容抽取方法需要针对不同的数据源构造不同的包装器的缺点,具有简单、准确的特点,试验表明该方法的抽取准确率可以达到95%以上。采用该方法实现的网页文本抽取工具目前为一个面向旅游领域的问答系统提供语料支持,很好的满足了问答系统的需求。","计算机应用,中文信息处理,网页数据抽取,包装器"
2004-01-11,汉语隐喻理解的逻辑描述初探,"张威,周昌乐","隐喻在语篇中出现非常普遍,是语言认知和计算机语篇理解中重要的一环。但无论是其逻辑基础或实验系统的探索都处于初级阶段。本文从逻辑角度给出了隐喻逻辑的定义、建构和性质。同时,针对汉语文本中名词性隐喻、动词性隐喻等类型,细化了隐喻逻辑的规则,并利用细化后的规则,分析了隐喻句中隐含信息的发掘方法,为隐喻的计算提供了新的解决方案。分析的结果表明,本文所提出的隐喻逻辑对汉语隐喻意义的生成有很好的解释能力,为后续隐喻句的计算机处理提供了理论基础。","人工智能,自然语言处理,隐喻逻辑,隐喻理解"
2004-02-16,一种基于Web的汉英CAPP应用研究及开发,"杨雨图,叶文华,王宁生","经济全球化要求CAPP支持远程设计和产品数据共享,而传统的CAPP系统只支持本地设计,生成的工艺文件也是单一语言的,通用翻译软件又不能直接被CAPP系统所使用,因此对开发基于WEB的中英文工艺设计和计算机辅助翻译软件提出需求。本文在分析国内外计算机辅助翻译(computer aided translation , CAT)的现状和工艺语言特点的基础上,对中英文工艺编制和机助翻译系统的总体结构、功能及关键技术的实现方法做了一些探索,提出了一种基于WEB的汉英CAPP系统。在此基础上,针对某航空企业的特点开发了一套工艺编制与计算机辅助翻译软件,并取得很好的使用效果。","计算机应用,中文信息处理,计算机辅助工艺设计,计算机辅助翻译,全球制造,敏捷制造"
2004-04-15,一种符合ISO14651语义的藏文排序实现方法,"林河水,程伟,曹晖,李文波,吴健,孙玉芳","本文介绍了一种实现藏文字典序排序的方法,它针对藏文“大字丁字符集”编码方案。通过引入有(无)前加基字符的概念,它把待排序的藏字预处理为有(无)前加基字符、前加字符、基字(基字符或者字丁)、后加字符、再后加字符串后,再行比较,从而避免拆分字丁。本实现方法符合ISO/IEC14651标准语义。","计算机应用,中文信息处理,藏文,字典序,机器排序"
2004-02-24,自然场景文本定位,"欧文武,朱军民,刘昌平","随着自然场景文本识别研究的不断深入,建立标准的场景文本图像库和了解该领域的研究现状变得越来越重要。为此,2003年国际文档分析和识别大会专门建立了一个这样的图像库,并组织了自然场景文本识别比赛,我们参加了其中的自然场景文本定位分赛。本文对我们参加这次比赛的算法做了介绍并给出了比赛结果,在文章最后,对参赛算法做了比较,指出了场景文本定位的发展现状。","人工智能,模式识别,文本定位,边缘密度,字符识别,图像处理"
2004-03-16,基于最大熵模型的韵律短语边界预测,"李剑锋,胡国平,王仁华","语音合成系统中,由于韵律短语边界预测的水平不高,阻碍了合成语音自然度的进一步提高。本文根据韵律短语边界预测的特点,提出了基于最大熵模型的预测方法。为考察该方法的能力,在较大规模的数据集上,使用相同的属性集,对比了其与主流的决策树方法的预测效果。还考察了词面信息的贡献,以及选择特征时的不同阈值对最大熵模型的影响。实验表明,使用相同的属性信息,最大熵方法比传统的决策树方法在F-Score上有5.5%的提高,加入了词面信息的最大熵模型则有9.4%的提高。最后指出,最大熵模型相当于一个带权重的规则系统,可以很好的解决规则冲突问题。","计算机应用,中文信息处理,韵律短语边界预测,最大熵,决策树"
2003-08-10,基于语音增强失真补偿的抗噪声语音识别技术,"丁沛,曹志刚","本文提出了一种基于语音增强失真补偿的抗噪声语音识别算法。在前端,语音增强有效地抑制背景噪声;语音增强带来的频谱失真和剩余噪声是对语音识别不利的因素,其影响将通过识别阶段的并行模型合并或特征提取阶段的倒谱均值归一化得到补偿。实验结果表明,此算法能够在非常宽的信噪比范围内显著的提高语音识别系统在噪声环境下的识别精度,在低信噪比情况下的效果尤其明显,如对-5dB的白噪声,相对于基线识别器,该算法可使误识率下降67.4%。","计算机应用,中文信息处理,语音增强,倒谱均值归一化,并行模型合并,语音识别"
2003-08-11,噪音环境下基于高阶谱的端点检测算法,"王卓,苏牧,李鹏,徐波","本文将现有的各种端点检测方法分为鲁棒特征,特征滤波,模型匹配三种方法,并列举了各种端点检测算法,分析不同算法的优缺点。在此基础上,深入分析了信号域噪声与语音的本质区别,引入了数学上的高阶累计量的思想,建立了基于高阶谱的高维抗噪特征,利用轴向积分映射的方法将高维高阶谱空间转换为一维空间,利用非平凡谱点的非线性组合建立一维的高阶谱特征,同时并建立基于抗噪特征的完善的搜索算法。大量实验证明该算法在各种噪音,不同信噪比的条件下都取得了非常好的效果。","计算机应用,中文信息处理,鲁棒特征,高阶累计量,轴向积分映射"
2004-08-11,汉英双语混合声学建模方法研究,"于胜民,张树武,徐波","本文从直接合并汉英双语的phone set入手,对三种不同的汉英双语混合声学建模方法进行了研究。这三种方法分别是: (1) 直接合并二者的phone set进行声学建模; (2) 基于IPA映射的统一声学表示; (3) 对汉英双语的Phone进行自动合并聚类。实验结果表明,方法(1)的声学模型较为鲁棒,但是建模单元也最多,模型不够紧凑;方法(2)具有紧凑的模型,但是鲁棒性较差;方法(3)以较少的Phone进行双语混合声学建模,不仅保持了(2)中模型紧凑的特点,而且基本达到(1)的识别率;特别是当使用声学似然度准则时,英语的识别率甚至超过了方法(1)。","计算机应用,中文信息处理,语音识别,声学建模,汉语双语,合并聚类,似然度"
2004-07-28,面向Internet的中文新词语检测,"邹纲,刘洋,刘群,孟遥,于浩,西野文人,亢世勇","随着社会的飞速发展,新词语不断地在日常生活中涌现出来。搜集和整理这些新词语,是中文信息处理中的一个重要研究课题。本文提出了一种自动检测新词语的方法,通过大规模地分析从Internet上采集而来的网页,建立巨大的词和字串的集合,从中自动检测新词语,而后再根据构词规则对自动检测的结果进行进一步的过滤,最终抽取出采集语料中存在的新词语。根据该方法实现的系统,可以寻找不限长度和不限领域的新词语,目前正应用于《现代汉语新词语信息(电子)词典》的编纂,在实用中大大的减轻了人工查找新词语的负担。","计算机应用,中文信息处理,新词语,自动检测"
2004-03-02,结合决策树方法的中文姓名识别,"王振华,孔祥龙,陆汝占,刘绍明","中文姓名识别是自然语言处理中专名识别的一个重要的子问题,本文将中文姓名的识别过程细分为三个步骤:抽取阶段、分类阶段和消歧阶段。利用中文姓和名的用字概率信息,在文本中抽取潜在的中文姓名,以及其相关的上下文词法、语法和语义特征,并将潜在姓名是否是真实姓名的判别看作是两分类问题,并利用决策树算法来实现初步判别,最后消除初步判别结果中的歧义现象。实验结果表明,该方法的召回率和准确率都可达到90%以上。","人工智能,自然语言处理,中文姓名识别,决策树,自然语言处理"
2004-05-10,基于词汇吸引与排斥模型的共现词提取,"郭锋,李绍滋,周昌乐,林颖,李胜睿","共现词提取在信息挖掘和自然语言处理中有着十分重要的地位。而传统的共现词提取方法仅仅局限在单一的一种统计量上,其结果十分不精确,需要人工再进行整理。本文提出了一种基于词汇吸引与排斥模型的共现词提取算法,并通过将多种常用统计量进行组合,改进了算法的效果。在开放测试环境下,所提取的共现词其用户感兴趣度为60.87%。将该算法应用于基于Web的共现词检索系统,在速度和共现词的提取精度上均取得了比较好的效果。","计算机应用,中文信息处理,共现词,词汇吸引与排斥模型,共现距离"
2004-03-31,基于小规模语料库和机器可读词典的二元分布语义获取,"郝秀兰,杨尔弘","本文提出了一种基于小规模语料库和机器可读词典(Machine Readable Dictionary ,MRD)的无指导的动词语义获取方法。该方法不需要使用有义项标注的语料库,而是使用从语料中获得的V+N搭配以及MRD中多义词定义的应用实例中获得的知识。使用两种方法解决数据稀疏问题:首先,将词的相似性度量由直接共现扩展到共现词的共现,以共现聚类而不是共现词来计算词的相似度。其次,从MRD定义中获取名词的IS- A关系。通过这些方法,即使两个词不共享任何词,也可认为是相似的。实验表明,该方法可从很小规模的语料中获取知识,并在不限制词义的情况下达到85.7%的正确排歧率。","人工智能,自然语言处理,机器可读词典,二元分布,语义,知识获取"
2004-04-08,结合类频率的关联中文文本分类,"钱铁云,王元珍,冯小年","该文提出一种词类频率和关联中文文本分类相结合的算法ARCTC。此算法将文档视作事务,关键词视作项,并针对文本事务的特性,提出利用词的类频率筛选与分类相关性不大的词汇,然后将改进的关联规则挖掘算法用于挖掘项和类别间的相关关系。挖掘出的规则用于形成类别特征词的集合,可用来和类标号未知文档的词的集合求交集,交集元素个数最多者即为所分类别。实验证明,该算法在提高训练时间和测试时间的同时具有较好的召回率、准确率和F-Measure。","计算机应用,中文信息处理,基于关联的分类,中文文本分类,词类频率,类别特征词集合"
2004-03-22,基于HNC理论的句法结构歧义消解,张克亮,"歧义消解是自然语言理解和处理所面对的核心问题。基于词组和短语的消歧不能保证消歧结果的正确,歧义的成功消解基于对语境或上下文(context)的正确理解。HNC理论采取的概念基元化、层次化、网络化、形式化策略以及在此基础上建立的句类和句式体系,为自然语言的歧义消解提供了最大的可能。基于HNC理论的歧义消解的总体原则是,以语句为基础,充分利用语句语境提供的句类知识,采取宏观消歧与微观消歧相结合的策略。对于经典句法歧义结构V+NP1+的+NP2,本文描述了其三重性歧义性质,并提出了三条准则和十个推论以实现对其歧义的消解。","人工智能,自然语言处理,HNC理论,句法结构歧义,V+NP1+的+NP2,消解策略,消解准则"
2004-03-09,语言工程的软件体系结构研究综述,"冯冲,陈肇雄,黄河燕","语言工程的软件体系结构已经逐渐发展成为语言工程的主要研究领域之一。它面向通用的自然语言应用,为其提供架构层次的参考方案。研究内容涵盖与体系结构相关的计算资源、语言资源、方法和应用等多个方面。在一定意义上,可以把它看作是在语言工程领域内的特定领域软件体系结构(DSSA)。本文概要介绍了该领域的发展历程和研究意义,然后对其基本概念和当前主要研究进展进行了阐述和分析,并展望了进一步的发展趋势。","人工智能,自然语言处理,综述,语言工程,软件体系结构,自然语言处理"
2002-08-24,维吾尔语词切分方法初探,"古丽拉·阿东别克,米吉提·阿布力米提","维语词的词干-词附加成分切分、音节切分的规律对维吾尔语自然语言处理方面提供更多方便。本文提出了以“词=词根+附加成分”结构。维语附加成分种类繁多,连接形式各式各样,在句子中起着非常重要的作用,同时有相当的规律性。本文提出了维语中可能出现的基本语音规律的处理方法,如:语音同化、音节切分、语音和谐规律处理。本文对维文词的词法和语音法结构进行了归纳,提出了维语词切分的一些规律和实现方法。以新疆高校学报为语料来测试,对规则词准确率达到95%。","人工智能,自然语言处理,维吾尔语,词干,词附加成分,切分"
2004-08-01,上海普通话与普通话元音系统的声学特征对比研究,"于珏,李爱军,王霞","本文通过对单音节字的声学测量及分析,对比上海市的地方普通话和普通话的元音系统,从而为汉语口语处理基本元音系统提供可靠的声学参数。发现:1.受上海话自身元音系统的影响,上海普通话男女声学元音图都表现出一定的外延性。2.上海普通话无论男女,在声学元音图上的分布都有很大的重叠区。3.从共振峰模式图上看:上海普通话[y ,i]的F1 - F2 距离都较标准普通话的大,其中的共振峰模式几乎接近于。41 多数发音人的[?]都或多或少表现出双元音化趋势。","计算机应用,中文信息处理,上海普通话,标准普通话,声学元音图,共振峰模式,口音"
2003-08-01,嵌入式语音识别系统的研究和实现,"方敏,浦剑涛,李成荣,台宪青","本文首先给出了一种适合于在嵌入式平台上实现的可变命令集的非特定人语音识别系统,同传统的基于PC的非特定人语音识别系统相比,该系统具备内存消耗小,运算速度快的优点。然后给出了该语音识别系统在多种嵌入式平台上的实现和评估结果,论证了非特定人语音识别系统在嵌入式平台上实现的可行性及其对硬件的最低配置要求,在技术层次上分析了目前实现高性能语音识别SOC的主要问题和困难,并指出了今后相关的研究方向。","计算机应用,中文信息处理,嵌入式平台,非特定人语音识别,语音识别SOC"
2003-08-09,分级语音识别研究,"徐明星,杨大利,吴文虎","分级识别的策略在模式识别领域中提出相当长的时间了。尽管人类可以训练地使用这个策略进行识别,但对语音识别而言,缺少一个有效的系统化的方法来实现它。本文给出了我们最近在这方面做的一些研究工作,使用了子空间划分原理来实现一个分级识别器,并用树型结构来组织多个识别器。实验结果表明,该方法与传统方法相比,误识率降低10%。我们将在未来的研究工作中,测试全部汉语音节,并将该方法扩展到连续语音识别。","计算机应用,中文信息处理,语音识别,分级识别,空间划分"
2015-11-30,《中文信息学报》征稿简则,,一、《中文信息学报》主要刊登中文信息的基础理论、应用技术、中文信息处理系统及设备、中文信息的自动输入和人工编码输入、汉字字形信息、自然语言处理、计算语言学及民族语言文字信息处理及网上信息处理等方面的研究论文、技术报告、综述、通讯、简报、国内外学术活动等。,
2005-05-11,基于语料库的高频最大交集型歧义字段考察,"李斌,陈小荷,方芳,徐艳华","交集型歧义是中文分词的一大难题,构建大规模高频最大交集型歧义字段(MOAS)的数据库,对于掌握其分布状况和自动消歧都具有重要意义。本文首先通过实验指出,与FBMM相比,全切分才能检测出数量完整、严格定义的MOAS,检测出的MOAS在数量上也与词典规模基本成正比。然后,在4亿字人民日报语料中采集出高频MOAS14906条,并随机抽取了1354270条带有上下文信息的实例进行人工判定。数据分析表明,约70%的真歧义MOAS存在着强势切分现象,并给出了相应的消歧策略。","计算机应用,中文信息处理,最大交集型歧义字段,全切分,强势切分"
2005-05-03,面向商务信息抽取的产品命名实体识别研究,"刘非凡,赵军,吕碧波,徐波,于浩,夏迎炬","市场信息化使得商务信息抽取、市场内容管理日益成为信息科学领域的一个研究热点。产品命名实体识别作为其中非常重要的关键技术之一也逐渐受到人们的关注。本文面向商务信息抽取对产品命名实体进行了定义并系统分析了其识别任务的特点和难点,提出了一种基于层级隐马尔可夫模型(hierarchical hidden Markov model)的产品命名实体识别方法,实现了汉语自由文本中产品命名实体识别和标注的原型系统。实验表明,该系统在电子数码和手机领域均取得了令人满意的实验结果,对产品名实体、产品型号实体、产品品牌实体整体识别性能的F值分别为79.7% ,86.9% ,75.8%。通过和最大熵模型相比较,验证了HHMM对于处理多尺度嵌套序列有更强的表征能力。","计算机应用,中文信息处理,产品命名实体识别,商务信息抽取,层级隐马尔可夫模型"
2005-05-07,基于HowNet的词汇语义倾向计算,"朱嫣岚,闵锦,周雅倩,黄萱菁,吴立德","在互联网技术快速发展、网络信息爆炸的今天,通过计算机自动分析大规模文本中的态度倾向信息的技术,在企业商业智能系统、政府舆情分析等诸多领域有着广阔的应用空间和发展前景。同时,语义褒贬倾向研究也为文本分类、自动文摘、文本过滤等自然语言处理的研究提供了新的思路和手段。篇章语义倾向研究的基础工作是对词汇的褒贬倾向判别。本文基于HowNet,提出了两种词汇语义倾向性计算的方法:基于语义相似度的方法和基于语义相关场的方法。实验表明,本文的方法在汉语常用词中的效果较好,词频加权后的判别准确率可达80%以上,具有一定的实用价值。","计算机应用,中文信息处理,态度分类,语义倾向,知网"
2005-05-20,基于多策略优化的分治多层聚类算法的话题发现研究,"骆卫华,于满泉,许洪波,王斌,程学旗","话题发现与跟踪是一项评测驱动的研究,旨在依据事件对语言文本信息流进行组织利用。自1996年提出以来,该研究得到了越来越广泛的关注。本文在研究已有成熟算法的基础上,提出了基于分治多层聚类的话题发现算法,其核心思想是把全部数据分割成具有一定相关性的分组,对各个分组分别进行聚类,得到各个分组内部的话题(微类) ,然后对所有的微类再进行聚类,得到最终的话题,在聚类的过程中采用多种策略进行优化,以保证聚类的效果。基于该算法的系统在TDT4中文语料上进行了测试,结果表明该算法属于目前结果最好的算法之一。","计算机应用,中文信息处理,话题发现与跟踪,分治多层聚类,系统聚类"
2006,中文语言资源联盟,,"在国家高科技研究规划发展项目(863)和国家重点基础研究发展规划项目(973)以及其他项目的支持下,由中国中文信息学会语言资源建设和管理工作委员会发起,由中文语言(包括文本、语音、文字等)资源建设和管理领域的科技工作者自愿组成了中文语言资源联盟(英文译名Chinese Linguistic Data Consortium ,缩写为CLDC),该联盟是学术性、公益性、非盈利性的社会团体。本团体隶属于中国中文信息学会,接受中国中文信息学会语音资源建设和管理工作委员会的业务指导和监督管理。",
2005-05-13,现代藏语动词的句法语义分类及相关语法句式,江荻,"本文突破了传统藏文文法关于动词分类的简单描述,建立起以句法语义为纲要的动词类别和相关句法规则。本文区分了藏语12大类动词,各类动词都有不同论元数量和不同句法性质的要求。因此,动词的句法语义类别划分能够较细致和全面反映各种类型藏语句式的语法结构框架,包括句子的语序、词格标记和句法助词。动词的句法语义分类结果可以直接应用于藏语语法信息词典的构建,是藏语计算处理的重要基础。","计算机应用,中文信息处理,藏语,动词句法语义分类,句法结构,句法标记"
2004-11-02,汉语自动分词和词性标注评测,"杨尔弘,方莹,刘冬明,乔羽","本文介绍了2003年“863中文与接口技术”汉语自动分词与词性标注一体化评测的一些基本情况,主要包括评测的内容、评测方法、测试试题的选择与产生、测试指标以及测试结果,并对参评系统的切分和标注错误进行了总结。文中着重介绍了测试中所采用的一种柔性化的自动测试方法,该方法在一定程度上克服了界定一个具体分词单位的困难。同时,对评测的结果进行了一些分析,对今后的评测提出了一些建议。","计算机应用,中文信息处理,自动分词,词性标注,评测"
2004-09-28,基于Multigram语言模型的主动学习中文分词,"冯冲,陈肇雄,黄河燕,关真珍","分词是中文处理中的重要基础问题。为了克服Web文本分析中传统方法在适应繁杂的专业领域和多变的语言现象时存在的困难,本文以无督导分词方法为基本框架,使用EM算法建立n元multigram语言模型,提出了一种基于置信度的主动学习分词算法,使得系统在主要利用大量未标注数据的同时,还能够主动选择少量最有价值的数据提交人工标注。实验结果表明算法性能优于相关的几种无督导分词算法。","计算机应用,中文信息处理,分词,无督导机器学习,主动学习,EM算法"
2004-09-21,基于双层级联文本分类的简历信息抽取,"于琨,管刚,周明,王煦法,蔡庆生","本文提出了一种基于双层级联文本分类的方法,用于简历信息的自动抽取。本方法将简历文本分解为文本块和文本串,并将简历中包含的信息分解为概要信息与详细信息。首先对简历文本中的文本块进行切分与分类,抽取出概要信息,然后选择可能包含详细信息的文本块,将其切分为文本串,再通过对文本串的分类抽取出详细信息。对1200份中文简历的实验结果表明,本方法适用于简历信息的自动抽取和管理。","计算机应用,中文信息处理,信息抽取,文本分类,简历管理"
2005-01-10,基于模式分类的汉语时态确定方法研究,"林达真,李绍滋","汉语时态是中文信息处理领域的一个难点。基于规则的处理方法在无时态特征词的句子,多时态特征词的句子处理等方面存在很大问题。本文从统计的角度,提出一种基于模式分类的时态确定方法,该方法综合评价句子中每个词对时态确定所作的贡献,能够处理无时态特征词的句子和多时态特征词的句子,并且该方法使用线性判别函数,具有对多维数据分析,训练与判别速度快的特性。在开放测试环境下,对单句的汉语时态确定正确率与召回率分别为79.8%和95.3%。","计算机应用,中文信息处理,汉语,时态,特征词,线性判别函数,感知器准则函数"
2004-12-28,基于知网的文本推理,"石晶,戴国忠","文本推理在自然语言处理的应用中占有极为重要的位置,本文介绍了基于知网的一种推理方法,该方法以语义网络的形式表示知网中的知识,利用“标记传递”实现推理。其特点是引入构造-融合模型的思想,动态生成知识结构,有引导地在文本词汇间建立推理路径。利用16种推理类的实例对其进行测试,结果表明在有足够上下文的条件下,该方法能够得出较为理想的推理,并且代价不高。","计算机应用,中文信息处理,文本推理,构造-融合模型,标记传递,语义网"
2004-12-02,手写中文信封的地址行字符切分算法,"韩智,刘昌平,殷绪成","在手写体中文信封处理系统中,地址行字符切分是实现地址行识别的关键步骤。本文根据邮政信封地址行字符的特点,有针对性的提出了一种字符切分算法。首先对地址行图像利用投影、求连通区域、笔划穿越数分析等基于字符结构的方法进行初始切分,得到基本字段序列;然后通过对相邻的基本字段进行组合形成多条候选切分路径,再通过识别的可信度和邮政目标地址库的先验知识信息对路径进行评价分析,从而得到最优的切分路径。该算法经过邮政分拣机采集的实际信封图像测试,纯地址行识别正确率达到78.61%,地址行识别与邮政编码识别相结合的分拣正确率达到95.42%。","人工智能,模式识别,邮政信封地址,脱机手写体汉字,字符切分,OCR"
2015-11-30,北京大学软件与微电子学院与北京大学计算语言学研究所联合新建语言技术系,,"中国第一个以语言信息处理技术领域的工程硕士为培养目标的语言技术系即将在北京大学软件与微电子学院成立。这是11月12日在北京大学召开的“语言学研究手段现代化问题学术研讨会”上,北京大学计算语言学研究所的俞士汶教授宣布的。",
2004-10-20,噪声环境下的鲁棒性说话人识别,"白俊梅,张世磊,张树武,徐波","在实际应用中,噪声或信道干扰导致说话人识别(SR)识别性能急剧下降。针对该问题,本文分析传统方法的优缺点并提出相应的系统解决方案:采用维纳滤波对语音信号进行前端处理;以MFCC声道特征结合基频(F0)韵律特征来提高识别系统的鲁棒性。实验结果表明:维纳滤波能有效地消除噪声影响;经维纳滤波处理后,使得F0-MFCC联合模型能更好的区分说话人。可以看出在噪声环境下系统的综合性能得到很大改善。","计算机应用,中文信息处理,说话人辨认,维纳滤波,F0-MFCC"
2015-11-30,汉语韵律词F0曲线的优化,"刘浩杰,杜利民","汉语韵律词内部音节重音的强弱对总的F0曲线的特征有很大影响。本文参考生成F0曲线的数学优化模型,提出了对由孤立单音节调型曲线串接而成的汉语韵律词的F0曲线的连续性、平滑性、曲线形状、平均值进行整体优化的x2估计方法,实现了在重音作用下的F0曲线的优化。在谐波+噪声合成系统上实验研究了汉语三音节韵律词的64种不包含轻声的调型组合和10种结尾为轻声的调型组合的F0曲线的优化效果,展示优化过程中三个控制参数——平滑因子(smooth)、音节重音强度(stress)、音节F0形状失真度(Distor-tion)对F0曲线整体形状的控制效果和参数取值的有效范围。非正式的听觉实验表明合成语音的自然度有明显提高。","计算机应用,中文信息处理,语音合成,F0曲线,优化,x2估计"
2003-11-05,基于规则的自动分类在文本分类中的应用,"李渝勤,孙丽华","文本自动分类是指将文本按一定的策略归于一个或多个类别中的应用技术。本文首先介绍三种基于统计的自动分类技术(k近邻分类器、支持向量机分类器和朴素贝叶斯分类器),剖析了基于统计的自动分类的优势及不足。基于统计的自动分类的不足主要表现为:当类别之间分类特征的交叉变大时,分类精度呈下降趋势,在多层分类的情况下,此局限尤为突出。针对此局限性,为了提高自动分类的精度,我们引入了基于规则的自动分类来对其进行改进和扩充,并整合两种自动分类技术的优点,设计出了混合分类器系统,从而获得了比较理想的分类效果。","计算机应用,中文信息处理,文本挖掘,文本分类,规则分类"
2004-04-09,半结构化中文信息检索中查询结果相关度算法的研究,"曲卫民,孙乐,孙玉芳","本文研究了对富含文本信息的XML数据进行基于关键字的查询时,查询结果与查询条件之间相关度的计算问题,分析了利用传统信息检索技术解决该问题时存在的一些不足,提出了一种基于节点的动态的关键字权重计算法,以及综合考虑关键字在查询结果中的频率分布特征和结构分布特征的查询结果相关度计算法,有效解决了XML数据中的结构信息对相关度计算的影响,实验证明本文中的方法取得了较好的检索性能。","计算机应用,中文信息处理,XML,信息检索,相关度算法"
2004-03-18,基于转换的时间-事件关系映射,"王昀,苑春法","近些年来,中文时间信息抽取和处理已经变得越来越重要。然而,很少有研究者关注中文文本中事件信息所对应的时间信息的识别和分析。本文的目的就是确定文本中时间信息和事件信息之间的映射关系。区别于传统的基于规则的方法,本文采用了一种机器学习的方法—基于转换的错误驱动学习—来确定事件相应的时间表达,这种学习算法可以自动的获取和改进规则。使用训练得到的转换规则集后,系统的时间-事件映射错误率减少了9.74%,实验结果表明本系统对基于规则的方法有很好的改进效果。","计算机应用,中文信息处理,时间信息处理,基于转换的错误驱动学习,信息抽取"
2004-03-30,基于互连网的术语定义获取系统,"许勇,荀恩东,贾爱平,宋柔","文中介绍了一个实验性的基于互联网的术语定义获取系统,可以方便、迅速的从互连网上查找术语的定义以及与定义有关的内容,给用户迅速获得新生术语以及新技术词汇的定义方面的知识提供方便。系统采用一组术语定义的语言学模式,以多线程方式高效下载网页,并从中匹配符合术语定义模式的文本段落,再经一定后续处理,形成返回给用户的结果。系统中使用的语言学模式是在一定量的科技期刊语料库中获取的。试验结果表明系统的运行效率高,结果的准确度比较令人满意。","人工智能,自然语言处理,术语定义,信息抽取"
2004-03-17,灰度图像中字符切分方法的研究,"陈艳,孙羽菲,张玉志","字符切分目前已经成为限制OCR技术发展的瓶颈,对于图像质量较差、中英文混排和背景色变化的文本图像,传统切分方法造成的切分错误使得文字识别率大大降低。针对这些问题,本文提出了新型文字切分方法。该方法先将灰度图像的灰度值进行分级处理,再根据分级连通域的概念把整个图像构造成树状结构,然后确定主层次级别,根据一定的规则在部分节点上进行合并、分割等进一步处理,最后得到最优的切分结果。实验结果表明,该方法能够取得比常规切分方法更好的切分效果。","人工智能,模式识别,字符切分,灰度图像,OCR"
2003-09-09,基于规则库的汉字输入法自动评测系统的设计,"张玉华,周克兰","汉字编码输入法是汉字输入电脑的主要方式。对输入法进行科学评价,从而帮助软件开发人员和输入法用户进行自我改进或评估,有其十分积极的意义。本文在实际应用基础上,提出了通过汉字输入系统输入规则库的建立,在选定的输入法状态下,通过计算机自动模拟汉字输入得到输入法码本,并以码本为基础根据信息技术国家标准完成输入法性能自动评价的思路。","计算机应用,中文信息处理,码本,规则库,自动评测系统"
2004-01-11,一种计算汉字串之间相关程度的新方法,"曹娟,周经野","本文提出了一种能更准确的反映两个汉字串之间相关程度的新概念——黏结度,并给出了其计算方法。该方法把需要计算相关程度的汉字串放在一个大环境中进行讨论,通过加入上下文信息来提高分词的准确度;另外,该方法在引用汉字词频时,增加了对动态词频的考虑,可以自动识别未登陆的专业词汇。文中同时给出了黏结度在分词领域中的应用实例。通过与前人提出的相关信息的方法相比较,这种计算方法能够解决分词中一些难于解决的问题并提高分词的精确度。","计算机应用,中文信息处理,黏结度,相关信息,分词"
2004-02-18,《信息处理用GB13000.1字符集汉字部件规范》在输入法应用中的难点讨论,张小衡,"《信息处理用GB13000.1字符集汉字部件规范》对于规范汉字形码输入法具有非常重要的意义。然而,在实际运用上却存在着部件数量太大,部件定义难以操作,部件拆分组合不易掌握等难处。造成困难的原因主要有: (1) 基础部件主要靠列表来确定, (2) 部件强调按理切分和成字组合, (3) 过多依赖“组字能力”的判别, (4) 过分注重部件数量的限制。要走出“难”的困境,应该在现有规范的基础上根据汉字的形态特征制定出简便可靠的部件识别规则和切分规则。实验证明,这种方法是行之有效的。","计算机应用,中文信息处理,汉字输入,汉字部件,规范"
2003-08-10,自然言语的韵律组织中的不确定性及其在语音合成中的应用,初敏,"本文对自然言语的韵律组织中的不确定性及其对合成语音自然度的影响进行了初步探讨,并在此基础上,提出在韵律预测中用最小错误概率准则代替传统的最大生成概率准则,从而在预测结果中保留多种等价的韵律实现。本文还进一步提出一种将基于最小错误准则的韵律预测与单元选择结合的算法,首先根据最小错误准则在所有候选单元中筛选出最不可能造成韵律错误的样本,然后再依据最平滑拼接准则从各种韵律等价的路径中选出一条能达到最平滑拼接的作为最后输出。","计算机应用,中文信息处理,言语,韵律的不确定性,单元选择,最小错误准则"
2003-08-20,盲人用计算机软件系统中的语音和自然语言处理技术,"庄丽,包塔,朱小燕","本文介绍了智能技术与系统国家重点实验室开发的“北极光”盲人用计算机软件系统中涉及的语音和语言处理技术。该系统能够获取和分析需要反馈的屏幕信息,通过语音合成平台将其内容朗读出来,对用户进行语音提示;与汉语自动分词、语言模型等自然语言处理技术的结合,使系统能够进行汉字和盲文的转换,反馈信息可以通过盲文点显器输出,使用户能够摸读盲文点字来获取所需要的信息,用户也可以采用盲文输入法进行输入,输入结果可转换为汉字文本形式。","计算机应用,中文信息处理,语音合成,文本分析,汉语自动分词,语言模型"
2003-08-11,多模式汉语连续语音识别中视觉特征的提取和应用,"刘鹏,王作英","本文对在汉语多模式汉语语音识别系统中利用视觉特征进行了研究,给出了基于多流隐马尔科夫模型(Multi-stream HMM, MSHMM)的听视觉融合方案,并对有关视觉特征的两项关键技术:嘴唇定位和视觉特征提取进行了详细讨论。首先,我们研究了基于模板匹配的嘴唇跟踪方法;然后研究了基于线性变换的低级视觉特征,并与基于动态形状模型的特征作了比较;实验结果表明,引入视觉信息后无噪环境下语音识别声学层首选错误率相对下降36.09%,在噪声环境下的鲁棒性也有明显提高。","计算机应用,中文信息处理,多模式,听-视觉融合,视觉特征提取,鲁棒性"
2004-06-20,实体关系自动抽取,"车万翔,刘挺,李生","实体关系抽取是信息抽取领域中的重要研究课题。本文使用两种基于特征向量的机器学习算法,Winnow 和支持向量机(SVM) ,在2004 年ACE(Automatic Content Extraction) 评测的训练数据上进行实体关系抽取实验。两种算法都进行适当的特征选择,当选择每个实体的左右两个词为特征时,达到最好的抽取效果,Winnow和SVM算法的加权平均F-Score 分别为73108 %和73127 %。可见在使用相同的特征集,不同的学习算法进行实体关系的识别时,最终性能差别不大。因此使用自动的方法进行实体关系抽取时,应当集中精力寻找好的特征。","计算机应用,中文信息处理,实体关系抽取,ACE 评测,特征选择"
2004-06-10,利用未标注语料改进实体名识别性能,"陈宁昱,周雅倩,黄萱菁,吴立德","本文主要介绍了一个利用最大熵进行实体名识别的系统以及所采用的模型和选取的特征。这些特征包括单词本身的词法词态特征和上下文信息。利用这些在任何语言的文本上都极易获得的特征,我们采用最大熵分类器构建了一个基准系统。在此基础上,我们首先通过网络资源建立了实体名词典知识库;并利用词典和基准系统在未标注语料上抽取出现的实体名作为辅助的训练语料;最后再将这些语料加入训练。实验结果表明,辅助的训练语料能够在一定程度上提高系统的性能。","计算机应用,中文信息处理,实体名识别,最大熵,未标注语料"
2004-06-15,短语结构制导的范畴表达式演算,"赵章界,白硕","对于语言表达式的组成成分及它们间的关系的刻画,目前大多数语法研究都着重在句法层面,而本文的范畴表达式演算理论则着重在语义层面。我们首先考察了完全表达式与不完全表达式、句法类型与语义类型、继承、顺序、提取、并列等若干重要的语言现象以及各种语法理论对这些现象的解释,然后提出范畴表达式的形式化定义,分析了句法层面的形式约束对语义层面的内容组织的制导作用,并且用典型的语言例子直观的说明了如何利用短语结构制导,进行范畴表达式的演算。这种机制可形式化、可验证,能很好的捕捉语言的组成成分及它们间的相互关系,揭示一个句子所说的内容。","人工智能,自然语言处理,语法,范畴表达式,短语结构制导,信息处理"
2004-06-21,XML内容筛选中的快速串匹配算法,"刘萍,谭建龙","本文提出了一种对XML 文本进行快速串匹配的算法- XMatch。在对于XML 文本的含路径信息的模式串匹配中,由于XML 文本的结构化特点,使得传统的串匹配算法不能直接有效的使用;而现有的大部分XML 内容筛选方法都是基于SAX 分析的事件驱动过程,效率普遍较低。XMatch 在对XML 文本的结构-schema 进行分析的同时,结合模式串的路径信息,建立一个扫描自动机的有限状态自动机;此外,算法还支持带循环引用路径信息的模式串匹配。XMatch 容易扩展,可以支持普通的结构化文本的串匹配。实验结果显示,本算法的效率比使用SAX事件驱动的方法有明显的提高。","计算机应用,中文信息处理,XML 数据处理,串匹配,多关键词匹配"
2004-06-15,一种义项矩阵模型SMM,孙斌,"本文介绍了一个同时利用词语和义项来索引和检索文档的信息检索模型,称为“义项矩阵模型”SMM(Sense Matrix Model) . 利用词语和义项的关联提出了一种新的文档表示,即把文档表示成为一个term ×sense 矩阵,由此引进或建立起一些很有效用的数据分析技术,包括基于矩阵范数的文档相似度计算、文档向量和矩阵的离散余弦变换(DCT) 、多维数据正交分解(MAD) 等,并提供了一种新的、无需翻译或者模型训练集的跨语言检索和多语言文本分类的技术。另外,还讨论了对文档进行DCT的部分试验结果。","计算机应用,中文信息处理,信息检索,检索模型,义项矩阵"
2004-06-22,文本的图表示初探,"周昭涛,卜东波,程学旗","文本表示是文本信息处理中的基础问题,以向量空间模型(VSM) 为代表的多数文本表示模型没有考虑文本中特征项之间的序关系,这样的表示造成文本语义信息的损失。我们尝试在文本表示中引入序关系,用图结构来表示文本,提出了一种新的文本表示模型—图表示模型,并对该模型的表示效果进行了验证。实验结果表明目前我们的表示模型仍达不到VSM模型所取得的表示效果。本文总结了文本表示过程,提出了一种新颖的用于度量文本表示模型表示能力的方法,同时也提出了一系列与文本图表示相关的值得探讨的问题。","计算机应用,中文信息处理,文本表示,VSM模型,图表示"
2004-06-17,利用虚拟站点定位技术的网络信息检索研究,"刘奕群,张敏,马少平","虚拟组织是网格体系结构中的基本组织单元,借鉴网格研究中对虚拟组织的特性分析,可以在网络信息检索研究中定义虚拟站点的概念。实验发现,虚拟站点入口页面是网络信息环境中具有较高质量的一个网页集合:实验表明,仅为全部页面数量21 %的此类页面就涵盖了70 %以上的超链接,对这个集合进行的内容检索也比对网页全集的检索有超过60 %的性能提高。这提供了一种在减少索引规模前提下提高网络信息检索性能的解决方案。","计算机应用,中文信息处理,网络信息检索,非内容特征,虚拟组织"
2004-06-20,基于Ontology的信息检索技术研究,"陈康,武港山","随着Web 的迅速发展,网上信息资源越来越丰富,网络已经成为了一个全球最大的信息库。而用户要从中得到所需的信息一般是通过各种信息检索工具。但是现有的信息检索工具都存在着检索精度不高等问题。本文针对这些问题,提出了将Ontology 融合到信息检索技术中的思路。利用Ontology 中拥有的领域知识,可以大大提高检索系统对自然语言文本的理解能力,同时方便用户以自然语言的方式提出检索请求,从而提高检索的效果。","人工智能,自然语言处理,信息检索,Ontology ,自然语言理解"
2004-06-28,词性标注对信息检索系统性能的影响,"苏祺,昝红英,胡景贺,项锟","在信息检索中引入NLP 技术是信息检索发展的主要趋势,本文将NLP 中较为成熟的词性标注技术加入信息检索,采用大规模TREC 数据集,试图发现词性标注对检索系统性能的影响。笔者在SMART 检索系统上使用不同标注集、不同索引项权重进行了检索实验。实验表明,在信息检索中加入词性标注信息可能会对某些特定Topic 和Document 的检索效果有所改进,但词性标注的影响能力弱于索引项权重选择的影响能力。词性标注对检索性能的影响涉及到Topic 和Document 中的具体用词,普遍规律有待进一步研究。","人工智能,自然语言处理,信息检索,向量空间模型,词性标注,SMART"
2004-06-20,基于粗糙集的文本分类方法研究,"卢娇丽,郑家恒","本文旨在利用粗糙集优越的约简理论对文本进行分类。主要完成了以下几个方面的任务:对文本进行了预处理;改进了Okapi 权重计算公式,并对权值进行了离散化;实现了属性约简和规则抽取,首先利用区分矩阵对特征向量维数进行了初次压缩,然后通过相对约简计算再次压缩了特征向量维数,并生成了决策规则;采取了规则合成的策略,生成最终的决策规则;设计了一种文本与规则的匹配算法,使匹配过程尽可能简单有序。试验结果表明该方法是行之有效的。","人工智能,自然语言处理,文本分类,粗糙集,决策规则"
2004-06-18,一种自举的二元关系和二元关系模式获取方法,"姜吉发,王树西","本文提出一种自举的二元关系和二元关系模式获取方法BRPAM,并根据该法设计了一个能够从自由文本中进行二元关系抽取的IE 系统BRPAM2Texts。将BRPAM2Texts 用于从自由文本中抽取〈组织、组织总部所在地〉类二元关系的实验表明,BRPAM2Texts 能够根据用户初始给出的几个种子二元关系从一个大的自","人工智能,自然语言处理,信息抽取,二元关系,模式获取"
2004-06-16,基于语料库的字母词语自动提取研究,"郑泽之,张普,杨建国","目前,很多最新的术语和专有名词,首先以字母词语的形式出现在汉语中,并日益广泛应用。而字母词语多数是汉语自动分词中的未登录词,其正确识别,将有助于提高中文分词、信息检索、搜索引擎、机器翻译等应用软件的质量。本文在对字母词语进行先期考察的基础上,分析了字母词语组成情况的复杂特征和自动识别的难点,结合字母词语的各种统计特征和其独有的特点———字母串“锚点”,提出了从中心往两边扩展的规则加统计辅助的字母词语自动提取的算法。并且对字母词语的双语同现问题进行了处理。算法简单,但有效。召回率为100 % ,准确率在80 %以上。","人工智能,自然语言处理,字母词语,自动提取"
2004-06-15,基于Bootstrapping的文本分类模型,"陈文亮,朱慕华,朱靖波,姚天顺","本文提出一种基于Bootstrapping 的文本分类模型,该模型采用最大熵模型作为分类器,从少量的种子集出发,自动学习更多的文本作为新的种子样本,这样不断学习来提高最大熵分类器的文本分类性能。文中提出一个权重因子来调整新的种子样本在分类器训练过程中的权重。实验结果表明,在相同的手工训练语料的条件下,与传统的文本分类模型相比这种基于Bootstrapping 的文本分类模型具有明显优势,仅使用每类100 篇种子训练集,分类结果的F1 值为70156 % ,比传统模型高出4170 %。该模型通过使用适当的权重因子可以更好改善分类器的训练效果。","计算机应用,中文信息处理,文本分类,最大熵模型,权重因子"
2004-06-23,一种新的句子相似度度量及其在文本自动摘要中的应用,"张奇,黄萱菁,吴立德","本文提出了一种新的句子相似度度量的方法并应用于文本自动摘要中。其创新处在于相似度计算不仅考虑句子中的unigram ,还考虑了bi-gram 和tri-gram ,通过回归方法将这几种相似度结果综合起来。实验证明这种相似度计算方法是有效的。同时本文还提出了一种新的,利用句子间相似度以及句子的权重的抽句","计算机应用,中文信息处理,文本自动摘要,向量模型,相似度计算"
2004-06-21,基于改进贝叶斯模型的问题分类,"张宇,刘挺,文勖","随着计算机及互联网络技术的发展,开放域问答系统越来越受到人们的关注,因为它能够给用户提供相对简洁、准确的结果。开放域问答系统通常包括问题分类、问题扩展、搜索引擎、答案抽取和答案选择五个主要部分。问题分类在问答系统中起着很重要的作用,它的准确性直接影响到最终抽取的答案的准确性。","计算机应用,中文信息处理,贝叶斯模型,问题分类,问答系统"
2004-06-17,问答式检索技术及评测研究综述,"吴友政,赵军,段湘煜,徐波","问答式检索系统(简称问答系统) 是集自然语言处理技术和信息检索技术于一身的新一代搜索引擎。它的出现旨在提供更有力的信息获取工具,以应对信息爆炸带来的严重挑战。经过这几年的发展,问答系统已经成为自然语言处理领域和信息检索领域的一个重要分支和新兴的研究热点,其“通过系统化、大规模地定量评测推动研究向前发展”的发展轨迹,以及某些成功的启示,如基于字符表层的文本分析技术(模板技术) 的有效性,快速、浅层自然语言处理技术的必要性,都极大地推动了自然语言处理研究的发展,促进了NLP研究与应用的紧密结合。回顾问答系统研究的历史,总结问答技术的研究现状,将有助于这方面工作向前发展。","人工智能,自然语言处理,综述,问答系统,问答评测,信息抽取,信息检索"
2004-05-19,基于主题语言模型的中文信息检索系统研究,"张俊林,孙乐,孙玉芳","准确的文档语言模型估计对于改善语言模型检索系统的性能是非常重要的。在本文中我们提出了基于主题语言模型的信息检索系统,首先设计了“改进的两阶段K2Means 聚类算法”来对文档集合进行聚类,通过引入Aspect Model 结合聚类结果可以得到基于主题的语言模型。这个新的语言模型较深入地刻画了词汇在不同主题下的分布规律以及文档所蕴含不同主题的分布规律。将主题语言模型和文档本身的语言模型通过线性插值可以更准确地估计文档语言模型。实验结果表明我们提出的这个方法显著改善了检索系统的性能,与Jelinek2Mercer 模型方法相比较,主题语言模型检索系统的平均精度提高大约16117 % ,召回率提高大约","人工智能,自然语言处理,主题语言模型,信息检索"
2004-06-06,短语树到依存树的自动转换研究,"党政法,周强","不同标注体系的树库之间的相互转换是计算语言学研究的重要内容之一。本文在总结国内外几种树库标注体系及相互转换实践的基础上,结合清华汉语树库(Tsinghua Chinese Treebank ,简称TCT) 标注体系的特点,提出了一种将TCT从短语结构转换成依存结构(Dependency Structure) 的算法。这种算法充分利用了TCT具有的功能、结构的双重标记,转换得到的依存树不仅包含了各个节点之间相互依存的层次关系,更包含了相互依存的两个节点的具体的依存关系类型。我们对转换的效果进行了抽样评估,准确率可以达到97137 %。","人工智能,自然语言处理,树库,短语树,依存树,自动转换"
2005,“第十届少数民族语言信息处理研讨会”征文通知,,"为了交流民族语言文字及多语言信息处理的最新研究成果,促进中国少数民族语言文字信息技术发展,中国中文信息学会、青海省教育厅五省区藏族教育协作领导小组办公室、青海师范大学、中国科学院软件研究所联合举办“第十届少数民族语言信息处理研讨会”。会议定于2005 年7 月16 - 18 日在青海师范大学召开,会期三天。会议将邀请著名中文信息处理专家做专题报告。",
2004-06-02,面向语言信息处理的朝鲜语知识库研究,毕玉德,"在自然语言处理系统(包括机器翻译系统) 中,语法、语义信息词典是必不可少的构件。本文以国内外语义工程研究成果为基础,通过对朝鲜语谓词进行句法语义一体化描述,建立面向信息处理的朝鲜语知识库。该研究的语言学理论根据是论元结构理论和语义场理论。我们首先对谓词进行语义分类,然后再对谓词义项作详细的属性描述。在知识库构建上,采用结构体方式将谓词的句法、语义等属性整合在一起。","计算机应用,中文信息处理,朝鲜语,知识库,一体化描述,结构体在"
2004-05-30,汉字双向有穷自动机的研究,"蔡增玉,谷文祥","汉字的计算机输入是中文信息处理的关键问题之一,而汉字计算机输入的数学模型对汉字的计算机输入的研究有重要的意义。本文从自动机理论的角度对汉字输入的数学模型进行了研究,把控制操作引入了输入模型,并给出确定汉字双向有穷自动机和不确定汉字双向有穷自动机的模型。新的模型较之以前的数学模型,能刻画出汉字输入的控制操作,表达能力进一步增强,是对以前汉字键盘输入数学模型的推广。","计算机应用,中文信息处理,汉字输入,数学模型,双向有穷自动机"
2004-07-29,机器翻译评测中的模糊匹配,"刘洋,刘群,林守勋","目前,大多数机器翻译自动评测方法都没有考虑在未匹配的词语中可能包含被忽略的信息。本文提出一种在参考译文和待评测译文之间自动搜索模糊匹配词对的方法,并给出相似度的计算方法。模糊匹配和计算相似度的整个过程将通过一个例子进行说明。实验表明,我们的方法能够较好地找到被忽略的、有意义的词对。更重要的是,通过引入模糊匹配,BLEU 的性能得到显著的提高。模糊匹配可以用来提高其他机器翻译自动评测方法的性能。","人工智能,机器翻译,机器翻译评测,模糊匹配"
2005,《统计自然语言处理基础》,,"　这本书是美国的Christopher D. Manning 教授和德国的Hinrich Schutze 教授合著、清华大学苑春法教授组织翻译、并负责对全书进行了统一修改、审阅及定稿。该书是电子工业出版社的国外计算机科学教材系列中的一本系统介绍统计自然语言处理(或统计语言学) 专著,在国外已经被许多大学用来作为教材。在我国统计语言学也已成为自然语言处理研究中的主流,希望这本书对大家的研究及教学工作有所帮助。",
2004-08-04,基于多层过滤的统计机器翻译,"周玉,宗成庆,徐波","本文提出了一种基于多层过滤的算法。该算法主要实现从对齐的中英文句子中自动的抽取与对齐双语语块。根据不同语块具备的不同特性,采用不同的层次对其处理。该算法不同于传统的算法,它不需要对句子进行标注,句法分析,词法分析甚至不需要对汉语句子进行分词等操作。初步的实验结果表明该算法性能较好,测试的结果是:抽取语块的准确率能达到F = 0170 ,对齐语块的准确率能达到F = 0180 ;而且将此算法获得的对齐双语语块用于统计机器翻译系统,跟基于词的系统做对比,结果表明基于语块的翻译系统明显提高了翻译水平,差不多能提高10 %。","人工智能,机器翻译,多层过滤,双语语块识别与对齐"
2004-07-28,融合丰富语言知识的汉语统计句法分析,"熊德意,刘群,林守勋","知识获取一直以来是自然语言处理中的瓶颈,基于树库的统计句法分析也不例外。树库中潜在隐含的语言知识是非常丰富的,但它们并不是可以直接得到,往往需要特定的策略才能将它们融合到模型中。我们的汉语统计句法分析模型从3 个方面融合潜在的丰富语言知识:1) 重新标注树库中的非递归名词短语和非递归动词短语;2) 设计新的中心词映射表;3) 引进上下文配置框架以更具体地描述二元依存结构。由于融合了以上三种潜在语言知识,模型的F1 值提高了2137 % ,完全匹配正确率提高了5136 %。","人工智能,自然语言处理,统计句法分析,非递归短语,中心词映射表,上下文配置框架"
2004-07-27,一种基于可信度的人名识别方法,"罗智勇,宋柔","专名识别技术是影响中文自动分词精度的一个重要方面,也是自动分词技术的难点之一。本文以人名识别为例,分析了目前流行的基于语料库和统计语言模型的专名识别方法中在概率估值问题上存在的弊端;同时在规则和统计相结合的基础上,提出了一种基于可信度的人名识别方法,并给出了一个渐进式模型训练方法,克服了人工标注语料库规模的限制。从我们对《人民日报》1998 年1 月、2000 年12 月(共约379 万字) 语料的测试结果来看,基于可信度的人名识别方法比传统的概率估值方法识别效果有一定的提高。","计算机应用,中文信息处理,自动分词,人名识别,统计方法,可信度"
2004-07-25,Co-training机器学习方法在中文组块识别中的应用,"刘世岳,李珩,张俐,姚天顺","采用半指导机器学习方法co2training 实现中文组块识别。首先明确了中文组块的定义,co-training 算法的形式化定义。文中提出了基于一致性的co-training 选取方法将增益的隐马尔可夫模型(Transductive HMM) 和基于转换规则的分类器(fnTBL) 组合成一个分类体系,并与自我训练方法进行了比较,在小规模汉语树库语料和大规模未带标汉语语料上进行中文组块识别,实验结果要比单纯使用小规模的树库语料有所提高,F 值分别达到了85134 %和83141 % ,分别提高了2113 %和7121 %。","计算机应用,中文信息处理,co2training 算法,中文组块,分类器"
2004-07-29,现代汉语介词短语边界识别研究,"王立霞,孙宏林","汉语中介词结构右边界歧义是汉语结构歧义中最突出的现象之一,这给汉语的句法分析带来了很大的困难。本文研究的目标是:在不引进复杂的句法分析的前提下实现介词短语边界的自动识别,期望其作为句法分析预处理的一部分为句法分析提供一定的帮助。本文对汉语中最常用的介词“在”进行了实验,封闭测试和开放测试的准确率分别达到97 %和93 %。与前人的同类研究相比,准确率有了较大的提高,解决了过去遗留的一些问题。","计算机应用,中文信息处理,右边界,概率信息,删除插值法"
2004-07-27,基于HowNet概念获取的中文自动文摘系统,"王萌,何婷婷,姬东鸿,王晓荣","本文提出了一种中文自动文摘的方法。不同于其它的基于词频统计的一般方法,运用概念(词义)作为特征取代词语。用概念统计代替传统的词形频率统计方法,建立概念向量空间模型,计算出句子重要度,并对句子进行冗余度计算,抽取文摘句。对于文摘测试,采用两种不同的方法进行测试:一是用机器文摘和专家文摘进行比较的内部测试;二是对不同文摘方法进行分类,通过对分类正确率的比较的外部评测方法。","计算机应用,中文信息处理,HowNet ,自动文摘,概念向量空间模型"
2015-12-01,《中文信息学报》征稿简则,,一、《中文信息学报》主要刊登中文信息的基础理论、应用技术、中文信息处理系统及设备、中文信息的自动输入和人工编码输入、汉字字形信息、自然语言处理、计算语言学及民族语言文字信息处理及网上信息处理等方面的研究论文、技术报告、综述、通讯、简报、国内外学术活动等。,
2005-03-13,一种基于概率上下文无关文法的汉语句法分析,"林颖,史晓东,郭锋","本文研究了PCFG独立性假设的局限性,并针对这一局限性提出了句法结构共现的概念以引入上下文信息,给出了计算方法;为了打破中文树库规模过小的局限性,对于句法规则参数的获取,本文利用Inside-Outside算法进行迭代,最后提出了一个基于统计模型的自顶向下的汉语句法分析器。在封闭测试下,其标记精确率和标记召回率分别为88.1%和86.8%。实验结果表明,这种方法确实能够提高标记的精确率和召回率,值得深入研究。","人工智能,自然语言处理,统计句法分析,概率上下文无关文法,汉语自动分析"
2005-01-19,基于语义分类树的汉语口语理解方法,"左云存,宗成庆","口语理解在口语自动翻译和人机对话系统中具有非常重要的作用。本文面向口语自动翻译提出了一种统计和规则相结合的汉语口语理解方法,该方法利用统计方法从训练语料中自动获取语义规则,生成语义分类树,然后利用语义分类树对待解析的汉语句子中与句子浅层语义密切相关的词语进行解析,最后再利用统计理解模型对各个词语的解析结果进行组合,从而获得整个句子的浅层语义领域行为。实验结果表明,该方法具有较高的准确率和鲁棒性,适合应用在限定领域的汉语口语浅层语义理解。","人工智能,自然语言处理,语义分类树,浅层语义分析,口语理解"
2005-02-02,基于Web数据的特定领域双语词典抽取,"张永臣,孙乐,李飞,李文波,西野文人,于浩,方高林","双语词典是跨语言检索以及机器翻译等自然语言处理应用的基础资源。本文提出了一种从非平行语料中抽取特定领域双语词典的算法。首先给出了算法的基本假设并回顾了相关的研究方法,然后详细给出了利用词间关系矩阵法从特定领域非平行语料中抽取双语词典的过程,最后通过大量实验分析了种子词选择对词典抽取结果的影响,实验结果表明种子词的数量和频率对词典抽取结果有积极作用。","计算机应用,中文信息处理,双语词典,词间关系矩阵,非平行语料,种子词"
2005-03-09,中文文本体裁的自动分类机制,"方鸷飞,林鸿飞,杨志豪,赵晶","文本按体裁自动分类属于按文本的形式分类的范畴,所以它与按内容自动分类问题有许多的不同之处,本文提出了一种关于中文文本体裁自动分类的新机制。在体裁分类过程中首要的问题是分类特征的选取,体裁分类特征项分为两种方式加以描述,一是集合形式,如基于分类词典和语料统计的政论性词汇和情感词汇等,二是规则形式,如公文标识信息和条文句等。基于根据特征之间的关联性和差异性,采用样本分布决策的方法抽取相应的特征项。最后利用支撑向量机算法进行自动分类。该机制已经在五类体裁的语料上得到实现,并获得了较好的效果。","计算机应用,中文信息处理,体裁分类,特征项选取,样本分布决策,支撑向量机"
2005-03-12,基于句法结构分析的中文问题分类,"文勖,张宇,刘挺,马金山","问题分类是问答系统中重要的组成部分,问题分类结果的好坏直接影响问答系统的质量。本文提出了一种用于问题分类的特征提取的新方法,该方法主要使用句法分析的结果,提取问题的主干和疑问词及其附属成分作为分类的特征,此方法大幅度地减少了噪音,突出了问题分类的主要特征,利用贝叶斯分类器分类,有效地提高了问题分类的精度。实验结果证明了该方法的有效性,大类和小类的分类精度分别达到了86.62%和71.92%,取得了较好的效果。","计算机应用,中文信息处理,问答系统,问题分类,特征提取,句法分析"
2005-03-15,基于事件框架的信息抽取系统,"梁晗,陈群秀,吴平博","信息抽取技术能够提供高质量的检索服务。本文提出一种基于框架的信息抽取模式并建立统一的灾难性事件框架,利用框架的继承-归纳特性简化系统实现过程,概括事件信息,并提出按时间流顺序的线索性文件抽取的输出方式。本文使用这种方法建立了一个灾难性事件信息抽取系统。实验证明本文中的方法是有效的。","计算机应用,中文信息处理,信息抽取,框架,继承,灾难性事件"
2005-09-20,《说文解字》音义关系的产生式表达,"宋继华,李国玉,王宁","汉语语义关系的探求离不开汉字音义关系的探求,汉字的音义关系分为同音、同义和同源三种。探求汉字之间的音义关系、利用汉字的字音来推求字义之间的关系,是《说文解字》研究的一项重要内容。为了便于基于计算机技术更全面地探求音义关系尤其是同源关系中的“音近”、“义通”关系,本文对音韵通转规则进行了形式化表述。在《说文》知识库中,建立了《说文》双声规则库和叠韵规则库(含8个规则表) ,它们通过“规则槽”与传统框架表示法中的“属性槽”和“属性库”共同构成产生式框架,有效地表达了《说文》中的各项描述性知识和规则性知识,为后续研究奠定了基础。","计算机应用,中文信息处理,说文解字,音义关系,同源,产生式规则,产生式框架"
2005-01-25,基于小波分析的大词汇汉语连续语音识别系统鲁棒性的研究,"颜龙,刘刚,郭军","本文提出一种基于小波分析的大词汇汉语连续语音识别的方法,即采用一维小波变换将原始语音信号进行五层小波分解,然后对各层小波系数进行重构,得到五层语音信号,分别对各层语音信号进行训练,得到各层的声学模型,然后结合语言模型对各层声学模型的性能进行测试。通过对纯净语音和带噪语音的各层重构语音数据进行测试。结果表明对于含有高斯白噪声的带噪语音,该方法能使系统性能有所提高,但对于粉红噪声,该方法效果不明显。对于含有真实环境噪声的带噪语音,该方法能获得比基线系统更好的性能。","计算机应用,中文信息处理,大词汇连续语音识别,小波分析,声学模型"
2005-02-23,印刷体朝鲜文字符中字母的分割与识别研究,"许日俊,刘昌平","朝鲜文是一种由元音和辅音构成的字母文字。因此经常使用的一种朝鲜文识别方法是:从朝鲜文字符中分离出每一个字母,然后对这些字母进行识别,最后确定识别字符。本文结合结构分析法,通过对字符图像背景进行细化处理,找到字母之间的分割线分离出了每个字母,并且利用两层外围距离特征对这些字母进行了识别。在对4种经常使用的朝鲜文印刷字体进行初步实验的结果表明,字母分割正确率平均达到了97.4% ,而字母样本集识别率为99%以上。","人工智能,模式识别,字母分割,字母识别,朝鲜文字符识别"
2005-02-18,维吾尔文手机输入关键技术研究与实现,"热依曼.吐尔逊,吾守尔.斯拉木","维吾尔文,汉文和英文等多文种手机,对于发展少数民族地区通讯和经济,有非常重要的实用和商业价值。针对以上实际情况,本文首先研究了维吾尔文的书写特点、手机输入法设计中的难点、以及不等宽、不同输入方向的汉、英、维多文种信息的屏幕混合显示问题。根据维吾尔文的特征和手机显示屏幕的物理特征设计了维吾尔文的手机键盘布局,实现了支持多文种混合显示的维吾尔文手机输入法,给出了实现其关键模块功能的程序流程图。","计算机应用,中文信息处理,维吾尔文,输入法,自动选型,多文种混合显示,手机键盘"
2005-05-16,藏文计算机通用键盘布局与输入法研究,卢亚军,"为了改进现有各种藏文计算机键盘布局与输入法,本文依据键盘布局的基本理论、若干原则、相关科学数据和基于藏文语料库的字符、部件、音节、词汇统计数据,遵循藏语语法规则及其特殊性,在对键盘键位的属性进行专门研究的基础上,研制出“一键多符”和“一键到位”的智能化藏文计算机通用键盘布局与输入法,其藏文文本的键盘输入速度和效率成倍提高,对藏文印刷、办公自动化和信息处理具有广泛的使用价值。","计算机应用,中文信息处理,藏文,计算机键盘,键盘布局,输入法"
2005-01-27,国际化文字处理综述,"芮建武,吴健,孙玉芳","计算机与不同用户的交互通常必须实现通过多种文字信息的输入/输出以实现,因此操作系统对多种文字的支持程度是其功能性的一个衡量标准。各种文字特征的巨大差异导致现代操作系统的文字处理实现非常复杂。本文总结了操作系统文字处理的范围与内容,包括文本输入与存储,文本处理以及用户交互处理;归纳了通用的文字处理模型和可能采取的技术途径及其优缺点;分析了常用操作系统的文字处理实现;最后展望了文字处理仍面临的挑战。","计算机应用,中文信息处理,综述文字处理,复杂文字,字体模型"
2004-12-28,消除同音查询的死角——汉语言文字多音同音查询解决方案,蒋晓京,"本文通过分析汉语言文字“读音-字形”之间的“多-多”对应关系,阐明了现有数据库同音查询技术中因忽视多音字问题而导致漏查的缺陷,提出了以汉字字形输入代替拼音字母输入,同时在查询逻辑上加以扩展和改造,从而弥补上述缺陷,实现汉语言文字多音同音查询的完整解决方案。此外,本文试图通过对本方案的扩展及延伸,从汉语言文字问题推导出方言、译音、乃至其它种类语言文字类似问题的解决思路,进而为发掘事物的内在联系、发展人工智能提供参考。","计算机应用,中文信息处理,汉字,多音字,同音字,拼音,数据库"
2005-07-20,应用二叉树剪枝识别韵律短语边界,"荀恩东,钱揖丽,郭庆,宋柔","句子的韵律短语识别是语音合成的重要研究内容。本文提出了应用统计语言模型生成的二叉树,结合最大熵方法识别待合成汉语句子的语音停顿点。文中给出了二叉树相关的模型训练和生成算法;二叉树与语音停顿点之间的关系;在最大熵方法中应用二叉树剪枝识别句子的韵律短语。实验结果表明,在搜索算法中,利用二叉树进行剪枝,可以很大程度上提高语音停顿预测的正确率和召回率,基于试验数据的f-Score提高了近35%。","人工智能,自然语言处理,统计语言模型,二叉树,韵律短语,最大熵"
2005-05-26,基于AdaBoost.MH算法的汉语多义词消歧,"刘风成,黄德根,姜鹏","本文提出一种基于AdaBoost MH算法的有指导的汉语多义词消歧方法,该方法利用AdaBoost MH算法对决策树产生的弱规则进行加强,经过若干次迭代后,最终得到一个准确度更高的分类规则;并给出了一种简单的终止算法中迭代的方法;为获取多义词上下文中的知识源,在采用传统的词性标注和局部搭配序列等知识源的基础上,引入了一种新的知识源,即语义范畴,提高了算法的学习效率和排歧的正确率。通过对6个典型多义词和SENSEVAL3中文语料中20个多义词的词义消歧实验,AdaBoost MH算法获得了较高的开放测试正确率(85.75%)。","人工智能,自然语言处理,词义消歧,AdaBoost MH算法,多知识源"
2005-06-20,基于粗糙集的基本名词短语识别,"郭永辉,杨红卫,马芳,王炳锡","本文提出了一种基于粗糙集的基本名词短语(BaseNP)识别方法。该方法首先进行BaseNP标注,然后实现BaseNP识别。它把BaseNP标注看作一个决策问题用粗糙集理论解决,因而具有特征约简和规则优化的特点。文章介绍了基于粗糙集的规则学习方法和相应的算法,同时也给出了BaseNP标注和识别的算法流程,提出了解决实例冲突问题的方法,并提高了识别效果。文章最后给出了详细的实验步骤和结果,并与几个典型系统进行了比较与分析,提出了进一步改进的方向。","人工智能,自然语言处理,基本名词短语,粗糙集,机器学习,规则方法,算法"
2005-03-21,中文文本分类中基于概念屏蔽层的特征提取方法,"廖莎莎,江铭虎","本文提出了一种新的基于概念抽取和屏蔽层的特征选择方法。该方法利用HowNet概念词典中的概念树,通过义原在概念树中的位置信息进行概念抽取,并赋予其适当权值来说明其描述能力。对于权值低于屏蔽层的义原,我们不将其选入特征集,并相应保留原词。具体到每个词,我们计算其DEF条目中的权值,决定是将原词选入特征集还是进行概念抽取。本文重点研究了如何给义原设定一个合适的权值,如何在选取原词和概念之间取得平衡以及针对非概念词的加权处理。实验证明,设定合适的屏蔽层,不仅可以缩小特征维数,使分类正确率得到一定的提高,而且可以减少不同类别间的分类正确率的差别。","计算机应用,中文信息处理,文本分类,特征提取,概念抽取,属性特征树,屏蔽层,描述能力"
2005-03-18,基于n-gram语言模型和链状朴素贝叶斯分类器的中文文本分类系统,"毛伟,徐蔚然,郭军","本文提出了一个基于n-gram语言模型进行文本表示,采用链状朴素贝叶斯分类器进行分类的中文文本分类系统。介绍了如何用n-gram语言模型进行文本表示,阐述了链状朴素贝叶斯分类器与n-gram语言模型相结合的优势,分析了n-gram语言模型参数的选取,讨论了分类系统的若干重要问题,研究了训练集的规模和质量对分类系统的影响。根据863计划文本分类测评组所提供的测试标准、训练集以及测试集对本文所设计的分类系统进行测试,实验结果表明该分类系统有良好的分类效果。","计算机应用,中文信息处理,中文文本分类,n-gram语言模型,链状朴素贝叶斯分类器"
2005-01-28,《元朝秘史》电子文本检索系统的研制,"江荻,严海林,孙伯君,斯钦朝克图,孟达来","本文概要地介绍13世纪《元朝秘史》的文献背景及原文所独有的复杂文本形式,通过对文本的内涵分析和版面分析,设计了关于《元朝秘史》电子检索系统的研制方案。其中主要解决了原文三行一体显示格式的还原问题,而且系统可以分别对原文汉字音写、汉语译文、汉字旁译、语音语法标注等不同部分进行检索和统计。检索输出结果包括研究者最重视的传统学术章节号、卷页码、在电子文本出现的具体位置。另外,系统对检索词采用了上下文检索技术,输出文本包括检索词的部分上下文内容。本系统基本满足历史、文学和语言研究的应用需求。","计算机应用,中文信息处理,元朝秘史,复杂文本,电子检索系统"
2005-08-21,面向自然语言信息处理的维吾尔语名词形态分析研究,"阿依克孜·卡德尔,开沙尔·卡德尔,吐尔根·依布拉音","名词是人类语言中的基本词类之一。维吾尔语是一种形态变化很复杂的语言,其中名词是一种形态变化复杂的词类。因此名词的形态分析研究无论在语法研究还是在语言信息处理中都非常重要。本文对维吾尔语名词的形态变化(名词的数、人称、格等语法范畴)进行了形式化的描述和分析。指出了维吾尔语名词的基本形态参数,总结出参数的组配规律并统计了其类型,探索了维吾尔语名词的削尾方法。这些工作将为维吾尔语名词形态处理提供有效的方法和新的思路。","人工智能,自然语言处理,维吾尔语信息处理,名词,形态"
2005-01-26,藏文支持在OpenOffice.org办公套件中的实现,"贾彦民,吴健,欧珠,孙玉芳","办公套件是人们日常应用最为广泛的信息处理软件之一,但真正意义的藏文办公套件至今都尚未问世,成为藏文信息技术发展的“瓶颈”。开源项目OpenOffice.org的不断发展和日益成熟,为藏文办公套件的研制开发提供了有利的契机。以OpenOffice.org为源代码基础,采用藏文编码字符集(扩充集A)国家标准,研制的藏文办公套件可支持藏文排版习惯和藏文本地环境,着重解决了藏文文本自动断行的问题,能够满足藏语文用户日常办公需要。","计算机应用,中文信息处理,藏文字符集,办公套件,藏文信息处理,文本断行"
2005-07-01,文档聚类综述,"刘远超,王晓龙,徐志明,关毅","聚类作为一种自动化程度较高的无监督机器学习方法,近年来在信息检索、多文档自动文摘等领域获得了广泛的应用。本文首先讨论了文档聚类的应用背景和体系结构,然后对文档聚类算法、聚类空间的构造和降维方法、文档聚类中的语义问题进行了综述。最后还介绍了聚类质量评测问题。","计算机应用,中文信息处理,综述,文档聚类,降维,概念相关,聚类算法"
2005-11-01,以本体构造中文信息过滤中的需求模型,"袁兴宇,王挺,周会平,肖君","在信息过滤系统中,用户模板是机器可理解的用户需求表示形式,是否能准确地反映出用户的真实需求将直接影响着过滤系统的性能。在向量空间模型中,用户的模板表现为一组带权重的特征词集,但由于在这样的用户模板中缺少必要的语义信息,很难准确地反映出用户的需求。本文提出了以本体构造需求模板的方法,以本体的形式定义需求中概念间的语义关联关系,将向量空间模型中的特征向量定义为本体中的实例,通过实例间的关联路径计算特征项间的语义关联,并通过特征项间的语义关联计算出文档与模板的语义关联度。","计算机应用,中文信息处理,信息过滤,本体,语义关联,用户模板"
2005-11-03,面向信息检索需要的网络数据清理研究,"刘奕群,张敏,马少平","Web数据中的质量参差不齐、可信度不高以及冗余现象造成了网络信息检索工具存储和运算资源的极大浪费,并直接影响着检索性能的提高。现有的网络数据清理方式并非专门针对网络信息检索的需要,因而存在着较大不足。本文根据对检索用户的查询行为分析,提出了一种利用查询无关特征分析和先验知识学习的方法计算页面成为检索结果页面的概率,从而进行网络数据清理的算法。基于文本信息检索会议标准测试平台的实验结果证明,此算法可以在保留近95%检索结果页面的基础上清理占语料库页面总数45%以上的低质量页面,这意味着使用更少的存储和运算资源获取更高的检索性能将成为可能。","计算机应用,中文信息处理,网络信息检索,数据清理,机器学习"
2005-11-02,基于相关文档池建模的查询扩展,"吕碧波,赵军","在信息检索领域,相关反馈是提高检索性能的有效方法之一。所谓相关反馈,指用户按照一定策略从查找到的相关文档中选择一些和主题相关的词进行查询扩展的技术。本文介绍了概率模型和向量空间模型下的常用查询扩展方法,并提出了一种基于语言模型的相关反馈方法,该方法同时考虑了扩展词应该具备的两个特征,即相关性和覆盖性。在TREC测试集上对这些算法进行了比较,结果表明这种新算法在平均准确率上比传统方法有所提高。","计算机应用,中文信息处理,信息检索,相关反馈,查询扩展"
2005-11-02,一种基于局部共现的查询扩展方法,"丁国栋,白硕,王斌","针对信息检索中文档与查询之间的词不匹配问题,本文提出了一种基于局部共现的查询扩展方法LOCOOC。LOCOOC利用词项与所有查询词在局部文档集合中的共现程度来评估扩展词的质量,并整合了词项在语料集中的全局统计信息,使得选取的扩展词与初始查询所表征的主题或概念具有更好的相关性。实验结果表明:与未进行查询扩展时相比,采用LOCOOC方法进行扩展后,平均准确率提高40%以上;与传统的局部反馈方法以及局部上下文分析方法(LCA,Local Context Analysis)相比,LOCOOC不仅具有更优的检索性能,而且有着更好的鲁棒性。","计算机应用,中文信息处理,信息检索,局部共现,查询扩展,LOCOOC"
2005-11-02,基于反馈学习自适应的中文话题追踪,"王会珍,朱靖波,季铎,叶娜,张斌","在话题追踪研究领域,由于话题是动态发展的,在追踪过程中会产生话题漂移的问题。针对该问题以及现有自适应方法的不足,本文提出基于反馈学习的自适应方法。该方法采用增量学习的思想,对话题追踪任务中的自适应学习机制提出了新的算法。该算法能够解决话题漂移现象,并能够弥补现有自适应方法的不足。该算法中还考虑了话题追踪任务的时序性,将时间信息引入到了算法中。本文实验采用TDT4语料中的中文部分作为测试语料,使用TDT2004的评测方法对基于反馈学习的自适应的中文话题追踪系统进行评价,实验数据表明基于反馈学习的自适应方法能够提高话题追踪的性能。","计算机应用,中文信息处理,话题追踪,基于反馈学习的自适应方法,增量学习"
2005-11-02,语音识别准确率与检索性能的关联性研究,"周梁,高鹏,丁鹏,徐波","对海量语音进行基于内容的检索需要语音识别技术和检索技术的结合。本文通过调节语言模型的途径研究在不同识别率的语音识别文本上进行关键词检索的差异,由此研究语音识别性能和检索性能之间的关联性。通过对114小时语音数据的实验表明:语音识别性能与检索性能有一定的相关性,同时也说明改进检索的方法可以消除一部分由于语音识别所带来的误差。研究结果为进一步针对性地改进识别引擎、语音识别输出的表示和相应的快速检索方法提供了基础。","计算机应用,中文信息处理,语音识别,关键词检索,查全率,查准率"
2004-08-30,多策略机器翻译系统IHSMTS中实例模式泛化匹配算法,"张孝飞,陈肇雄,黄河燕,胡春玲","基于精确匹配的EBMT,由于翻译覆盖率过低,导致其难以大规模实际应用。本文提出一种实例模式泛化匹配算法,试图改善EBMT的翻译覆盖率:以输入的待翻译句子为目标导向,对候选翻译实例有针对性地进行实时泛化,使得算法既能满足实时文档翻译对速度的要求,又能充分利用系统使用过程中用户新添加和修改的翻译知识,从而总体上提高了系统的翻译覆盖率和翻译质量。实验结果表明,在语料规模为16 万句对的情况下,系统翻译覆盖率达到了75 %左右,充分说明了本文算法的有效性。","人工智能,机器翻译,基于实例的机器翻译,泛化匹配,翻译覆盖率"
2004-07-07,基于k-means聚类的无导词义消歧,"陈浩,何婷婷,姬东鸿","无导词义消歧避免了人工词义标注的巨大工作量,可以适应大规模的多义词消歧工作,具有广阔的应用前景。这篇文章提出了一种无导词义消歧的方法,该方法采用二阶context 构造上下文向量,使用k-means算法进行聚类,最后通过计算相似度来进行词义的排歧. 实验是在抽取术语的基础上进行的,在多个汉语高频多义词的两组测试中取得了平均准确率82167 %和80187 %的较好的效果。","计算机应用,中文信息处理,词义消歧,HowNet ,二阶context ,k-means 聚类"
2004-07-28,汉语介词短语的自动识别,"干俊伟,黄德根","本文运用规则和统计相结合的方法构造了一个汉语介词短语识别算法。首先,根据介词和介词短语右边界组成的搭配模板自动提取可信搭配关系,并用这些搭配关系对介词短语进行识别。之后,用基于词性的三元边界统计模型和规则相结合的方法识别其它未处理的介词短语。通过对含有7323 个介词短语的语料作交叉测试,精确率达到87148 % ,召回率达到87127 %。","计算机应用,中文信息处理,短语识别,介词短语"
2004-12-05,采用优先选择策略的中文人称代词的指代消解,"李国臣,罗云飞","指代是自然语言中常见的语言现象,指代消解是文本信息处理中的一个重要任务。随着篇章处理相关应用日益广泛,指代消解也显示出前所未有的重要性。本文针对中文人称代词的指代特点,提出了一种基于语料库的,运用决策树机器学习算法并结合优先选择策略,进行指代消解的方法。该方法充分考虑了与指代相关的若干属性,及相互之间的影响。实验表明,对中文人称代词的消解特别是第三人称的消解获得了一定的效果。","计算机应用,中文信息处理,语料库,人称代词,指代消解,最优选择"
2004-06-21,词语间依存关系的定量识别,"王建会,王雷,胡运发","本文扩展和改进了现有的词语间依存关系定量识别算法,充分考虑词项概率分布的影响;明确区分词项之间的搭配关系、并列关系和从属关系,针对它们不同的特点,提出不同的识别算法;提出字串匹配模型;充分考虑两个词项之间相互位置的离散分布和距离的影响、以及它们的概率分布特性, 提出词项间的依存强度模型,并据此构建词语间依存关系树;提出更新策略,对已经建好的依存关系树进行裁剪,并挖掘出潜在的依存关系。应用实验结果表明,本文提出的算法可以有效地识别出词语间的依存关系。","计算机应用,中文信息处理,词语搭配,依存关系,定量识别"
2004-07-26,用逻辑和篇章知识来约束模板匹配——逻辑结构和篇章结构知识在信息抽取中的运用,袁毓林,"本文以文献[2 ]的语料为主要对象,讨论语句的逻辑结构和篇章结构怎样约束信息模板的类型,并约束对当前句中缺失的或以代词等形式表达的信息项目的求解。首先说明什么是基于论元结构的逻辑结构和篇章结构知识,然后分析否定算子、时体成分怎样改变事件的类型及其跟有关事件模板的匹配关系。接着,讨论动词的论元结构的内嵌和名词化等句法操作,怎样造成有关论元及相应的信息项目的分布位置发生变化。最后,讨论怎样利用篇章结构知识来求解本句中缺失的或以代词、指示词形式表达的信息项目。","计算机应用,中文信息处理,信息抽取,论元结构,逻辑结构,篇章结构,代词,指示词"
2004-06-23,规则加权的文本关联分类,"陈晓云,胡运发","近年来,基于关联规则的文本分类方法受到普遍关注。虽然在一般情况下这种方法可获得较好的分类效果。但当样本特征词分布明显不均时,分类规则在各类别的分布也出现不均,从而导致分类准确率下降。本文设计和实现的基于规则权重调整的关联规则文本分类算法可有效地解决这一问题。该算法根据误分类训练样本的数量定义规则强度。对强规则通过乘以小于1 的调整因子降低其权重,而弱规则乘以大于1的调整因子提高其权重。实验结果表明经过规则权重的调整,分类质量显著提高。","计算机应用,中文信息处理,关联分类,规则强度,权重"
2004-07-04,基于投影寻踪的中文网页分类算法,"万中英,王明文,廖海波","随着Web 信息迅猛发展,网络用户对网页自动分类器的需求日益增长。为了提高分类精度,本文提出了一种新的基于投影寻踪(Projection Pursuit , 简称PP) 的中文网页分类算法。我们首先利用遗传算法找到一个最好的投影方向,然后将已被表示成为n 维向量的网页投影到一维空间。最后采用KNN 分类算法对其进行分类。此方法能解决“维数灾难”问题。实验结果表明,我们提出的算法是可行而且是有效的。","计算机应用,中文信息处理,投影寻踪,网页分类,遗传算法,KNN 算法"
2004-07-13,一种基于超链接结构的向量空间模型改进算法,"原福永,褚蓓蓓","在基于向量空间模型的信息检索系统中,TF2IDF 算法被广泛的应用在基于关键字的信息检索中。然而,对于网页独特的超链接结构,需要有一种技术在表示网页内容的同时将与它相邻链接的网页内容考虑进去。本文分析了向量空间模型的实质,并找出了其精度低的原因,在传统模型基础上提出了一种基于网页超链接结构的向量空间模型改进算法。实验分析表明改进后的算法与原算法相比检索精确度提高了10 % ,在一定程度上改善了检索效果。","计算机应用,中文信息处理,搜索引擎,信息检索,向量空间模型,超链接"
2004-07-20,基于遗传和BP算法的车牌图像快速匹配,"刘栓,孟庆春","将基于遗传的BP 神经网络算法用于智能交通中的车牌图像匹配,结合了遗传算法和BP 算法的优点。先采用遗传学习算法进行全局寻优、再利用BP 算法进行精确训练、优化BP(Back Propagation) 神经网络权重学习和训练的神经网络图像匹配算法。实验结果表明:本文设计算法较好地达到了匹配要求,能够对目标图像与样本图像进行正确匹配,匹配概率达到了92 % ,而传统的BP 神经网络仅有79 % ,并且在匹配速度上也明显优于传统的BP 神经网络及其他改进算法,具有精确性、收敛性和匹配快等特点。","人工智能,模式识别,遗传算法,BP(back propagation) 神经网络,图像匹配,小波变换"
2004-11-21,一种有效的手写体汉字组合特征的抽取与识别算法,"孙权森,金忠,王平安,夏德深","基于特征融合的思想,从有利于模式分类的角度,推广了典型相关分析的理论,建立了广义的典型相关分析用于图像识别的理论框架。在该框架下,首先利用广义的典型相关判据准则函数,求取两组特征矢量的广义投影矢量集,构成一对变换矩阵;然后根据所提出的新的特征融合策略,对两种手写体汉字特征进行融合,所抽取的模式的相关特征矩阵,在普通分类器下取得了良好的分类效果,优于已有的特征融合方法及基于单一特征的PCA 方法和FLDA 方法。","人工智能,模式识别,手写体汉字识别,广义的典型相关分析,特征融合"
2004-07-01,语音识别中的一种说话人聚类算法,"肖述才,欧智坚,王作英","本文介绍了稳健语音识别中的一种说话人聚类算法,包括它在语音识别中的作用和具体的用法,聚类中常用的特征、距离测度,聚类的具体实现步骤等。我们从两个方面对该算法的性能进行了测试,一是直接计算句子聚类的正确率,二是对说话人自适应效果的改进的作用,即比较使用此算法后系统性能的改进进行评价。实验表明:在使用GLR 距离作为距离测度的时候,该算法对句子的聚类正确率达85169 %;在识别实验中,该聚类算法的使用,使得用于说话人自适应的数据更加充分,提高了自适应的效果,系统的误识率已经接近利用已知说话人信息进行自适应时的误识率。","计算机应用,中文信息处理,说话人聚类,说话人自适应,语音识别"
2004-08-11,口语对话中的语句主题分析,"徐为群,徐波,黄泰翼","本文研究如何根据浅层的语义分析确定自然口语对话中的语句主题。首先将对话中的语句主题定义为说话者所关注的显著语义实体,并讨论了这样的语句主题所具有的两个特点(即话语性和连续性) 以及语句主题跟(扩展) 句子类型的关系(因而也介绍了句子类型及其扩展和扩展句子类型的识别) 。然后根据这些建立了语句主题分析算法,并在实际的对话语料中进行分析。实验结果表明,语句主题的分析正确率可达到6111～8716 % ,取决于不同的扩展句子类型和不同的正确率定义。","人工智能,自然语言处理,语句主题,句子类型,对话,计算分析"
2004-08-24,X-Window核心系统的民文支持,"谢谦,吴健,孙玉芳","Linux 系统对少数民族文字的支持需要建立在国际化机制基础上,本文在总结现有Linux 国际化框架层次结构基础上,分析了X核心系统国际化的一些关键问题,并以增加藏文支持的实践为例,系统说明了增加民族文字支持所需对X核心系统进行的改动,对在相关项目中的实施情况和效果进行了评估,最后结合其他民族文字系统的研究,对这些工作的局限性进行了深入分析,并提出了今后的工作方向。","计算机应用,中文信息处理,X窗口系统,国际化,藏文"
2005-07-07,基于规则与统计相结合的中文文本自动查错模型与算法,"张仰森,曹元大,俞士汶","中文文本自动校对是自然语言处理领域具有挑战性的研究课题。本文提出了一种规则与统计相结合的中文文本自动查错模型与算法。根据正确文本分词后单字词的出现规律以及“非多字词错误”的概念,提出一组错误发现规则,并与针对分词后单字散串建立的字二元、三元统计模型和词性二元、三元统计模型相结合,建立了文本自动查错模型与实现算法。通过对30篇含有578个错误测试点的文本进行实验,所提算法的查错召回率为86.85%、准确率为69.43% ,误报率为30.57%。","计算机应用,中文信息处理,中文文本自动查错,规则与统计相结合,非多字词错误,真多字词错误"
2005-06-24,隐喻的计算研究与进展,王治敏,"隐喻作为自然语言处理最棘手的问题之一逐渐引起了学者们的关注,国外学者在隐喻模型设计和隐喻知识库建设方面进行了很多尝试,也取得了一定效果。相比较而言,中文隐喻计算研究却显得有些薄弱,因此本文在传统隐喻研究的基础上综述了隐喻计算模型和隐喻知识库建设方面的进展,重点介绍了基于优选限制思想的Met5系统,基于实例方法的MIDAS系统,以及以统计为手段,基于大规模语料库提取的隐喻分析模型CorMet系统。通过总结国外的相关研究成果,探索面向信息处理的汉语隐喻形式化的研究方向。","计算机应用,中文信息处理,综述,隐喻计算模型,源域,目标域,隐喻概念"
2005-08-09,复述技术研究综述,"刘挺,李维刚,张宇,李生","复述是自然语言中比较普遍的一个现象,它集中反映了语言的多样性。复述研究的对象主要是短语或者句子的同义现象。自然语言处理各种底层技术的不断发展和成熟,为复述研究提高了可能,使之受到越来越多的关注。在英文和日文方面,复述技术已经被成功的应用到信息检索、自动问答、信息抽取、自动文摘以及机器翻译等多个领域,有效地提高了系统的性能。本文主要对复述实例库的构建、复述规则的抽取以及复述的生成等几方面的最新研究进展进行详细的综述,并简要介绍了我们在中文复述方面进行的初步研究工作。在文章的最后一部分,我们对复述技术的难点及未来的发展方向进行了展望,并对全文进行了总结。","人工智能,自然语言处理,综述,句子复述,复述语料库,复述抽取,复述生成"
2005-06-09,重新审视跨语言信息检索,"闵金明,孙乐,张俊林","阻碍互联网资源在世界范围内广泛共享的一个主要障碍是多语言问题,而跨语言信息检索是解决这个问题的有效方法之一。本文从定义跨语言信息检索系统开始,给出了一个标准的跨语言信息检索系统框架和评价方法,对主流研究方法进行了重新审视,进一步明确指出了跨语言信息检索中必须解决的核心问题,最后通过分析研究现状给出了未来可能的重点研究方向。","计算机应用,中文信息处理,跨语言信息检索,未登录词,词义消歧,多语言信息检索"
2005-07-06,基于混合语言模型的文档相似性计算模型,"李晓光,于戈,王大玲","为了克服现有文档相似性模型对文档特性拟合的不完全性和缺乏理论根据的弱点,本文在统计语言模型的基础上,提出了一种基于混合语言模型(Mixture Language Model,MLM)文档相似性计算模型。MLM利用统计语言模型描述文档特征,将相关影响因素作为模型的潜在子模型,文档语言模型由各子模型混合构成,从而准确和全面地反映文档特征。由于MLM根据具体应用确定相关影响因素,并以此构建相应文档描述模型,因此具有很强的灵活性和扩展性。在MLM的基础上,本文给出了一个基于文档主题内容相似性的实例,在TREC9数据集上的实验表明MLM优于向量空间模型(VSM)。","人工智能,自然语言处理,文档相似性,统计语言模型,混合模型,EM算法"
2005-07-22,基于概念匹配的中文问答处理模型核心问题探讨,"吴晨,张全","为了解决问答处理系统中的语义模糊问题,提高问答处理的性能,研究人员尝试采用概念作为系统处理的对象,而不再是语言表层符号,然而,在引入概念进行处理的同时引来了一些新的问题,如概念的抽取、概念关联计算以及特定于问答系统的问题理解、问题求解、答案生成等问题。在概念抽取、概念关联计算方面,已有一些比较成功的算法。本文将在此基础上,针对实现这样一个问答处理系统所存在的一些未涉及的核心问题进行一个探讨,同时提出解决以上问题的方法。实验及实际应用表明基于所提出算法的概念问答系统具有较强的性能,系统总体自动处理准确率将近达到40%。在实际应用中也表现出较高的应用价值。","计算机应用,中文信息处理,中文问答系统,语言概念空间,核心问题研究,概念匹配,算法"
2005-05-18,蒙古语语言-文字的自动化处理,"伊·达瓦,张玉洁,上园一知,大川茂树,章森,井佐原均,白井克彦","本文首先叙述了蒙文电子化的意义以及蒙文电子化数据的现状。然后重点讨论了在不同地区和国家使用的蒙文书面语以及口语的不同和蒙文在计算机处理时所面临的问题。最后,介绍了我们在日本建设的针对蒙古语语言信息处理的两种语言资源:蒙古语多方言口语语料库和蒙文多文种-多语言并行语法标注电子词典,后者得到了2005年中日蒙韩国际合作课题“蒙文自然语言处理技术的研究”的资助。","计算机应用,中文信息处理,蒙文语言文字信息处理,文本-口语语料库,多文种-多语言电子词典"
2005-05-20,基于HMM的满文文本识别后处理的研究,"赵骥,李晶皎,王丽君,张继生","将满文单词识别系统的识别信息和满文的词组信息有机的结合起来,建立满文词组和待定词集统计信息库,采用基于统计的隐马尔可夫模型的方法,依据贝叶斯准则,综合满文待定词的后验概率和词组的先验概率信息,建立合理有效便于实现的数据结构,采用动态规划法对满文单词识别系统输出存在的拒识词和错识词进行检测和纠正,从而有效的提高满文文本识别系统的识别率。实验表明:后处理性能除取决于语言模型外,还取决于概率的精确估计。另外,在单词识别系统识别率高的情况下,后处理的纠错能力会增强。","计算机应用,中文信息处理,满文,后处理,模糊矩阵,贝叶斯准则,特征矢量"
2005-06-25,基于trigram语体特征分类的语言模型自适应方法,"梁奇,郑方,徐明星,吴文虎","本文从书面语和口语存在的差异出发,提出了语言模型的语体自适应方法。自适应采用了几种不同的计数意义上的插值算法。考虑Katz平滑的插值算法根据trigram单元的可信度来分配权值。基于trigram语体特征分类的自适应算法根据trigram单元的语体特征倾向动态分配权值,并选取了几种不同的权值生成函数。对口语语料做音转字的实验证明,使用这几种自适应算法可以让基准模型的性能有不同程度的提高,其中综合考虑单元可信度和特征倾向的算法效果最好,相对于本文的两个基准的汉字错误率下降率分别达到了50.2%和23.7%。","计算机应用,中文信息处理,统计语言模型,trigram,自适应,语体,插值算法"
2005-07-15,基于HMM的可训练中文语音合成,"吴义坚,王仁华","本文将基于HMM的可训练语音合成方法应用到中文语音合成。通过对HMM建模参数的合理选择和优化,并基于中文语音特性设计上下文属性集以及用于模型聚类的问题集,提高其建模和训练效果。从对比评测实验结果来看, 98.5%的合成语音在改进后其音质得到改善。此外,针对合成语音节奏感不强的问题,提出了一种基于状态和声韵母单元的两层模型用于时长建模和预测,集外时长预测RMSE由29,56ms降为27.01ms。从最终的合成系统效果来看,合成语音整体稳定流畅,而且节奏感也比较强。由于合成系统所需的存贮量非常小,特别适合嵌入式应用。","计算机应用,中文信息处理,语音合成,HMM,可训练语音合成,时长模型"
2005-06-28,一种新的基于主题的语言模型自适应方法,"任纪生,王作英","基于主题的语言模型自适应方法应尽可能提高语言模型权重系数的更新速度并降低语言模型的调用量以满足语音识别实时性要求。本文采用基于聚类的方法实现连续相邻二元词对的量化表示并以此刻画语音识别预测历史和各个文本主题中心,依据语音识别历史矢量和各个文本主题中心矢量的相似度更新语言模型权重系数并摒弃全局语言模型。同传统的基于EM算法的自适应方法相比,实验表明该方法明显提高了语音识别性能和实时性,识别错误率相对下降5.1% ,说明该方法可比较准确地判断测试内容所属文本主题。","计算机应用,中文信息处理,语言模型,主题自适应,语音识别,文本分类"
2005-04-11,LINUX下维、哈、柯文多语种图形化处理平台的设计与实现,"苏国平,缪成,夏国平","针对维吾尔文字、哈萨克文字、柯尔克孜文字(以下简称“维哈柯文”)的特点以及进行维哈柯文、西文等多语种混合处理时的特殊需求,本文通过对Linux的I18N体系中NLS(National Language Support)研究分析,提出了基于Linux的多语种图形化处理平台的设计目标与总体架构。该平台由维哈柯文本地化环境、维哈柯文显示、自适应维哈柯文输入和维哈柯文打印输出等4个子系统的十余个模块组成。本文详细介绍了各子系统主要模块的实现技术。通过在redhat linux 810、turbolinux上测试表明,该平台在桌面环境、编辑软件、网络浏览、数据库软件、多媒体软件、图形处理软件等应用中均能较好的实现维哈柯文、汉文、西文的混合输入、显示、编辑、排版、打印等功能。","计算机应用,中文信息处理,多语种,图形化处理平台,Linux"
2005-05-14,基于Qt的国际化图形用户界面设计与实现,"刘汇丹,芮建武,姚延栋,吴健","一次开发多语言使用是国际化软件开发的主要目标。但是世界上的文字多种多样,它们的书写方向也有所不同,除了水平从左向右书写的英文、水平从右往左书写的阿拉伯文外,还有类似蒙古文这样垂直排列的文字,这对计算机图形用户界面提出了更高的要求,现有的计算机系统将这类垂直排列的文字沿水平方向输出,极不符合少数民族人民的习惯。在分析现有Qt库对类似阿拉伯文这样从右向左书写的文字的部分支持机制的基础上,我们设计并实现了支持四种方向模式的国际化的图形用户界面,现在它已经能够适应世界上几乎所有的文字。这对于软件国际化以及民族语言信息处理有重要意义。","计算机应用,中文信息处理,图形用户界面,Qt库,国际化,民族文字处理"
2005-05-23,智能型汉字数码输入技术的研究,"顾平,朱巧明,李培峰,钱培德","针对数字编码的特点,本文提出了一种在不改变编码方案的情况下通过改进输入规则,结合语言模型,实现汉字数字编码的智能输入技术。文章首先讨论了怎样设计字词码本结构,使之能够满足灵活多样的输入方式,继而设计了一种动态自学习语言模型,重点分析了数据平滑算法在语言模型中的应用与改进,最后通过一个输入法示例程序,对改进前后不同情况下的输入效果进行了测试。实验表明,这种输入技术不但降低了输入法的平均码长,而且显著地提高了首字命中率。","计算机应用,中文信息处理,汉字输入,数字编码,智能输入,动态自学习语言模型"
2005-12-14,2005统计机器翻译研讨班研究报告,"徐波,史晓东,刘群,宗成庆,庞薇,陈振标,杨振东,魏玮,杜金华,陈毅东,刘洋,熊德意,侯宏旭,何中军","2005年7月13日至15日,中国科学院自动化研究所、计算技术研究所和厦门大学计算机系联合举办了我国首届统计机器翻译研讨班。本文主要介绍本次研讨班参加单位的测试系统和实验结果,并给出相应的分析。测试结果表明,我国的统计机器翻译研究起步虽晚,但已有快速进展,参评系统在短期内得到了较好的翻译质量,与往年参加863评测的基于规则方法的系统相比性能虽还有差距,但差距已经不大。从目前国际统计机器翻译研究的现状和发展趋势来看,随着数据资源规模的不断扩大和计算机性能的迅速提高,统计机器翻译还有很大的发展空间。在未来几年内,在基于短语的主流统计翻译方法中融入句法、语义信息,必将成为机器翻译发展的趋势。","人工智能,机器翻译,统计机器翻译,基于短语的翻译模型,机器翻译评测"
2005-09-09,基于规则和统计的中文自动文摘系统,"傅间莲,陈群秀","自动文摘是自然语言处理领域里一个重要课题,本文在传统方法基础上提出了一种中文自动文摘的方法。在篇章结构分析里,我们提出了基于连续段落相似度的主题划分算法,使生成的文摘更具内容全面性与结构平衡性。同时结合了若干规则对生成的文摘初稿进行可读性加工处理,使最终生成的文摘更具可读性。最后提出了一种新的文摘评价方法(F-new-measure)对系统进行测试。系统测试表明该方法在不同文摘压缩率时,评价值均较为稳定。","计算机应用,中文信息处理,自动文摘,向量空间模型,主题划分,可读性,评价"
2005-09-07,一种快速获取领域新词语的新方法,刘华,"本文提出一种新词语识别新方法。该方法直接抽取分类网页上人工标引的关键词,并按照其网页栏目所属类目存储进各分类词表,从而快速完成新词语识别和聚类任务。该方法简单快捷。我们利用该方法从15类6亿字网页中抽取到229237个词条,其中新词语175187个,新词率为76.42% ,其中游戏类新词率最高,时政_社会类新词率最低。新词语以命名实体为主,结构固定,意义完整性和专指性强,有助于解决歧义切分和未登录词问题,并能提高文本表示如分类和关键词标引的效果。","人工智能,自然语言处理,新词语,识别,聚类"
2005-06-20,汉语词典的快速查询算法研究,"李江波,周强,陈祖舜","汉语词典查询是中文信息处理系统的重要基础部分,对系统效率有重要的影响。本文对汉语词典查询算法研究作了简要回顾,设计实现了基于双数组TRIE机制的汉语词典查询算法,并提出了基于双编码机制的词典查询算法。最后以逐字二分法查询性能为基准,使用这两种词典询机制进行了词语直接查询和分词查询两种应用的性能测试。经过实验分析,双数组TRIE机制的词典查询算法在查询速度上提高明显,查询速度约是逐字二分法的5倍。双编码机制的的词典查询算法查询速度有一定提高,而且调整机制更加灵活。","计算计应用,中文信息处理,汉语词典查询,双数组TRIE,双编码算法"
2005-08-13,一种基于信息熵的中文高频词抽取算法,"任禾,曾隽芳","为扩展分词词典,提高分词的准确率,本文提出了一种基于信息熵的中文高频词抽取算法,其结果可以用来识别未登录词并扩充现有词典。我们首先对文本进行预处理,将文本中的噪音字和非中文字符转化为分隔符,这样文本就可以被视为用分隔符分开的中文字符串的集合,然后统计这些中文字符串的所有子串的相关频次信息,最后根据这些频次信息计算每一个子串的信息熵来判断其是否为词。实验证明,该算法不仅简单易行,而且可以比较有效地从文本中抽取高频词,可接受率可达到91.68%。","人工智能,自然语言处理,分词,中文抽词,信息熵,高频词"
2005-09-14,边界模板和局部统计相结合的中国人名识别,"李中国,刘颖","本文提出了一种基于篇章信息的中国人名识别算法。我们从标注语料中提取人名左右边界词语及人名用字频度作为系统知识源。识别过程是:首先利用带有频度的边界模板识别出可能的人名,并把识别结果扩散到整篇文章以召回数据稀疏导致的遗漏人名。然后应用上下文局部统计量及几条启发式规则对识别结果进行边界校正。该算法具有线性时间复杂度,大规模开放测试(针对1354篇新闻报道约304万字,含人名3.7万个)的正确率为94.52%,召回率为98.97%,效果非常令人满意。","计算机应用,中文信息处理,人名识别,命名实体识别,边界模板,局部统计量,词法分析"
2006,中文语言资源联盟,,"在国家高科技研究规划发展项目(863)和国家重点基础研究发展规划项目(973)以及其他项目的支持下,由中国中文信息学会语言资源建设和管理工作委员会发起,由中文语言(包括文本、语音、文字等)资源建设和管理领域的科技工作者自愿组成了中文语言资源联盟,该联盟是学术性、公益性、非盈利性的社会团体。本团体隶属于中国中文信息学会,接受中国中文信息学会语音资源建设和管理工作委员会的业务指导和监督管理。",
2005-05-26,SVM与规则相结合的中文地名自动识别,"李丽双,黄德根,陈春荣,杨元生","在分析中文文本中地名特点的基础上,提出了一种支持向量机(SVM)与规则相结合的中文地名自动识别方法:按字抽取特征向量的属性,然后将这些属性转换成二进制向量并建立训练集,采用多项式Kernel函数,得到SVM识别地名的机器学习模型;通过对错误识别结果的分析,构建规则库对识别结果进行后处理,弥补了机器学习模型获取知识不够全面导致召回率偏低的不足。实验表明,用SVM与规则相结合的机制识别中文文本中的地名是有效的:系统开式召回率、精确率和F-值分别达89.57%、93.52%和91.50%。","计算机应用,中文信息处理,中文地名识别,支持向量机,机器学习,基于规则的后处理"
2005-10-07,面向连续字符识别的手写汉字部件集及统计规律,"赵巍,李春娣,刘家锋,唐降龙","本文面向手写字符序列输入信号连续识别研究,分析了汉字及联机手写文本的特点,提出并构建了手写汉字部件集。基于该部件集,完成了GB2312-80的6,763个汉字的部件拆分编码和部件集的测试。统计编码数据发现,汉字依手写部件数的分布规律呈对数正态分布。本文从统计学和字符识别技术的角度对手写部件的构字能力作了分析和讨论,部件集的设计方案在部件选择和汉字拆分上均满足设计要求。实验表明,基于手写部件构造的部件识别器对手写汉字和连续汉字的部件识别率分别达到70.21%和58.49%。","人工智能,模式识别,连续字符识别,手写汉字部件,对数正态分布"
2005-12-23,基于高阶统计的手写字符形变弹性匹配法,"马瑞,杨静宇","针对传统弹性匹配法在手写字符识别中存在着由于过匹配而造成误识别的不足,提出一种基于高阶统计的形变弹性匹配法。根据高阶统计量包含字符形状上的细节变化信息,采用独立分量分析抽取出每个字符类的内在变化方向,并将其应用到弹性匹配的形变模型中。字符的任意种形状变化由这组独立分量的线性叠加来表示。通过形变模型,类模板字符发生形变逐次向输入待识别字符趋近,从而在两个字符之间求得一种最佳匹配。在实验结果中,识别率达到92.81%,得到了提高,表明该方法的有效性。","人工智能,模式识别,手写字符识别,高阶统计,弹性匹配,内在形变,独立分量分析(ICA)"
2005-09-20,基于时域单元融合的拼接平滑算法,"郭武,吴义坚","针对基于大语料库的拼接合成系统中经常出现的拼接单元不匹配问题,特别是浊音拼接处不匹配对合成效果会产生较大的损伤,本文提出一种基于时域单元融合技术的平滑算法。它通过模板匹配选取合适的过渡段模板作为融合单元,并同时进行相位对齐,然后采用TD-PSOLA的方法对拼接单元和融合单元进行时域上的基音同步迭加融合。它的优点是对音质损伤很小,而且直接在时域上进行,效率高。通过对平滑前后语谱及主观听感两个方面的对比评测,平滑后的效果比平滑前有明显改善。","计算机应用,中文信息处理,时域单元融合,拼接单元,融合单元"
2005-07-08,编码字符集标准及分类研究,"谢谦,芮建武,吴健","编码字符集标准是计算机处理文字信息的基础,本文提出了编码字符集三元组抽象,对现有编码字符集标准进行了简单回顾和总结,深入剖析了影响巨大的ISO2022标准及其派生标准,对ISO2022编码机制应用于多语言环境的局限性进行了探讨,阐明了使用通用编码字符集UCS的必要性,并对其进行了分析。探讨了现有编码分类方法存在的问题,引入了一种对编码字符集以及实现方法进行分类的新方法,使用该方法对现有标准进行了归类;最后对汉字字符集相关的国家标准进行了分析评介。","计算机应用,中文信息处理,编码字符集"
2005-06-17,一个基于ISO/IEC10646的汉字输入模型,"李培峰,朱巧明,钱培德","计算机中各国文字编码的统一是必然趋势,而ISO/IEC10646正是顺应这种趋势而诞生的一个国际标准。现有的输入法绝大多数是基于本地代码页(ANSI CODE),存在着移植困难、不能跨语言平台以及向国际化标准过渡困难等缺点。本文首先分析了现有本地化输入法存在的问题,并在此基础上阐述了基于ISO10646的汉字输入法的实现方法,并给出了一个以ISO10646为核心的通用汉字输入法模型和原理,该模型由输入法管理/服务器、ISO10646输入码对照表、码本检索/过滤模块、输入法与OS接口模块、输入法内核和本地化接口六部分构成。最后,本文重点论述了输入法的核心—输入码对照表的设计和检索技术。","计算机应用,中文信息处理,输入法模型,ISO/IEC10646,Unicode,输入码对照表"
2005-07-20,汉字输入编码优劣评测方法的探讨,孙基寿,"字形编码的优劣必须进行科学的评测。编码规则的轻松性和速度潜力是评价字形编码优劣的两个关键指标。本文共分四部分,第一部分简单地陈述了什么是简单、规范、易学、轻松,提出了选择轻松的理由;第二部分通过具体的例子说明了导致轻松与不轻松的内在因素,提出了评测轻松的实验草案;第三部分分析了考核一种通用键盘汉字输入系统速度素质的现状,认为字形编码应将编码层次和软件层次分割开来进行性能考核,编码层次应评测编码规则的轻松性和速度潜力;第四部分从实践和理论两个方面分析了平均偏移量与速度潜力之间的关系,即平均偏移量越小,速度潜力就越大,并提出了反映速度潜力的参数指标。","计算机应用,中文信息处理,汉字编码,评测方法,轻松性,速度潜力"
2005-10-09,基于双层决策的新闻网页正文精确抽取,"胡国平,张巍,王仁华","本文提出了基于双层决策的新闻网页正文的精确抽取算法,双层决策是指对新闻网页正文所在区域的全局范围决策和对正文范围内每段文字是否确是正文的局部内容决策。首先根据实际应用的需要给出了新闻网页正文的严格界定,然后分析了新闻网页及其正文的特性,提出了基于双层决策的正文抽取策略,基于特征向量提取和决策树学习算法对上述双层决策进行了建模,并在国内10个主要新闻网站的1687个新闻页面上开展了模型训练和测试实验。实验结果表明,上述基于双层决策的方法能够精确地抽取出新闻网页的正文,最终正文抽取与人工标注不完全一致的网页比例仅为18.14% ,比单纯局部正文内容决策的方法相对下降了29.85% ,同时抽取误差率大于10%的网页比例更是仅为7.11% ,满足了实际应用的需要。","计算机应用,中文信息处理,信息抽取,特征向量,决策树,正文抽取"
2005-10-19,应用于长频繁集挖掘的基于变动邻域搜索的遗传算法设计,"章舜仲,王树梅,黄河燕,陈肇雄","提出了一种基于变动邻域搜索的长频繁集挖掘方法(VNS-GA),利用遗传算法的高效搜索性能快速挖掘最大频繁集。在遗传算法的适应度函数设计中,综合考虑项集支持度、长度以及项集支持度和邻域中心支持度的距离,算法一次运行可找出邻域内的最大频繁集,改变邻域中心即可找到我们需要的最大频繁集。算法有效性通过实验得到了验证,且实验表明该算法的时间复杂度与支持度阈值大小无关,因此对于长模式挖掘问题具有很高的效率。","计算机应用,中文信息处理,遗传算法,频繁集,搜索空间,邻域搜索,apriori性质"
2005-09-26,基于SVM的组块识别及其错误驱动学习方法,"黄德根,王莹莹","给出了一种错误驱动学习机制与SVM相结合的汉语组块识别方法。该方法在SVM组块识别的基础上,对SVM识别结果中的错误词语序列的词性、组块标注信息等进行分析,获得候选校正规则集;之后按照阈值条件对候选集进行筛选,得到最终的校正规则集;最后应用该规则集对SVM的组块识别结果进行校正。实验结果表明,与单独采用SVM模型的组块识别相比,加入错误驱动学习方法后,组块识别的精确率、召回率和F值均得到了提高。","计算机应用,中文信息处理,组块分析,错误驱动学习,支持向量机(SVM),规则集"
2005-11-21,一种基于词汇链的关键词抽取方法,"索红光,刘玉树,曹淑英","关键词在文献检索、自动文摘、文本聚类/分类等方面有十分重要的作用。词汇链是由一系列词义相关的词语组成,最初被用于分析文本的结构。本文提出了利用词汇链进行中文文本关键词自动标引的方法,并给出了利用《知网》为知识库构建词汇链的算法。通过计算词义相似度首先构建词汇链,然后结合词频与区域特征进行关键词选择。该方法考虑了词汇之间的语义信息,能够改善关键词标引的性能。实验结果表明,与单纯的词频、区域方法相比,召回率提高了7.78%,准确率提高了9.33%。","计算机应用,中文信息处理,关键词标引,关键词抽取,词汇链,词义相似度,知网"
2005-01-05,自动获取汉语词语搭配,"王素格,杨军玲,张武","作为一种词汇现象,词语搭配在自然语言处理的许多领域具有重要的应用。本文对4种词语相关性度量和3种词语结构分布度量分别进行了比较分析,并提出了一种基于互信息与熵融合的获取词语搭配的方法。实验结果表明:在同现频率较高情况下,互信息、Cosine系数、x2测试和似然比测试4种相关性度量对搭配判定有大致相同的效果;在度量词语的结构分布方面,熵要优于方差和离散度。本文所提方法依赖度量指标少,阈值容易选取,且与其他已有的方法具有同等效果。","计算机应用,中文信息处理,词语搭配,互信息,熵"
2005-10-31,基于词语属性的计算机辅助获取流行词语研究,"何婷婷,朱薏,张勇,任函","本文以2005年的1月1日至6月25日新浪网上下载的各类页面上的文本内容为研究资源集合,从中提取出有效词语,对词语的流行程度的判定属性做了定性定量的分析研究,对词语的流行特性进行了定义,在此基础上,引入衡量关注程度的量化方法,并配合依据词语判定属性与时间关系而绘制的走势曲线图,设置淘汰机制与评分机制,得到了候选流行词语,验证了流行词语判定属性规范的合理性,为机器辅助判定词语特性提供了参考数据。","计算机应用,中文信息处理,流行词语,词语活动能力,走势曲线图"
2005-10-08,基于Web的文摘技术研究,"耿增民,贾云得,刘万春,朱玉文","Web文档的迅猛增长使Web文摘技术成了当今的一个研究热点。由于Web文档的特殊性,使得Web文摘不同于传统的文本自动文摘。本文分析了Web文档的特点;给出了Web文摘的定义;提出了基于句子抽取的Web文摘生成算法。算法中将每个Web句子权重分解为Web特征词权重和Web句子结构权重,并用机器学习的方法来计算二者所占的比重。Web特征词权重根据文档分类树图进行权值调整,Web句子结构权重充分考虑排版格式和超连接属性。通过对1000篇Web文档的文摘实验,证明文中所提Web文摘算法切实可行。","计算机应用,中文信息处理,Web文摘,文本文摘,Web文档预处理,文摘后处理"
2005-12-11,基于语义分析的作者身份识别方法研究,"武晓春,黄萱菁,吴立德","作者身份识别是一项应用广泛的研究,身份识别的关键问题是从作品中提取出代表语体风格的识别特征,并根据这些风格特征,评估作品与作品之间的风格相似度。传统的身份识别方法,主要考察作者遣词造句、段落组织等各种代表文体风格的特征,其中基于标点符号和最常见功能词频数的分析方法受到较为普遍的认同。本文依据文体学理论,利用HowNet知识库,提出一种新的基于词汇语义分析的相似度评估方法,有效利用了功能词以外的其他词汇,达到了较好的身份识别性能。","计算机应用,中文信息处理,身份识别,语义分析,文档相似度"
2005-12-19,手写中文地址识别后处理方法的研究,"龙翀,庄丽,朱小燕,黄开竹,孙俊,堀田悦伸,直井聡","OCR(光学字符识别技术)作为方便有效的字体识别技术,在办公自动化、信息恢复、数字图书馆等方面发挥着日益重要的作用。语言模型在OCR后处理,特别是在中文的文字识别后处理方面有着广泛的应用。本文针对手写中文地址的后处理,讨论了语言模型的粒度对识别正确率的影响,分析了基于字和基于词的语言模型各自的优点和缺点,并采用了基于词的语言模型,在此基础上提出了加权词图搜索算法。实验证明,在58269条中文手写地址的测试集上,手写地址的整体识别率由原来的28.56%上升到了75.66% ,错误率下降了65.93% ,大大提高了系统的性能。","人工智能,模式识别,OCR,语言模型,后处理"
2005-09-28,汉语连续语音中HMM模型状态数优化方法研究,"何珏,刘加","为了优化汉语连续语音中HMM模型系统以提高识别性能,提出了分别为每个声母和韵母半音节声学模型选择最优的状态数的方法。通过综合考虑每个声母和韵母半音节声学模型在不同状态数下的段长均值、方差以及各自识别率这三者信息,作为进行最优模型状态数的选择准则。优化后的声学模型系统由状态数各不相同的声母半音节声学模型组成,同未优化前状态数统一的模型系统相比,音节识别性能提高了5.07个百分点。研究表明,每个声母和韵母半音节志学模型应根据情况选择不同的状态数,优化后的模型系统识别性能得到了提高。","计算机应用,中文信息处理,声学模型,隐型Markov模型,语音识别"
2005-11-02,普通话水平测试电子化系统,"魏思,刘庆升,胡郁,王仁华","普通话水平测试电子化系统有助于高效地进行普通话水平测试。本文在100小时标准发音人数据库的基础上,针对汉语发音特点,利用语言学专家知识,引入语料选择的自适应算法改进了传统的语音评测算法。在500人普通话水平测试数据库上的测试结果表明,新评测算法能有效提升评测性能。经过分段线性映射,机器评分和人工评分的误差(2.44)和人工与人工评分之间的误差(2.30)相当。这表明可以使用机器代替人工进行普通话水平测试的前三项评分工作。","计算机应用,中文信息处理,普通话水平测试,语音评测,普通话水平测试数据库,电子化系统"
2005-09-07,ISO 2022的有限状态机描述,"谢谦,芮建武,吴健","ISO 2022编码体系对字符集国家标准的制订有很大影响,然而标准条款存在不确定性,有时难于理解。本文引入有限状态机(FSM)模型来形式化地刻画ISO 2022的特征。针对FSM五元组,详细说明了其状态空间的构成,提出了输入字母表的等效分类方法,给出了初始状态以及终结状态集合,分析了状态转移函数的规模,并采用FSM描述方法分析了ISO-2022-CN、EUC-CN、复合文本等标准,揭示了这些标准与ISO 2022的内在联系。这些工作有助于ISO 2022标准符合性检测、扩展标准的制订与系统实现复杂度评估。鉴于形式化描述方法在编码字符集标准领域未得到广泛应用,本文工作为该类研究引入了新的思路和方法。","计算机应用,中文信息处理,编码字符集,ISO 2022,有限状态机"
2005-06-21,一个适用于手持设备的多层汉字输入法模型,"朱晓旭,李培峰,朱巧明,刁红军","随着以PDA和智能手机为代表的手持设备快速发展,汉字输入法选择余地小的缺点已经成为影响其普及的障碍之一。究其原因是手持设备中的操作系统和物理设备类型多,而开发的汉字输入法在不同手持设备中不通用,造成开发效率低。本文介绍了一个适用于手持设备的多层的通用汉字输入法模型,详细描述了模型中每一层的功能和特点,讲解了如何基于该模型实现一个输入法,并概要论述了本模型的优点。","计算机应用,中文信息处理,手持设备,汉字输入法"
2015-12-03,《中文信息学报》征稿简则,,一、《中文信息学报》主要刊登中文信息的基础理论、应用技术、中文信息处理系统及设备、中文信息的自动输入和人工编码输入、汉字字形信息、自然语言处理、计算语言学及民族语言文字信息处理及网上信息处理等方面的研究论文、技术报告、综述、通讯、简报、国内外学术活动等。,
2004-09-02,基于内容的垃圾邮件过滤技术综述,"王斌,潘文锋","垃圾邮件问题日益严重,受到研究人员的广泛关注。基于内容的过滤是当前解决垃圾邮件问题的主流技术之一。目前基于内容的垃圾邮件过滤主要包括基于规则的方法和基于概率统计的方法。本文综述了目前用于垃圾邮件过滤研究的各种语料和评价方法,并总结了目前使用的垃圾邮件过滤技术以及它们之间的对比实验,包括Ripper、决策树、Rough Set 、Rocchio 、Boosting、Bayes、kNN、SVM、Winnow 等等。实验结果表明,Boosting、Flexible Bayes、SVM、Winnow 方法是目前较好的垃圾邮件过滤方法,它们在评测语料上的结果已经达到很高水平,但是,要走向真正实用化,还有很多的工作要做。","计算机应用,中文信息处理,综述,垃圾邮件,反垃圾邮件,信息过滤,文本分类"
2004-07-06,一种文本分类的在线SVM学习算法,"代六玲,黄河燕,陈肇雄","本文提出了一种用于文本分类的RBF 支持向量机在线学习算法。利用RBF 核函数的局部性,该算法仅对新训练样本的某一大小邻域内且位于“可能带”中的训练样本集进行重新训练,以实现对现有SVM的更新。为高效的实现该邻域大小的自适应确定,使用ξa 泛化错误估计在所有现有训练样本集上对当前SVM的泛化错误进行定性估计。同时引入泛化能力进化因子,使得结果SVM在分类效果上具有自动调整能力,并防止分类能力的退化。在TREC - 5 真实语料上的对比测试结果表明,该算法显著地加速了增量学习的过程而同时保证结果SVM的分类效果。","计算机应用,中文信息处理:文本分类,在线学习,增量学习,支持向量机,SMO"
2004-07-21,基于后缀树模型的文本实时分类系统的研究和实现,"郭莉,张吉,谭建龙","本文在面向网络内容分析的前提下,提出了一种基于后缀树的文本向量空间模型(VSM) ,并在此模型之上实现了文本分类系统。对比基于词的VSM,该模型利用后缀树的快速匹配,实时获得文本的向量表示,不需要对文本进行分词、特征抽取等复杂计算。同时,该模型能够保证训练集中文本的更改,对分类结果产生实时影响。实验结果和算法分析表明,我们系统的文本预处理的时间复杂度为O(N) ,远远优于分词系统的预处理时间复杂度。此外,由于不需要分词和特征抽取,分类过程与具体语种无关,所以是一种独立语种的分类方法。","计算机应用,中文信息处理,实时文本分类,向量空间模型,后缀树"
2004-09-01,面向体育领域的句子主干翻译技术研究,"薛永增,杨沐昀,赵铁军,韩习武,齐浩亮","为了有效翻译体育领域文本,特别是文本中的长句,本文提出了一种面向体育领域的句子主干翻译方法。该方法采用模板来表示句子主干,主要包括句法主干分析、模板转换和句子主干译文生成三个步骤。本文研究中特别针对体育领域的语言特点进行了模板的设计和获取;在译文生成过程中,则分别利用规则和模板,采用了短语级全译和句子级摘译相结合的混合生成策略,并引入翻译函数来处理形态变化。实验结果表明句子主干翻译方法能够获取句子的关键信息,在可懂度上优于完全翻译,其忠实度也令人满意,是处理体育领域文本的有效方法。","人工智能,机器翻译,句子主干翻译,模板,体育领域"
2004-09-08,基于长度的扩展方法的汉英句子对齐,"张艳,柏冈秀纪","本文提出了一种用于汉英平行语料库对齐的扩展方法。该扩展方法以基于长度的统计对齐方法为主,然后根据双语词典引入了词汇信息,而基于标点的方法作为对齐的后处理部分。这种扩展方法不仅避免了复杂的中文处理,例如,汉语分词和词性标注,而且在统计方法中引入了关键词信息,以提高句子对齐的正确率。本文中所用的双语语料是LDC 的关于香港的双语新闻报道。动态规划算法用于系统的实现。和单纯的基于长度的方法和词汇方法相比,我们的扩展方法提高了句子对齐的正确率,并且结果是比较理想的。","人工智能,机器翻译,句子对齐,中文处理,双语语料库"
2004-07-26,用动词的论元结构跟事件模板相匹配——一种由动词驱动的信息抽取方法,袁毓林,"本文以文献[2 ]中信息抽取模型( InfoX) 的测试语料(职务变动文本) 为主要对象,具体说明怎样建立从动词的论元结构到相关的事件模板的匹配关系。首先根据职务变更动词的有关句法、语义特点,把它分成六个小类:任命、担任、免职、辞职、调遣、受命;然后,分别描写每一小类动词的论元结构,特别是它们所支配的论元角色及其句法配置方式。最后,建立动词的论元角色跟事件模板元素的匹配关系,并揭示动词对文本筛选和合并都有导向作用,说明发展由动词驱动的信息抽取方法的可行性。","计算机应用,中文信息处理,信息抽取,事件模板,论元结构,模板元素,论元角色"
2004-09-10,面向主题会话的扩展语义框架,"施海虎,邢宣宇,李冬梅","随着人工智能技术的发展,基于人机会话的智能化技术成为当前研究热点,知识表示是人机会话领域的研究难点之一。在众多的知识表示方法中,框架表示法由于具有适应性强、概括性高、结构化良好、推理方式灵活的特点而得到人们的广泛应用。本文在基于篮球运动主题会话课题研究中,提出了一种扩展语义框架表示方法。与传统的表示方法相比,该扩展语义框架能够解决基于领域的知识处理、常识推理和语句生成问题,能够很好地满足受限人机会话系统的要求。","计算机应用,中文信息处理,语义框架,扩展语义框架,人机会话,主题会话"
2004-08-25,对蒙古语语料库基本名词短语的定界与统计分析,"华沙宝,达胡白乙拉","解决蒙古语基本名词短语的定界问题,是在蒙古语词性标注语料库的基础上进行的探索性研究。基本名词短语的内部结构信息对其定界问题具有重要作用。确定基本名词短语内部结构的因素有多种,但基本名词短语成分的词类信息是最基本的因素。我们以词类信息为核心,附加一些限定条件,构建识别基本名词短语的形式规则集,并在实际语料中进行基本名词短语标注测试。","计算机应用,中文信息处理,蒙古语,基本名词短语,短语结构,形式化描述"
2004-09-09,基于ISO/IEC10646标准的藏文操作系统若干问题研究,"芮建武,吴健,孙玉芳","长期以来尚未有完整的藏文操作系统,原因是藏文文字的特性要求特定的文字处理。本文基于ISOPIEC 10646 的藏文字符集标准,结合藏文正字法要求,详细分析了藏文操作系统实现中的关键问题: (1) 藏文字符集方案比较与藏文存储; (2) 藏文输入; (3) 藏文显现。藏文显现是公认的“瓶颈”问题。对此,本文提出基于音节划分、使用OpenType 字体及相应的文本引擎来解决藏文“叠加”字符的显现。此方案应用于Qt 库的实验及相关测试证明基于ISOPIEC 10646 标准的藏文操作系统实现是较合理的方案。","计算机应用,中文信息处理,藏文字符集,藏文叠加字符,OpenType"
2004-09-20,图文互斥版面中文字阅读顺序的确定,"贾娟,陈堃銶,周东浩","图文互斥版面中确定文字的阅读顺序是排版及版面理解过程中的一个难点。尤其是中文等东方文字特有的分栏串文互斥,其空间关系的复杂性使得阅读顺序存在歧义。针对此问题,建立新的版面布局模型,并引入新的版面对象PMRegion。给出了版面逐层快速分解构造版面对象和基于有序树的阅读顺序确定算法。已成功运用于专业中日文排版系统,取得了满意的效果,并对更深入研究文档图像理解具有十分重要的理论和实践意义。","计算机应用,中文信息处理,阅读顺序,图文互斥,有序树"
2004-06-12,印刷维吾尔文本切割,"靳简明,丁晓青,彭良瑞,王华","我国新疆地区使用的维吾尔文借用阿拉伯文字母书写。因为阿拉伯文字母自身书写的特点,造成维文文本的切割和识别极其困难。本文在连通体分类的基础上,结合水平投影和连通体分析的方法实现维文文本的文字行切分和单词切分。然后定位单词基线位置,计算单词轮廓和基线的距离,寻找所有可能的切点实现维文单词过切割,最后利用规则合并过切分字符。实验结果表明,字符切割准确率达到99 %以上。","计算机应用,中文信息处理,文本切割,字符切割,字符识别,维吾尔文"
2004-06-11,一个基于多代码页的中文屏幕实时解释引擎的设计,"李培峰,朱巧明,钱培德","目前,在计算机中汉字有多种代码页,汉字的多代码页并存现象将长期存在。为了实现汉字多代码页并存,需要汉字代码页自动识别技术的支撑。屏幕实时解释引擎是目前各种在线字典、词典以及教学软件的核心技术,此技术目前存在不能跨代码页,取词不全面、不正确等缺陷。本文主要针对以上情况,描述了采用汉字内码的代码页自动识别技术以及优化的自动屏幕取词技术的中文屏幕实时解释引擎的系统架构,并阐述了数据词典的设计以及在设计中采用的关键技术。对五百万汉字样本的测试中,应用此引擎的在线词典对有意义短字符串(不包括单字) 代码页的识别率可以达到99 %以上。","计算机应用,中文信息处理,汉字代码页自动识别,屏幕取词,ISO10646"
2004-05-25,HCL2000手写汉字数据库的更新及相关研究,"任俊玲,郭军","HCL2000 是目前最具影响力的手写汉字数据库之一,基于研究手写汉字规律的设计初衷,该数据库采用了以书写者为单位按文件形式组织和存放的方式。本文则从研究样本选择的应用角度出发,对HCL2000中的样本进行了重新组织,同时对该数据库中的错误进行了纠正,生成了一个新的手写汉字数据库HCL2004。文章最后基于HCL2004 数据库和方向线素特征进行了有关训练样本数对识别性能影响的研究,给出了3755类大字符集情况下的最佳训练样本数为300 的结论,同时还对识别过程中的样本选择问题进行了探讨。","人工智能,模式识别,HCL2000 ,手写汉字数据库,样本选择,HCL2004"
2004-10-18,基于统计学习的机器翻译模板自动获取方法,"胡日勒,宗成庆,徐波","本文提出了一种从未经深层次处理的双语口语语料库中自动获取机器翻译模板的方法。这种算法是一种无监督的、基于统计的、数据驱动的方法。这种方法有两个基本的步骤。首先,通过语法归纳分别从源语言和目标语言中获取语义类和短语结构类。然后,利用双语划界文法将短语结构类进行对齐。对齐的结果经过后处理就可以得到翻译的模板。初步的试验结果表明,本方法是有效的和切实可行的。","人工智能,机器翻译,双语语法归纳,翻译模板获取,结构对齐"
2004-11-12,班智达汉藏公文翻译系统中基于二分法的句法分析方法研究,"才藏太,华关加","机器翻译系统是一种典型的自然语言处理系统,语言技术是机器翻译系统中居于核心地位的技术。本文结合863 项目《班智达汉藏公文机器翻译系统》的研制实践,论述了词项信息同语法规则相结合的原则,提出了以动词为中心的句法分析二分法,从而在受限语言的范围内,为建立有较大适应性的机器翻译规则系统,有效地提高机器翻译语法分析的效率提供了有益的方法。","人工智能,机器翻译,二分法,语句结构,句法分析"
2004-11-22,多文档自动文摘综述,"秦兵,刘挺,李生","多文档文摘是将同一主题下的多个文本描述的主要的信息按压缩比提炼为一个文本的自然语言处理技术。随着互联网上信息的日益丰富,多文档文摘技术成为新的研究热点。本文介绍了多文档文摘的产生和应用背景,阐述了多文档文摘和其他自然语言处理技术的关系,对多文档文摘国内外研究现状进行了分析,在此基础上汇总提出了多文档文摘研究的基本路线及关键技术,并总结了多文档文摘的未来及发展趋势。","人工智能,自然语言处理,多文档文摘,自然语言处理,文本压缩"
2004-11-24,基于类别特征域的文本分类特征选择方法,"赵世奇,张宇,刘挺,陈毅恒,黄永光,李生","特征选择是文本分类的关键问题之一,而噪音与数据稀疏则是特征选择过程中遇到的主要障碍。本文介绍了一种基于类别特征域的特征选择方法。该方法首先利用“组合特征抽取”[1 ]的方法去除原始特征空间中的噪音 ,从中抽取出候选特征。这里“, 组合特征抽取”是指先利用文档频率(DF)的方法去掉一部分低频词,再用互信息的方法选择出候选特征。接下来,本方法为分类体系中的每个类别构建一个类别特征域,对出现在类别特征域中的候选特征进行特征的合并和强化,从而解决数据稀疏的问题。实验表明,这种新的方法较之各种传统方法在特征选择的效果上有着明显改善,并能显著提高文本分类系统的性能。","计算机应用,中文信息处理,文本分类,特征选择,类别特征域"
2005-01-11,自动文摘系统中的主题划分问题研究,"傅间莲,陈群秀","随着网络的发展,电子文本大量涌现,自动文摘以迅速、快捷、有效、客观等手工文摘无可比拟的优势,使得其实用价值得到充分体现。而主题划分是自动文摘系统中文本结构分析阶段所要解决的一个重要问题。本文提出了一个通过建立段落向量空间模型,根据连续段落相似度进行文本主题划分的算法,解决了文章的篇章结构分析问题,使得多主题文章的文摘更具内容全面性与结构平衡性。实验结果表明,该算法对多主题文章的主题划分准确率为9212 % ,对单主题文章的主题划分准确率为9911 %。","计算机应用,中文信息处理,自动文摘,向量空间模型,段落相似度,主题划分"
2004-12-26,词表的自动丰富——从元数据中提取关键词及其定位,王军,"词表和分类法是传统纸质文献环境下最重要的知识组织工具。它的更新和维护一直依靠手工进行。这限制了它在数字图书馆和网络信息环境下的应用。本文介绍了一项基于统计的、从元数据的标题中抽取关键词并定位在词表中的方法。定位的依据是抽取出的关键词所对应的标引词集的收敛性质。标引词是用于标引文献主题的、来自于词表的受控词汇,即主题词。在《中国分类主题词表》和北京大学图书馆提供的5 千余条计算机科技领域的书目数据上所进行实验证明了文中所述的方法是可行的、有效的。这一方法可以直接用来实现基于已标引语料库的自动编目和元数据自动生成。","计算机应用,中文信息处理,词表,元数据,关键词提取"
2004-10-15,特征词提取中同义处理的新方法,"邹娟,周经野,邓成,高南莎","本文利用文本分类中文本的特点提出了一种基于模糊集的同义词处理的新方法。本方法充分考虑不同文本类型中同义(近义) 词之间的差别,在训练中自动计算不同类型文本中特征词对其对应的同义概念的隶属度,从而实现了用模糊集来定义同义概念;然后应用同义概念来提取文本中的特征值。另外,本系统还利用模糊集来处理多义词的问题。文中给出了系统的处理算法。比较试验的结果表明该方法提高了分类的正确率,效果是令人满意的。整个系统达到了较高的自动化水平和较强的可移植性。","人工智图,自然语言处理,文本分类,特征值提取,同义词"
2005-01-27,汉语语义分析模型研究述评,"由丽萍,范开泰,刘开瑛","这篇述评的目的是为汉语语义处理的研究工作提供参考。我们首先分别分析了三种语义分析模型———词语依存(WD) 、概念依存(CD) 和核心依存(KD) 的理论基础和表达方式;然后,重点从功能和可操作性方面比较三者在语义表示方面的特点。结论是(1) 词语依存可操作性好但功能弱,概念依存功能强但可操作性差,二者的缺点都是极难解决的问题,核心依存兼顾词语和概念,可能是最适合汉语语义处理需要的; (2) 要使模型达到实用要求,需要在句法标注、词典编纂和规范化方面做大量复杂的工作。","计算机应用,中文信息处理,依存语法,概念依存理论,框架语义学,语义表示"
2004-10-10,潜在语义分析权重计算的改进,"刘云峰,齐欢,Xiang’en Hu,Zhiqiang Cai","自从潜在语义分析方法诞生以来,被广泛应用于信息检索、文本分类、自动问答系统等领域中。潜在语义分析的一个重要过程是对词语文档矩阵作加权转换,加权函数直接影响潜在语义分析结果的优劣。本文首先总结了传统的、已成熟的权重计算方法,包括局部权重部分和词语全局权重部分,随后指出已有方法的不足之处,并对权重计算方法进行扩展,提出文档全局权重的概念。在最后的实验中,提出了一种新的检验潜在语义分析结果优劣的方法———文档自检索矩阵,实验结果证明改进后的权重计算方法提高了检索效率。","计算机应用,中文信息处理,潜在语义分析,权重,文档全局权重,文档自检索矩阵"
2004-12-01,层次型金融票据图像分类方法,"殷绪成,江世盛,韩智,刘昌平","金融票据图像识别处理是当今的一个热点研究方向,而票据分类是其中重要的基础部分。针对种类繁多、数量巨大、版面复杂和噪声干扰严重的金融票据彩色图像,本文提出了一种基于二叉树决策的层次型票据类型匹配方法。该方法利用三个类型判断器:基于票据版面结构的松弛匹配、基于OCR 的票据标题识别和基于票据颜色的色彩分析,层次化的进行票据类型判断。实验表明,层次型金融票据图像分类方法具有良好的效果;基于该方法的银行票据识别处理系统已经广泛应用于各大银行的相关业务系统中。","人工智能,模式识别,金融票据图像识别,票据分类,层次型"
2004-11-04,汉语朗读话语重音自动分类研究,"胡伟湘,董宏辉,陶建华,黄泰翼","汉语的重音由于受到声调、语调以及韵律单元层级的干扰和制约,对于重音的自动感知一直是比较困难的问题。针对标准的朗读普通话语,本文在广义韵律结构的框架下研究了重音的声学表现,设计并实现了重音的自动感知模型。本文提出的基于分类树结构的区分度模型能有效地结合韵律单元结构对重音的制约。研究结果表明,音高高线、调域、音长是表达重音最重要线索,利用这些线索能有效地实现对重音的自动感知。我们的模型能一般能达到80 %左右的重音检出水平。","计算机应用,中文信息处理,重音,韵律结构"
2004-09-27,多预测子融合实时连续语音识别输出词正误判别,"付跃文,杜利民","本文在采用堆栈译码词网重估输出作为识别最终输出的连续语音识别实时解码条件下,利用决策树方法将多个预测子融合,对识别输出词进行正确和错误的判别。本文首先构造了词后验概率、词长、相邻词的后验概率、词的声学和语言得分等共13 个预测子,然后利用决策树方法,通过选择不同的预测子组合方式和适当的决策树建树参数,筛选出预测子的最佳组合,建立优化的决策树进行输出词的正误判别。实验结果表明:利用局域词图计算的词后验概率与词长、相邻词的后验概率等几种实时预测子融合后,对识别输出词的正误判别能力得到提高,并且在实时性和分类效果两个方面优于n - best 输出的相应结果,相对于基线系统","计算机应用,中文信息处理,连续语音识别,预测子,决策树"
2004-11-24,藏文键盘布局的优化设计方法,"高定国,龚育昌","键位设计是实现藏字编码输入的关键步骤。由于藏字构件数多于标准键盘的可用键位数,较好地解决方法是把几个构件归并到一个键位上,但键位的归并可能会带来重码。为了有效地解决这一矛盾,本文采用了键位布局的优化设计方法,利用图论和概率方法求出藏字构件的极大独立集,以使得键位归并所产生的重码现象降到最低。文中详细介绍了求极大独立集的算法、矛盾构件的查找法、极大独立集数目的控制法、极大独立集最优划分的选择以及算法的流程图。并根据工程心理学方法把现代藏字的构件布局到标准键盘上,使得该布局的标准键盘可以一键一构件地输入现代藏字,且仅产生二对重码。","计算机应用,中文信息处理,优化设计方法,藏字,键位布局"
2004-07-30,基于词联接的诗词风格评价技术,"李良炎,何中市,易勇","在当前自然语言处理的研究状况下,文学语言处理应当受到足够的重视。诗词艺术集中体现了文学语言的形象性、情感性、个性等特征,是文学语言处理研究很好的切入点。风格评价是文学语言处理的重要课题,极具挑战性。本文以诗词语言为具体研究对象,以基于词联接的自然语言处理技术为技术背景,着重介绍并验证基于词联接的诗词风格评价技术。提出了计算方法,设计了诗词风格评价问卷调查实验。结果表明,人的诗词风格评价共性大于个性,基于词联接的诗词风格评价技术能够有效地评价诗词风格。","计算机应用,中文信息处理,文学语言处理,诗词风格,评价技术,词联接"
2015-07-20,“X什么”类否定义构式探析,"夏 雪,詹卫东","该文区分了“言语行为否定”和“命题真值否定”两类否定义: 前者表达对某种行为状态的否定态度(谴责、拒绝、禁止等);后者否定某个命题: 或者否定命题的“真值条件”、或者否定命题的“适宜条件”、或者否定命题主目的“典型条件”,表达“X未达某标准”。此外,进一步对两类否定义的基本要素及要素间的关系进行分析,讨论了表达每一类语义的“X什么”类构式的变项选择限制与实际使用情况,并总结了“X什么”类构式间的异同。","构式,否定义,什么"
2015-08-15,名词词义描写和研究需要什么样的语义学知识？,"李 强,袁毓林","该文主要讨论名词的词义描写和研究问题。首先通过对几种主要的词汇语义学理论(包括结构主义语义学、生成主义语义学、概念语义学和自然语义元语言理论)进行介绍和评述,指出它们在对名词进行语义刻画方面存在缺陷和不足;然后,重点引入生成词库理论的物性结构的描写方式,阐明它与前几种理论的区别及其自身的特点;最后,在生成词库理论的基础上,展示物性结构知识在有关名词分析中的四个研究案例(词语缺省、隐喻义生成、供用句、中动句)和在自然语言处理中的可能应用。","词汇语义学理论,生成词库理论,物性结构,名词分析,自然语言处理"
2015-08-10,用语图分析揭示语言系统中的隐性规律 ——赢家通吃和赢多输少算法,"陈振宁,陈振宇","该文用“图”这一数学工具,通过定量分析来揭示语言系统中的隐性规律,设计了“赢家通吃”和“赢多输少”两种生成算法,将理想算法“步步竞争、择优而行”的博弈论思路贯彻到非理想状态。两种新算法都较前人有更好的概括能力。赢多输少算法更兼顾了充分概括和适度概括均衡。生成语图后,该设计着重准确率的最小简图和着重覆盖率的最大简图归纳算法,挖掘控制的主流规则、分析语言系统的语言学规律。在最小简图基础上提出控制度公式以评价语言系统。","隐性规律,图论,博弈论,规则挖掘"
2015-07-21,花园幽径模式行进错位的量化研究: 计算语言学视角,"杜家利,于屏方","该文讨论了花园幽径模式行进错位过程中的困惑商指数。非对称性信息断层的存在导致解码呈现否定之否定的螺旋上升态势。行进错位的潜在效应幅度可通过困惑商指数得到测定。基于大数据语料库统计方法和在线剖析器分析方法,我们测算出优选结构困惑商指数介于(-∞,1];非优选结构困惑商指数介于[1,2];两结构临界值分别为0.72和1.28;歧义域为[0.72,1.28]。结论认为,多结构频数差异是导致困惑商指数变化的根本;行进错位的幅度和非对称性信息补偿的强度均与困惑商指数相关;基于统计的困惑商指数可对局部歧义的复杂句结构提供前瞻性解码信息。","计算语言学,花园幽径模式,行进错位,局部歧义,困惑商"
2015-08-10,基于聋人案例的空间隐喻语义认知计算,"姚登峰,江铭虎,阿布都克力木·阿布力孜,侯仁魁,哈里旦木·阿布都克里木","我们从心理语言学的角度,对空间隐喻进行相似性分类和计算,使用多维量表和聚类方法,以聋人为被试分别进行了两个实验。实验结果表明,聋人为实现空间隐喻理解的计算,使用了地形空间和语法空间的特征信息,同时受手语语言特点的影响,其空间隐喻的认知主题包括手势者自身参照系、参照物的相对坐标系、手势空间的饱和度、以手部或胸部为边界。同时表明由于两种空间的存在,聋人大脑对空间隐喻的理解存在着层次,并且在长期使用手语交流的过程中,其地形空间和语法空间相互作用,影响了聋人大脑空间隐喻的结构和表征,从而导致了独特的高效快速空间隐喻计算。","空间隐喻,语义计算,聋人"
2015-07-26,基于感知器的中文分词增量训练方法研究,"韩 冰,刘一佳,车万翔,刘 挺","该文提出了一种基于感知器的中文分词增量训练方法。该方法可在训练好的模型基础上添加目标领域标注数据继续训练,解决了大规模切分数据难于共享,源领域与目标领域数据混合需要重新训练等问题。实验表明,增量训练可以有效提升领域适应性,达到与传统数据混合相类似的效果。同时该文方法模型占用空间小,训练时间短,可以快速训练获得目标领域的模型。","中文分词,领域适应,增量训练"
2015-08-15,基于Active Learning的中文分词领域自适应,"许华婷,张玉洁,杨晓晖,单 华,徐金安,陈钰枫","在新闻领域标注语料上训练的中文分词系统在跨领域时性能会有明显下降。针对目标领域的大规模标注语料难以获取的问题,该文提出Active learning算法与n-gram统计特征相结合的领域自适应方法。该方法通过对目标领域文本与已有标注语料的差异进行统计分析,选择含有最多未标记过的语言现象的小规模语料优先进行人工标注,然后再结合大规模文本中的n-gram统计特征训练目标领域的分词系统。该文采用了CRF训练模型,并在100万句的科技文献领域上,验证了所提方法的有效性,评测数据为人工标注的300句科技文献语料。实验结果显示,在科技文献测试语料上,基于Active Learning训练的分词系统在各项评测指标上均有提高。","中文分词,领域自适应,主动学习"
2015-07-10,面向普通未登录词理解的二字词语义构词研究,"吉志薇,冯敏萱","把词素作为基本资源,从语义上寻找他们组合成词的规律,可以辅助自然语言理解。该文首先参照《现代汉语词典》和知网标注了二字词的词素意义,继而从意合结构、意根分布、意指方式、意变类型四个角度标注了词素间的词化意义,最后综合词素意义和词化意义,在定量统计的基础上建立了一个二字词的语义描写体系。通过对论坛及《现代汉语词典》的新词进行实验,我们发现二字词的语义构词研究在普通未登录词的理解中具有一定的应用价值。","二字词,普通未登录词,语义构词"
2015-07-10,多领域中文依存树库构建与影响统计句法分析因素之分析,"邱立坤,史林林,王厚峰","为提升依存分析并分析影响其精度的相关因素,该文构建了大规模中文通用依存树库和中等规模领域依存树库。基于这一系列树库,通过句法分析实验考察质量、规模、领域差异等因素对中文依存分析的影响,实验结果表明: (1)树库规模和质量均与句法分析精度成正相关关系,质量应先于规模因素被优先考虑;(2)通用树库和领域树库之间的差异程度与前者对后者的替代性成相关关系;(3)两种树库混合使用的效果同样与领域差异有关。","依存树库,领域迁移,依存句法分析"
2015-07-10,基于word2vec的大中华区词对齐库的构建,"王明文,徐雄飞,徐 凡,李茂西","该文针对大陆、香港和台湾地区(简称大中华区)存在同一种语义但采用不同词语进行表达的语言现象进行分析。首先,我们抓取了维基百科以及简繁体新闻网站上的3 200 000万组大中华区平行句对,手工标注了一致性程度达到95%以上的10 000组大中华区平行词对齐语料库。同时,我们提出了一个基于word2vec的两阶段大中华区词对齐模型,该模型采用word2vec获取大中华区词语的向量表示形式,并融合了有效的余弦相似度计算方法以及后处理技术。实验结果表明我们提出的大中华区词对齐模型在以上两种不同文体的词对齐语料库上的F1值显著优于现有的GIZA++和基于HMM的基准模型。此外,我们在维基百科上利用该词对齐模型进一步生成了90 029组准确率达82.66%的大中华区词语三元组。","大中华区,词对齐,最长公共子序列,word2vec"
2015-08-14,基于单语语料的面向日语假名的日汉人名翻译对抽取方法,"王东明,徐金安,陈钰枫,张玉洁","命名实体的翻译等价对在跨语言信息处理中非常重要。传统抽取方法通常使用平行语料库或可比语料库,此类方法受到语料库资源的质量和规模的限制。在日汉翻译领域,一方面,双语资源相对匮乏;另一方面,对于汉字命名实体,通常使用汉字对照表;对于日语纯假名的命名实体,通常采用统计翻译模型,此类方法受到平行语料库的质量和规模的限制,且精度低下。针对此问题,该文提出了一种基于单语语料的面向日语假名的日汉人名翻译对自动抽取方法。该方法首先使用条件随机场模型,分别从日语和汉语语料库中抽取日语和汉语人名;然后,采用基于实例的归纳学习法自动获取人名实体的日汉音译规则库,并通过反馈学习来迭代重构音译规则库。使用音译规则库计算日汉人名实体之间的相似度,给定阈值判定人名实体翻译等价对。实验结果表明,提出的方法简单高效,在实现系统高精度的同时,克服了传统方法对双语资源的依赖性。","机器翻译,命名实体,日语假名,归纳学习法,音译"
2015-07-08,中文维基百科的实体分类研究,"徐志浩,惠浩添,钱龙华,朱巧明","维基百科实体分类对自然语言处理和机器学习具有重要的作用。该文采用机器学习的方法对中文维基百科的条目进行实体分类,在利用维基百科页面中半结构化信息和无结构化文本作为基本特征的基础上,结合中文的特点使用扩展特征和语义特征来提高实体分类性能。在人工标注的语料库上的实验表明,这些额外特征有效地提高了ACE分类体系上的实体分类性能,总体F1值达到96%,同时在扩展实体分类上也取得了较好的效果,总体F1值达95%。","维基百科,实体分类,半结构化信息,信息框"
2015-07-31,话题内相关文本的内容计算,"刘冬明,杨尔弘","信息的暴涨给文本处理带来了更多的挑战。话题检测能够把大量的信息以话题为单位有效地组织起来,然而最终用户有可能并不需要涉及某一话题的所有文本,而是仅仅关心该话题的具体内容。在我们根据相关文本智能表达话题内容推送给用户之前,自动从相关文本中挑选符合用户需求的文本是一个非常有意义的工作。本文致力于相同话题文本之间的内容比较,目的是有效地选出满足需求的文本。我们通过对话题进行重新定义,并根据此定义设定了话题和文本的表示方法,给出了基于该表示方法的话题和文本之间的内容比较计算方法。最后,通过实验说明了这一系列方法的有效性。","话题定义,文本表示, 话题检测,文本内容计算"
2015-07-31,一种改进的社交媒体文本规范化方法,"宋亚军,于中华,陈 黎,丁革建,罗 谦","社交媒体具有文本不规范的特点,现有自然语言处理工具直接应用于社交媒体文本时效果不甚理想,并且基于","社交媒体,文本规范化,自然语言处理,词嵌入"
2015-07-31,高斯加权的重构性K-NN算法研究,"刘作国,陈笑蓉","该文提出基于高斯加权距离以及聚类重构机制的K-NN文本聚类算法。文章提出K-NN近邻域的概念,通过高斯加权的近邻域算法实施K-NN聚类。利用高斯函数根据样本与聚类中心的距离为样本赋权,计算聚类距离。基于近邻域权重和聚类密度对形成的聚类实施重构,实现聚类数目的自适应调整。使用拆分算子拆分稀疏聚类并调整异常样本;使用合并算子合并相似聚类。实验显示聚类重构机制能够有效地提高聚类的准确率及召回率,增加聚类密度,使得形成的聚类结果更加合理。","文本聚类,K-NN算法,高斯加权,近邻域规则,聚类重构"
2015-07-10,基于多源知识和Ranking SVM的中文微博命名实体链接,"陈万礼,昝红英,吴泳钢","命名实体是文本中承载信息的重要单元,正确分析存在歧义的命名实体对文本的理解起着关键性作用。该文提出基于多源知识和Ranking SVM的中文微博命名实体链接,结合同义词词典、百科资源等知识产生初始候选实体集合,同时从文本中抽取多种组合特征,利用Ranking SVM对候选实体集合进行排序,从而得到目标实体。在NLP&amp;CC2014①中文微博实体链接评测数据集上进行了实验,获得了89.40%的平均准确率,与NLP&amp;CC2014中文微博实体链接评测取得最好成绩的系统相比,本文的系统具有一定的优势。","命名实体,中文微博实体链接,同义词词典,百科资源,Ranking SVM,语义特征"
2015-06-15,面向篇章机器翻译的英汉翻译单位和翻译模型研究,"宋 柔,葛诗利","篇章机器翻译的首要问题是确定翻译单位。基于汉语和英语的语言知识和英汉翻译的实践,该文提出面向篇章机器翻译的基本单位和复合单位的双层单位体系,讨论了这两种单位支持篇章翻译应满足的性质,并据此勾画了篇章机器翻译的拆分、翻译、装配三步模型(PTA模型)。该文提出,汉语篇章机器翻译的复合单位为广义话题结构对应的文本块,基本单位则是根据广义话题结构流水模型得到的话题自足句;英语篇章机器翻译的复合单位为句号句,基本单位为naming-telling小句(NT小句),即指称性成分加上对它的陈述或后修饰成分所构成的小句。该文展示了在这样的翻译单位体系下采用PTA模型的英汉翻译过程实例,规划了面向篇章翻译的英汉小句对齐语料库的建设任务,讨论了PTA模型的可行性。","翻译单位,翻译模型,广义话题结构,naming-telling小句"
2015-07-10,利用Markov网络抽取复述增强机器译文自动评价方法,"翁 贞,李茂西,王明文","在机器译文自动评价中,匹配具有相同语义、不同表达方式的词或短语是其中一个很大的挑战。许多研究工作提出从双语平行语料或可比语料中抽取复述来增强机器译文和人工译文的匹配。然而双语平行语料或可比语料不仅构建成本高,而且对少数语言对难以大量获取。我们提出通过构建词的Markov网络,从目标语言的单语文本中抽取复述的方法,并利用该复述提高机器译文自动评价方法与人工评价方法的相关性。在WMT14 Metrics task上的实验结果表明,我们从单语文本中提取复述方法的性能与从双语平行语料中提取复述方法的性能具有很强的可比性。因此,该文提出的方法可在保证复述质量的同时,降低复述抽取的成本。","复述,机器译文自动评价,Markov网络,相关性"
2015-06-01,面向主题的微博热门话题舆情监测研究——以“北京单双号限行常态化”舆情分析为例,"张 瑜,李 兵,刘晨玥","社交媒体舆情监测是社交媒体分析的热点研究问题,学界和工业界取得了很多研究成果。但目前针对热门话题舆情监测研究中,往往只在整体上关注事件舆情趋势,而没有对事件内部不同的讨论主题进行分析。鉴于此,该研究将主题分类模型引入到舆情监测中来,并在此基础上,以时间为脉络进行面向主题的情感分析。并以“北京市单双号限行常态化”这一微博话题为例进行实证研究,通过各个时段 “北京市单双号限行常态化”这一微博话题群体情感倾向变化的分析,为舆情的监测提供对象和时点选择的参考建议。","舆情监测,短文本情感分析,朴素贝叶斯"
2015-07-08,基于极性转移和LSTM递归网络的情感分析,"梁 军,柴玉梅,原慧斌,高明磊,昝红英","长短时记忆(long short term memory,LSTM)是一种有效的链式循环神经网络(recurrent neural network,R2NN①),被广泛用于语言模型、机器翻译、语音识别等领域。但由于该网络结构是一种链式结构,不能有效表征语言的结构层次信息,该文将LSTM扩展到基于树结构的递归神经网络(Recursive Neural Network,RNN)上,用于捕获文本更深层次的语义语法信息,并根据句子前后词语间的关联性引入情感极性转移模型。实验证明本文提出的模型优于LSTM、递归神经网络等。","LSTM,递归神经网络,情感分析"
2015-07-30,基于模糊推理机的汉语主观句识别,"宋洪伟,宋佳颖,付国宏","该文提出一种基于词汇模糊集合的模糊推理机以识别汉语主观句。首先,根据主、客观词概念的模糊性,我们定义了两个相应的模糊集合,并在模糊统计方法下,利用TF-IDF从训练语料中获取隶属度函数。然后制定了两个模糊IF-THEN规则,并据此实现了一个模糊推理机以识别汉语主观句。NTCIR-6中文数据上的实验结果表明我们的方法具有一定的可行性。","主观句识别,模糊集合,模糊IF-THEN规则,模糊推理机"
2015-07-21,基于聚类和分类的金庸与古龙小说风格分析,"肖天久,刘 颖","该文以金庸与古龙的小说作为语料,从计算风格学的角度考察二人的风格差异。对比了两人小说的文本从众性、句子破碎度,同时,使用文本聚类的方法对词和词类的N元文法,标点符号的N元文法以及多种特征的总体情况进行了考察,还使用主成分分析和文本分类对八种特征从总体上进行了比较,结果证实金庸与古龙小说风格存在较大差异:金庸小说从众性大于古龙,较多使用俚语方言,口语性更强,同时在语法结构、短语结构、文本节奏以及文本可读性和语言变化程度上也有较大的差异。","计算风格学,N元文法,聚类,分类,句子破碎度"
2015-06-29,利用词的分布式表示改进作文跑题检测,"陈志鹏,陈文亮,朱慕华","作文跑题检测任务的核心问题是文本相似度计算。传统的文本相似度计算方法一般基于向量空间模型,即把文本表示成高维向量,再计算文本之间的相似度。这种方法只考虑文本中出现的词项(词袋模型),而没有利用词项的语义信息。该文提出一种新的文本相似度计算方法:基于词扩展的文本相似度计算方法,将词袋模型(Bag-of-Words)方法与词的分布式表示相结合,在词的分布式表示向量空间中寻找与文本出现的词项语义上相似的词加入到文本表示中,实现文本中单词的扩展。然后对扩展后的文本计算相似度。该文将这种方法运用到英文作文的跑题检测中,构建一套跑题检测系统,并在一个真实数据中进行测试。实验结果表明该文的跑题检测系统能有效识别跑题作文,性能明显高于基准系统。","文本相似度,词分布式表示,跑题检测,文本表示"
2015-06-26,《红楼梦》中社会权势关系的提取及网络构建,"陈 蕾,胡亦旻,艾 苇,胡俊峰","社会地位与权势的研究一直是社会语言学领域的一个热点话题。该文借助数据挖掘中的关系提取方案雪球算法(Snowball Algorithm),实现了《红楼梦》文本中候选的特征语言模式(pattern)和人物关系对之间的相互定位与赋权,对小说中频繁同现的人物对之间的社会等级关系进行挖掘,以此建立了能反映人物等级关系的有向加权人际关系网络。进一步应用最小树形图算法,生成了涵盖192个《红楼梦》主要人物的单向联通的树状社会关系图。通过这种方法生成的社会关系图不但能有效反映人际交往亲密度与社区影响力,同时还透视了人与人之间的社会等级差异。相较于单纯基于人际交往亲密程度的无向关系网络,能更加客观地表达出社会交往中人际关系网络的真实图景。","关系提取,权势关系,社会关系网络,最小树形图"
2015-08-16,限定领域口语对话系统中超出领域话语的协处理方法,"王俊东,黄沛杰,林仙茂,徐禹洪,李凯茵","领域外话语的开放性、口语化以及表达多样性,使得现有的限定领域口语对话系统不能很好地处理超出领域话语。该文提出了一种限定领域口语对话系统协处理方案,基于人工智能标记语言AIML,设计一套理解开放语义用户话语的理解模板,并对未匹配话语基于话语相似度进行理解模板分类,进而采用扩展有限状态自动机处理模式,结合对话流程上下文的状态及信息,实现理解模板到应答模板的转换,改变了单纯模板匹配方法在对话流程控制方面的相对缺失。中文手机导购领域的测试表明,该文所提出的协处理方法能有效地辅助口语对话系统完成限定领域完整对话流程,得到更好的用户满意度。","超出领域话语,协处理,AIML,有限状态自动机,口语对话系统"
2015-08-06,融合多策略的维吾尔语词干提取方法,"赛迪亚古丽·艾尼瓦尔,向 露,宗成庆,艾克白尔·帕塔尔,艾斯卡尔·艾木都拉","维吾尔语是形态变化复杂的黏着性语言,维吾尔语词干词缀切分对维吾尔语信息处理具有非常重要的意义,但到目前为止,维吾尔语词干提取的性能仍存在较大的改进空间。该文以N-gram模型为基本框架,根据维吾尔语的构词约束条件,提出了融合词性特征和上下文词干信息的维吾尔语词干提取模型。实验结果表明,词性特征和上下文词干信息可以显著提高维吾尔语词干提取的准确率,与基准系统比较,融入了词性特征和上下文词干信息的实验准确率分别达到了95.19%和96.60%。","维吾尔语,形态,词干提取,N-gram模型,词性特征,上下文词干信息"
2015-09-01,基于藏语字性标注的词性预测研究,"龙从军,刘汇丹,诺明花,吴 健","该文选取了藏语文中小学教材的部分语料,构建了带有藏语字性标记、词边界标记和词性标记的语料库,通过比较不同的分词、标注方法,证明分词、词性标注一体化效果比分步进行的效果好,准确率、召回率和F值分别提高了0.067、0.073和0.07。但词级标注模型难以解决词边界划分的一致性和未登录词的问题。基于此,作者提出可以利用字性和字构词的规律预测合成词的词性,既可以融入语言学知识又可以减少由未登录词导致的标注错误,实验结果证明,作为词性标注的后处理模块,基于字性标注的词性预测准确率提高到了0.916,这个结果已经比分词标注一体化结果好,说明字性标注对纠正词性错误标注有明显的效果。","藏语, 语字标注, 分词, 词性标注"
2015-07-05,三位一体字标注的汉语词法分析,"于江德,胡顺义,余正涛","针对汉语词法分析中分词、词性标注、命名实体识别三项子任务分步处理时多类信息难以整合利用,且错误向上传递放大的不足,该文提出一种三位一体字标注的汉语词法分析方法,该方法将汉语词法分析过程看作字序列的标注过程,将每个字的词位、词性、命名实体三类信息融合到该字的标记中,采用最大熵模型经过一次标注实现汉语词法分析的三项任务。并在Bakeoff2007的PKU语料上进行了封闭测试,通过对该方法和传统分步处理的分词、词性标注、命名实体识别的性能进行大量对比实验,结果表明,三位一体字标注方法的分词、词性标注、命名实体识别的性能都有不同程度的提升,汉语分词的F值达到了96.4%,词性标注的标注精度达到了95.3%,命名实体识别的F值达到了90.3%,这说明三位一体字标注的汉语词法分析性能更优。","汉语词法分析,最大熵模型,三位一体,字标注"
2015-07-15,汉语复句关系的特征结构,冯文贺,"通常复句关系分析基于分类机制,由于缺乏统一逻辑,面临不少分歧。该文提出基于特征结构描写复句关系。复句关系的特征结构由[特征: 值]元组构成,该文初步构拟汉语复句关系的特征结构系统,并用于具体分析。较之分类机制,特征结构对复句关系的描写深刻,且分析判断准确、易行。目前特征结构系统开放,但特征调整,可以完善而不大量更改已有特征描写结果。特征结构可用于复句关系的深度语义分析资源构建与计算研究。","复句关系,特征结构,语义分析"
2015-07-10,基于知网义原词向量表示的无监督词义消歧方法,"唐共波,于 东,荀恩东","词义消歧一直是自然语言处理领域中的重要问题,该文将知网(HowNet)中表示词语语义的义原信息融入到语言模型的训练中。通过义原向量对词语进行向量化表示,实现了词语语义特征的自动学习,提高了特征学习效率。针对多义词的语义消歧,该文将多义词的上下文作为特征,形成特征向量,通过计算多义词词向量与特征向量之间相似度进行词语消歧。作为一种无监督的方法,该方法大大降低了词义消歧的计算和时间成本。在SENSEVAL-3的测试数据中准确率达到了37.7%,略高于相同测试集下其他无监督词义消歧方法的准确率。","词向量,《知网》,词义消歧,无监督方法"
2015-06-15,基于语义依存图库的兼语句句模研究,"郑丽娟,邵艳秋","句子语义分析是语言研究深入发展的客观要求,也是当前制约语言信息处理技术深度应用的主要因素。在探索深层语义分析方法的基础上,该文根据汉语的特点,提出了一整套语义依存图的构建方法,并建立了一个包含30 000个句子的语义依存图库。以兼语句为重点研究对象,该文研究了语料库中所有纯粹的兼语句所对应的句模情况,进而试图构建基于语义依存图的句模系统,总结句型和句模的映射规则,从而为更好的建立语义自动分析模型提供相应的知识库。","句模,语义分析,语义依存图,兼语句"
2015-07-06,汉语形名复合词的语义建构: 基于物性结构与概念整合理论,"张念歆,宋作艳","该文用定量和定性分析相结合的方法,考察了现代汉语双音节形名复合词的物性修饰关系,发现形语素有选择地约束名语素的不同物性角色。当形语素修饰名语素的形式角色或构成角色时,语义解读时常需要补充名词;当形语素修饰名语素的施成角色、功用角色或规约化属性时,语义解读时常需要补充动词。形名复合词的语义建构是物性结构和概念整合共同作用的结果,当形语素激活的物性角色或物性值不止一个时,就会出现多义或歧义。","形名复合词,语义建构,生成词库理论,物性结构,概念整合理论"
2015-07-16,语言网络研究的数学模型从复杂网络、社会网络到语言网络,"赵怿怡,刘海涛","复杂网络技术的发展为大数据时代的语言研究提供了新的视角。网络方法应用到语言研究的重要目的是探索语言网络的结构特征规律和功能演化规律。该文综述了以图论为基础的复杂网络发展及社会网络、语言网络的主要数学模型,试图从复杂网络共性特征小世界、无标度特征中进一步剥离出语言网络的个性特征,为语言符号多层级网络结构、功能研究提供参考。","语言网络,网络技术,网络演化,图论,复杂网络特征"
2015-07-10,面向汉语(二语)教学的语法点知识库构建及语法点标注研究,"谭晓平,杨丽姣,苏靖杰","语法是汉语(二语)教学中的重点和难点,而面向语法教学领域的知识库、语料库较少,不能满足汉语国际教育事业发展的需求。该文首先根据三个平面理论和对外汉语教学语法理论提出了面向汉语(二语)教学的语法点描述框架,建立了包含121个教学常用语法点的知识库。其次,在141 464条对外汉语教材语料和新HSK样题文本语料中对121个语法点进行了句法语义信息的综合标注,共获得95 592个句次的标注语料,涉及形式类别580项,语义类别233项,形成了与语法点知识库配套的语法点标注语料库。最后,讨论了语法点知识库和语法点标注语料库在汉语(二语)教学及教材研究领域的应用。","语法点,知识库,标注,语料库,汉语国际教育"
2015-07-05,对外汉语教学领域话题语料库的研究与构建,"胡韧奋,朱 琦,杨丽姣","对外汉语教学领域,教材上的课文通常围绕一个话题展开,话题是教学内容的集中体现,也与词汇、语法等不同层面的语言知识间有着密切关联。该文基于大规模教材语料库研究教学话题分类体系,设计了一个包含四个一级话题、23个二级话题和246个三级话题的三层话题框架,并据此对197册汉语经典教材中的5 457个文段进行了人工标注及校对,构建了一个规模约12万句的面向对外汉语教学的话题语料库。为了更好地服务于汉语教学及相关研究工作,还抽取、计算了文段的语法点和新HSK词语等级信息,作为话题标注的补充维度加入资源库,以期为汉语教学领域的教师、研究者及教材编写者提供较为全面的话题信息参考。","对外汉语,话题,语料库"
2014-07-25,领域相关的汉语情感词典扩展,"宋佳颖,贺 宇,付国宏","动态情感知识的获取,特别是领域相关极性词典的构建一直是意见挖掘和情感分析系统在开放应用时面临的主要挑战之一。该文面向产品评价文本提出一种汉语情感极性词典扩展方法。该方法首先采用序列标注方法从意见文本中抽取产品意见要素,同时构建属性-评价对;然后,对抽取的属性-评价对进行正规化,以减少词典扩展中的复杂性和噪声;最后,改进PolarityRank算法的构图方式以使其适用于汉语文本,从而完成词典扩展。在汽车和手机两个领域的意见文本的实验结果表明领域相关的情感极性词语的扩展有利于情感极性分类性能的提高。","情感分析,情感词典扩展,PolarityRank,意见要素一致化"
2014-09-05,中文模糊限制语语料库的研究与构建,"周惠巍,杨 欢,张 静,亢世勇,黄德根","模糊限制语常用来表示不确定性和可能性的含义,由模糊限制语所引导的信息为模糊限制信息。为进行中文事实信息的抽取,应将模糊限制信息与事实信息区分开来。然而中文模糊限制语语料资源却十分缺乏,影响了中文模糊限制语和模糊限制信息检测的研究。该文研究了中文模糊限制语的分类,并在生物医学和维基百科两个领域,设计构建了一个具有2.4万句规模的中文模糊限制语语料库。统计分析了语料标注的一致性,以及模糊限制语的类型和领域之间的关系。这些资源对于中文模糊限制信息检测研究,以及中文事实信息的抽取具有重要意义。同时,为语言学家从语义和语用等方面进行模糊限制语的研究提供了强大的知识库支持。","中文模糊限制语,分类,语料库,一致性分析"
2015-07-10,基于全局/局部共现词对分布的汉越双语新闻事件线索分析,"高盛祥,余正涛,龙文旭,丁 硙,闫春婷","针对汉越双语新闻事件线索分析,提出了基于全局/局部共现词对分布的汉越双语事件线索生成方法。该方法首先将新闻话题词语分布作为全局词语表征全局事件,然后用一定时间粒度下新闻片段特有的时间、人物、地点等事件元素作为局部词语,分析新闻片段中全局词语和局部词语的共现关系,将全局/局部词语的共现规律作为监督信息,结合RCRP算法和汉越双语新闻的对齐语料,构建有监督话题生成主题模型,获得相应时间跨度下代表事件发展进程的子话题分布,通过子话题的分布反映事件发展的线索,从而构建出在线汉越双语事件线索生成模型。实验在汉越混合新闻数据集上进行,事件线索生成对比实验结果证明了提出的方法的有效性。","汉语-越南语,新闻事件线索,全局/局部共现词对,子话题分布,双语主题模型 "
2015-07-10,基于框架的汉语篇章结构生成和篇章关系识别,"吕国英,苏 娜,李 茹,王智强,柴清华","针对汉语篇章分析的三个任务: 篇章单元切割、篇章结构生成和篇章关系识别,该文提出引入框架语义进行分析研究。首先基于框架构建了汉语篇章连贯性描述体系以及相应语料库;然后抽取句首、依存句法、短语结构、目标词、框架等特征,分别训练基于最大熵的篇章单元间有无关系分类器和篇章关系分类器;最后采用贪婪算法自下向上生成篇章结构树。实验证明,框架语义可以有效切割篇章单元,并且框架特征可以有效提升篇章结构以及篇章关系的识别效果。","篇章单元,篇章结构,篇章关系,贪婪算法 "
2015-07-15,面向不平衡数据的隐式篇章关系分类方法研究,"朱珊珊,洪 宇,丁思远,姚建民,朱巧明","隐式篇章关系分类是篇章分析领域的一个重要研究子任务,大部分已有研究都假设参与分类的正类样本和负类样本数量相等,采用随机欠采样等不平衡数据处理方法保持训练样本中数据平衡,然而,在实际语料中正类样本和负类样本的分布是不平衡的,这一现象往往制约隐式篇章关系分类性能的有效提升。针对该问题,该文提出一种基于框架语义向量的隐式篇章关系分类方法,该方法借助框架语义知识库,将论元表示成框架语义向量,在此基础上,从外部数据资源中挖掘有效的篇章关系样本,对训练样本进行扩展,解决数据不平衡问题。在宾州篇章树库(Penn Discourse Treebank, PDTB)语料上的实验结果表明,相较于目前主流的不平衡数据处理方法,该文方法能够明显提高隐式篇章关系分类性能。","隐式篇章关系分类,不平衡数据,框架语义向量"
2015-06-05,基于知识话题模型的文本蕴涵识别,"任 函,盛雅琦,冯文贺,刘茂福","该文分析了现有基于分类策略的文本蕴涵识别方法的问题,并提出了一种基于知识话题模型的文本蕴涵分类识别方法。 其假设是: 文本可看作是语义关系的组合,这些语义关系构成若干话题;若即若文本T蕴涵假设H,说明 T 和 H 具有相似的话题分布,反之说明T 和 H 不具有相似的话题分布。基于此,我们将 T 和 H 的蕴涵识别问题转化为相关话题的生成过程,同时将文本推理知识融入到抽样过程,由此建立一个面向文本蕴涵识别的话题模型。实验结果表明基于知识话题模型在一定程度上改进了文本蕴涵识别系统的性能。","文本蕴涵识别,话题模型,蕴涵分类,推理知识"
2015-07-05,文言信息的自动抽取: 基于统计和规则的尝试,"虞宁翌,高琦,恩东","文言信息的自动抽取有利于语言监测和语料库构建。同时该文的计算研究也验证了语言学界关于汉语文白系统连续性的自省结论。该文将从混合语料中标注文言文的问题视为短文本分类的问题进行处理。使用基于规则和基于统计的方法对文言文、白话文本进行分类。在基于规则的方法中,考虑文言常用虚词和句式的影响,对N-gram、朴素贝叶斯、最大熵、决策树模型的性能进行了研究。结果表明监测虚词系统的一元语言模型的F值达到了0.98。","文言标注,文本分类,规则模型,统计模型"
2015-06-18,基于超图的文本摘要与关键词协同抽取研究,"莫 鹏,胡 珀,黄湘冀,何婷婷",,"超图,文本摘要,关键词抽取,协同抽取"
2015-07-10,基于自然标注信息和隐含主题模型的无监督文本特征抽取,"饶高琦,于东,荀恩东","术语和惯用短语可以体现文本特征。无监督的抽取特征词语对诸多自然语言处理工作起到支持作用。该文提出了聚类-验证过程,使用主题模型对文本中的字符进行聚类,并采用自然标注信息对提取出的字符串进行验证和过滤,从而实现了从未分词领域语料中无监督获得词语表的方法。通过优化和过滤,我们可以进一步获得了富含有术语信息和特征短语的高置信度特征词表。在对计算机科学等六类不同领域语料的实验中,该方法抽取的特征词表具有较好的文体区分度和领域区分度。","自然标注信息,自然语块,隐含主题模型,领域特征,文体特征"
2015-07-10,融合热点话题的微博转发预测研究,"陈 江,刘 玮,巢文涵,王丽宏","微博转发行为是实现信息传播的重要方式,微博转发预测对微博影响力分析、微博话题分析具有重要价值。现有微博转发预测研究大多围绕消息属性、用户属性等微博自身特征,该文提出融合热点话题的微博转发预测方法,对背景热点话题内容和传播趋势对用户转发行为的影响进行量化分析,提出融合背景热点信息的转发兴趣、转发活跃度、行为模式等特征,并基于分类算法建立了面向热点话题相关微博的转发预测模型,在真实数据上的实验结果表明,该方法的预测准确性达到96.6%,提升幅度最高达到12.14%。","转发行为,转发预测,热点话题"
2015-07-03,基于卷积神经网络的微博情感倾向性分析,"刘龙飞,杨 亮,张绍武,林鸿飞","微博情感倾向性分析旨在发现用户对热点事件的观点态度。由于微博噪声大、新词多、缩写频繁、有自己的固定搭配、上下文信息有限等原因,微博情感倾向性分析是一项有挑战性的工作。该文主要探讨利用卷积神经网络进行微博情感倾向性分析的可行性,分别将字级别词向量和词级别词向量作为原始特征,采用卷积神经网络来发现任务中的特征,在COAE2014任务4的语料上进行了实验。实验结果表明,利用字级别词向量及词级别词向量的卷积神经网络分别取得了95.42%的准确率和94.65%的准确率。由此可见对于中文微博语料而言,利用卷积神经网络进行微博情感倾向性分析是有效的,且使用字级别的词向量作为原始特征会好于使用词级别的词向量作为原始特征。","深度学习,情感倾向性分析,卷积神经网络,词向量"
2015-01-01,面向微博的社会情绪词典构建及情绪分析方法研究,"蒋盛益,黄卫坚,蔡茂丽,王连喜","该文旨在探索一种面向微博的社会情绪词典构建方法,并将其应用于社会公共事件的情绪分析中。首先通过手工方法建立小规模的基准情绪词典,然后利用深度学习工具Word2vec对社会热点事件的微博语料通过增量式学习方法来扩展基准词典,并结合HowNet词典匹配和人工筛选生成最终的情绪词典。接下来,分别利用基于情绪词典和基于SVM的情绪方法对实验标注语料进行情绪分析,结果对比分析表明基于词典的情绪分析方法优于基于SVM的情绪分析方法,前者的平均准确率和召回率比后者分别高13.9%和1.5%。最后运用所构建的情绪词典对热点公共事件进行情绪分析,实验结果表明该方法是有效的。","微博,社会情绪,词典,情绪分析"
2015-07-06,结合卷积神经网络和词语情感序列特征的中文情感分析,"陈 钊,徐睿峰,桂 林,陆 勤","目前基于词嵌入的卷积神经网络文本分类方法已经在情感分析研究中取得了很好的效果。此类方法主要使用基于上下文的词嵌入特征,但在词嵌入过程中通常并未考虑词语本身的情感极性,同时此类方法往往缺乏对大量人工构建情感词典等资源的有效利用。针对这些问题,该文提出了一种结合情感词典和卷积神经网络的情感分类方法,利用情感词典中的词条对文本中的词语进行抽象表示,在此基础上利用卷积神经网络提取抽象词语的序列特征,并用于情感极性分类。该文提出的相关方法在中文倾向性分析评测COAE2014数据集上取得了比目前主流的卷积神经网络以及朴素贝叶斯支持向量机更好的性能。","卷积神经网络,情感分析,词语情感序列特征"
2015-07-02,基于简介和评论的标签推荐方法研究,"褚晓敏,王中卿,朱巧明,周国栋","Web 2.0时代,社会标签是信息资源组织的一种重要方式。标签推荐能够有效的帮助用户收集、定位、查找和共享在线资源。以往的标签推荐算法只是基于一种文本信息,比如基于电影的简介文本来进行标签推荐。但是实际上电影往往存在多种文本信息,比如同时存在摘要信息和评论信息,不同类型的信息能够反映电影的不同方面的属性,因此为了提高电影标签推荐的准确率和有效性,我们同时根据电影的简介和短评进行电影标签自动推荐,并使用多种方法融合基于不同类型文本的标签推荐的结果,实验证明,使用不同类型信息进行标签推荐能够比单一使用一种文本信息进行标签推荐有很大的提升。","自然语言处理,社会标签,社会关系网络,分类器融合"
2015-08-05,TIP-LAS:一个开源的藏文分词词性标注系统,"李亚超,江 静,加羊吉,于洪志","TIP-LAS是一个开源的藏文分词词性标注系统,提供藏文分词、词性标注功能。该系统基于条件随机场模型实现基于音节标注的藏文分词系统,采用最大熵模型,并融合音节特征,实现藏文词性标注系统。经过试验及对比分析,藏文分词系统和词性标注系统取得了较好的实验效果,系统的源代码可以从网上获取。希望该研究可以推动藏文分词、词性标注等基础工作的发展,提供一个可以比较、共享的研究平台。","藏文,分词,词性标注,条件随机场,最大熵"
2015-07-10,基于形态分析的现代维吾尔语名词词干识别研究,"艾孜尔古丽,阿力木·木拉提,玉素甫·艾白都拉","现代维吾尔语名词词干识别是自然语言处理领域的重要基础性研究,主要目的是从句子中提取名词词干,提高名词识别效率。首先陈述形态分析概念,通过这些形态特征可以准确地识别其词性的意义;其次讨论维吾尔语的词类划分标准、名词的形态特征分析,总结词缀歧义及消解规则;该文提出研究总体思路,设计现代维吾尔语新词中名词识别算法,其中包括特征选择及参数估计、词内部特征、前后依存词特征等;最后将初中、高中物理维吾尔语教材作为验证对象,对名词词干进行统计与分析。","现代维吾尔语,形态分析,名词词干识别"
2015-07-15,基于知识融合的CRFs藏文分词系统,"洛桑嘎登,杨媛媛,赵小兵","藏文分词问题是藏文自然语言处理的基本问题之一,该文首先通过对35.1M的藏文语料进行标注之后,通过条件随机场模型对其进行训练,生成模型参数,再用模版对未分词的语料进行分词,针对基于条件随机场分词结果中存在的非藏文字符切分错误,藏文黏着词识别错误,停用词切分错误,未登录词切分错误等问题分别总结了规则,并对分词的结果利用规则进行再加工,得到最终的分词结果,开放实验表明该系统的正确率96.11%,召回率96.03%,F值96.06%。","藏文,分词,条件随机场,知识融合"
2015-08-17,基于SVM和泛化模板协作的藏语人物属性抽取,"朱 臻,孙 媛","该文提出了一种基于SVM和泛化模板协作的藏语人物属性抽取方法。该方法首先构建了基于藏语语言规则的模板系统,收集了包括格助词、特殊动词等具有明显语义信息的特征建设模板并泛化。针对规则方法的局限性,该文在模板的基础上,采用SVM机器学习方法,设计了一种处理多分类问题的层次分类器结构,同时对多样化的特征选取给予说明。最后,实验结果表明,基于SVM和模板相结合的方式可以对人物属性抽取的性能有较大提高。","人物属性抽取,藏语语言处理,SVM,层次分类器"
2013-08-10,一种短正文网页的正文自动化抽取方法,"郗家贞,郭 岩,黎 强,赵 岭,刘 悦,俞晓明,程学旗","随着互联网的发展,网页形式日趋多变。短正文网页日益增多,传统的网页正文自动化抽取方式对短正文网页抽取效果较差。针对以上问题,该文提出一种单记录(新闻、博客等)、短正文网页的正文自动化抽取方法,在该方法中,首先利用短正文网页分类算法对网页进行分类,然后针对短正文网页,使用基于页面深度以及文本密度的正文抽取算法抽取正文。","短正文,正文抽取"
2013-08-10,基于同义扩展的在线百科中实体属性抽取,"刘 倩,刘冰洋,贺 敏,伍大勇,刘 悦,程学旗","实体属性抽取是信息抽取、知识库构建等任务的重要基础。该文提出了一种利用在线百科获取实体属性的方法,该方法首先通过在线百科的结构特征和领域独立的抽取模式捕获可能的属性短语,然后根据同义扩展获取尽可能多的属性表述形式,并同时得到对应实体类别的同义属性集合。实验表明,该方法在保证属性抽取准确率不变的情况下,获得了比仅使用频率的方法覆盖范围更广的实体属性集合。","实体属性,同义属性,命名实体,信息抽取词"
2013-07-10,基于多核融合的中文领域实体关系抽取,"郭剑毅,陈 鹏,余正涛,线岩团,毛存礼,赵 君","针对传统径向基核函数的训练矩阵中所有元素都十分接近零而不利于分类的问题,该文提出了一种融合了改进的径向基核函数及其他核函数的多核融合中文领域实体关系抽取方法。利用径向基核函数的数学特性,提出一种改进的训练矩阵,使训练矩阵中的向量离散化,并以此改进的径向基核函数融合多项式核函数及卷积树核函数,通过枚举的方式寻找最优的复合核函数参数,并以上述多核融合方法与支持向量机结合进行中文领域实体关系抽取。在旅游领域的语料上测试,相对于单一核方法及传统多核融合方法,关系抽取性能得到提高。","关系抽取,径向基核函数,卷积核函数,多核融合"
2013-07-10,基于SAO的专利结构化相似度计算方法,"杜玉锋,季 铎,姜利雪,张桂平",该文提出了一种基于subject-action-object(SAO)的专利结构化相似度计算方法。传统的基于,"数据挖掘,专利相似度,Subject-Action-Object(SAO)技术,实体抽取工具,OLLIE"
2013-08-10,基于混合模型的生物事件触发词检测,"李浩瑞,王 健,林鸿飞,杨志豪,张益嘉","语义歧义增加了生物事件触发词检测的难度,为了解决语义歧义带来的困难,提高生物事件触发词检测的性能,该文提出了一种基于丰富特征和组合不同类型学习器的混合模型。该方法通过组合支持向量机(SVM)分类器和随机森林(Random Forest)分类器,利用丰富的特征进行触发词检测,从而为每一个待检测词分配一个事件类型,达到检测触发词的目的。实验是在BioNLP2009共享任务提供的数据集上进行的,实验结果表明该方法有效可行。","触发词,生物事件,歧义,丰富特征,组合学习器"
2013-08-17,一种无指导的子主题挖掘方法,"郭 程,白 宇,郑剑夕,蔡东风","为了解决用户查询经常存在表意模糊或歧义性等问题,明确用户的查询意图,该文提出了一种无指导的子主题挖掘方法。该方法首先在检索结果文档集中利用ATF &times; PDF模型挖掘候选主题词;其次,为保证子主题的多样性,该文基于HowNet语义相似度方法对候选主题词进行了层次聚类分析,进而得到潜在主题;最后,利用LCS算法生成多样性子主题。实验结果显示,系统平均D#-nDCG@10达到0.573,结果说明该方法在明确查询主题表意方面取得了较好效果。","子主题挖掘,查询意图,潜在主题"
2013-08-17,基于多层Markov网络的信息检索模型,"廖亚男,王明文,左家莉,吴根秀,甘丽新","随着信息检索技术的不断发展,挖掘更加有效的信息来提高检索精度成为研究热点,已有的研究表明在检索过程中有效地融合各种信息将得到更好的检索效果。对一个具体查询而言,可以充分利用与已有查询的相关性、词语相关性和文档相关性等信息进行查询扩展和重构。基于这种思路,该文分别构造查询网络、词网络和文档网络,提出了多层Markov网络的信息检索模型,模型可以融合词间关系、文档间关系和查询间关系,为了有效降低计算量,给出了基于团计算模型。在标准数据集上的实验表明该文的模型能够有效融合三类信息,并较大幅度地提高检索效果。","信息检索,多层Markov网络,查询扩展,团"
2013-10-12,基于时空局部性的层次化查询结果缓存机制,"朱亚东,,郭嘉丰,兰艳艳,程学旗","查询结果缓存可以对查询结果的文档标识符集合或者实际的返回页面进行缓存,以提高用户查询的响应速度,相应的缓存形式可以分别称之为标识符缓存或页面缓存。对于固定大小的内存,标识符缓存可以获得更高的命中率,而页面缓存可以达到更高的响应速度。该文根据用户查询访问的时间局部性和空间局部性,提出了一种新颖的基于时空局部性的层次化结果缓存机制。首先,该机制将固定大小的结果缓存划分为两层:页面缓存和标识符缓存。对于用户提交的查询,该机制会首先使用第一层的页面缓存进行应答,如果未能命中,则继续尝试使用第二层的标识符缓存。实验显示这种层次化的缓存机制较传统的仅依赖于单一缓存形式的机制,在平均查询响应时间上,取得了可观的性能提升:例如,相对单纯的页面缓存,平均达到9%,最好情况下达到11%。其次,该机制在标识符缓存的基础上,设计了一种启发式的预取策略,对用户查询检索的空间局部性进行挖掘。实验显示,这种预取策略的融合,能进一步促进检索系统性能的有效提升,从而最终建立起一套时空完备的、有效的结果缓存机制。","页面缓存,标识符缓存,启发式预取"
2013-09-15,搜索引擎的一种在线中文查询纠错方法,"胡 熠,刘云峰,杨海松,张小鹏,段建勇,张 梅,乔建秀","该文主要解决中文搜索引擎的查询纠错问题。错误的查询,已经偏离用户真实的搜索意图时,搜索质量很差,甚至导致搜索结果数为零。为此该文提出了一种服务于实际搜索引擎,较为完整的查询纠错方案。该文重点描述了纠错查询候选生成、纠错查询候选评价、以及基于核函数,挑选最优纠错查询候选等内容。通过在开放测试集上的准确率/召回率验证,以及在搜索引擎中实际的DCG评测,该文的方案都取得了较好的效果。","中文查询纠错, 多特征, 核函数排序"
2013-09-25,网页搜索中查询时效性的实时计算模型,"胡 熠,刘云峰,段建勇,熊展志,乔建秀,张梅","网页搜索中的查询时效性是指查询对新闻网页的需求。这种时间相关的因素,在网页排序过程中用于平衡其他非时间性因素,使排序更好地满足用户体验。为此该文提出了一种查询时效性的实时计算模型从用户搜索和媒体报道两个角度,分别对时效性建模,然后这两种不同来源的时效性相互补充,综合计算某个时刻用户搜索某个查询时,其综合时效性得分。这个量化得分在网页排序阶段用于提高或抑制新闻网页的露出;同时也为网页搜索结果中展现新闻直达区提供依据。在人工评测以及用户点击通过率统计上,该模型均取得了不错的实际效果。","查询时效性,时效性用户模型,时效性媒体模型"
2013-07-16,LinkMF: 结合Linked Data的协同过滤推荐算法,"黄山山,马 军,郭 磊,王帅强","协同过滤(CF)是推荐系统中应用最为广泛的推荐算法之一,然而数据稀疏性和冷启动问题是协同过滤方法的两个主要挑战。由于Linked Data整合了关于实体的丰富且结构化的特征,可以作为额外的信息源来缓解以上两种挑战。该文中我们首次提出了结合Linked Data改进CF推荐算法,基于矩阵分解提出了一种新的CF模型LinkMF,在保证推荐准确度的基础上利用Linked Data缓解数据稀疏性和冷启动问题。首先,我们从Linked Data中抽取项目的特征表示并为项目建模;然后提出新的相似度度量方法计算项目相似度;最后利用项目相似度约束和指导MF分解过程产生推荐。在MovielLens和YAGO标准数据集上的大量实验结果表明,LinkMF优于现有的一些CF方法,特别在缓解数据稀疏性和冷启动问题上取得很好地效果。","推荐系统,矩阵分解,Linked Data,数据稀疏性,冷启动"
2013-06-20,基于微博用户模型的个性化新闻推荐,"古万荣,董守斌,曾之肇,何锦潮,刘 崇","新闻推荐是互联网推荐系统的研究热点之一,传统的新闻推荐方法是在新闻网站内,通过记录用户浏览的新闻来实现推荐应用。然而,许多新闻网站并不强制要求用户必须注册才能浏览新闻。微博作为目前最主流的自媒体形式,它由用户自己发起或传递,进而实现草根媒体的职能。对新闻进行高效组织并使用微博进行新闻推荐,这是之前研究欠缺的。该文通过提出基于微博分析的新闻推荐,提出了基于新闻和微博本身特点的解决方法,从而实现微博和新闻的关联。实验表明,该文设计的各模块具备较高的效率和实用效果。 ","新闻推荐,文本分类,微博分析"
2014-01-05,基于同义词词林信息特征的语义角色自动标注,"李国臣,吕 雷,王瑞波,李济洪,李 茹","该文使用同义词词林语义资源库,以词林中编码信息为基础构建新的特征,使用条件随机场模型,研究了汉语框架语义角色的自动标注。该文在先前的基于词、词性、位置、目标词特征的基础上,在模型中加入不同的词林信息特征,以山西大学的汉语框架语义知识库为实验语料,研究了各词林信息特征分别对语义角色边界识别与分类的影响。实验结果表明,词林信息特征可以显著提高语义角色标注的性能,并且主要作用在语义角色分类上。","语义角色标注,同义词词林,条件随机场,正交表"
2013-06-08,中心修正增量主成分分析及其在文本分类中的应用,"陈素芬,曾雪强","增量式学习模型是挖掘大规模文本流数据的一种有效的数据处理技术。无偏协方差无关增量主成分分析(Candid Covariance-free Incremental Principal Component Analysis, CCIPCA)是一种增量主成分分析模型,具有收敛速度快和降维效果好的特点。但是,CCIPCA模型要求训练数据是已经中心化或中心向量固定的。在实际的应用中,CCIPCA往往采用一种近似的中心化算法对新样本进行处理,而不会对历史数据进行中心化修正。针对这一问题,该文提出了一种中心修正增量主成分分析模型(Centred Incremental Principal Component Analysis, CIPCA)。CIPCA算法不仅对新样本进行中心化处理,而且会对历史数据进行准确的中心化修正。在文本流数据上的实验结果表明,CIPCA算法的收敛速度和分类性能明显优于CCIPCA算法,特别是在原始数据的内在模型不稳定的情况下,新算法的优势更为明显。","主成分分析,中心化修正,流数据,维数约减,增量学习"
2013-06-25,基于主位-述位结构理论的英文作文连贯性建模研究,"徐 凡,王明文,谢旭升,李茂西,万剑怡","该文在研究了有监督的基于实体和基于篇章关系网格的篇章连贯性模型的基础上,提出了一个无监督的基于主位-述位结构理论的篇章连贯性模型。该模型通过引入词语的词干、上下位、近义和复述等语义方面的信息来计算相邻句子中主位和述位的相似度,并利用此相似度值来描述篇章的连贯性。同时,该文提出了一种简单有效的基于篇章关系计数的连贯性模型,并采用线性组合方法将其与基于主位-述位结构理论的连贯性模型加以集成。上述模型在国际基准英文作文语料上进行试验,实验结果表明采用线性组合的连贯性模型后,作文连贯性检测准确率与目前基于实体和篇章关系网格的模型相比得到显著提升。","衔接性,连贯性,主位-述位结构理论,篇章关系,线性组合"
2013-06-08,基于词干的蒙古语语音关键词检测方法的研究,"飞 龙,高光来,王宏伟",为了提高蒙古语语音,"蒙古语,词干,混淆网络,置信度"
2013-05-08,彝语言语料资源数据库的设计与共享的实现,王成平,"该文以收集整理翻译的彝语言语料为基础,在SQL Server 2008数据库环境下,通过ODBC,利用VC++ 6.0编写彝语言语料入库程序,实现了彝语言语料U文件(Unicode彝文)和Y文件(YIWIN彝文)的自动入库,完成了彝语言语料资源数据库的设计;通过编写WEB服务端的查询和统计程序,利用C/S方式实现了彝语言语料基于WEB浏览器的访问和远程共享,同时也为其他少数民族文字信息处理中的类似问题提供了一个可参考的解决方案。","彝语言,语料库,数据库设计,共享"
2014-09-01,基于N-Gram模型的蒙古语文本语种识别算法的研究,"马志强,张泽广,闫 瑞,刘利民,冯永祥,苏依拉","互联网上蒙古语文本正在不断地增加,如何让网络中的蒙古语内容为搜索引擎和舆情分析等应用提供服务引起了社会的高度关注。首先要解决如何采集网络中蒙古语文本数据,核心是准确识别网络中蒙古语文本的问题。该文提出了基于N-Gram模型的平均距离识别算法,建立了一个能够对目标语种识别的实验平台。实验结果表明,识别算法能够很好地从中文、英文、蒙古文以及混合语言文本中识别出蒙古语文本,准确率达到99.5%以上。","语种识别,N-Gram模型,平均距离识别算法,蒙古语文本"
2013-07-22,细粒度意见挖掘中维吾尔语文本情感分析研究,"罗亚伟,田生伟,禹 龙,吐尔根·依布拉音,艾斯卡尔·艾木都拉","传统的情感分析研究通过分析, 确定词语、句子或篇章的情感, 但忽略了情感表达的主题。针对这一不足, 该文提出了一种基于双层CRFs模型的细粒度意见挖掘中维吾尔语意见型文本陈述级情感分析方法。第一层模型识别意见型文本中的主题词和意见词, 确定意见陈述的范围, 并将识别结果传递给第二层模型, 将其作为重要特征之一, 用于陈述级情感分析。细粒度意见挖掘中情感分析的目标是构建&lt;意见陈述, 主题词, 意见词, 情感&gt;四元组。该方法用于维吾尔语陈述级情感分析的准确率为77.41%, 召回率为78.51%, 证明了该方法在细粒度意见挖掘中情感分析任务上的有效性。","细粒度, 陈述级, 情感分析, CRFs, 维吾尔语"
2013-06-11,维吾尔语比较句识别研究,"王慧云,禹 龙,田生伟, 加米拉·吾守尔,冯冠军","识别比较句并提取被比较事物之间的关系是细颗粒度意见挖掘的重要研究内容之一。该文给出维吾尔语比较句的范畴、语法特点,定义了维吾尔语比较句识别的任务。提出两层识别模型,第一层是基于比较词的粗识别,第二层提出双向CSR挖掘算法(Bidirectional CSR Mining),以挖掘的模式为特征,利用支持向量机(SVM)筛选得到比较句,实现维吾尔语比较句的识别。实验F值达到70.93%,证明提出的两层识别模型可以有效识别维吾尔语比较句。","维吾尔语,比较句识别,双向CSR挖掘算法, 文本分类"
2014-05-08,基于多模板归一化的维吾尔文字母识别算法,"刘  卫,李和成","该文针对手写维文字符识别中字符宽高比变化剧烈,单一模板归一化后提取字符特征,不能有效增加异类字符之间的差异性,提出了针对维文字形特点的多模板归一化算法。训练阶段,由多模板归一化字符图像,提取特征并训练对应分类器;识别阶段,用主笔画散度方向作为维文字形参数, 对不同字形选用最优模板进行归一化处理后提取特征,并送入该模板对应的分类器。多模版归一化有效利用了手写维文字符字形特征,克服了单模板归一化时异类维文字符差异减小的不利影响。实验结果表明多模板归一化算法较单模板归一化算法在识别性能上有所提高。","维吾尔文字符,归一化,宽高比,分类器"
2013-05-10,交通数据中的会话识别,"娄新燕,刘  洋,禹晓辉","会话识别因其能够提供对用户行为模式的深入理解而备受关注。交通数据会话是指用户为了完成某个任务而经过的交通路口序列。该文中我们采用超时和统计语言模型两种方法来进行会话识别。超时方法主要考察相邻交通路口之间的时间间隔对会话识别的影响,而统计语言模型则考虑路口序列的全局规律性。我们在交通数据集上进行了大量的实验,并通过比较分析两种方法性能上的差异得知时间因素比全局规律性在会话识别中的影响更大。","会话识别,超时方法,统计语言模型"
2013-07-08,MBNER:面向生物医学领域的多种实体识别系统,"杨 娅,杨志豪,林鸿飞,宫本东,王 健","生物命名实体识别,就是从生物医学文本中识别出指定类型的名称。目前,面向生物医学领域的实体识别研究不断出现,从海量生物医学文本自动提取生物实体信息的技术变得尤为重要。该文介绍了一个面向生物医学领域的多实体识别系统MBNER(Multiple Biomedical Named Entity Recognizer)。该系统可以在生物医学文本中同时识别出基因(蛋白质)、药物、疾病实体,其对基因(蛋白质)、药物、疾病实体识别在各自数据集上分别得到了89.05%,76.73%,90.12%的综合分类率(F-score)。该系统以可视化的形式给出对三种命名实体的识别结果。","机器学习,特征耦合泛化,CRF,全称缩写对"
2014-01-08,基于语义资源的生物医学文献知识发现研究,"李宗耀,杨志豪,吴晓芳,林鸿飞,宫本东,王 健","目前,生物医学文献的数量正在呈指数的方式快速增长,这些文献中隐含着大量有用的信息,挖掘这些文献可以形成医学假设。但传统的基于简单共现的方法会产生大量的目标词,导致很难发现有用的假设。该文提出了一种基于语义资源的方法,利用SemRep工具抽取句子内实体之间的关系,结合语义类型、概念的信息量以及关联规则对连接词、目标词进行过滤,并根据统计量信息对目标词进行排序。通过对Swanson发现的经典病例进行验证,实验结果表明该方法取得很好的效果。","隐含知识发现,共现,语义关系"
2013-06-08,网络游戏案例研究: 用户行为分析和流失预测过,"过岩巍,吴悦昕,赵 鑫,闫宏飞,黄建兴","用户流失预测在很多领域得到关注,目前主流的用户流失预测方法是使用分类法。网络游戏领域发展迅猛,但用户特征选取、特征处理和流失预测的相关研究较少。本文以一款网页网络游戏的用户记录为数据,对用户游戏行为进行分析对比,发现流失用户在游戏投入、博彩热情、玩家互动方面与正常用户存在显著差异;同时发现网络游戏数据存在样本分布不平衡、候选特征库庞大和干扰差异多等难点。在此分析基础上,本文探讨了网游用户的关键特征提取的关注方向,以及归一化和对齐化在特征处理中的关键作用。实验表明,本文提取的特征具有很好的区分度。","行为分析,特征提取,流失预测,网络游戏"
2013-07-08,一种中文伪评论语料半自动获取方法,"郝秀兰,许方曲,蒋云良","该文提出了一种中文伪评论语料半自动收集方法,主要包括数据收集、句法分析、情感倾向性分析等方法,并对影响方法正确性的错误进行了总结。文中着重介绍了一种句法分析方法,在句法分析的基础上提出了&lt;评价对象,评价短语&gt;的提取方法。该提取方法简化了情感二元对的句法呈现模式。同时,对部分实验结果进行了分析,对提高文本情感分析的准确率提出了一些建议。","计算机应用,中文信息处理,倾向性分析,伪中文评论,半自动获取"
2013-06-05,基于维基百科的双语可比语料的句子对齐,"胡弘思,姚天昉","该文提出了一种从维基百科的可比语料中抽取对齐句子的方法。在获取了维基百科中英文数据库备份并进行一定处理后,重构成本地维基语料数据库。在此基础上,统计了词汇数据、构建了命名实体词典,并通过维基百科本身的对齐机制获得了双语可比语料文本。然后,该文在标注的过程中分析了维基百科语料的特点,以此为指导设计了一系列的特征,并确定了对齐、部分对齐、不对齐三分类体系,最终采用SVM分类器对维基百科语料和来自第三方的平行语料进行了句子对齐实验。实验表明:对于语言较规范的可比语料,分类器对对齐句的分类正确率可达到82%,对于平行语料,可以达到92%,这说明该方法是可行且有效的。","SVM,句子对齐,可比语料,维基百科,SVM"
2013-07-03,基于释义扩展的术语归类研究,"贺 刚,吕学强,肖诗斌,王 凡","术语归类研究对领域本体构建与特定领域词表扩展有十分重要的意义。该文针对中国知网概念知识元库中存在的术语归类错误问题,研究如何提高术语归类正确率。经分析发现术语具有释义文本短、所包含的能够区分术语类别的特征词较少的特点。该文提出一种基于释义扩展的术语归类方法,该方法引入了释义扩展思想,以搜索引擎为工具,获取术语相关的互联网知识,抽取查询结果的锚文本和摘要文本等内容扩展术语释义文本;采用向量距离算法计算术语释义文本特征向量与类中心向量之间的距离,实现对术语的归类。实验得到的术语归类总体正确率为73.32%,与未经释义扩展得到的术语归类正确率相比,提高了近10%。实验结果表明,该方法对提高术语归类正确率是有效的。","术语归类,释义扩展,向量距离,类中心向量"
2013-11-18,汉语概念复合块的自动分析,"仵永栩,吕学强,周 强,关晓炟","为解决句法分析任务中的块边界识别和块内结构分析问题,该文基于概念复合块描述体系进行了块分析探索。通过概念复合块与以往的基本块和功能块描述体系的对比分析,深入挖掘了概念复合块自动分析的主要难点所在,提出了一种基于“移进-归约”模型的汉语概念复合块自动分析方法。在从清华句法树库TCT中自动提取的概念复合块标注库上,多层次、多角度对概念复合块自动分析性能进行了纵向与横向评估,初步实验结果证明了该分析方法对简单概念复合块分析的有效性,为后续进行更复杂的概念复合块的句法语义分析研究打下了很好的基础。","句法分析,块识别,概念复合块,移进-归约分析"
2014-08-19,融合分词隐层特征的汉语基本块识别,"李国臣,刘展鹏,王瑞波,李济洪","该文以字为基本标注单位,构建了一种汉语基本块识别的神经网络学习模型。模型联合分词任务的神经网络学习模型与基本块识别任务模型,将分词任务模型中学习得到的隐层特征融入基本块识别的模型中,两模型相互交替优化学习模型参数,并实现了以整句似然函数(而非单字似然函数)作为优化目标的算法。实验结果表明:1)以整句似然函数为优化目标的基本块识别的F值比单字似然情形要高出1.33%,特别是在多字块识别中,其召回率比单字似然情形要高出4.68%;2)融合分词任务模型中的隐层特征的汉语基本块识别模型的结果比不做融合的模型要高出2.17%,说明融合分词隐层特征的交替联合学习方法是有效的。","分布表征,汉语基本块识别,神经网络模型,隐层特征,整句似然函数"
2013-09-09,基于PDTB的自动显式篇章分析器,"李 生,孔 芳,周国栋","自动篇章处理是自然语言处理中非常有挑战的一个任务,对自然语言处理的其他任务,如问答系统,自动文摘以及篇章生成都有重要的作用。近年来,大规模篇章语料PDTB的出现为篇章研究提供了一个公共的平台。该文在PDTB语料之上提出了一个完整的基于条件随机场模型的显式篇章分析平台,该平台包含连接词识别、篇章关系分类和关系论元提取三个子任务。给出了在PDTB上各模块的实验结果,并针对错误传播问题,给出了完整平台的性能及详细分析。","篇章处理,条件随机场,宾州篇章树库"
2013-01-08,一种基于特征映射的中文专家消歧方法,"潘 霄,余正涛,郭剑毅,毛存礼,杨秀贞","针对中文专家页面特点,以及用于消歧的基准专家页面中信息涵盖不全的问题,该文提出一种基于特征映射的中文专家消歧方法。首先,采用条件随机场模型,从基准专家页面和待消歧页面中提取出所定义的12维人物属性特征,并利用最大熵分类模型,结合已有消歧结果训练出各属性特征的权重;然后,针对某个专家的基准页面,计算待消歧页面与该页面的相似度,根据设定的阈值判断该页面是否单独成类,若不是单独成类,则利用特征映射,扩充该页面的属性特征,结合模糊聚类方法,得到与该页面为一类的页面。在“自然语言处理”及“机器学习”领域进行中文专家消歧实验,结果表明提出的方法能有效对中文专家页面进行消歧。","中文专家消歧,属性特征,特征映射,模糊聚类"
2013-11-21,基于多分类器投票集成的半监督情感分类方法研究,"黄 伟,范 磊","情感分类是目前自然语言处理领域的一个具有挑战性的研究热点,该文主要研究基于半监督的文本情感分类问题。传统基于Co-training的半监督情感分类方法要求文本具备大量有用的属性集,其训练过程是线性时间的计算复杂度并且不适用于非平衡语料。该文提出了一种基于多分类器投票集成的半监督情感分类方法,通过选取不同的训练集、特征参数和分类方法构建了一组有差异的子分类器,每轮通过简单投票挑选出置信度最高的样本使训练集扩大一倍并更新训练模型。该方法使得子分类器可共享有用的属性集,具有对数时间复杂度并且可用于非平衡语料。实验结果表明我们的方法在不同语种、不同领域、不同规模大小,平衡和非平衡语料的情感分类中均具有良好效果。","情感分类,集成学习,半监督学习"
2013-09-04,基于因果模型的主题热度计算与预测方法,"杜 慧,郭 岩,范意兴,张 瑾,余智华,程学旗","网络是目前最重要的信息传播渠道,其自由性和丰富性使得信息迅速传播。挖掘网络中的热点主题对政府政策的制定、企业经营决策的调整可以提供强有力的支持,并能够满足网民对热点主题的关注需求。主题数量的庞大使得主题热度值的计算尤为重要,该文分析热度的形成原因,基于因果模型并采用面板数据,给出一种较为客观可行的主题热度计算模型。该模型使用易于获取的数据进行计算,给出较为客观的热度度量,进而便于不同主题、不同日期间的热度对比。在此基础上,通过对热度变化规律的考察,提出一种基于多峰高斯曲线拟合热度变化进行主题热度预测的思路。","主题热度,因果模型,面板数据,热度预测,多峰高斯曲线"
2013-09-15,社交网络用户标签预测研究,"刘 列,邢千里,刘奕群,张 敏,马少平","随着社交网站的流行以及用户的大规模增加,社交网络用户行为分析已经成为社交网站进行网站维护、性能优化和系统升级的重要基础,也是网络知识挖掘和信息检索的重要研究领域。为了更好地理解社交网络用户添加个人标签的行为特征,该文基于大约263万个微博用户的真实数据,对用户标签的分布进行了研究和分析。我们主要考察了用户标签的宏观分布特征,以及用户标签与关注对象的标签分布之间的联系,发现微博用户给自己添加标签时,在开始阶段倾向于使用反映个性的标签,之后会出于从众心理而选用大众化标签。我们将研究发现运用到基于关注关系的标签预测算法中,结果证实相关分析对于社交网站的标签推荐等课题具有一定的参考意义。","社交网络,用户行为分析,标签预测"
2013-10-30,一种融合地理位置信息的协同过滤推荐算法,"鲁 骁,王书鑫,王 斌,鲁 凯","目前,基于用户消费数据构建的推荐系统在电子商务领域发挥着越来越大的作用,而在这些数据中,商家本身具有的地理位置信息忠实地记录了用户的消费痕迹,能够有效反映出用户在地理位置维度上的个人偏好信息,从而对推荐系统具有非常重要的意义。现有工作一般只利用了用户对地点的评价以及地点之间的距离,无法反映出不同地点之间的关联关系,以及用户在不同地点中的偏好权重问题。该文从地理区域划分的角度出发,研究了用户在区域范围内的消费兴趣偏好,以及不同粒度级别的区域划分方法对推荐模型的影响,探索了在推荐过程中有效融合地域信息的方法,考虑了包括地区的全局性影响、用户对地区的偏好等,结合这些因素提出了融合地理位置信息的推荐模型LGE、LGN及LRSVD。通过在Yelp数据集上的实验表明,这些模型相比于传统的推荐算法能够有效提高预测效果。","推荐系统,协同过滤,地理位置信息,邻居模型,隐参数模型"
2013-09-15,利用社交网络的影响力骨架探索信息传播,"黄俊铭,沈华伟,程学旗","理解社交网络上的信息传播机制,通常包括对拓扑结构的分析和对用户行为的分析。由于社交网络上连边的强度具有异质性,只有一部分连边对于信息传播有实质作用,构成隐藏在社交网络中的影响力骨架。对影响力骨架的拓扑研究可帮助我们获得比直接研究社交网络拓扑结构更深入的认识。我们从连边正负性和个体节点角色分化入手,探讨了微观层面连边和节点在信息传播中的作用,进而从宏观层面分析信息传播所依赖的影响力骨架的连通性和扩散效率,发现信息传播具有一定程度的脆弱性,且其传播效率低于对社交网络本身研究的预期。","信息传播,社交网络,影响力骨架"
2013-10-15,基于在线社会网络的用户影响力研究,"许丹青,刘奕群,张 敏,马少平","对大规模的在线社会网络图结构进行了较为系统的分析,结果表明社会网络的入度、出度、发文数等基本符合幂律分布。社会网络的小世界属性也使得强连通关系呈现“纺锤体”形状。该文从用户的阅读概率角度引入用户的发文行为、浏览行为与标签社区小世界属性等对用户的社会影响力模型进行建模。实验结果显示PTIM模型融合了发文行为与小世界属性等特性,在最具影响力用户节点、用户粉丝数、认证用户数与人工标注的相对用户影响力大小等指标上均表现出稳定的性能。","社会影响力,小世界属性,信息扩散,社会网络"
2013-09-15,基于用户相似性传递的跨平台交叉推荐算法,"李 超,周 涛,黄俊铭,程学旗,沈华伟","个性化推荐系统在电子商务领域中的广泛应用带来了巨大的经济效益和良好的用户体验。由于用户数据往往分布在多个不同的网站,单个网站的推荐系统受制于数据稀疏性的限制,难以获得准确的推荐效果。该文提出了一种基于传递相似性的交叉推荐系统算法,可以利用多个网站平台数据计算不同网站中的用户的相似度,从而很大程度上克服了推荐系统中的数据稀疏性以及冷启动问题。结果显示,该交叉推荐算法与传统的针对单个数据集的推荐算法相比,推荐的精确性有一至两倍的提高。","个性化推荐系统,协同过滤,多源数据,稀疏性,冷启动"
2013-08-09,一种支持混合语言的并行查询纠错方法,"颛 悦,熊锦华,马宏远,程舒杨,程学旗","中文信息检索系统中的查询语句包含中文字、拼音、英文等多种形式,而有些查询语句过长,不利于纠错处理。现有的查询纠错方法不能很好的解决中文检索系统中的混合语言与中文长查询的问题。为了解决上述两个问题,该文提出了一种支持混合语言的并行纠错方法。该方法通过对混合语言统一编码,建立统一编码语言模型和异构字符词典树,并根据语言特点制定相应的编辑规则对查询词语进行统一处理,其中,针对中文长查询,提出双向并行的纠错模型。为了并行处理查询语句,我们在字符词典树和语言模型的基础上提出了逆向字符词典树和逆向语言模型的概念。模型中使用的训练语料库是从用户查询日志、网页点击日志、网页链接信息等文件中提取的高质量文本。实验表明,与单向查询纠错相比,支持混合语言的并行纠错方法在准确率上提升了9%,召回率降低了3%,在速度上提升了40%左右。","查询纠错,词典树,语言模型,并行纠错"
2013-08-15,结合句子级别检索的信息检索模型,"左家莉,王明文,吴水秀,万剑怡","查询词之间的距离较为接近的文档,相关的可能性更大,将这种距离信息用于信息检索模型的构造可有效提高检索的性能。然而直接估计查询词在文档中的距离需要大量的训练文本,且计算复杂度高。该文提出了一种结合句子级别检索的信息检索模型,将文档分为若干个窗口,通过计算句子和查询的相关度考察查询词在给定窗口中的共现性,该方法可增大那些查询词彼此靠近的文档的相关度,从而使得检索模型可返回更为相关的文档。标准数据集上的实验结果表明所提出的模型可以取得较好的性能。","信息检索模型,句子级别检索,句子相关度"
2013-06-18,查询会话中带时间因子的隐式负反馈研究,"陈振宏,俞晓明,刘 悦,程学旗","隐式相关反馈常被用于提升检索系统的性能,目前大部分工作集中在研究隐式正反馈。该文同时考虑隐式正负反馈,将查询会话中被点击网页前的未被点击网页作为隐式负反馈信息,通过引入时间因子,估计用户在未被点击网页的标题和摘要上的停留时间,推断隐式负反馈与用户兴趣和行为的关系,达到优化检索结果的目的。在TREC Session 2011和2012数据集上的实验,验证了该文提出的带时间因子的隐式正负反馈算法TIPNF的有效性。","查询会话,隐式负反馈,时间因子,排序"
2013-06-08,基于内容和用户行为的查询聚类,"程舒杨,熊锦华,公 帅,程学旗","现有方法没有有效利用查询文本特征、点击行为和session信息来挖掘用户的搜索意图,获取的查询特征对于多意图查询在不同意图下的区分度不足,对于多意图查询的相关查询聚类效果不佳。针对以上问题,该文提出了基于查询图信息的GPLSI模型,并利用该模型学习所得的查询特征进行查询聚类。基于查询图信息的GPLSI模型利用查询的词语、点击和session共现现象,从查询的文本特征、点击行为和session信息等多个方面来模拟查询意图的产生和表现,学习查询在不同搜索意图上的概率分布。最后,实验结果验证了基于查询图信息的PLSI模型用于查询相似度计算和多意图查询聚类中的有效性。","查询聚类,多意图查询,搜索意图"
2013-09-15,基于用户偏好与语言模型的个性化引文推荐,"刘亚宁,严 睿,闫宏飞","根据引文上下文,自动为科研人员推荐备引用的论文列表具有很大的实用价值和研究意义。在科研人员写作时,一个为引用符自动推荐引文的系统,会为科研人员节省大量的时间。对于引文推荐问题,过去的工作均主要把注意力集中到基于内容的研究上。该文认为引文推荐,不能只根据内容进行通用推荐,还需要根据不同研究者的偏好进行个性化推荐。该文利用用户的发表及引用历史,结合语言模型,构建出一个个性化引文推荐模型——PCR模型。在结合用户引用倾向性与内容相关性后,与传统的基于内容的语言模型相比,PCR模型在recall@10上获得了71.01%的性能提升,在MAP上获得了70.23%的性能提升。","引文推荐,个性化"
2013-07-13,基于本体和语义文法的上下文相关问答,"王东升,王 石,王卫民,刘亮亮,符建辉","在问答系统中,用户的提问通常不是孤立的,而是使用连续的多个相关的问题来获取信息,用户在与这样的系统进行交互时,才会感觉更自然。在已构建的非上下文相关问答系统的基础上,该文提出了一种可以处理上下文相关问题的方法并开发了系统OSG-IQAs。方法首先识别当前问题是否是一个从问题(follow-up),并判别其与前面问题的具体的相关类别,然后根据相关类别,利用话语结构中的信息对当前的follow-up问题进行重构,并提交到非上下文相关问答系统中。最后,将方法在两个不同规模的领域进行测试,并与相关系统或方法进行比较,测试结果表明,该方法具有较好的可扩展性。在总体测试中,该方法比基线系统获得了更好地效果,同时利用手工将所有上下文相关问题进行上下文消解,系统与此也进行了比较,并获得了相近的性能。","本体,语义文法,上下文,问答"
2013-10-11,利用维基百科实体增强基于图的多文档摘要,"陈维政,严 睿,闫宏飞,李晓明","针对基于图的多文档摘要,该文提出了一种在图排序中结合维基百科实体信息增强摘要质量的方法。首先抽取文档集合中高频实体的维基词条内容作为该文档集合的背景知识,然后采用PageRank算法对文档集合中的句子进行排序,之后采用改进的DivRank算法对文档集合和背景知识中的句子一起排序,最后根据两次排序结果的线性组合确定文档句子的最终排序以进行摘要句的选取。在DUC2005数据集上的评测结果表明该方法可以有效利用维基百科知识增强摘要的质量。","多文档摘要,维基实体,基于图"
2013-12-10,基于迁移学习的蛋白质交互关系抽取,"李丽双,郭 瑞,黄德根,周惠巍","作为生物医学信息抽取领域的重要分支,蛋白质交互关系(Protein-Protein Interaction,PPI)抽取具有重要的研究意义。目前的研究大多采用统计机器学习方法,需要大规模标注语料进行训练。训练语料过少,会降低关系抽取系统的性能,而人工标注语料需要耗费巨大的成本。该文采用迁移学习的方法,用大量已标注的源领域(其它领域)语料来辅助少量标注的目标领域语料(本领域)进行蛋白质交互关系抽取。但是,不同领域的数据分布存在差异,容易导致负迁移,该文借助实例的相对分布来调整权重,避免了负迁移的发生。在公共语料库AIMed上实验,两种迁移学习方法获得了明显优于基准算法的性能;同样方法在语料库IEPA上实验时,TrAdaboost算法发生了负迁移,而改进的DisTrAdaboost算法仍保持良好迁移效果。","蛋白质交互关系抽取,迁移学习,负迁移"
2013-06-08,基于领域知识抽样的深网资源采集方法,"林海伦,熊锦华,王 博,程学旗","深网资源是指隐藏在HTML表单后端的Web数据库资源,这些资源主要通过表单查询的方式访问。然而,目前的网页采集技术由于采用页面超链接的方式采集资源,所以无法有效覆盖这些资源,为此,该文提出了一种基于领域知识抽样的深网资源采集方法,该方法首先利用开源目录服务创建领域属性集合,接着基于置信度函数对属性进行赋值,然后利用领域属性集合选择查询接口并生成查询接口赋值集合,最后基于贪心选择策略选择置信度最高的查询接口赋值生成查询实例进行深网采集。实验表明,该方法能够有效地实现深网资源的采集。","深网,置信度,抽样,领域知识"
2013-08-25,FPC: 大规模网页的快速增量聚类,"余 钧,郭 岩,张 凯,刘 林,刘 悦,俞晓明,程学旗","面向结构相似的网页聚类是网络数据挖掘的一项重要技术。传统的网页聚类没有给出网页簇中心的表示方式,在计算点簇间和簇簇间相似度时需要计算多个点对的相似度,这种聚类算法一般比使用簇中心的聚类算法慢,难以满足大规模快速增量聚类的需求。针对此问题,该文提出一种快速增量网页聚类方法FPC(Fast Page Clustering)。在该方法中,先提出一种新的计算网页相似度的方法,其计算速度是简单树匹配算法的500倍;给出一种网页簇中心的表示方式,在此基础上使用Kmeans算法的一个变种MKmeans(Merge-Kmeans)进行聚类,在聚类算法层面上提高效率;使用局部敏感哈希技术,从数量庞大的网页类集中快速找出最相似的类,在增量合并层面上提高效率。","DOM树分层向量,网页簇中心,局部敏感哈希,快速增量聚类"
2013-12-11,文本聚类的重构策略研究,"陈笑蓉,刘作国","该文提出面向文本距离并独立于聚类过程的聚类重构策略。提出邻近域的概念并阐述了邻近域规则,设计了高斯加权邻近域算法。利用高斯函数根据样本与聚簇中心的距离为样本赋权,计算聚簇间距。基于邻近域权重对文本聚类的结果实施重构。使用拆分算子拆分稀疏聚簇并调整异常样本;使用合并算子合并相似聚簇。实验显示聚簇重构机制能够有效地提高聚类的准确率及召回率,增加聚簇密度,使得形成的聚类结果更加合理。","文本聚类,聚簇重构,邻近域规则,高斯加权"
2013-09-20,Wikipedia跨语言链接发现中的锚文本译项选择,"郑剑夕,白 宇,郭 程,张桂平","Wikipedia跨语言链接发现主要研究从源语言Wikipedia文章中自动识别与主题相关的锚文本,并为锚文本推荐一组相关的目标语言链接。该研究涉及三个关键问题: 锚文本识别、锚文本翻译和目标链接发现。在锚文本翻译中,一个锚文本可能存在多个目标译项,如果其译项选择有误,将会直接影响目标链接发现中的链接推荐的准确性。为此,该文提出了一种基于上下文的锚文本译项选择方法,使用基于逐点互信息投票的方式确定锚文本的译项。 对中英文Wikipedia中的人名、术语以及缩略语的译项选择进行测试,实验表明该方法取得了较好的效果。","Wikipedia,跨语言链接发现,锚文本,译项选择,逐点互信息"
2013-10-21,藏文字符的向量模型及构件特征分析,"才智杰,才让卓玛","藏文字属性分析是藏文信息处理的一项基础性工作,对藏文信息处理的研究和藏语文教学具有重要的参考价值及指导意义。藏文字是一种特殊的拼音文字,由1～7个基本构件横向和纵向拼接而成。因而藏文字符的属性包括其组成的构件及其构件的位置特征,以及藏文字的使用频度、结构、字长等属性特征。该文通过分析藏文字的结构,分别建立了藏文字及藏文字符串的向量模型VMTT、VMTS和藏文字符串的稀疏域模型SLM,并在向量模型和稀疏域模型上研究了藏文字符的构件特征。","中文信息处理,向量模型,稀疏域模型,构件"
2013-10-17,面向维吾尔语关键词检索的等宽切词算法,"木合塔尔·沙地克,布合力齐姑丽·瓦斯力,李 晓","该文提出了面向维吾尔语关键词检索的两种切词算法,并给出MATLAB实现的算法代码及详细说明;在同等条件下对两种算法的切词效果和关键词识别效率进行对比分析;提出两种算法的优化方法和构想。","维吾尔语,敏感词检索,切词,广播新闻"
2013-11-14,汉蒙机器翻译中译文动词后处理研究,"王斯日古楞,王春荣,斯琴图,阿 荣,玉 霞","蒙古文的形态变化非常丰富,在动词词类上该特点更为明显。我们对蒙古文的动词自动生成方法进行了系统的研究。该文利用生成的蒙古语动词库,给出了对基于层次短语的汉蒙统计机器翻译译文中句尾错误词形动词进行纠正处理的方法。实验表明,该方法可以提高汉蒙机器翻译的性能和流利度。","蒙古文动词,汉蒙机器翻译,后处理,错误词形"
2014-06-09,计算语义合成性综述,"王超超,熊德意","随着自然语言处理技术的飞速发展,单纯在语法层上的研究已经不能解决目前的问题,语义层的研究逐渐成为热点。计算语义合成性作为语义学的关键部分,受到了诸多研究人员的关注。计算语义合成性的研究方法可以分为两大类: 语言学方法和分布式方法。该文详细介绍了它们各自具有代表性的工作,着重阐述了近年来使用广泛的深度学习方法在计算语义合成性研究中的应用,并对这两种方法进行了比较;然后对计算语义合成性在情感分析以及机器翻译中的应用做了细致分析;最后,展望了计算语义合成性未来的研究趋势。","语义合成,自然语言处理,分布式方法,深度学习"
2014-02-03,汉语篇章中零形式的识别与消解,"武娟,李茹,王智强","传统的语义角色标注只能为句中显式表达的句法论元分配语义角色,但是忽略了一些隐式的语义成分,即零形式。该文基于汉语框架语义研究了零形式的识别及消解。在识别阶段,首先使用规则方法进行零形式检测,然后运用筛选过滤的策略去除部分错误识别的零形式;在消解阶段,将篇章中显式表达的框架元素填充项作为零形式的候选先行语,提出结合框架元素语义类型与框架关系的消解方法。在构建的164篇中文语料上进行实验,与其他方法相比,该方法能获得更好的结果。","汉语框架网,零形式识别,零形式消解"
2014-02-24,汉语冒号标注与自动识别方法研究,"谷晶晶,周国栋","随着对篇章分析研究的逐步加深,标点符号研究成为了篇章分析与消歧的一个重要切入点。有效识别标点符号在句子中的作用,将有助于句法分析、篇章分析以及其他自然语言处理技术的发展。该文主要任务是实现汉语冒号的人工标注与自动识别,其中自动识别采取了规则法和基于统计的最大熵法。基于规则的方法比较简单且易于实现,最大熵方法把规则融入到统计之中,在实验结果中具有更好的识别效果。","汉语冒号分类,最大熵,篇章分析"
2014-02-24,基于轻语义&lambda;-演算的汉语陈述句灵活语序研究,"刘冬宁,邓春国,滕少华,张巍,梁路","目前,自然语言处理已经从句法、语法层面走向轻语义层面。对于汉语陈述句的处理,传统的方法是采用Lambek演算来进行处理。但是传统的Lambek演算无法处理汉语中的灵活语序问题,而现有的方法,如加入模态词、新连接词等,又因为其进一步使得本已是NP-hard的Lambek演算时间复杂度变大,并不适合当前的计算机处理。基于此,该文提出了&lambda;-Lambek演算,即采用Lambek演算来对汉语陈述句进行句法演算,并通过Curry-Howard对应理论与&lambda;-演算来对汉语陈述句进行轻语义模型的构建。&lambda;-Lambek演算不仅能够对汉语陈述句进行轻语义演算,而且还能对汉语陈述句灵活语序进行处理。","Lambek演算,&lambda;-演算,中文陈述句,灵活语序,语义"
2014-03-07,汉语析句的形式化问题,"彭炜明,宋继华,王宁","该文讨论了形式化析句的基本概念,从语言和言语、描写和解释、层次和线性、短语和句式、词法和句法等多个语言学视角梳理了汉语析句中的形式化问题,并介绍了在句本位语法图解析句形式化中总结的若干经验、原则和待解决问题。",
2014-02-26,一种基于维基百科的中文词语相关度学习算法,"黄岚,杜友福","词语相关程度计算是语义计算的基础。维基百科是目前最大、更新最快的在线开放式百科全书,涵盖概念广,概念解释详细,蕴含了大量概念间关联关系,为语义计算提供了丰富的背景知识。然而,中文维基百科中存在严重的数据稀疏问题,降低了中文词语相关度计算方法的有效性。针对这一问题,该文利用机器学习技术,提出一种新的基于多种维基资源的词语相关度学习算法。在三个标准数据集上的实验结果验证了新算法的有效性,在已知最好结果的基础上提升了20%—40%。","词语相关度,维基百科,中文信息处理,回归,链接结构"
2014-05-15,基于子词的历史典籍术语对齐方法,"车超,郑晓军","由于历史典籍术语存在普遍的多义性且缺少古汉语分词算法,使用基于双语平行语料的对齐方法来自动获取典籍术语翻译对困难重重。针对上述问题,该文提出一种基于子词的最大熵模型来进行典籍术语对齐。该方法结合两种统计信息抽取频繁在一起出现的字作为子词,使用子词对典籍进行分词,解决了缺少古汉语分词算法的问题。针对典籍术语的多义性,根据典籍术语的音译模式制定音译特征函数,并结合其他特征使用最大熵模型来确定术语的翻译。在《史记》双语平行语料上的实验表明,使用子词的方法远远优于未使用子词的方法,而结合三种特征的最大熵模型能有效的提高术语对齐的准确率。","子词,术语对齐,最大熵模型,音译特征"
2014-01-09,基于熵模型的英汉人名对齐,"刘颖,曹项","该文使用熵模型来对中英文双语语料进行人名对齐。熵模型综合利用双语人名词典、双语姓氏词典、词汇对齐概率、中英文人名的共现特征、基于最小编辑距离的音译相似度和基于语音匹配的音译相似度。实验结果表明,基于熵模型的中英文人名对齐在大规模语料库的实验中达到了较好的人名对齐正确率和召回率。我们分析了人名对齐存在的主要错误,并针对主要错误给出了可能的解决方案。","人名对齐,熵模型,音译相似度,最小编辑距离,词典"
2014-08-25,拉丁化维吾尔文字特征及其基于规则的正规化,"赛牙热·依马木,于斯音·于苏普,阿不都萨拉木·达吾提","结合网络上流通的拉丁化维吾尔文字特征,以拉丁化维吾尔文单词作为研究单位,首先,通过大规模文本语料库建立了固定词库、词首字母序列库、词尾字母序列库以及特殊词库等正规化规则库。然后,利用维吾尔单词中的字母序列结构特征和相邻字母上下文信息进行了拉丁化维吾尔文的正规化,同时引用最小编辑距离的方法进一步提高了正规化正确率,并用Visual C# 编程工具实现了基于规则的拉丁化维吾尔文的正规化算法。最后,给出了实验结果,并分析了结果不佳的原因及相应的对策。","维吾尔语,拉丁化维吾尔文,正规化,规则库,最小编辑距离,文字转写"
2014-05-30,哈萨克语IT领域术语识别研究与实现,"木合亚提·尼亚孜别克,古力沙吾利·塔里甫","该文阐述了基于统计方法进行哈萨克语IT领域术语识别的研究,并在已有的训练语料基础之上,采用最大熵模型进行标注识别和结合人工方式对错误识别结果进行后处理的分析实验,阐述了该平台的研究和设计思路,系统的总体框架、基本结构、功能模块以及实现方法等相关的问题。实验结果显示该方法识别哈萨克语IT领域术语是有效的,封闭测试结果达到了82.6%。","哈萨克语,IT术语,术语管理平台,最大熵模型"
2014-05-20,基于小字符集藏文拉丁转写系统的设计与实现,"陈小莹,艾金勇","随着藏语语言信息技术的迅速发展,藏文拉丁转写成为迫切需要解决的重要课题之一。该文在前人有关藏文拉丁转写研究的基础上,设计并实现了基于小字符集方案的藏文拉丁转写系统。文章通过对小字符集编码方案的特征分析,同时根据藏文正字法知识,提出了基于小字符集编码的藏文拉丁转写算法,并对具体算法策略进行了分析和说明,最后在Windows平台进行了程序的实现。藏文拉丁转写方案的设计与实现,可以解决藏文多编码系统之间的兼容性问题。","藏文,拉丁转写,小字符集,占位辅音"
2014-04-21,说话人自适应技术在维吾尔语语音识别中的应用研究,"努尔麦麦提·尤鲁瓦斯,张力文,吾守尔·斯拉木","该文针对维吾尔语说话人之间的发音差异会在一定程度上影响维吾尔语语音识别系统的性能这一情况研究了说话人自适应技术,将目前较为常用的MLLR和MAP以及MLLR和MAP相结合的自适应方法应用于维吾尔语连续语音识别的声学模型训练中,并用这三种方法自适应后的声学模型分别在测试集上进行识别实验。实验结果表明MLLR、MAP以及MAP+MLLR自适应方法使基线识别系统的单词错误识别率分别降低了0.6%、2.34%和2.57%。","维吾尔语,语音识别,说话人自适应,MLLR,MAP"
2013-11-28,关于朝鲜文信息技术标准化,"玄龙云,崔荣一","信息技术标准是我国普及应用信息技术、弘扬民族文化、取得市场主动性的关键。该文分析了朝鲜语信息技术标准化国内外现状,论述了朝鲜语信息处理的必要性,并提出信息技术标准化工作的具体建议。该文认为统一的中国少数民族文字信息技术基础标准体系亟待完善,朝鲜语的信息技术标准化对我国朝鲜族文化的传承与发展具有深远意义,对形成系统、完整的中华语言文字信息处理统一平台、扩大国际影响、维护国家统一而言是不可或缺的工作。",
2014-01-08,基于条件随机场的维吾尔文组块分析,"艾山·吾买尔,吐尔根·依布拉音,卡哈尔江·阿比的热西提,早克热·卡德尔,买合木提·买买提,亚森·艾则孜","该文对维吾尔语树库标注体系进行分析,根据组块划分原则,在短语标记集的基础上制定了维吾尔语组块标记集,从已完成标注的3 000句语料库构建组块库。根据维文语言的特点,在英汉组块识别特征基础上,增加了词干、词缀、同义词标记等特征。该文中的性能评价指标采用了国际通用的准确率,召回率和F值,3 000个标注句子作为训练和测试语料库用,实验采用了交叉验证法,训练和测试语料库的比例分别为9∶1,8∶2,2∶1,召回率分别为80.34%,76.87%,66.76%。实验表明,语料库规模对模型性能影响较大。","条件随机场,维吾尔,组块分析"
2014-07-22,基于音节划分及短语表优化的英汉人名音译研究,"王丹丹,黄德根,高扬","把英汉人名音译问题转换为以音节为基本单位的翻译问题,将连续的音节组合看作短语,引入一种基于短语的统计机器翻译方法,实现英汉人名的音译。首先,针对现有音节划分方法存在的问题,提出一种改进的音节划分方法;其次,该文提出去除低频词法及基于C-value方法对短语表进行优化,解决了训练语料偏小导致短语表中出现杂质信息的问题;之后,融入了汉语人名中首字(词)及尾字(词)的位置特征,改善了生成的音译候选中汉字选取的不合理性;最后,提出了两阶段音节划分方法,缓解了音节划分粒度过大导致的音译错误。与基准方法相比,其音译准确率ACC由63.78%提高到67.56%。","英汉人名音译,音节划分,短语表优化,C-value"
2014-04-07,基于神经网络的统计机器翻译的预调序模型,"杨南,李沐","长距离调序是统计机器翻译的一个主要挑战。之前的研究工作表明预调序是解决这个问题的一个可能的途径。在该工作中,我们沿着预调序这个研究方向,将神经网络建模结合到线性排序的框架之下,提出了一个基于神经网络的预调序模型。这个的预调序模型能够利用从海量未标注数据中抽取的句法和语意信息,从而更好的对不同语言之间的语序差异进行预测。我们在中文到英文以及日文到英文的机器翻译任务上进行了实验,实验结果表明了该方法的有效性。","统计机器翻译,预调序,神经网络 "
2014-03-20,中文专利文献术语自动识别研究,"杨双龙,吕学强,李卓,徐丽萍","中文专利文献中含有大量领域术语,对这些术语进行自动识别是信息抽取、文本挖掘等领域的重要任务。该文提出了基于专利文献标题的术语词性规则自动生成方法以及针对候选术语排序的TermRank算法。该方法首先从大量的中文专利文献标题中自动生成词性规则;然后利用生成的词性规则对中文专利文献正文部分进行规则匹配获得候选术语表;再利用提出的TermRank排序算法对候选术语表排序,最终得到术语列表。通过在9 725篇中文专利文献数据上实验,证实了该方法的有效性。","术语自动识别,专利文献,信息抽取,文本挖掘"
2014-02-04,中医针灸领域术语自动抽取研究,"孙水华,黄德根,牛萍","针对中医针灸领域术语的构成特点,该文建立了一种基于规则的领域术语抽取算法模型,该模型首先对中医针灸领域术语种子集进行有限次的迭代,生成中医针灸领域术语构件集;然后,以术语构件集为领域词典,采用最大向前匹配算法对中文针灸医学文献中的句子进行切分,并抽取候选术语;最后,利用语言规则对候选术语进行过滤处理,筛选出中医针灸领域专业术语。分别以关键字集和中医词典为种子集进行实验,开式测试的F值分别达到76.96%和35.59%。","中医针灸领域术语,术语种子集迭代算法,术语过滤规则"
2014-02-08,一种交互式事件常识知识的获取方法,"曹聪,曹存根,臧良军,王石","赋予机器常识知识是使机器具有真正智能的必备条件之一,而获得这些常识一直是人工智能研究的一个重要课题。该文提出了一种通过交互的方式来引导知识贡献者给出关于事件的常识知识的方法。方法获取过程是一个机器与贡献者的交互过程: 机器动态地生成问题,对知识贡献者进行提问;知识贡献者通过回答问题给出常识知识。交互过程通过包含提示信息的提问问题对知识贡献者进行提示,运用七种类型问题层层递进地引导知识贡献者思考,以此唤醒他们大脑中的常识知识;通过动态变化的问题改善知识贡献者贡献常识知识过程的趣味性。同时,该文还引入可接受性和有效性两个定量标准评价提问问题,用于进一步改善交互过程。实验结果表明,知识贡献者运用此方法给出的知识量增加了451.61%,同时知识的正确率也达到了92.5%。","常识知识获取,事件常识,交互过程"
2014-02-24,中文微博故事线生成方法,"李培,翁伟,林琛","新浪微博、腾讯微博等微博平台已经成为国内重要的网络媒体。随着海量的实时信息在微博上分享和传播,为每个用户提供更多方便,展现一目了然的实事资讯的任务已经迫在眉睫。这就需要在微博中理出重大事件的发展进程。该文中,我们将利用最小权重支配集和有向斯坦纳树在给定查询的微博数据集上生成故事线。该文的工作由三部分组成:第一部分是在Lucene检索出来的结果集上构建多视点图;其次,通过在图中寻找最小权重支配集来选出具有代表性的微博;最后,通过求解有向斯坦纳树问题来平滑地连接这些已挑选的微博,形成故事线。在实际数据集上的实验验证了该文提出系统的高效性和有效性。","微博故事线,最小权重支配集,有向斯坦纳树"
2014-01-17,基于三维坐标的模糊量化情感分类方法,"林明明,邱云飞,邵良杉","针对微博情感分类问题,构造了基于三维坐标的模糊量化情感分类算法,通过将情感模糊量化,对微博进行多情感分类。首先对情感模糊处理,将情感分为六大类,根据六大类,定义并计算句子的模糊情感;其次将情感量化处理,根据情感类别构造三维坐标模型,将模糊情感值作为句子的坐标,通过坐标将句子映射到三维坐标模型中,使其量化;最后通过模糊量化处理后,根据与坐标轴的夹角判断句子最终的情感分类。通过实验,对三个作者的微博进行模糊量化处理,对其情感分类,实验结果的F值达到85%以上,同时与三种经典算法进行对比实验,准确率有了明显的提高。","微博情感,模糊量化,情感分类,模糊情感,三维坐标"
2014-02-23,基于短文本隐含语义特征的文本蕴涵识别,"张晗,盛雅琦,吕晨,姬东鸿","该文采用基于短文本隐含空间语义特征改进文本蕴涵识别,该方法通过构造句子的隐含变量模型,并融合基于该模型的句子之间相似度特征,和词汇重叠度、N元语法重叠度、余弦相似度等字符串特征,以及带标记和未标记的子树重叠度句法特征一起利用SVM进行分类。基于该分类算法,我们对RTE-8任务进行了测试,实验表明短文本的隐含语义特征可有效改进文本蕴涵关系识别。","文本蕴涵,隐含语义特征,短文本,支持向量机"
2014-09-11,一种长短期兴趣结合的个性化检索模型,"王晓春,李生,杨沐昀,赵铁军","个性化信息检索针对用户个人兴趣优化文档排序,被认为是改善用户检索体验的一种有效途径。为提高个性化检索模型的检索性能,该文提出了一种将用户的长短期兴趣结合的通用方法,利用用户长期兴趣和短期兴趣对查询模型进行改进。大规模真实搜索日志数据上的实验结果显示,利用长短期兴趣能够获得准确表达信息需求的查询模型,相对于传统的个性化检索模型取得了更好的效果。","个性化信息检索,长期兴趣,短期兴趣"
2014-07-28,一种基于事实知识的实体相关度计算方法,"孙叔琦,孙珂,赵世奇,李生,王海峰,杨沐昀","在近来出现的面向实体的搜索服务中,准确地预测实体间的相关程度是至关重要的。该文提出了一种基于实体的事实知识,即利用 “实体—属性—属性值”(SPO)记录进行实体相关度计算的方法。该文通过基于属性和属性值的两步概率估计,将实体表示为一个属性值词的概率分布列,并通过比对两个实体共享的属性值词汇得出二者的相关度。实验表明,在用于面向实体搜索的相关实体排序问题上,该文方法达到了80.9%的平均top-5准确率,优于词袋方法和基于查询日志共现的方法。此外,该文通过定量分析,考察了不同领域的用户需求特性对实体相关度计算结果的影响。","实体相关度,实体—属性—属性值(SPO)记录,用户需求,面向实体的搜索,"
2016,一种基于用户互动话题的微博推荐算法,"鲁骁,李鹏,王斌,李应博,房婧",随着社交网络的发展，微博逐渐成为人们获取信息的重要来源。然而随着用户的增多，微博中的信息过载问题也越来越严重，如何快速准确地为用户推荐感兴趣的微博已经成为研究的热点。与传统的推荐技术不同，微博中的用户具有天然的社交关系，这为推荐算法提供了额外的用户信息，因此，融合了用户社交关系的社会化推荐方法日益受到重视。但是，现有的方法大多只利用了固定的用户社交关系或简单的互动行为，事实上，用户互动行为的出发点必然是用户与好友的共同兴趣，具有明显的话题相关性。该文从话题层面来分析用户的互动关系，提出了度量互动关系在话题上强弱度的方法，通过有效地融合互动关系的话题特征，最终提出了改进的微博推荐模型IBCF。实验结果表明，与现有的社会化推荐方法相比，该文提出的新方法在MAP和NDCG等指标上取得了更好的推荐效果，而且为推荐结果提供了更明确的可解释性。,"互动关系,互动话题,社会化推荐,协同过滤,微博推荐"
2014-01-20,汉语谓词组合范畴语法词库的自动构建研究,周强,"谓词词库是深层语法模型分析和理解的核心资源。近年来的常规方法是人工构建或从标注语料库中自动获取,标注规模和信息容量的扩大受制于巨大的人工投入量和标注库体系设计。该文提出了一种多资源融合自动构建汉语谓词组合范畴语法(CCG)词库的新方法。从知网、北大语法信息词典和大规模事件句式实例中提取汉语谓词的不同句法语义分布特征,融合形成CCG原型范畴表示,将它们指派给各资源信息完全重合的谓词形成核心词库。然后通过自动分类和隶属度分析相结合方法对其他谓词的CCG范畴进行预测,并对两者结果进行融合得到扩展词库,最终合并形成包含约15,000个词条的汉语谓词CCG词库。通过在随机均匀抽样的1000个谓词上通过多人独立标注形成的标准测试库上进行不同角度的性能分析实验,表明该词库的预期准确率达到了96.3%。","组合范畴语法,汉语谓词词库,多资源融合"
2014-06-04,基于数据场和全局序列比对的大规模中文关联数据模型,"王汀,徐天晟,冀付军","目前关联数据的研究工作主要集中在实例级别上展开,而在模式级别(Schema-Level)上的关联数据构建则易被忽视。本体映射是解决本体异构问题的重要途径和手段,同时,本体映射也可视为模式级别关联数据构建的典型情景。特别是在中文知识库方面,中文知识是关联数据网中的重要组成部分,但现有的中文本体映射系统在面对大规模本体映射任务时,显得效率较低且可用性不高,目前仍缺乏针对中文大规模本体映射的相关系统。为了解决在模式级别上的中文大规模关联数据构建问题,提出了一种新的基于数据场和序列比对思想的大规模中文关联数据构建模型。首先,基于改进的融合概念相似度和相异度的拟核力场势函数对大规模中文本体映射规模进行约简和压缩;其次,通过引入序列比对算法,对组合概念进行相似度的度量;最后,将本系统与相似度计算相关典型算法进行比较,表明其具备一定的可用性和较高的总体性能。","语义网,关联数据,本体映射,同义词词林,相似度计算"
2014-05-09,中文文本的事件时空信息标注,"张春菊,张雪英,王曙,廖建平,陈晓丹","基于文本数据源的地理空间信息解析研究侧重于地名实体、空间关系等空间语义角色的标注和抽取,忽略了丰富的时间信息、主题事件信息及其时空一体化信息。该文通过分析中文文本中事件信息描述的语言特点和事件的时空语义特征,基于地名实体和空间关系标注研究成果,制定了中文文本的事件时空信息标注体系和标注模式,并以GATE(General Architecture for Text Engineering)为标注平台,以网页文本为数据源,构建了事件时空信息标注语料库。研究成果为中文文本中地理信息的语义解析提供标准化的训练和测试数据。","中文文本,时空信息,事件,标注体系,标注语料库"
2014-09-25,篇章关系分析研究综述,"严为绒,徐 扬,朱珊珊,洪 宇,姚建民,朱巧明","篇章关系研究,旨在推断同一篇章内相邻或跨度在一定范围内的文本片段之间的语义连接关系。语义连接关系对篇章内容理解和结构分析都具有重要作用,成为目前篇章分析领域的重点研究内容。该文针对三个中英文篇章关系研究领域的语料库: 基于修辞结构理论的篇章树库(Rhetorical Structure Theory Discourse Treebank,RSTDT)、宾州篇章树库(Penn Discourse Treebank,PDTB)和哈尔滨工业大学中文篇章关系语料库(HIT Chinese Discourse Treebank,HIT-CDTB),主要介绍篇章关系分析理论的语料资源与研究背景、标注与评测体系以及国内外研究现状。此外,总结相关工作,指出目前篇章关系,尤其是隐式篇章关系研究的主要难题。","篇章关系,篇章修辞结构,RSTDT,PDTB,CDTB"
2014-09-05,汉语“比”字句关键要素的常规序列模式探索,"朴敏浚,李 强,袁毓林","表达“差比”义的“比”字句,是比较句的主要句型,也是比较句关键要素抽取研究中不可回避的主要课题。该句型的关键要素(SUB、BI、OBJ、ITM、DIM、RES、EXT)在语义上互相交织,在表层句法上可以实现为多种多样的序列模式。该文面向中文“比”字句关键要素抽取这个目标,对于表示“差比”义的460多个“比”字句文本进行了七种关键要素的标注。在此基础上,利用Apriori和PrefixSpan算法找出这些要素的关联规则及其序列模式,并归纳出六种“比”字句关键要素的分布规律。此外,该文还进一步说明了产生这六种模式规则的动因,为“比”字句特征选取和处理提供了重要的语言学理论依据。","“比”字句,关键要素,关联规则,序列模式,分布规律"
2014-08-05,基于马尔科夫逻辑网的中文专利最大名词短语识别,"蔡东风,赵奇猛,饶 齐,王裴岩","缺少标注语料和难以识别动词和名词类是阻碍中文专利最大名词短语识别的主要问题。针对上述问题,该文提出了一种基于马尔科夫逻辑网的中文最大名词短语识别方法。该方法避免对开放类的名词短语的识别,而将主要精力放在了相对封闭的分隔符的识别上,利用句子自身特征、领域迁移特征以及双语对齐特征来识别最大名词短语的边界。结果说明,双语信息较好地促进了动词、介词、连词等MNP边界的识别。MNP识别的F值可达83.27%。","最大名词短语,马尔科夫逻辑网,中文专利"
2014-08-20,专利中基于语义角色的术语相似度计算方法,"姜利雪,季 铎,蔡东风","术语是由一个到多个单词按照某种语义角色组合而成的,传统的基于统计的相似度计算方法,将术语看作一个基本单元来进行计算,忽略了术语内部的语义角色,且对于上下文信息不丰富的术语,无法利用统计的方法取得理想的效果;基于语义资源的相似度计算方法,所涵盖的词语有限,因此不包含在语义资源中的术语便无法计算相似度。针对这些问题,该文针对专利提出了基于语义角色的术语相似度计算方法,该方法弥补了传统方法的不足。该文对术语内部的单词进行语义角色标注,通过共享最近邻方法计算单词的相似度,然后根据不同的语义角色,利用单词相似度来计算术语相似度。实验表明,该方法与传统方法相比,取得了较好的效果。","术语,内部语义角色,共享最近邻,术语相似度,专利文本"
2015-08-09,一种基于主动学习的框架元素标注,"屠寒非,李 茹,王智强,周铁峰","框架元素标注是中文FrameNet众多任务中亟待解决的一个问题,目前仍主要采用有监督的机器学习方法,即依赖大规模人工标注的例句作为训练语料。但例句标注又是一件费时费力的工作,所以为了降低人工标注的代价,该文将主动学习应用到框架元素标注中,优先选择训练模型预测最不准的例句交由人工标注。该文以条件随机场为标注模型,并提出了进行样本选择时所依赖的准则。实验表明,一方面,与随机选择样本进行标注相比,当使用相同数量的例句训练模型时,主动学习使框架元素标注的性能最高提升4.83%;另一方面,主动学习使框架元素标注达到同等F值时只需更少的标注例句,人工标注量最高可减少30%。","主动学习,框架元素标注,条件随机场,不确定性度量"
2014-03-08,基于复杂网络理论的汉语复句关系词搭配网的统计特征研究,"胡 泉,谢 芳,李 源,刘延申","汉语复句关系词是汉语复句在语表形式上的标记,是复句中标识关系的重要构件,在现代汉语复句研究领域起着关键作用。汉语复句关系词的搭配是指在汉语语篇中两个或两个以上的复句关系词形成的句法共现形式,它不仅影响着分句的语义,而且影响着复句层次关系的划分。该文利用复杂网络的理论,基于已获取的470个复句关系词构建了一个“现代汉语复句关系词搭配网络”。通过对该网络中的平均路径长度、聚集系数和度分布等特征的统计,用来发现汉语复句关系词之间的搭配能力和搭配强度,这些结果能够帮助复句层次关系和复句逻辑语义的自动识别。","汉语复句关系词搭配,复杂网络,平均路径长度,聚集系数,度分布"
2014-01-04,基于WordNet的中泰文跨语言文本相似度计算,"石 杰,周兰江,线岩团,余正涛","文本相似度在信息检索、文本挖掘、抄袭检测等领域有着广泛的应用。目前,大多数研究都只是针对同一种语言的文本相似度计算,关于跨语言文本相似度计算的研究则很少,不同语言之间的差异使得跨语言文本相似度计算很困难,针对这种情况,该文提出一种基于WordNet的中泰文跨语言文本相似度的计算方法。首先对中泰文本进行预处理和特征选择,然后利用语义词典WordNet将中泰文本转换成中间层语言,最后在中间层上计算中泰文本的相似度。实验结果表明,该方法准确率达到82%。","WordNet,中间层语言,跨语言文本相似度"
2014-07-08,一种基于复杂网络的短文本语义相似度计算,"詹志建,杨小平","将传统的文本相似度量方法直接移植到短文本时,由于短文本内容简短的特性会导致数据稀疏而造成计算结果出现偏差。该文通过使用复杂网络表征短文本,提出了一种新的短文本相似度量方法。该方法首先对短文本进行预处理,然后对短文本建立复杂网络模型,计算短文本词语的复杂网络特征值,再借助外部工具计算短文本词语之间的语义相似度,然后结合短文本语义相似度定义计算短文本之间的相似度。最后在基准数据集上进行聚类实验,验证本文提出的短文本相似度计算方法在基于F-度量值标准上,优于传统的TF-IDF方法和另一种基于词项语义相似度的计算方法。","复杂网络,综合特征值,短文本,语义相似度"
2014-05-05,基于PDTB体系的隐式篇章关系识别,"李 生,孔 芳,周国栋","识别隐式篇章关系是篇章分析领域中非常有挑战的一个任务。该文基于PDTB语料提出一个隐式篇章分析识别方法,使用传统的特征如动词,极性和句法推导规则等,系统分析了它们对隐式篇章分析的影响。我们利用全部标注数据构建多个分类器并使用加法规则融合分类结果,此外还通过前向特征选择算法确定各分类任务最优的特征集。实验结果表明该方法能显著提升隐式篇章分析的性能。","篇章处理,隐式篇章关系,宾州篇章树库"
2014-06-25,基于中心理论和话语结构的交互式问答文本指代消解,"李 映,孔 芳","与传统新闻文本相比,交互式问答中蕴含着更为丰富的语言现象。在传统的针对新闻文本的指代消解方案的基础上,融入了交互式问答特有的特征集,给出了一个适于交互式问答文本的指代消解方案。具体而言,基于浅层语义角色分析的结果进行话语结构的识别,根据识别出的话语结构进行话语中心及中心跳转的识别。将获取到的话语中心及跳转信息组织成交互式文本特有的特征集,使用交互式问答领域广泛使用的TREC2004和TREC2007的评测语料进行指代消解的实验,结果表明给出的方案能大大提高交互式问答文本中指代消解的性能,系统F值提高了3.2%。","指代消解,交互式问答,中心理论,话语结构"
2014-06-09,藏文字形结构分布研究,"才智杰,才让卓玛","字是语言文字的基本组成单位,字形结构统计研究是自然语言处理的基础,为字属性分析、输入法设计、排序、语音合成和字符信息熵研究等提供理论依据。该文通过分析藏文字形结构的特征,对藏文字的字形结构分成独体字和合体字,合体字按其构件的结构位和所含构件数进行分类。设计了藏文字形结构统计系统模型和算法,从约含8 500万藏文字的450M语料中对藏文字形结构进行统计,建立了藏文字形结构分布统计表,并对统计结果进行了分析。","中文信息处理,字形结构,独体字,合体字,频度统计"
2014-02-17,基于拓扑特征的纳西东巴文象形文字输入方法研究,"王海燕,王红军,徐小力","纳西东巴文字是一种比甲骨文还要原始的图画象形文字,该文针对大量纳西经典古籍资料需要录入、整理、分析的需要,设计一种普通用户即可使用的基于拓扑特征的输入方法。首先针对纳西东巴象形文字的1 561个基本字形的五个拓扑特征-块数、孔数、端点数、三叉点数和四叉点数进行了统计和分析,然后基于Java程序结合TTF字库文件进行了测试,证明了该方法可行。统计结果表明,50%以上的纳西东巴象形文字通过这五个特征可以唯一识别,80%以上的东巴文字通过该方法识别时重复数不高于4,人工输入、识别的效率较高,为纳西东巴象形文字的输入方法提供一种新的思路。","纳西,东巴,象形文字,输入方法"
2014-03-20,基于层叠条件随机场的高棉语分词及词性标注方法,"潘华山,严 馨,周 枫,余正涛,郭剑毅","针对高棉语分词及词性标注问题,提出一种基于层叠条件随机场模型的自动分词及词性标注方法。该方法由三层条件随机场模型构成: 第一层是分词模型,该模型以字符簇为粒度,结合上下文信息与高棉语的构词特点构建特征模板,实现对高棉语句子的自动分词;第二层是分词结果修正模型,该模型以词语为粒度,结合上下文信息与高棉语中命名实体的构成特点构建特征模板,实现对第一层分词结果的修正;第三层是词性标注模型,该模型以词语为粒度,结合上下文信息与高棉语丰富的词缀信息构建特征模板,实现对高棉语句子中的词语进行自动标注词性。基于该模型进行开放测试实验,最终准确率为95.44%,结果表明该方法能有效解决高棉语的分词和词性标注问题。","高棉语,层叠条件随机场,分词,词性标注"
2014-09-01,机器译文自动评价中基于IHMM的近义词匹配方法研究,"李茂西,徐 凡,王明文","机器译文的自动评价推动着机器翻译技术的快速发展与应用,在其研究中的一个关键问题是如何自动的识别并匹配机器译文与人工参考译文之间的近义词。该文探索以源语言句子作为桥梁,利用间接隐马尔可夫模型(IHMM)来对齐机器译文与人工参考译文,匹配两者之间的近义词,提高自动评价方法与人工评价方法的相关性。在LDC2006T04语料和WMT 数据集上的实验结果表明,该方法与人工评价的系统级别相关性和句子级别相关性不仅一致的优于在机器翻译中广泛使用的BLEU、NIST和TER方法,而且优于使用词根信息和同义词典进行近义词匹配的METEOR方法。","机器译文自动评价, 近义词匹配, 间接隐马尔可夫模型, 单语句子词对齐, 相关性"
2014-10-09,基于多特征融合和图匹配的维汉句子对齐,"倪耀群,许洪波,程学旗","维吾尔语新闻网页与对应的中文翻译网页在内容上往往并非完全可比,主要表现为双语句子序列的错位甚至部分句子缺失,这给维汉句子对齐造成了困难。此外,作为新闻要素的人名地名很多是未登录词,这进一步增加了维汉句子对齐的难度。为了提高维汉词汇的匹配概率,作者自动提取中文人名、地名并翻译为维吾尔译名,构造双语名称映射表并加入维汉双语词典。然后用维文句中词典词对应的中文译词在中文句中进行串匹配,以避免中文分词错误,累计所有匹配词对得到双语句对的词汇互译率。最后融合数字、标点、长度特征计算双语句对的相似度。在所有双语句子相似度构成的矩阵上,使用图匹配算法寻找维汉平行句对,在900个句对上最高达到95.67%的维汉对齐准确率。","句子对齐,人名、地名翻译,多特征融合,二部图最佳匹配"
2014-08-14,基于词重要性的信息检索图模型,"王明文,洪 欢,江爱文,左家莉","在信息检索建模中,确定索引词项在文档中的重要性是一项重要内容。以词袋(bag-of-word)的形式表示文档来建立检索模型的方法中大多是基于词项独立性假设,用TF和IDF的函数来计算词项的重要性,并未考虑词项之间的关系。该文采用基于词项图(graph-of-word)的文档表示形式来捕获词项间的依赖关系,提出了一种新的基于词重要性的信息检索图模型TI-IDF。根据词项图得到文档中词项的共现矩阵和词项间的概率转移矩阵,通过马尔科夫链计算方法来确定词项在文档中的重要性(Term Importance, TI),并以此替代索引过程中传统的词项频率TF。该模型具有更好的鲁棒性,我们在国际公开数据集上与传统的检索模型进行了比较。实验结果表明,该文提出的模型都要优于BM25,且在大多数情况下优于BM25的扩展模型、TW-IDF等模型。","词项重要性,词项图,检索模型,TI-IDF"
2014-09-25,基于Tri-training与噪声过滤的弱监督关系抽取,"贾 真,冶忠林,尹红风,何大可","弱监督关系抽取利用已有关系实体对从文本集中自动获取训练数据,有效解决了训练数据不足的问题。针对弱监督训练数据存在噪声、特征不足和不平衡,导致关系抽取性能不高的问题,文中提出NF-Tri-training(Tri-training with Noise Filtering)弱监督关系抽取算法。它利用欠采样解决样本不平衡问题,基于Tri-training从未标注数据中迭代学习新的样本,提高分类器的泛化能力,采用数据编辑技术识别并移除初始训练数据和每次迭代产生的错标样本。在互动百科采集数据集上实验结果表明NF-Tri-training算法能够有效提升关系分类器的性能。","关系抽取, 弱监督学习, Tri-training, 数据编辑"
2014-09-25,基于CRFs和领域本体的中文微博评价对象抽取研究,"丁晟春,吴婧婵媛,李 霄","微博情感分析是对微博内容进行细粒度的挖掘,有着重要的研究价值。微博评价对象的抽取是微博情感分析研究的关键问题之一。为了提高中文微博评价对象抽取的准确率,该文在中文微博特征分析和微博评论本体构建研究的基础上,尝试从词、词性、情感词以及本体四个方面进行特征选择,采用CRFs模型对评价对象进行抽取。该文将提出的方法运用到COAE2014测评的Task5评价对象抽取任务中,宏平均准确率达到61.20%,在所有测评队伍中居第一。实验结果表明,将本体特征引入到CRFs模型中,能够有效地提高评价对象抽取的准确率。","CRFs模型,本体,特征选择,评价对象抽取,信息抽取"
2014-10-15,一种基于事件本体的文本事件要素提取方法,"刘 炜,刘菲京,王 东,刘宗田","在事件信息的抽取中,事件要素的提取是一个难点。现有的事件要素抽取主要是基于机器学习的方法,这类方法容易受到语料稀疏性的影响。该文提出一种基于事件本体的事件要素提取方法,该方法将事件要素推理分为两步: 一、通过事件要素词和事件指示词的位置关系来初步填充要素值,并将得出的置信度较高的事件作为种子事件;二、利用第一步得出的种子事件,查询事件本体中的事件类约束和基于事件非分类关系的推理规则,并对要素进行推理,进一步对事件要素进行填充和修正。实验结果表明,该方法能较好地提升事件要素提取的准确度。","事件本体,事件要素,事件要素推理"
2014-09-15,融合显性和隐性特征的中文微博情感分析,"陈铁明,缪茹一,王小号","微博情感分析是研究社交网络舆情的一项关键技术。微博表情符号和情感词汇等是一类直观显性的情感特征,而微博的内容语义则可视为隐性特征,且对情感判定往往具有决定性作用,因此本文提出将两类特征因素融合的微博情感分析方法。首先构建情感分析词典、网络用语词典以及表情符号库,定义微博频繁特征词集,再根据频繁特征词集,利用最大频繁项集获得微博初始情感簇;针对初始簇间存在文本重叠情况,提出基于短文本扩展语义隶属度的簇间重叠消减算法,获得完全分离的初始簇;最后根据簇语义相似度矩阵,给出一种凝聚式情感聚类方法。利用NLPCC2013 评测所提供的训练语料进行情感分类实验,说明了分析该文方法的性能优势,并以2014年3月8日马航事件微博数据为例,给出了利用微博情感分析公众随事态发展的情感变化,说明了该文方法的实用效果。","表情符号,情感词典,语义,频繁项集,聚类"
2014-09-15,中文微博情感词提取: N-Gram为特征的分类方法,"刘德喜,聂建云,张 晶,刘晓华,万常选,廖国琼","情感词典是文本情感分析的基础资源,但采用手工方式构建工作量大,且覆盖有限。一种可行的途径是从新情感词传播的重要媒介-微博数据-中自动抽取情感词。该文以COAE 2014评测任务3提供的中文微博数据为统计对象,发现传统的基于共现的方法,如点互信息等,对中文微博数据中的新情感词发现是无效的。为此,设计一组基于上下文词汇的分类特征,即N-Gram特征,以刻画情感词的用词环境和用词模式,并以已知情感词为训练数据训练分类器,对候选情感词进行分类。实验结果表明,该方法较传统基于共现的方法要好。实验还发现,与英语不同的是,中文情感词通常会以名词词性出现,而基于共现的方法无法有效地区分该类情感词,这是造成其失效的主要原因,而该文提出的分类特征能解决这一问题。","情感词提取,中文微博,分类方法,N-Gram特征"
2014-09-10,基于隐马尔可夫模型的主观句识别,"刘培玉,荀 静,费绍栋,朱振方","文本情感倾向分析是意见挖掘和情感文摘中的一个重要环节,而在情感倾向分析中涉及到的是主观性文本,这就需要进行主客观文本分类。当前的主客观文本分类方法主要是基于特征词典的概率统计方法,并没有考虑特征之间的语法与语义关系。针对该问题,该文提出一种基于隐马尔可夫模型(HMM)的主观句识别方法。该方法首先从训练语料中抽取具有明显分类效果的七类主客观特征,然后每个句子应用HMM进行特征角色类别标注,并依据标注的结果计算句子的权重,最终识别主观句。该方法在第六届中文倾向性分析评测任务中能够有效地识别主观句。","隐马尔可夫模型,特征标注,主观句识别"
2014-09-10,在线游戏用户的流失预测： 基于不平衡数据的采样方法比较和分析,"吴悦昕,赵 鑫,过岩巍,闫宏飞","流失用户预测问题在很多领域都是研究重点。目前主流的流失用户预测方法是使用分类法,即把用户是否会流失看作一个二分类问题来处理。该文提出了一个基于二分类问题解决的在线游戏流失用户预测方法。此方法除了总结了一些对在线游戏而言比较重要的可以用于流失预测的特征之外,也考虑到流失用户相对稀少的问题,在流失用户预测问题中引入了不平衡数据分类的思想。该文主要在流失预测中结合使用了基于采样法的不平衡数据处理策略,并对现有主要的几种采样算法进行了对比实验和分析。","在线游戏,流失预测,不平衡数据,采样法"
2015-06-03,词汇语义表示研究综述,"袁书寒,向 阳","构建能够表达语义特征的词语表示形式是自然语言处理的关键问题。该文首先介绍了基于分布假设和基于预测模型的词汇语义表示方法,并给出目前词表示方法的评价指标;进而介绍了基于词汇表示所蕴含的语义信息而产生的新应用;最后,对词汇语义表示研究的方法和目前面临的问题进行了分析和展望。","词汇表示,语义,分布假设,深度学习"
2015-05-04,在线社交网络中的新兴话题检测技术综述,"笱程成,杜 攀,刘 悦,程学旗","新兴话题检测是社交网络研究的热点问题之一。在线社交网络特别是微博的开放性,给话题的流行和爆发提供了前所未有的便利条件。新兴话题是即将流行或爆发的话题,往往伴随着重大的事件或新闻的发生,会产生重大的社会影响,如何在早期识别此类话题,是新兴话题检测研究的主要内容。该文回顾了近年来在新兴话题检测方面的主要进展,分析了新兴话题检测领域面临的挑战,阐述了相关的概念、方法和理论,重点从内容突发特征和信息传播模型两个方面对影响新兴话题检测的方法进行了分析和讨论,并对新兴话题检测的前景做了展望。","新兴话题,话题检测,信息传播,社交网络"
2015-05-04,情感词典构建综述,"梅莉莉,黄河燕,周新宇,毛先领","文本情感分析是近年来迅速兴起的一个研究课题,具有显著的研究价值和应用价值。情感词典的构建在情感分析任务中发挥着越来越重要的影响力。该文对情感词典构建的研究进展进行了总结。首先重点介绍了情感词典构建的研究现状,将其归纳为四种方法,即基于启发式规则的方法、基于图的方法、基于词对齐模型的方法以及基于表示学习的方法,并对每种方法进行介绍和分析;然后对一些常见的语料库、词典资源以及评测组织进行介绍;最后,对情感词典的构建进行了总结,并对发展趋势进行了展望。","情感分析,情感词典,评测,语料,综述"
2015-08-19,面向短文本情感分类的特征拓扑聚合模型,"胡 杨,冯旭鹏,黄青松,付晓东,刘 骊,刘利军","由于短文本极稀疏性和特征分散的特点,短文本的情感分类效果总是不及篇章文本的情感分类,针对此问题,该文提出面向短文本情感分类的特征拓扑聚合模型。模型首先从特征点互信息,情感指向相似度,主题归属差异值三个维度整合计算情感特征的关联度,然后根据特征关联度建立拓扑聚合图模型,通过在图上求解强联通分量聚合高关联度情感特征,从大量未标注语料中提取相似特征对训练集特征进行补充,同时降低训练空间维度。实验将模型应用于短文本情感分类,与基准算法对比能提高分类准确率和召回率分别达0.03和0.027。验证了模型在缓解短文本极稀疏性和特征分散问题上的效果。","短文本,情感分类,特征关联度,强联通分量,拓扑聚合"
2015-05-20,基于统计的新浪微博动态传播规律研究,"王 怡,梁 循,周小平","社交网络是一个庞大的新型复杂系统,用户和信息常用作研究网络静态结构和动态传播过程的典型对象,它们的结构特点和传播规律处处体现出社会网络复杂的特点。该文利用新浪微博约三万名用户及其发信息的数据,从上述两方面进行了研究。首先基于统计,本文发现了新浪微博网络的紧密程度较弱,并实证了关注网络的关联密度是线性的。其次,通过研究单条微博的传播过程的用户影响曲线,我们发现10%的用户能影响其他的90%。第三,该文从时间和转发结构两方面对微博的传播模型进行了归纳。相关的结论能够为后续模型建立、舆情监控等提供支持。","新浪微博,线性关联密度,关键节点,传播模式"
2015-03-09,基于地域特征和异构社交关系的事件推荐算法研究,"乔 治,周 川,纪现才,曹亚男,郭 莉","近几年,在基于事件的社交网络(EBSNs)服务中,为便于增强用户体验,事件推荐任务一直被广泛研究。本文基于对EBSN中用户行为数据的详细分析,提出了一种新型的融合多种数据特征的潜在因子模型。该模型综合考虑EBSN中两种新型的数据特征: 异构的社交关系特征(线上社交关系+线下社交关系)和用户参与行为的地域性特征。基于真实的Meetup数据集,实验结果表明我们的算法在解决事件推荐问题时比传统的算法有更好的性能。","事件推荐,基于事件的社交网络,用户行为倾向,协从过滤,地域特征,异构社交关系"
2015-09-07,预测信息传播中的转发选择,"王永庆,沈华伟,程学旗","在信息传播中,用户在重复接收同一信息的情况下其转发行为会具有一定的倾向性。对这种转发的倾向性建模是影响力分析、传播动力学、社会推荐等一系列信息传播相关应用研究领域中的一个关键问题。本文假设用户的转发选择行为主要由用户间的人际影响力决定。人际影响力的大小由信息传播者的影响力和信息接收者的易感性共同作用。本文从真实的信息传播记录中推断出用户隐式的影响力和易感性,进而提出了一种转发选择模型。该模型能够有效解决目前方法存在的对转发选择行为建模不充分和模型泛化能力差的问题。本文选取典型的转发选择建模方法作为比较,将所提的转发选择模型在新浪微博数据上进行对比验证。实验表明,本文所提的模型在两种评价指标上均取得更好效果,证明了所提模型的有效性。","信息传播,转发选择,影响力,易感性"
2014-09-07,基于概率交易模型的线下百货推荐,"王鹏飞,郭嘉丰,兰艳艳,晏小辉,程学旗","该文提出了一种新颖的概率交易模型PTM,针对线下百货进行个性化的推荐。传统的推荐模型,如K-近邻算法、矩阵分解等,或者仅利用局部的数据,使得模型面临线下数据极大的稀疏性挑战,或者忽略百货数据中的交易维度,使得模型损失了同一交易中多商品共现的强相关信息,最终导致它们在面对线下百货推荐问题时性能低下。针对以上的问题,本模型从交易的维度出发,建模交易记录中的共现模式,并利用全局的交易数据来学习商品的相关分量,在此基础上推断出用户的兴趣分布,实现个性化的推荐。在真实的线下百货交易数据上的实验结果表明,该模型能够极大地提高线下百货领域个性化推荐的准确性。","PTM,概率交易模型,品牌共现"
2015-01-07,基于路径与深度的同义词词林词语相似度计算,"陈宏朝,李 飞,朱新华,马润聪","该文提出了一种基于路径与深度的同义词词林词语语义相似度计算方法。该方法通过两个词语义项之间的最短路径以及它们的最近公共父结点在层次树中的深度计算出两个词语义项的相似度。在处理两个词语义项的最短路径与其最近公共父结点的深度时,为提高路径与深度计算的合理性,为分类树中不同层之间的边赋予不同的权值,同时通过两个义项在其最近公共父结点中的分支间距动态调节词语义项间的最短路径,从而平衡两个词语的相似度。该方法修正了目前相关算法只能得出几个固定的相似度值,所有最近公共父结点处于同一层次的义项对之间的相似度都相同的不合理现象,使词语语义相似度的计算结果更为合理。实验表明,该方法对MC30词对的相似度计算值与人工判定值相比,取得了0.856的皮尔逊相关系数,该结果高于目前大多数词语相似度算法与MC30的相关度。","同义词词林,路径,深度,分支间距,最近公共父结点"
2015-04-14,基于高斯词长特征的中文分词方法,"张 义,李治江","中文分词是中文信息处理的基础,在语音合成、中外文翻译、中文检索、文本摘要等方面均有重要应用。在中文分词的任务中,存在的主要问题在于可用有效特征较少,分词准确率较低,如何有效的获取和使用分词特征是关键。该文从中文文本生成的过程出发,基于词长噪声的高斯分布特性,提出利用上下文的词长特征作为分词特征。实验表明,在封闭测试中,采用条件随机场模型,使用该特征对现有的实验结果有提高作用。","高斯词长,条件随机场,中文分词,自然语言处理"
2015-03-01,基于Markov逻辑网的虚假评论识别方法,行娟娟,"为解决虚假评论识别的问题,该文提出一种基于Markov逻辑网的虚假评论识别方法。首先,对虚假评论内容和评论者行为的特点进行分析,选取评论内容特征和评论者行为特征;然后,根据特征定义一阶逻辑谓词和逻辑公式,并介绍了权重学习和推理的过程;最后,进行了对比实验,结果表明该方法的虚假评论识别取得了较好的效果。","Markov逻辑网,虚假评论,权重学习,自适应聚类"
2015-06-03,基于属性主题分割的评论短文本词向量构建优化算法,"李志宇,梁 循,周小平","从词向量的训练模式入手,研究了基于语料语句分割(BWP)算法,分隔符分割(BSP)算法以及属性主题分割(BTP)算法三种分割情况下的词向量训练结果的优劣。研究发现,由于评论短文本的自身特征,传统的无分割(NP)训练方法,在词向量训练结果的准确率和相似度等方面与BWP算法、BSP算法以及BTP算法具有明显的差异。通过对0.7亿条评论短文本进行词向量构建实验对比后发现,该文所提出的BTP算法在同义词(属性词)测试任务上获得的结果是最佳的,因此BTP算法对于优化评论短文本词向量的训练,评论短文本属性词的抽取以及情感倾向分析等在内的,以词向量为基础的应用研究工作具有较为重要的实践意义。同时,该文在超大规模评论语料集上构建的词向量(开源)对于其他商品评论文本分析的应用任务具有较好可用性。","在线评论,短文本,词向量,相似度计算"
2014-12-25,基于训练样本集扩展的隐式篇章关系分类,"朱珊珊,洪 宇,丁思远,严为绒,姚建民,朱巧明","隐式篇章关系分类主要任务是在显式关联线索缺失的情况下,自动检测特定论元之间的语义关系类别。前人研究显示,语言学特征能够有效辅助隐式篇章关系的分类。目前,主流检测方法由于缺少足够的已标注隐式训练样本,导致分类器无法准确学习各种分类特征,分类精确率仅约为40%。针对这一问题,该文提出一种基于训练样本集扩展的隐式篇章关系分类方法。该方法首先借助论元向量,以原始训练样本集为种子实例,从外部数据资源中挖掘与其在语义以及关系上一致的“平行训练样本集”;然后将“平行训练样本集”加入原始训练样本集中,形成扩展的训练样本集;最后基于扩展的训练样本集,实现隐式篇章关系的分类。该文在宾州篇章树库(Penn Discourse Treebank, PDTB)上对扩展的训练样本集进行评测,结果显示,相较于原始训练样本集,使用扩展的训练样本集的实验系统整体性能提升8.41%,在四种篇章关系类别上的平均性能提升5.42%。与现有主流分类方法性能对比,识别精确率提升6.36%。","隐式篇章关系,语义向量,训练样本集扩展,篇章分析"
2015-04-07,基于语义和图的文本聚类算法研究,"蒋 旦,周文乐,朱 明","传统的文本聚类往往采用词包模型构建文本向量,忽略了词语间丰富的语义信息。而基于中心划分的聚类算法,容易将概念相关的自然簇强制分开,不能很好地发现人们感兴趣的话题。该文针对传统文本聚类算法的缺点,提出一种基于语义和完全子图的短文本聚类算法,通过对目前主流的三大语义模型进行了实验和对比,选择了一种较为先进的语义模型,基于该语义模型进行了聚类实验,发现新算法能较好地挖掘句子的语义信息且较传统的K-means有更高的聚类纯度。","文本聚类,完全子图,语义相似度,词向量"
2014-02-28,基于全局优化的中文事件时序关系推理方法,"郑 新,李培峰,朱巧明","近年来,越来越多的研究关注事件时序关系,但大多数工作集中于提高事件关系分类器的性能,忽略了分类器错误所造成的事件关系间不一致的问题。该文利用了一个全局优化的推理模型来解决这一问题,将事件时序关系全局优化看成整数线性规划问题,使用了自反性、传递性、同指性、时序连接词、事件类型对等多个约束条件。实验结果表明,该文的全局推理方法与分类器相比,F1值提高了3.56%。","事件,时序关系,推理"
2014-12-04,产品评论中领域情感词典的构建,郗亚辉,"领域情感词典是情感分析最重要的基础。由于产品评论的数量巨大、领域众多,如何自动构建领域情感词典已经成为近年来的一个研究热点。该文提出了一个两阶段的领域情感词典构建算法。第一阶段,利用情感词间的点互信息和上下文约束,使用基于约束的标签传播算法构造基本情感词典;第二阶段,根据情感冲突的频率来识别领域相关情感词,并根据其上下文约束以及修饰的特征完善领域情感词典。实验结果表明,该方法在实际产品评论数据集上取得了较好的效果。","情感分析,领域情感词典,上下文约束,基于约束的标签传播算法"
2015-07-31,基于翻译模型和语言模型相融合的双语句对选择方法,"姚 亮,洪 宇,刘 昊,刘 乐,姚建民","双语句对选择方法旨在从大规模通用领域双语语料库中,自动抽取与待翻译文本领域相关性较高的句对,以缓解特定领域翻译模型训练语料不足的问题。区别于原有基于语言模型的双语句对选择方法,该文从句对生成式建模的角度出发,提出一种基于翻译模型和语言模型相融合的双语句对选择方法。该方法能够有效评价双语句对的领域相关性及互译性。实验结果显示,利用该文所提方法选择双语句对训练所得翻译系统,相比于基准系统,在测试集上性能提升3.5个BLEU值;此外,针对不同句对质量评价特征之间的权重调节问题,该文提出一种基于句对重排序的特征权重自动优化方法。基于该方法的机器翻译系统性能继续提升0.68个BLEU值。","双语句对选择,生成式建模,翻译模型,语言模型,权重调节"
2015-02-13,二分图顶点配对模型下的英汉句子对齐研究,严灿勋,"英汉平行文本句子对齐可以视为一个二分图顶点配对模型。利用完全基于英汉词典的双语句子相关性评价函数,能够对二分图的“顶点对”进行加权。该文提出的顶点配对句子对齐方法首先获取二分图全局最大权重顶点配对作为临时锚点;在此基础上,根据句子先后顺序,局部最大权重顶点配对和英汉句长比的值域范围,纠正临时锚点中的错误,补充锚点序列未覆盖的合法顶点对,同时划分句对,实现句子对齐处理。在对比实验中该句子对齐方法优于Champollion句子对齐系统。从实验对比结果和实践效果看,该句子对齐方法可行。","句子对齐,双语词典,平行文本,二分图,顶点配对,顶点对"
2015-10-08,异源语料融合研究,"吕学强,仵永栩,周 强,刘 殷","语料资源与自然语言处理领域的各项研究息息相关,具有很大的应用价值。由于不同的研究机构对于语料标注的规则和标记的类型不尽相同,使得不同的语料库很难组合为一个更大的语料库来进行使用。针对该问题,该文从不同标注库及词类映射层面考虑,对其产生的词性歧义问题进行了研究,提出了一种将异源语料融合到一种体系下的方法,对词类信息进行映射和消歧,并进行了实验验证,融合后的词性信息准确率可达87%,实验结果表明该方法具有一定的有效性和可扩展性。","语料建设,语料融合,词类映射,词性消歧,"
2015-11-15,基于查询性能预测的鲁棒检索排序研究,"薛源海,俞晓明,刘 悦,关 峰,程学旗","信息检索技术致力于从海量的信息资源中为用户获取所需的信息。相较于传统的简单模型,近些年来的大量研究工作在提升了检索结果平均质量的同时,往往忽略了鲁棒性的问题,即造成了很多查询的性能下降,导致用户满意度的显著下降。本文提出了一种基于排序学习的查询性能预测方法,针对每一个查询,对多种模型得到的检索结果列表进行预测,将其中预测性能最优的检索结果列表展示给用户。在LETOR的三个标准数据集OHSUMED、MQ2008和MSLR-WEB10K上的一系列对比实验表明,在以经典的BM25模型作为基准的情况下,与当前最好的检索模型之一LambdaMART相比,该方法在提升了检索结果平均质量的同时,显著地减少了性能下降的查询的数量,具备较好的鲁棒性。","查询性能预测,排序学习,鲁棒检索排序"
2015-09-21,一种半监督的中文垃圾微博过滤方法,"姚子瑜,屠守中,黄民烈,朱小燕","微博作为目前国内外最活跃的信息分享平台之一,其中却充斥着大量的垃圾内容。因此,如何从给定话题的微博数据中,过滤掉与话题不相关的垃圾微博、保留话题相关微博,成为迫切需要解决的问题。该文提出了一种半监督的中文微博过滤方法,基于朴素贝叶斯分类模型和最大期望算法,实现了利用少量标注数据的垃圾微博过滤算法,其优势是仅仅利用少量标注数据就可以获得较为理想的过滤性能。分别对十个话题140 000余条新浪微博数据进行过滤,该文提出的模型准确度和F值优于朴素贝叶斯和支持向量机模型。","垃圾微博过滤,半监督学习,EM算法,朴素贝叶斯"
2016-01-01,一种基于云模型的文摘单元选取方法研究,陈劲光,"该文提出了一种基于云模型的文摘单元选取方法,利用云模型,全面考虑文摘单元的随机性和模糊性,提高面向查询的多文档自动文摘系统的性能。首先计算文摘单元和查询条件的相关性,将文摘单元和各个查询词的相关度看成云滴,通过对云的不确定性的计算,找出与查询条件真正意义相关的文摘单元;随后利用文档集合重要度对查询相关的结果进行修正,将文摘句和其他各文摘句的相似度看成云滴,利用云的数字特征计算句子重要度,找出能够概括尽可能多的文档集合内容的句子,避免片面地只从某一个方面回答查询问题。为了证明文摘单元选取方法的有效性,在英文大规模公开语料上进行了实验,并参加了国际自动文摘公开评测,取得了较好的成绩。","云模型,自动文摘,不确定性"
2015-10-15,一种基于神经网络模型的句子排序方法,"康世泽,马 宏,黄瑞阳","句子排序是多文本摘要中的重要问题,合理地对句子进行排序对于摘要的可读性和连贯性具有重要意义。该文首先利用神经网络模型融合了五种前人已经提出过的标准来决定任意两个句子之间的连接强度,这五种标准分别是时间、概率、主题相似性、预设以及继承。其次,该文提出了一种基于马尔科夫随机游走模型的句子排序方法,该方法利用所有句子之间的连接强度共同决定句子的最终排序。最终,该文同时使用人工和半自动方法对句子排序的质量进行评价,实验结果表明该文所提出方法的句子排序质量与基准算法相比具有明显提高。","句子排序,多文本摘要,神经网络模型,马尔科夫随机游走模型"
2015-10-15,藏文国际音标(拉萨音)自动转换研究,"龙从军,刘汇丹,吴 健","该文旨在实现从藏文文本到国际音标的自动转换,在一定程度上解决获取较大规模的藏文国际音标标注文本的问题。在国际音标转换系统中,采用了基于规则和统计融合的方法,实现了文语语音词自动切分;利用辅音、元音和声调对应规则表实现了藏语音节的国际音标自动转换;利用声调变化规则、辅音和元音变化规则实现了基于语音词的声调变调、辅音和元音的变化。从自动标注的结果来看,达到了实用效果。 ","藏语,国际音标,自动转换,分词"
2015-10-25,基于笔画曲率特征的笔迹鉴别方法,"李庆武,马云鹏,周 妍,周亮基","现有的手写汉字脱机笔迹鉴别方法存在只能针对特定字符或需要大量样本字符等问题,为此提出一种基于笔画曲率特征的笔迹鉴别方法。首先运用数学形态学对采集的笔迹图像进行预处理,在横、竖、撇、捺四个方向提取具有代表性的笔画骨架,然后对笔画骨架进行圆的重构,提取四个方向笔画圆的曲率作为特征值组成笔迹特征矩,根据待鉴别的笔迹特征矩与数据库中笔迹特征矩向量夹角相似性度量结果对样本做出判断。实验结果表明该文方法对于待鉴别样本字符的内容没有要求,样本字符数量要求低、应用范围广、鲁棒性强。","笔迹鉴别,数学形态学,笔画圆重构,笔画曲率,夹角相似性"
2016-09-27,借重于人工知识库的词和义项的向量表示: 以HowNet为例,"孙茂松,陈新雄","该文旨在以HowNet为例,探讨在表示学习模型中引入人工知识库的必要性和有效性。目前词向量多是通过构造神经网络模型,在大规模语料库上无监督训练得到,但这种框架面临两个问题: 一是低频词的词向量质量难以保证;二是多义词的义项向量无法获得。该文提出了融合HowNet和大规模语料库的义原向量学习神经网络模型,并以义原向量为桥梁,自动得到义项向量及完善词向量。初步的实验结果表明该模型能有效提升在词相似度和词义消歧任务上的性能,有助于低频词和多义词的处理。作者指出,借重于人工知识库的神经网络语言模型应该成为今后一段时期自然语言处理的研究重点之一。","词向量,义项向量,义原向量,HowNet,神经网络语言模型"
2016-09-27,语义角色映射为句法成分的词汇语义制约规律及特点,"亢世勇,张 晨","该文以联接理论、事件结构理论为指导,进行词汇语义类、语义角色、句法成分对应关系的研究。选择人教社中小学语文课文语料,标注语义角色、句法成分及中心词的词汇语义类。在标注语料库的基础上,统计分析了词汇语义类与语义角色的对应关系,重点分析各语义类语义角色映射为句法成分的规律,并进一步总结了各词汇语义类的语义角色与句法成分的对应的特点。尽管词汇语义类、语义角色、句法成分之间存在错综复杂的关系,但还是有规律的,可以为计算机句法分析提供一些依据。","词汇语义,语义角色,句法成分"
2016-09-27,细粒度与可视化的比字句分析模型及计算应用,"朴敏浚,袁毓林","针对现有五元组比较句语义要素框架的缺陷,该文引进了提升语义分辨率的七元组语义要素分类模板。在此基础上建立了一个可视化的比字句结构分析模型,用以总结出比较对象之间的三种对应模式,并确立了判定不对称比较的形式标准。该文的可视化分析模型可以明确阐述比字句内部的多重述谓结构,有助于获取容易被忽略或认错的隐含成分及比较关系。而且,它立足于谓词逻辑的基础形式,所以与OWL本体语言相兼容。作为模型的应用实践,该文还建设了小规模知识本体(ontology),演示了比字句语义要素的自动识别过程。","比字句,语义要素,比较关系,细粒度,可视化,知识本体"
2016-09-27,汉语未登录词的词义知识表示及语义预测,"田元贺,刘 扬","在此前的汉语未登录词语义预测中,构词相关的知识一直被当做预测的手段,而没有被视为一种有价值的知识表示方式,该文在语素概念基础上,深入考察汉语的语义构词知识,给出未登录词的多层面的词义知识表示方案。针对该方案,该文采用贝叶斯网络方法,构建面向汉语未登录词的自动语义构词分析模型,该模型能有效预测未登录词的多层面的词义知识。这种词义知识表示简单、直观、易于拓展,实验表明对汉语未登录词的语义预测具有重要的价值,可以满足不同层次的应用需求。","未登录词,词义知识表示,语义预测,语义构词"
2016-09-27,基于声调核参数及DNN建模的韵律边界检测研究,"林 举,解焱陆,张劲松,张 微","韵律边界对言语表达的自然度和可理解度有着重要作用。韵律建模也是语音合成、语音理解中的重要方面。该文从相邻声调的相互作用角度出发,提出基于深度神经网络(DNN)及声调核声学特征的汉语韵律边界检测方法。该方法首先采用声调核部分的声学特征来计算边界检测相关参数。然后,利用深度神经网络进行建模。作为对比,实验中采用了以整个音节的声学特征为输入特征的基线系统。结果表明,只使用调核部分声学特征的系统优于使用整个音节的系统,韵律边界检测正确率相对提高了4%,这表明该文提出的汉语韵律边界检测方法的有效性。","韵律边界建模,声调核,深度神经网络"
2016-09-27,面向深层语义表示的否定义表达规律探析,"邱立坤,黄 焜,何保荣,亢世勇","否定义是深层语义表示中的一个重要组成部分。该文基于语料库的方法对现代汉语中的否定表达形式及其使用规律进行深入分析。首先,系统地收集否定表达形式,将之分为显性否定词、隐性否定词、否定结构三类,并讨论否定表达形式的非否定用法。其次,对否定表达形式的使用规律进行归纳与总结,涉及单动核结构、情态成分、述补结构、动词性并列结构、连谓结构、兼语结构等,重点分析多动核结构中否定对命题义的影响,并总结在深层语义标注框架下否定义的标注规则。最后,基于多领域句法树库考察否定表达形式的领域分布差异。","现代汉语,深层语义表达,否定词"
2016-09-27,基于70年报刊语料的现代汉语历时稳态词抽取与考察,"饶高琦,李宇明","该文基于70年跨度的历时报刊语料库,使用九种统计方法计算了词语历年的使用情况,并通过对稳定性、覆盖度和时间区分性能的考察筛选获得了规模为3 013词的历时稳态词候选词集。该词集中动词与名词各占约三分之一(其余为形容词、副词与虚词),平均词长约1.7字,前密后疏地分布于历时语料库总频序表的前7 609位,覆盖了总语料的近九成。该部分词语中包含大量构造句子结构的核心词语。它们塑造了稳态词在词长和词类上的特性。稳态词的提取可以加深对语言生活底层与基础词汇的认识,对汉语教学、中文信息处理和语言规划都具有重要意义。","稳态词,历时语料库,语言监测"
2016-09-27,CRFs融合语义信息的英语功能名词短语识别,"马建军,裴家欢,黄德根","名词短语识别在句法分析中有着重要的作用,而英汉机器翻译的瓶颈之一就是名词短语的歧义消解问题。研究英语功能名词短语的自动识别,则将名词短语的结构消歧问题转化成名词短语的识别问题。基于名词短语在小句中的语法功能来确定名词短语的边界,选择商务领域语料,采用了细化词性标注集和条件随机域模型结合语义信息的方法,识别了名词短语的边界和句法功能。在预处理基于宾州树库细化了词性标注集,条件随机域模型中加入语义特征主要用来识别状语类的名词短语。实验结果表明,结合金标准词性实验的F值达到了89.04%,改进词性标注集有助于提高名词短语的识别,比使用宾州树库标注集提高了2.21%。将功能名词短语识别信息应用到NiuTrans统计机器翻译系统,英汉翻译质量略有提高。","功能名词短语,名词短语识别,条件随机域模型,语义信息"
2016-09-27,限定领域口语对话系统中的商品属性抽取,"叶大枢,黄沛杰,邓振鹏,黄 强","按功能或问题域划分,商品属性抽取(product feature mining)在限定领域的对话系统中属于口语语言理解(spoken language understanding, SLU)的范畴。商品属性抽取任务只关注自然文本中描述商品属性的特定部分,它是细粒度观点抽取(fine-grained opinion mining)的一个重要的子任务。现有的商品属性抽取技术主要建立在商品的评论语料上,该文以手机导购对话系统为背景,将商品属性抽取应用到整个对话过程中,增强对话系统应答的针对性。使用基于CBOW (continuous bag of words)语言模型的word2vector(W2V)对词汇的语义层面建模,提出一个针对口语对话的指数型变长静态窗口特征表达框架,捕捉不同距离词语组合的重要特征,使用卷积神经网络(convolutional neural network, CNN)结合词汇的语义和上下文层面对口语对话语料中的商品属性进行抽取。词嵌入模型给出了当前词和所给定的属性类别是否存在相关性的证据,而所提出的特征表达框架则是为了解决一词多义的问题。实验结果表明,该方法取得了优于研究进展中方法的商品属性识别效果。","商品属性抽取,词向量,卷积神经网络,特征表达,口语对话系统"
2016-09-27,基于DNN的汉语框架识别研究,"赵红燕,李 茹,张 晟,张力文","框架识别是语义角色标注的基本任务,它是根据目标词激起的语义场景,为其分配一个合适的语义框架。目前框架识别的研究主要是基于统计机器学习方法,把它看作多分类问题,框架识别的性能主要依赖于人工选择的特征。然而,人工选择特征的有效性和完备性无法保证。深度神经网络自动学习特征的能力,为我们提供了新思路。该文探索了利用深度神经网络自动学习目标词上下文特征,建立了一种新的通用的框架识别模型,在汉语框架网和《人民日报》2003年3月新闻语料上分别取得了79.64%和78.58%的准确率,实验证明该模型具有较好的泛化能力。","汉语框架,框架识别,深度神经网络,分布式表征"
2016-09-01,基于分布式表示和多特征融合的知识库三元组分类,"安 波,韩先培,孙 乐,吴 健","三元组分类是知识库补全及关系抽取的重要技术。当前主流的三元组分类方法通常基于TransE来构建知识库实体和关系的分布式表示。然而, TransE方法仅仅适用于处理1对1类型的关系,无法很好的处理1对多、多对1及多对多类型的关系。针对上述问题,该文在分布式表示的基础上,提出了一种特征融合的方法TCSF,通过综合利用三元组的距离、关系的先验概率及实体与关系上下文的拟合度进行三元组分类。在四种公开的数据集(WN11、WN18、FB13、FB15K)上的测试结果显示,TCSF在三元组分类上的效果超过现有的state-of-the-art模型。","知识库,深度学习,三元组分类"
2016-09-27,基于认知属性库的原型范畴研究,"李 斌,宋 丽,银思琪,曲维光,王 萌","原型范畴是认知科学研究中的重要理论,使用属性来区分范畴中心成员及边缘成员有着较强的解释力,但该理论一直缺乏基于频率信息的属性数据支撑。该文借助认知属性库的23万条数据,对原型理论研究中经常讨论的鸟、水果、交通工具等范畴的典型成员和非典型成员进行分析验证。认知属性库的数据显示,在汉语中,鸟的典型成员是麻雀、燕子等,和鸟具有较多的共同属性;而企鹅、鸵鸟则只共享了鸟很少的属性,且缺少关键的属性飞。大体上验证了原型理论的观点。同时,我们也发现小鸟的属性特别丰富,具有典型成员的特性。在进一步观察了水果和交通工具两个范畴后,我们探讨了范畴的跨类现象,进而从数学模型上区分了树结构的层次分类体系和图结构的范畴化体系。","认知属性,原型范畴,语义分类,语义计算"
2016-09-27,中国英语学习者花园幽径句错位效应强度研究: 计算语言学视角,"杜家利,于屏方","该文借助126名英语专业大二学生对100个花园幽径句和对照句的限时理解实验,讨论了中国英语学习者在解读花园幽径句过程中产生的错位效应,测算了效应强度,并与stanford parser的自动翻译进行了人机对比研究。花园幽径现象是一种有意识的受控行为。其编码和解码具有行进错位和认知过载现象,并能反映人类复杂的心理认知活动。实验证明: 在划分的引导词类错位、宾语辖域错位、嵌套错位和兼类错位四类中,错位效应呈现非对称性,其中兼类错位频数最高,错位效应强度也最大。在人机对照中,机器的程序解码错位和学习者认知解码错位不具有完全联动性和绝对共时性。","计算语言学,花园幽径句,行进错位,认知过载,斯坦福解析器"
2016-09-27,双语者加工汉语母语语义时对英语的ERP激活效应的研究,"杨思琴,江铭虎","本研究采用ERP实验,以被试的反应时间、错误率和脑电成分N400为参考因素,探索高级双语者在加工第一语言时是否自动检索第二语言。结果显示,内隐的英语首发音条件引起的效应没有体现在反应时间上。在ERP实验结果中,被试在判断语义相关的词语时,大脑语言区域的N400在词语英译首发音一致与否的情况下差异不显著;而判断语义无关的词语时,N400在该条件下显著。实验结果分析表明,高级双语者在深度加工第一语言时,大脑可能无意识地检索第二语言。","ERP,语义,反应时间,N400 "
2016-09-27,基于语义角色标注的汉语句子相似度算法,"田 堃,柯永红,穗志方","在语义角色标注过程中,经常需要检索相似的已标注语料,以便进行参考和分析。现有方法未能充分利用动词及其支配的成分信息,无法满足语义角色标注的相似句检索需求。基于此,本文提出一种新的汉语句子相似度计算方法。该方法基于已标注好语义角色的语料资源,以动词为分析核心,通过语义角色分析、标注句型的相似匹配、标注句型间相似度计算等步骤来实现句子语义的相似度量。为达到更好的实验效果,论文还综合比较了基于知网、词向量等多种计算词语相似度的算法,通过分析与实验对比,将实验效果最好的算法应用到句子相似度计算的研究中。实验结果显示,基于语义角色标注的句子相似度计算方法相对传统方法获得了更好的测试结果。","语义角色标注,词语相似度,知网,词向量,标注句型匹配"
2016-09-27,网络用语词典的构建及问题分析,"昝红英,许鸿飞,张坤丽,穗志方","随着互联网应用的快速发展,网络用语的使用越来越普遍,网络新词层出不穷。网络文本中大量的网络用语,对基于自然语言处理的情感分析、产品推荐、问答系统等应用带来了一定的挑战,而收集并构建网络用语词典及相关语料则是解决此类问题的突破点。该文以微博语料为出发点,综合多类网络资源,收集并整理了较为全面的网络用语词典及相关语料。同时,对网络用语词典构建中遇到的问题进行了分析和总结,并对其潜在应用进行了初步的探讨。","网络用语,词典构建,标注"
2016-09-27,谈话节目语料库的构建与会话结构分析,"王 珊,刘 锐","口语语料库的建设是口语研究的基础工作,该文选择具有代表性的交谈式谈话节目《锵锵三人行》和对谈式谈话节目《鲁豫有约》作为语料,建立了一个小型的谈话节目语料库,并构建了包含五大类16小类的会话结构标注体系,对语料进行了会话结构的标注。统计得到打断结构309例,插入结构141例,重复结构111例,问答结构653/589例,阻碍修正结构51/21例,反映了会话结构在数量上的不均衡分布,节目的形式、性质以及交际任务是会话结构分布的主要影响因素。会话结构组合具有模式性,该文使用Trigram方法对其组合情况进行了分析,发现语料中的高频组合是问答毗邻对,此外有大量的非毗邻性组合。会话结构组合模式不但反映出谈话节目的风格特点,还有助于分析会话中的功能性模块、会话策略的形成,进而更加深入地了解会话的运作机制。","谈话节目,会话结构,组合模式"
2016-09-27,基于远监督的语义知识资源扩展研究,"卢达威,王星友,袁毓林","语义知识资源蕴含了深刻的语言学理论,是语言学知识和语言工程的重要接口。该文以形容词句法语义词典为研究对象,探索对语义知识资源自动扩展的方法。该文的目标是利用大规模语料库,扩展原有词典的词表及其对应的句法格式。具体方法是根据词的句法格式将词典的词分类,将待扩展的新词通过分类器映射到原有词典的词中,以此把词典扩展问题转化为多类分类问题。依据的原理是词典词和待扩展新词在大规模语料中句法结构的相似性。该文通过远监督的方法构造训练数据,避免大量的人工标注。训练过程结合了浅层机器学习方法和深度神经网络,取得了有意义的成果。实验结果显示,深度神经网络能够习得句法结构信息,有效提升匹配的准确率。",
2016-09-27,基于伪文档的伪相关反馈方法,"闫 蓉,高光来","传统的伪相关反馈(Pseudo Relevance Feedback, PRF)方法通常是以文档作为扩展源单元提取扩展词,提取粒度过大造成扩展源质量下降,使得检索结果鲁棒性差。该文研究利用主题分析技术,尝试将文本语义内容作为扩展源单元,缓解扩展源质量不高的问题。提出并实现了对文本集中各文档内容的伪文档描述,通过对其进行隐式多样化处理,实现了从更细微的文本内容角度出发提取扩展词。通过在真实NTCIR8中文语料的检索结果表明,该方法可以有效地提升伪相关反馈的检索性能。","伪相关反馈,伪文档,主题分析,隐含主题"
2016-09-27,基于框架语义的高考语文阅读理解答案句抽取,"李国臣,刘姝林,杨陟卓,李 茹,张 虎,钱揖丽","高考语文阅读理解问答相对普通阅读理解问答难度更大,问句抽象表述的理解需要更深层的语言分析技术,答案候选句抽取更注重与问句的关联分析,答案候选句排序更注重答案句之间的语义相关性。为此,该文提出借助框架语义匹配和框架语义关系抽取答案候选句,在排序时引入流形排序模型,通过答案句之间的框架语义相关度将排序分数进行传播,最终选取分数较高的Top-4作为答案句。在北京近12年高考语文阅读理解问答题上的准确率与召回率分别达到了53.65%与79.06%。","高考语文,阅读理解,框架语义,答案句抽取,流形排序"
2016-07-21,基于文本语义离散度的自动作文评分关键技术研究,"王耀华,李舟军,何跃鹰,巢文涵,周建设","该文尝试从文本语义离散度的角度去提升自动作文评分的效果,提出了两种文本语义离散度的表示方法,并给出了数学化的计算公式。基于现有的LDA模型、段落向量、词向量等具体方法,提取出四种表征文本语义离散度的实例,应用于自动作文评分。该文从统计学角度将文本语义离散度向量化,从去中心化的角度将文本语义离散度矩阵化,并使用多元线性回归、卷积神经网络和循环神经网络三种方法进行对比实验。实验结果表明,在50篇作文的验证集上,在加入文本语义离散度特征后,预测分数与真实分数之间均方根误差最大降低10.99%,皮尔逊相关系数最高提升2.7倍。该表示方法通用性强,没有语种限制,可以扩展到任何语言。","作文评分,语义离散度,神经网络"
2016-09-27,限定领域口语对话系统中超出领域话语的对话行为识别,"黄沛杰,王俊东,柯子烜,林丕源","由于领域外话语具有内容短小、表达多样性、开放性及口语化等特点,限定领域口语对话系统中超出领域话语的对话行为识别是一个挑战。该文提出了一种结合外部无标签微博数据的随机森林对话行为识别方法。该文采用的微博数据无需根据应用领域特点专门收集和挑选,又与口语对话同样具有口语化和表达多样性的特点,其训练得到的词向量在超出领域话语出现超出词汇表字词时提供了有效的相似性扩展度量。随机森林模型具有较好的泛化能力,适合训练数据有限的分类任务。中文特定领域的口语对话语料库测试表明,该文提出的超出领域话语的对话行为识别方法取得了优于最大熵、卷积神经网络等短文本分类研究进展中的方法的效果。","对话行为识别,超出领域话语,随机森林,词向量,口语对话系统"
2016-05-31,汉维时间数字和量词的识别与翻译研究,"阿依古丽·哈力克,艾山·吾买尔,吐尔根·伊布拉音,卡哈尔江·阿比的热西提,买合木提·买买提","统计机器翻译对时间、数字、量词的泛化能力较弱,为了提高汉维机器翻译系统对时间、数字和量词短语的翻译性能,该文利用双语语料库挖掘并提取汉语时间、数字、量词表达与翻译模式,实现了基于模板的时间、数字、无歧义量词翻译方法及基于上下文的有歧义量词翻译方法。时间、数字、无歧义量词、有歧义量词的翻译F值达到了93.23%、90.15%、96.55%、87.58%,实验证明,该方法具有简单高效的优点。","时间数字,无歧义量词,有歧义量词,翻译规则,翻译模板"
2016-09-27,融合被动和可能态模型的日汉统计机器翻译,"王 楠,徐金安,明 芳,陈钰枫,张玉洁","日语中谓词语态有不同的词尾变形,其中被动态和可能态具有相同的词尾变化,在统计机器翻译中难以对其正确区分及翻译。因此,该文提出一种利用最大熵模型有效地对日语可能态和被动态进行分类,然后把日语的可能态和被动态特征有效地融合到对数线性模型中改进翻译模型的方法,以提高可能态和被动态翻译规则选择的准确性。实验结果表明,该方法可以有效提升日语可能态和被动态句子的翻译质量,在大规模日汉语料上,最高翻译BLEU值能够由41.50提高到42.01,并在人工评测中,翻译结果的整体可理解度得到了2.71%的提升。","被动态,可能态,统计机器翻译,最大熵模型"
2016-09-27,基于条件随机场的评价对象缺省项识别,唐文武;过 弋;徐永斌;方 旭,"在电商网站评论文本中,评价对象和评价属性的缺省识别对文本情感分析具有重要地作用。针对电商网站评论文本中评价对象和评价属性缺省问题,该文提出了一种基于条件随机场的评价对象缺省项识别方法。首先利用情感词典识别观点句,将缺省项识别问题转换成序列标注问题,综合词法特征和依存句法特征,使用条件随机场模型进行训练,并在测试集上对待识别的观点句进行序列标注,通过标注结果判定缺省项的位置。实验结果表明,该方法具有较高的准确率和召回率,验证了该方法的有效性。","条件随机场,评价对象,缺省识别,序列标注"
2016-09-27,基于多特征融合的混合神经网络模型讽刺语用判别,"孙 晓,何家劲,任福继","在社交媒体中,存在大量的反讽和讽刺等语言现象,这些语言现象往往表征了一定的情感倾向性。然而这些特殊的语言现象所表达的语义倾向性,通常与其浅层字面含义相去甚远,因此加大了社交媒体中文本情感分析的难度。鉴于此,该文主要研究中文社交媒体中的讽刺语用识别任务,构建了一个覆盖反讽、讽刺两种语言现象的语料库。基于此挖掘反讽和讽刺的语言特点,该文通过对比一些有效领域特征,验证了在反讽和讽刺文本的识别中,其结构和语义等深层语义特征的重要性。同时,该文提出了一种有效的多特征融合的混合神经网络判别模型,融合了卷积神经网络与LSTM序列神经网络模型,通过深层模型学习深层语义特征和深层结构特征,该模型获得了较好的识别精度,优于传统的单一的神经网络模型和BOW(Bag-of-Words)模型。","讽刺,神经网络,多特征融合,情感分析"
2016-09-27,藏文复合句的依存句法分析,"华却才让,赵海兴","为解决藏文复合句引起的依存句法分析性能下降的主要问题,该文提出了一种基于判别式的藏文复合句切分标注方法,先根据藏文固有的虚词语法结构和连词特征,将复合句子切分标注为句法分析的基本单元,然后将句法分析之后的各个部分依据主分句关系进行合并,生成复合句的完整分析结果。实验结果表明该方法在一定程度上降低了藏文复合句依存句法分析的复杂度,最终句法分析的准确率达到88.72%。","句法分析,依存句法,藏文分句,藏文复合句"
2016-09-27,基于蒙古语名词语义网的同形词歧义消除研究,"哈 斯,布音其其格","蒙古文同形词歧义消除问题是蒙古文信息处理的难点之一。该文提出了基于蒙古语名词语义网的同形词歧义消除方法,设计实现了同形词歧义消除算法,最后给出了语料库中同形词歧义消除实验的设计过程及结果分析。","蒙古文,名词语义网,同形词,歧义消除"
2016-07-24,基于问题与答案联合表示学习的半监督问题分类方法,"张 栋,李寿山,王晶晶","问题分类旨在对问题的类型进行自动分类,该任务是问答系统研究的一项基本任务。该文提出了一种基于问题和答案联合表示学习的问题分类方法。该方法的特色在于利用问题及其答案作为共同的上下文环境,学习词的分布式表示,从而充分利用未标注样本中问题和答案隐含的分类信息。具体而言,首先,我们引入神经网络语言模型,利用问题与答案联合学习词向量表示,增加问题词向量的信息量;其次,加入大量未标注的问题与答案样本参与词向量学习,进一步增强问题词向量表示能力;最后,将已标注的问题样本以词向量形式表示作为训练样本,采用卷积神经网络建立问题分类模型。实验结果表明,该文提出的基于半监督问题分类方法能够充分利用词向量表示和大量未标注样本来提升性能,明显优于其他基准半监督分类方法。","问题分类,联合表示,半监督"
2016-09-15,面向阅读理解复杂问题的句子融合,"谭红叶,赵红红,李 茹","阅读理解是目前NLP领域的一个研究热点。阅读理解中好的复杂问题解答策略不仅要进行答案句的抽取,还要对答案句进行融合、生成相应的答案,但是目前的研究大多集中在前者。该文针对复杂问题解答中的句子融合进行研究,提出了一种兼顾句子重要信息、问题关联度与句子流畅度的句子融合方法。该方法的主要思想为: 首先,基于句子拆分和词重要度选择待融合部分;然后,基于词对齐进行句子相同信息的合并;最后,利用基于依存关系、二元语言模型及词重要度的整数线性规划优化生成句子。在历年高考阅读理解数据集上的测试结果表明,该方法取得了82.62%的F值,同时更好地保证了结果的可读性及信息量。","阅读理解,复杂问题,句子融合,文本生成"
2016-09-10,基于事件元素无向图的查询扩展方法,"叶 雷,高盛祥,余正涛,秦广顺,洪旭东","借助新闻事件元素之间的关联特性,提出了基于事件元素无向图的查询扩展方法,利用新闻事件元素之间的关联关系进行查询扩展提升新闻事件检索效果。首先分析候选事件文档与查询项的关系,确定待扩展的元素;然后利用事件元素之间的关联关系构建无向图,通过事件向量空间计算边的权重;最后,利用无向图节点权重模型计算事件元素权重,依据权重进行事件元素扩展。在新闻事件查询扩展方面进行了对比试验,结果表明该文提出的查询扩展方法取得了较好的效果。","新闻事件,查询扩展,事件元素,事件元素无向图"
2016-09-10,基于文档发散度的作文跑题检测,"陈志鹏,陈文亮","作文跑题检测是作文自动评分系统的重要模块。传统的作文跑题检测一般计算文章内容相关性作为得分,并将其与某一固定阈值进行对比,从而判断文章是否跑题。但是实际上文章得分高低与题目有直接关系,发散性题目和非发散性题目的文章得分有明显差异,所以很难用一个固定阈值来判断所有文章。该文提出一种作文跑题检测方法,基于文档发散度的作文跑题检测方法。该方法的创新之处在于研究文章集合发散度的概念,建立发散度与跑题阈值的关系模型,对于不同的题目动态选取不同的跑题阈值。该文构建了一套跑题检测系统,并在一个真实的数据集中进行测试。实验结果表明基于文档发散度的作文跑题检测系统能有效识别跑题作文。","跑题检测,文档发散度,文本相似度"
2016-09-15,利用词表示和深层神经网络抽取蛋白质关系,"李丽双,蒋振超,万 佳,黄德根","蛋白质关系抽取是生物医学信息抽取领域的重要分支。目前研究中,基于特征和核函数方法的蛋白质关系抽取已被充分研究,并且达到了很高的F-值,通过改进特征和核函数进一步优化实例表示变得十分困难。该文结合词表示和深层神经网络,提出了一种实例表示模型。该模型能够充分利用词表示的语义表示能力和深层神经网络的表示优化能力;同时引入主成分分析和特征选择进行特征优化,并且通过比较多种传统的分类器,寻找适合蛋白质关系抽取的分类器。该方法在AIMed语料、BioInfer语料和HPRD50语料上的F-值分别取得了70.5%、82.2%和80.0%,在蛋白质关系抽取任务上达到了目前最好的抽取水平。","蛋白质关系抽取,词表示,深层神经网络"
2016-09-10,汉语词汇测试自动命题研究,胡韧奋,"为了提升汉语词汇测试的命题效率,该文从汉语语言特性和二语教学需求出发,对词语听力、多空词语选择、词语排序和单空词语选择四种词汇测试题型进行自动命题尝试,以满足不同语言信息、不同难度的词汇知识考查。在词语特征的提取上,构建了一个覆盖词音、词形、词义、语法、搭配、偏误各层次信息的词汇知识库,在句子特征的提取上,实现了语法项目自动识别、句子难度分析等算法,为自动命题中的题干句、目标词和干扰项选择提供依据。通过词句选择和语块合成等步骤,生成四种题型共计7 263道词汇测试题。人工测试数据显示,词汇测试自动命题的初步尝试取得了较好的效果,约58%的试题被评价为完全合理,经人工简单调整,试题接受率达到75.7%。","二语教学,词汇测试,自动命题"
2016-09-15,英汉《小王子》抽象语义图结构的对比分析,"李 斌,闻 媛,卜丽君,曲维光,薛念文","AMR(抽象语义表示)是国际上一种新的句子语义表示方法,有着接近于中间语言的表示能力,其研发者已经建立了英文《小王子》等AMR语料库。AMR与以往的句法语义表示方法的最大不同在于两个方面,首先采用图结构来表示句子的语义;其次允许添加原句之外的概念节点来表示隐含的语义。该文针对汉语特点,在制定中文AMR标注规范的基础上,标注完成了中文版《小王子》的AMR语料库,标注一致性的Smatch值为0.83。统计结果显示,英汉双语含图结构句子具有很高的相关性,且含有图的句子比例高达40%左右,额外添加的概念节点则存在较大差异。最后讨论了AMR在汉语句子语义表示以及跨语言对比方面的优势。","抽象语义表示,语义图,英汉对比,自然语言处理"
2016-09-15,基于点关联测度矩阵分解的中英跨语言词嵌入方法,"于 东,赵 艳,韦林煊,荀恩东","研究基于矩阵分解的词嵌入方法,提出统一的描述模型,并应用于中英跨语言词嵌入问题。以双语对齐语料为知识源,提出跨语言关联词计算方法和两种点关联测度的计算方法: 跨语言共现计数和跨语言点互信息。分别设计目标函数学习中英跨语言词嵌入。从目标函数、语料数据、向量维数等角度进行实验,结果表明,在中英跨语言文档分类中以前者作为点关联测度最高得到87.04%的准确率;在中英跨语言词义相似度计算中,后者作为点关联测度得到更好的性能,同时在英—英词义相似度计算中的性能略高于主流的英语词嵌入。","点关联测度,词嵌入,跨语言,矩阵分解"
2016-09-20,利用源域结构的粒迁移学习及词性标注应用,"孙世昶,林鸿飞,孟佳娜,刘洪波","迁移学习在一定程度上减轻了目标域的数据稀疏问题对泛化能力的影响,然而泛化能力的提高仍然受到负迁移等问题的影响。为了解决负迁移问题,该文提出使用源域结构的文本语料的信息粒化方法,用区间信息粒表示出源域数据集的结构对数据集中统计量的影响。然后提出区间二型模糊隐马尔可夫模型(Interval Type-2 fuzzy Hidden Markov Model, IHMM) 以处理区间信息粒。给出了IHMM的构建方法和去模糊化方法。在文本的词性标注任务中进行了多个实验,可以证实利用源域结构信息的粒迁移学习方法避免了负迁移,提高了模型的泛化能力。","迁移学习,粒计算,区间信息粒,词性标注"
2016-09-15,基于BCC的离合词离析形式自动识别研究,"臧娇娇,荀恩东","该文从中文信息处理角度对动宾型离合词自动识别进行研究。通过分析离合词在实际语料中的使用特点以及离合词离析成分在大规模语料库中的表现形式,从离合词内部入手,形式化地表示离合词的离析形式,总结自动识别的规则,设计基于规则的自动识别算法。经过优化后,该算法在20亿字的语料中达到了91.6%的正确率。离合词语素构词能力强,分词与词性标注错误,规则的不完整性,语料本身的错误,以及人工标注的疏漏等是影响实验结论的主要因素。","离合词,BCC,离析形式,自动识别"
2016-09-20,基于规则的“把”字句语义角色标注,"何保荣,邱立坤,徐德宽","“把”字句是现代汉语中一种重要的特殊句式,该文尝试用基于知识库的规则方法对把字句进行语义角色自动标注。首先,我们从《人民日报》语义角色标注语料库中收集把字句例句,形成一个覆盖范围较广的把字句例句库;之后,对例句库中把字句的句法和语义构成规律进行手工标注,标注内容包括谓语动词的配价类型、把字句谓语结构类型、把字句句模类型等。在上述标注的基础上,对把字句的句模构成规律进行分析,总结出若干条语义角色标注规则;最后,在测试数据上对前述规则进行验证,语义角色标注的最终正确率为98.61%,这一结果说明该文所提出的规则在把字句语义角色标注上是有效的。","把字句,语义角色标注,句模"
2016-09-18,基于语义构词的汉语词语语义相似度计算,"康司辰,刘 扬","汉语词语语义相似度计算,在中文信息处理的多种应用中扮演至关重要的角色。基于汉语字本位的思想,我们采用词类、构词结构、语素义等汉语语义构词知识,以“语素概念”为基础,计算汉语词语语义相似度。这种词义知识表示简单、直观、易于拓展,计算模型简洁、易懂,采用了尽可能少的特征和参数。实验表明,该文方法在典型“取样词对”上的表现突出,其数值更符合人类的感性认知,且在全局数据上也表现出了合理的分布规律。","词语语义相似度计算,语义构词,词义知识表示,语素概念"
2016-05-18,藏汉跨语言话题模型构建及对齐方法研究,"孙 媛,赵 倩","如何获取藏文话题在其他语种中的相关信息,对于促进少数民族地区的社会管理科学化水平、维护民族团结和国家统一、构建和谐社会具有重要意义。目前大多数研究集中在英汉跨语言信息处理方面,针对藏汉跨语言研究较少。如何根据藏语、汉语的特点,并结合目前藏语信息处理的研究现状,实现藏汉多角度的社会网络关系关联,同步发现关注话题并进行数据比较,是迫切需要解决的问题。该文在藏汉可比语料的基础上,利用词向量对文本词语进行语义扩展,进而构建LDA话题模型,并利用Gibbs sampling进行模型参数的估计,抽取出藏语和汉语话题。在LDA话题模型生成的文档-话题分布的基础上,提出一种基于余弦相似度、欧氏距离、Hellinger距离和KL距离四种相似度算法的投票方法,来实现藏汉话题的对齐。","藏汉跨语言,话题抽取,LDA,话题对齐"
2016-06-01,基于词向量的藏文词性标注方法研究,"郑亚楠,珠 杰","藏文词性标注是藏文信息处理的基础,在藏文文本分类、自动检索、机器翻译等领域有广泛的应用。该文针对藏文语料匮乏,人工标注费时费力等问题,提出一种基于词向量模型的词性标注方法和相应算法,该方法首先利用词向量的语义近似计算功能,扩展标注词典;其次结合语义近似计算和标注词典,完成词性标注。实验结果表明,该方法能够快速有效地扩大了标注词典规模,并能取得较好的标注结果。","词向量,藏文,词性标注"
2016-09-18,蒙古文原始语料统计建模研究,白双成,"蒙古文字符编码与字形之间的多对多复杂转换关系及录入不规范等众多原因导致原始语料存在严重的拼写多样化现象和字形拼写错误,成为大数据处理瓶颈。该文以蒙古文输入法为例,利用大词库和形码生成器,将原本基于读音正确的词晶格最佳路径搜索问题转换为基于形码词晶格路径搜索问题,很好地解决了原始文本统计建模问题。实验结果证明,该方法及字形归并的模型优化方法可显著提高输入效率,对所有蒙古文“音词转换”和“形词转换”研究都有广泛的参考价值。","蒙古文原始文本,统计建模,读音错误,字形错误,智能输入"
2016-08-05,基于语法的维吾尔语情感词汇自动获取,"玛尔哈巴·艾赛提,艾孜尔古丽,玉素甫·艾白都拉","情感词汇的获取是文本倾向性分析的基础。为了解决人工识别方法低效的不足,并为维吾尔语情感词的研究及情感词词典的创建提供一些可供选择的方法和思路,该文首先分析了维吾尔语情感词汇在上下文中表现的特征,并结合维吾尔语本身的语法特征,建立了扩展的维吾尔语新增特征模型,与词频逆文档频率(TF-IDF)算法相结合,实现了维吾尔语情感词汇的识别。实验结果指出该特征模型有效地提高了情感词汇的识别率。","情感词汇,维吾尔语,语法,自动获取"
2016-09-27,基于多策略的维吾尔文网页识别方法,"阿力木·木拉提,艾孜尔古丽,杨雅婷,李 晓","经过对大量维吾尔文网站的调查与分析,该文从多语种混合网页中针对维吾尔文网页识别进行了研究,这对维吾尔语信息处理工作起着关键作用。首先该文探讨了维吾尔文不规范网页的字符编码转换规则及原理,以此对不规范维吾尔文字符进行了相应的处理,之后介绍了基于修改的N-Gram方法和基于维吾尔语常用词特征向量的两种方法,其中后者融合了维吾尔文常用候选词语料库及向量空间模型(Vector Space Model)。使用三种不同类型的维吾尔文网页文本作为本研究的数据集,在此基础上验证了该文提出的网页识别方法,以及采用不同的方法进行了网页识别的实验。实验结果表明,基于N-Gram的方法对正文较长的新闻或论坛网页的识别性能最佳,反而基于常用词特征向量的方法对短文本的网页识别性能优越N-Gram。所提方法对维吾尔文网页识别的整体性能达到90%以上,并验证了这两种方法的有效性。","维吾尔文,网页识别,N-Gram方法,常用词,向量空间模型"
2016-09-20,知识图谱中实体相似度计算研究,"李 阳,高大启","实体相似度的计算有诸多应用,例如,电商平台的相似商品推荐,医疗疗效分析中的相似病人组等。在知识图谱的实体相似度计算中,给出了每个实体的属性值,并对部分实体进行相似度的标注,要求能得到其他实体之间的相似度。该文把该问题归结为监督学习问题,提出一种通用的实体相似度计算方法,通过清洗噪声数据,对数值、列表以及文本等不同数据类型进行预处理,使用SVM, Logistic回归等分类模型、Random Forest等集成学习模型以及排序学习模型进行建模,得到了较好的结果。","实体相似度,监督学习,分类模型,集成学习"
2016-09-16,基于Dropout正则化的汉语框架语义角色识别,"王瑞波,李济洪,李国臣,杨耀文","汉语框架语义角色识别是汉语框架语义分析的重要任务之一。该文基于汉语词语、词性等特征的分布式表示,使用一种多特征融合的神经网络结构来构建汉语框架语义角色识别模型。鉴于可用的训练语料规模有限,该文采用了Dropout正则化技术来改进神经网络的训练过程。实验结果表明,Dropout正则化的加入有效地缓解了模型的过拟合现象,使得模型的F值有了近7%的提高。该文进一步优化了学习率以及分布式表示的初始值,最终的汉语框架语义角色识别的F值达到70.54%,较原有的最优结果提升2%左右。","汉语框架网络,语义角色识别,Dropout正则化"
2016-09-08,基于神经网络的语义选择限制知识自动获取,"贾玉祥,许鸿飞,昝红英","语义选择限制刻画谓语对论元的语义选择倾向,对自然语言的句法语义分析有重要作用,语义选择限制知识的自动获取也成为一个重要的研究课题。鉴于神经网络模型在自然语言处理的很多任务中都有出色的表现,该文提出基于神经网络的语义选择限制知识获取模型,设计了引入预训练词向量的单隐层前馈网络和两层maxout网络。在汉语和英语的伪消歧实验中神经网络模型取得了较好的效果,优于基于隐含狄利克雷分配的模型。","语义选择限制,词汇知识获取,神经网络,伪消歧"
2016-09-17,一种针对短文本的主题情感混合模型,"谢 珺,郝 洁,苏婧琼,邹雪君,李思宇","主题情感混合模型可以同时提取语料的主题信息和情感倾向。针对短文本特征稀疏的问题,主题情感联合分析方法较少的问题,该文提出了BJSTM模型(Biterm Joint Sentiment Topic Model),在BTM模型(Biterm Topic Model)的基础上,增加情感层的设置,从而形成“情感-主题-词汇”的三层贝叶斯模型。对每个双词的情感和主题进行采样,从而对整个语料的词共现关系建模,一定程度上克服了短文本的稀疏性。实验表明,BJSTM模型在无监督情感分类和主题提取方面都有不错的表现。","主题情感混合模型,情感分类,BTM"
2016-09-15,基于深度表示学习和高斯过程迁移学习的情感分析方法,"吴冬茵,桂 林,陈 钊,徐睿峰","情感分析是自然语言处理领域的重要研究问题。现有方法往往难以克服样本偏置与领域依赖问题,严重制约了情感分析的发展和应用。为此,该文提出了一种基于深度表示学习和高斯过程知识迁移学习的情感分析方法。该方法首先利用深度神经网络获得文本样本的分布式表示,而后基于深度高斯过程,从辅助数据中迁移与测试集数据分布相符的高质量样例扩充训练数据集用于分类器训练,以此提高文本情感分类系统性能。在COAE2014文本情感分类数据集上进行的实验结果显示,该文提出的方法可以有效提高文本情感分类性能,同时可以有效缓解训练数据的样本偏置以及领域依赖问题的影响。","情感分析,深度表示学习,高斯过程,迁移学习"
2016-09-10,词典与机器学习方法相结合的维吾尔语文本情感分析,"热西旦木·吐尔洪太,吾守尔·斯拉木,伊尔夏提·吐尔贡","随着互联网整体水平的提高,大量基于维吾尔文的网络信息不断建立,引起了对不同领域的信息进行情感倾向性分析的迫切需要。该文考虑到维吾尔文没有足够的情感训练语料和完整的情感词典,结合机器学习方法和词典方法的优点,构建一个分类器模型 LCUSCM(Lexicon-based and Corpus-based Uyghur Text Sentiment Classification Model),先用自己构建的维吾尔文情感词典对语料进行高质量的情感分类,分类过程中对词典进行递归扩充,再根据每条句子的情感得分,从词典分类的结果中选择一部分语料来训练一个分类器并改进第一步的分类结果。此方法的正确率比单独使用机器学习方法提高了9.13%, 比词典方法提高了1.82%。","维吾尔文,情感词典,情感分析,机器学习"
2016-09-03,基于语言现象的文本蕴涵识别,"任 函,冯文贺,刘茂福,万 菁","该文提出一种基于语言现象的文本蕴涵识别方法,该方法建立了一个语言现象识别和整体推理判断的联合分类模型,目的是对两个高度相关的任务进行统一学习,避免管道模型的错误传播问题并提升系统精度。针对语言现象识别,设计了22个专用特征和20个通用特征;为提高随机森林的泛化能力,提出一种基于特征选择的随机森林生成算法。实验结果表明,基于随机森林的联合分类模型能够有效识别语言现象和总体蕴涵关系。","文本蕴涵识别,语言现象,随机森林"
2016-09-16,基于稀疏主成分分析的非正式语词的心理-人格特征研究,"钟 毓,费定舟","针对社会媒体中非正式文本的数据分析经常出现的稀疏数据矩阵,在应用文本分析工具的基础上使用稀疏主成分分析这一特征,降维分析方法分析现实情况下聊天文本中非正式语词表现的认知语用特征、描述非正式语词与人格的关系。使用短文本主题模型、心理距离问卷、大五人格问卷测量人格和背景变量,使用计算机文本分析工具对被试提供的即时聊天文本内的语词计频,使用简体中文版语词查询与字词计数字典和认知语用学对稀疏主成分分析后非正式语词维度进行特征表征。在非正式语词降维上,稀疏主成分分析比主成分分析在因子载荷数上更稳定,在累积方差解释率上也相对更优(24.54% &gt;23.40%);降维所得的6因子中“主观评价”与宜人性正相关(r0.05=.16, p =.03&lt;0.05),“随意社交”与宜人性负相关(r0.05=-.16, p=.03&lt;0.05),“认知愉悦”与性别显著正相关(r0.05=.43, p=.00&lt;0.001)。使用稀疏主成分分析对非正式语词的降维效果较好,并且比较简体中文版语词查询与字词计数字典的非正式语词维度和降维后所得非正式语词维度,两者在和人格的相关上是相符的,且后者能探索出更多信息。","文本分析,稀疏主成分分析,非正式语词"
2016-09-20,基于偏向相似性的自然语言关联和聚类研究,"陈振宁,陈振宇","聚类按关联进行分类,关联和聚类分析的基础是相似性计算。通常相似性是指绝对相似性,具有对称性。但自然语言研究中发现大部分规律都是偏向的,具有不对称性,需要用偏向的思路来考察不对称的关联和聚类策略: 以类似条件概率的概率蕴涵指标来描写特征间的不对称关联,并在此基础上定义优势关系、紧密关系、控制中心、中途岛等关联特性;基于偏向相似性的聚类策略,从而能更好地处理语言本体研究中的“假性孤立点”、数据稀疏问题和家族象似性类型的聚类。","不对称性,条件概率,关联,聚类"
2016-09-20,《世说新语》的篇章连接词,"冯文贺,郭海芳,李玉静,任 函","该文标注《世说新语》的篇章结构,据此研究其连接词的显隐、语义及用法。研究发现: 1)隐式关系(3 346,81.4%)多于显式关系(786,18.6%),17类关系仅有三类(假设,选择,让步)显多隐少;2)各类关系的同义连接词种数与使用有差异,其中种数最多36(顺承),最少则无(总分,背景);3)连接词(90种)单义为多(55),多义为少(35),义项最多为八种(乃),分布也有差异。对比发现,《世说新语》与同时期《文心雕龙》的连接词使用有一定差异。","《世说新语》,篇章结构,连接词,语义分析"
2016-09-15,汉语二语教学领域词义标注语料库的研究及构建,"王 敬,杨丽姣,蒋宏飞,苏靖杰,付静玲","词汇教学在汉语二语教学领域占有极为重要的地位,其中多义词又是词汇教学的重点和难点。该研究通过分析三部经典领域词表,选取了1 181个重点多义词,以《现代汉语词典(第6版)》为标注体系,制定了适合实际标注的多义词标注规范和形式,在197册经典汉语二语教材上进行了多义词词义标注,构建了一个规模约350万字的面向汉语二语教学领域的词义标注语料库,并在此基础上对1 811个多义词、4 323个多义词义项进行了计量统计,分析了多义词不同词义的出现情况及其分布规律。为了更好地服务于汉语二语教学,开发了语料库检索系统,设计并实现了多义词义项的查询功能。","汉语二语教学,语料库,多义词标注"
2016-09-20,从短语到构式: 构式知识库建设的若干理论问题探析,詹卫东,"构式语法(construction grammar)在汉语语法学界已引起持续关注,但在自然语言处理领域,将构式语法理论应用到计算机自动句法语义分析中的研究还很少见。该文提出构建现代汉语构式知识库的语言工程任务,讨论了构式与传统语法单位的关系、构式的形式表示、构式的内部小类及主要特征等。","构式,知识库,组合性,非递归性"
